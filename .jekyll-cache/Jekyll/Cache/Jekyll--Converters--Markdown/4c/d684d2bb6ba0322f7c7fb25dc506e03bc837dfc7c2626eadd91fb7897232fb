I"GŒ<h1 id="ç›®å½•"><strong>ç›®å½•</strong></h1>

<ul>
  <li><a href="#æ–‡çŒ®æ•´ç†">æ–‡çŒ®æ•´ç†</a>
    <ul>
      <li><a href="#è¦æ±‚">è¦æ±‚</a></li>
      <li><a href="#æœé›†åˆ°ç›¸å…³æ–‡çŒ®æ ‡é¢˜å’Œåœ°å€">æœé›†åˆ°ç›¸å…³æ–‡çŒ®æ ‡é¢˜å’Œåœ°å€</a></li>
    </ul>
  </li>
  <li><a href="#ç¬¬ä¸€ç¯‡">ç¬¬ä¸€ç¯‡</a>
    <ul>
      <li><a href="#title">Title</a></li>
      <li><a href="#author">Author</a></li>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#introduction">Introduction</a></li>
      <li><a href="#bert-distractor-generation">BERT distractor generation</a>
        <ul>
          <li><a href="#1bert-based-distractor-generationbdg">1)BERT-based distractor generation(BDG)</a></li>
          <li><a href="#2multi-task-with-parallel-mlm">2)Multi-task with Parallel MLM</a></li>
          <li><a href="#3answer-negative-regularization">3)Answer Negative Regularization</a></li>
        </ul>
      </li>
      <li><a href="#multiple-distractor-generation">Multiple Distractor Generation</a>
        <ul>
          <li><a href="#1selecting-distractors-by-entropy-maximization">1)Selecting Distractors by Entropy Maximization</a></li>
          <li><a href="#2bdg-em">2)BDG-EM</a></li>
        </ul>
      </li>
      <li><a href="#performance-evaluation">Performance Evaluation</a>
        <ul>
          <li><a href="#1datasets">1)datasets</a></li>
          <li><a href="#2implementation-details">2)implementation details</a></li>
          <li><a href="#3compared-methods">3)compared methods</a></li>
          <li><a href="#4token-score-comparison">4)token score comparison</a></li>
          <li><a href="#5mcq-model-accuracy-comparison">5)MCQ Model Accuracy Comparison</a></li>
          <li><a href="#6parameter-study-on-Î³">6ï¼‰Parameter Study on Î³</a></li>
        </ul>
      </li>
      <li><a href="#conclusion">Conclusion</a></li>
      <li><a href="#æˆ‘çš„çœ‹æ³•">æˆ‘çš„çœ‹æ³•</a></li>
    </ul>
  </li>
  <li><a href="#ç¬¬äºŒç¯‡">ç¬¬äºŒç¯‡</a>
    <ul>
      <li><a href="#title-1">Title</a></li>
      <li><a href="#author-1">Author</a></li>
      <li><a href="#abstract-1">Abstract</a></li>
      <li><a href="#method">Method</a>
        <ul>
          <li><a href="#1question-generation">1)question generation</a></li>
          <li><a href="#2distractor-generation">2)distractor generation</a></li>
          <li><a href="#3qa-filtering">3)QA filtering</a></li>
        </ul>
      </li>
      <li><a href="#results">Results</a>
        <ul>
          <li><a href="#1quantitative-evaluation">1)quantitative evaluation</a></li>
          <li><a href="#2question-answering-ability">2)question answering ability</a></li>
          <li><a href="#3human-evaluation">3)human evaluation</a></li>
        </ul>
      </li>
      <li><a href="#conclusion">conclusion</a></li>
    </ul>
  </li>
  <li><a href="#ç¬¬ä¸‰ç¯‡">ç¬¬ä¸‰ç¯‡</a>
    <ul>
      <li><a href="#title-2">Title</a></li>
      <li><a href="#author-2">Author</a></li>
      <li><a href="#abstract-2">Abstract</a></li>
      <li><a href="#framework-description-ç½‘ç»œç»“æ„">Framework Description ç½‘ç»œç»“æ„</a>
        <ul>
          <li><a href="#1task-definition">1)Task Definition</a></li>
          <li><a href="#2framework-overview">2)Framework overview</a></li>
          <li><a href="#3hierarchical-encoder">3)Hierarchical encoder</a></li>
          <li><a href="#4static-attention-mechanism">4)static attention mechanism</a></li>
          <li><a href="#5encoding-layer">5)encoding layer</a></li>
          <li><a href="#6matching-layer">6)matching layer</a></li>
          <li><a href="#7nomalization-layer">7)nomalization layer</a></li>
          <li><a href="#8distractor-decoder">8)distractor decoder</a></li>
          <li><a href="#9question-based-initializer">9)question-based initializer</a></li>
          <li><a href="#10dynamic-hierarchical-attention-mechanism">10)dynamic hierarchical attention mechanism</a></li>
          <li><a href="#11training-and-inference">11)training and inference</a></li>
        </ul>
      </li>
      <li><a href="#experimental-setting-å®éªŒè®¾ç½®">experimental setting å®éªŒè®¾ç½®</a>
        <ul>
          <li><a href="#1dataset">1)dataset</a></li>
          <li><a href="#2implementation-details-1">2)implementation details</a></li>
          <li><a href="#3baselines-and-ablations">3)baselines and ablations</a></li>
        </ul>
      </li>
      <li><a href="#results-and-analysis-ç»“æœä¸åˆ†æ">results and analysis ç»“æœä¸åˆ†æ</a></li>
      <li><a href="#æˆ‘çš„çœ‹æ³•-1">æˆ‘çš„çœ‹æ³•</a></li>
    </ul>
  </li>
  <li><a href="#ç¬¬å››ç¯‡">ç¬¬å››ç¯‡</a>
    <ul>
      <li><a href="#title-3">Title</a></li>
      <li><a href="#author-3">Author</a></li>
      <li><a href="#abstract-3">Abstract</a></li>
      <li><a href="#proposed-framework-ç½‘ç»œç»“æ„">Proposed Framework ç½‘ç»œç»“æ„</a>
        <ul>
          <li><a href="#1notations-and-task-definition">1)notations and task definition</a></li>
          <li><a href="#2model-overview">2)model overview</a></li>
          <li><a href="#3encoding-article-and-question">3)encoding article and question</a></li>
          <li><a href="#4co-attention-between-article-and-question">4)Co-attention between article and question</a></li>
          <li><a href="#5merging-sentence-representation">5)Merging sentence representation</a></li>
          <li><a href="#6question-initialization">6)question initialization</a></li>
          <li><a href="#7hierarchical-attention">7)hierarchical attention</a></li>
          <li><a href="#8semantic-similarity-loss">8)semantic similarity loss</a></li>
        </ul>
      </li>
      <li><a href="#experimental-settings">Experimental Settings</a>
        <ul>
          <li><a href="#1dataset-1">1)dataset</a></li>
          <li><a href="#2baselines-and-evaluation-metrics">2)baselines and evaluation metrics</a></li>
          <li><a href="#3implementation-details">3)implementation details</a></li>
        </ul>
      </li>
      <li><a href="#results-and-analysis-ç»“æœä¸åˆ†æ">Results and Analysis ç»“æœä¸åˆ†æ</a></li>
      <li><a href="#æˆ‘çš„çœ‹æ³•-2">æˆ‘çš„çœ‹æ³•</a></li>
    </ul>
  </li>
</ul>

<h1 id="æ–‡çŒ®æ•´ç†">æ–‡çŒ®æ•´ç†</h1>

<h2 id="è¦æ±‚">è¦æ±‚</h2>

<p><img src="../assets/img/posts/20211130/requirements.jpg" /></p>

<h2 id="æœé›†åˆ°ç›¸å…³æ–‡çŒ®æ ‡é¢˜å’Œåœ°å€">æœé›†åˆ°ç›¸å…³æ–‡çŒ®æ ‡é¢˜å’Œåœ°å€</h2>
<ul>
  <li><a href="https://arxiv.org/pdf/2010.05384.pdf">A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies</a></li>
  <li><a href="https://arxiv.org/pdf/2010.09598.pdf">Better Distractions: Transformer-based Distractor Generation and Multiple Choice Question Filtering</a></li>
  <li><a href="https://ojs.aaai.org//index.php/AAAI/article/view/4606">Generating Distractors for Reading Comprehension Questions from Real Examinations</a></li>
  <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6522">Co-attention hierarchical network: Generating coherent long distractors for reading comprehension</a></li>
  <li><a href="https://aclanthology.org/2020.coling-main.189.pdf">Automatic Distractor Generation for Multiple Choice Questions in Standard Tests</a></li>
  <li><a href="https://aclanthology.org/W18-0533.pdf">Distractor Generation for Multiple Choice Questions Using Learning to Rank</a></li>
  <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/16559">Knowledge-Driven Distractor Generation for Cloze-style Multiple Choice Questions</a></li>
</ul>

<h1 id="ç¬¬ä¸€ç¯‡">ç¬¬ä¸€ç¯‡</h1>
<h2 id="title">Title</h2>
<p>A BERT-based Distractor Generation Scheme with Multi-tasking and
Negative Answer Training Strategies</p>
<h2 id="author">Author</h2>
<p>Ho-Lam Chung, Ying-Hong Chan, Yao-Chung Fan</p>
<h2 id="abstract">Abstract</h2>
<p>ç°æœ‰çš„DG<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>å±€é™åœ¨åªèƒ½ç”Ÿæˆä¸€ä¸ªè¯¯å¯¼é€‰é¡¹ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆå¤šä¸ªè¯¯å¯¼é€‰é¡¹ï¼Œæ–‡ç« ä¸­æåˆ°ä»–ä»¬å›¢é˜Ÿç”¨multi-taskingå’Œnegative answer trainingæŠ€å·§æ¥ç”Ÿæˆå¤šä¸ªè¯¯å¯¼é€‰é¡¹ï¼Œæ¨¡å‹ç»“æœè¾¾åˆ°äº†å­¦ç•Œé¡¶å°–ã€‚</p>

<h2 id="introduction">Introduction</h2>
<p>DGæ•ˆæœä¸å¥½ï¼Œæ–‡ç« æå‡ºäº†ä¸¤ä¸ªæå‡çš„ç©ºé—´ï¼š</p>
<ol>
  <li>DGè´¨é‡æå‡ï¼š<br />
 BERTæ¨¡å‹æ¥æå‡è¯¯å¯¼é€‰é¡¹è´¨é‡</li>
  <li>å¤šä¸ªè¯¯å¯¼é€‰é¡¹ç”Ÿæˆï¼š
 è¿ç”¨äº†è¦†ç›–çš„æ–¹æ³•æ¥é€‰æ‹©distractorï¼Œè€Œä¸æ˜¯é€‰æ‹©æ¦‚ç‡æœ€é«˜ä½†æ˜¯è¯­ä¹‰å¾ˆç›¸è¿‘çš„distractor</li>
</ol>

<h2 id="bert-distractor-generation">BERT distractor generation</h2>
<h3 id="1bert-based-distractor-generationbdg">1)BERT-based distractor generation(BDG)</h3>
<p>è¾“å…¥ï¼šæ®µè½Pï¼Œç­”æ¡ˆAï¼Œé—®é¢˜Qï¼Œç”¨Cè¡¨ç¤ºè¿™ä¸‰è€…concatenateåçš„ç»“æœã€‚<br />
BDGæ¨¡å‹æ˜¯ä¸€ä¸ªè‡ªå›å½’æ¨¡å‹ï¼Œåœ¨é¢„æµ‹é˜¶æ®µï¼Œæ¯æ¬¡è¾“å…¥Cå’Œä¸Šä¸€æ¬¡é¢„æµ‹çš„è¯å…ƒï¼ŒBDGè¿­ä»£é¢„æµ‹è¯å…ƒï¼Œç›´åˆ°é¢„æµ‹å‡ºç‰¹æ®Šè¯å…ƒ[S]åœæ­¢ã€‚ä¸‹é¢è¿™å¼ å›¾ç®€å•ä»‹ç»äº†è¿™ä¸ªè¿‡ç¨‹ã€‚</p>

<p><img src="../assets/img/posts/20211130/2.jpg" /></p>

<p>ç½‘ç»œç»“æ„ç®€å•ä»‹ç»ï¼šh<sub>[M]</sub>è¡¨ç¤ºbertè¾“å‡ºçš„éšè—çŠ¶æ€ï¼Œéšè—çŠ¶æ€å†è¾“å…¥åˆ°ä¸€ä¸ªå…¨è¿æ¥å±‚ä¸­ç”¨æ¥é¢„æµ‹è¯å…ƒã€‚</p>

<p><img src="../assets/img/posts/20211130/3.jpg" /></p>

<h3 id="2multi-task-with-parallel-mlm">2)Multi-task with Parallel MLM</h3>
<p>MLMå…¨ç§°masked language modelï¼Œé®è”½è¯­è¨€æ¨¡å‹,é€šè¿‡å¹¶è¡ŒBDGå’ŒP-MLMæ¥è®­ç»ƒæ¨¡å‹è®©æ¨¡å‹æœ‰æ›´å¥½çš„æ•ˆæœã€‚</p>

<p><img src="../assets/img/posts/20211130/4.jpg" /></p>

<p>ä¸Šå›¾ä¸­å·¦è¾¹çš„sequential MLMå°±æ˜¯ä¹‹å‰æåˆ°çš„BDGï¼ŒBDGæ¨¡å‹æ˜¯ä¸€ä¸ªè¯æ¥ä¸€ä¸ªè¯çš„é¢„æµ‹ï¼ŒP-MLMæ˜¯å¯¹æ‰€æœ‰çš„masked tokenè¿›è¡Œé¢„æµ‹ï¼Œæœ€åçš„æŸå¤±å‡½æ•°æ˜¯è¿™ä¸¤è€…ç›¸åŠ <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>ï¼Œå…¬å¼å¦‚ä¸‹ï¼š</p>

<p><img src="../assets/img/posts/20211130/5.jpg" /></p>

<p><img src="../assets/img/posts/20211130/6.jpg" /></p>

<p><img src="../assets/img/posts/20211130/7.jpg" /></p>

<p>ä½œè€…å¦‚æ­¤è®¾è®¡çš„æ€è·¯æ˜¯ï¼šBDGå¯èƒ½ä¼šå¿½ç•¥æ•´ä½“è¯­ä¹‰è¯­ä¹‰ä¿¡æ¯ï¼Œä½†æ˜¯ä¼šè¿‡æ‹Ÿåˆå•ä¸ªè¯é¢„æµ‹ã€‚é‚£ä¹ˆå¹¶è¡Œä¸€ä¸ªP-MLMå¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚</p>

<h3 id="3answer-negative-regularization">3)Answer Negative Regularization</h3>
<p>ç›®å‰æœºå™¨é¢„æµ‹çš„distractorå’Œansweræœ‰å¾ˆé«˜çš„ç›¸ä¼¼åº¦ï¼Œä¸‹é¢ä¸€å¼ è¡¨å¯ä»¥å±•ç¤ºç›¸ä¼¼åº¦ã€‚å…¶ä¸­PMè¡¨ç¤ºæœºå™¨ï¼ŒGoldè¡¨ç¤ºäººå·¥ï¼Œä½œè€…å°†è¿™ç±»é—®é¢˜ç§°ä¸ºanswer copying problemã€‚</p>

<p><img src="../assets/img/posts/20211130/8.jpg" /></p>

<p>ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†answer negative lossæ¥è®©æœºå™¨æ›´å¤šçš„é€‰æ‹©ä¸answerä¸åŒçš„è¯æ¥è¡¨ç¤ºæ–°çš„distractorï¼Œå…¬å¼å¦‚ä¸‹ï¼š</p>

<p><img src="../assets/img/posts/20211130/9.jpg" /></p>

<p>å¯ä»¥çœ‹å‡ºBDGçš„lossæ›¿æ¢æˆäº†ANçš„lossï¼Œæ¯ä¸€é¡¹éƒ½å‡å»äº†Answer negative lossã€‚</p>

<h2 id="multiple-distractor-generation">Multiple Distractor Generation</h2>
<h3 id="1selecting-distractors-by-entropy-maximization">1)Selecting Distractors by Entropy Maximization</h3>
<p>é€‰æ‹©è¯­ä¹‰ä¸åŒçš„distractor setã€‚æ–‡ç« å€Ÿé‰´äº†MRC<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>çš„æ–¹æ³•ï¼Œè®©BDGmodelç”Ÿæˆå¾ˆå¤šdistractorç»„æˆ $\hat{D}$ = {$\hat{d}$<sub>1</sub>, $\hat{d}$<sub>2</sub>, $\hat{d}$<sub>3</sub>â€¦}ï¼Œç„¶åæ‰¾å‡ºæœ€å¥½çš„ä¸€ç»„é€‰é¡¹ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ç”±ä¸‰ä¸ªè¯¯å¯¼é€‰é¡¹å’Œä¸€ä¸ªç­”æ¡ˆç»„æˆã€‚é€‰æ‹©çš„ä¸€å¥æ˜¯æœ€å¤§åŒ–ä¸‹é¢è¿™ä¸ªå…¬å¼ï¼š</p>

<p><img src="../assets/img/posts/20211130/10.jpg" /></p>

<h3 id="2bdg-em">2)BDG-EM</h3>
<p>æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸åŒçš„BDGæ¨¡å‹æ¥ç”Ÿæˆä¸åŒçš„è¯¯å¯¼é€‰é¡¹æœ€åç»„åˆï¼Œä¸åŒçš„æ¨¡å‹åŒºåˆ«æ˜¯æœ‰æ²¡æœ‰answer negative/multi-task trainingï¼Œæ¯”å¦‚æˆ‘ä»¬æœ‰è¿™å‡ ä¸ªæ¨¡å‹:$\hat{D}$,$\hat{D}$<sub>PM</sub>,$\hat{D}$<sub>PM+AN</sub>ï¼Œå®ƒä»¬åˆ†åˆ«ä»£è¡¨å«PM<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>å’Œå«AN<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup></p>

<p><img src="../assets/img/posts/20211130/11.jpg" /></p>

<h2 id="performance-evaluation">Performance Evaluation</h2>
<h3 id="1datasets">1)datasets</h3>
<p>RACE,æ²¿ç”¨äº†<a href="https://ojs.aaai.org//index.php/AAAI/article/view/4606">Gao</a>é‚£ç¯‡è®ºæ–‡çš„å¤„ç†,åé¢ä¹Ÿä¼šæ¢³ç†é‚£ç¯‡è®ºæ–‡</p>

<p><img src="../assets/img/posts/20211130/12.jpg" /></p>

<h3 id="2implementation-details">2)implementation details</h3>
<ul>
  <li>tokenizer: wordpiece tokenizer</li>
  <li>framewordk:huggingface trainsformers</li>
  <li>optimizer:adamW(lr:5e-5)</li>
  <li>github_url: <a href="https://github.com/voidful/BDG">BDG</a></li>
</ul>

<h3 id="3compared-methods">3)compared methods</h3>
<p>æ¯”è¾ƒäº†ä¸åŒçš„distractor generation</p>
<ul>
  <li>CO-Attï¼šå‡ºè‡ª<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6522">Zhou</a></li>
  <li>DS-Att: å‡ºè‡ª<a href="https://ojs.aaai.org//index.php/AAAI/article/view/4606">Gao</a></li>
  <li>GPT:baseline</li>
  <li>BDG: æ²¡æœ‰åº”ç”¨P-MLMå’ŒAnswer negative</li>
  <li>BDG<sub>PM</sub></li>
  <li>BDG<sub>AN+PM</sub></li>
</ul>

<h3 id="4token-score-comparison">4)token score comparison</h3>
<p>BLEUå’ŒROUGE(L)ä¸¤ç§åˆ¤æ–­æŒ‡æ ‡</p>

<p><img src="../assets/img/posts/20211130/13.jpg" /></p>

<p>copying problemçš„æ•ˆæœ</p>

<p><img src="../assets/img/posts/20211130/14.jpg" /></p>

<h3 id="5mcq-model-accuracy-comparison">5)MCQ Model Accuracy Comparison</h3>
<p>ä¸å›ç­”ç³»ç»Ÿç›¸ç»“åˆï¼Œå°†ç”Ÿæˆå¥½çš„é€‰é¡¹ï¼ˆä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆä¸‰ä¸ªè¯¯å¯¼é€‰é¡¹ï¼‰æ”¾å…¥MCQ answering modelï¼Œä¸‹é¢æ˜¯å›ç­”æ­£ç¡®ç‡çš„è¡¨æ ¼</p>

<p><img src="../assets/img/posts/20211130/15.jpg" /></p>

<p>å¯ä»¥çœ‹å‡ºä½œè€…çš„æ¨¡å‹é€‰é¡¹çš„è¯¯å¯¼æ€§è¿˜æ˜¯å¾ˆé«˜çš„ã€‚</p>

<h3 id="6parameter-study-on-Î³">6ï¼‰Parameter Study on Î³</h3>
<p>ä¹‹å‰ä½¿ç”¨P-MLMå¹¶è¡Œè®­ç»ƒæ—¶å€™æœ‰ä¸ªæƒé‡å‚æ•°Î³ï¼Œä¸‹è¡¨æ˜¾ç¤ºäº†ä¸åŒÎ³å€¼çš„å½±å“ï¼Œå¯¹äºåªæœ‰PMçš„æ¨¡å‹æ¥è¯´ï¼ŒÎ³=6ï¼Œå¯¹äºæ—¢æœ‰ANå’ŒPMæ¥è¯´ï¼ŒÎ³=7</p>

<p><img src="../assets/img/posts/20211130/16.jpg" /></p>

<h2 id="conclusion">Conclusion</h2>
<p>ç°å­˜çš„DGå¯ä»¥åˆ†ä¸ºcloze-style distractor generationå’Œ reading comprehension distractor generationï¼Œå‰è€…ä¸»è¦æ˜¯word fillingï¼Œåè€…ä¸»è¦çœ‹é‡è¯­ä¹‰ä¿¡æ¯ï¼ŒåŸºäºä¸¤è€…çš„è®¾è®¡å‡ºäº†å¾ˆå¤šæ¨¡å‹ï¼Œç›®å‰æ¥çœ‹è¿˜æ˜¯è€ƒè™‘è¯­ä¹‰ä¿¡æ¯ç”Ÿæˆçš„è¯¯å¯¼é€‰é¡¹æ›´å¥½ã€‚</p>

<p><img src="../assets/img/posts/20211130/17.jpg" /></p>

<h2 id="æˆ‘çš„çœ‹æ³•">æˆ‘çš„çœ‹æ³•</h2>
<p>æ–‡ç« ä¸­çš„æ¨¡å‹æåˆ°äº†ä¸‰ç§æŠ€æœ¯ï¼Œç¬¬ä¸€æ˜¯berté¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨ã€‚ç¬¬äºŒæ˜¯P-MLMçš„å¹¶è¡Œä½¿ç”¨ï¼Œ å®ƒçš„ä½¿ç”¨è®©æ¨¡å‹å¯ä»¥è€ƒè™‘æ®µè½çš„è¯­ä¹‰ä¿¡æ¯ï¼Œé‚£ä¹ˆç”Ÿæˆçš„è¯¯å¯¼é€‰é¡¹æ˜¯sentence-levelè€Œä¸æ˜¯ä¹‹å‰æ¨¡å‹æ‰€ä½¿ç”¨çš„ç±»ä¼¼word-fillingè¿™ç§word-levelã€‚ç¬¬ä¸‰æ˜¯Answer negative lossçš„ä½¿ç”¨ï¼Œå®ƒçš„ä½¿ç”¨ç›¸å½“äºè®©æ¨¡å‹ä¸è¦è€ƒè™‘ä¸æ­£ç¡®ç­”æ¡ˆè¯­ä¹‰å¾ˆæ¥è¿‘çš„è¯¯å¯¼é€‰é¡¹ï¼Œå› ä¸ºç›®å‰å¤§å¤šæ•°DGç”Ÿæˆå¤šä¸ªé€‰é¡¹æ—¶è¯­ä¹‰ä¸æ­£ç¡®ç­”æ¡ˆéƒ½éå¸¸æ¥è¿‘ï¼Œè¿™ä¸å®é™…æƒ…å†µä¸ç¬¦ï¼ŒåŒæ—¶ä¹Ÿèµ·ä¸åˆ°è¯¯å¯¼çš„ä½œç”¨ã€‚  <br />
åŒæ—¶æ–‡ç« æå‡ºäº†ç”Ÿæˆå¤šä¸ªè¯¯å¯¼é€‰é¡¹æ—¶ä½¿ç”¨ä¸åŒæ¨¡å‹ç”Ÿæˆçš„è¯¯å¯¼é€‰é¡¹æ‹¼åœ¨ä¸€èµ·ä½œä¸ºé€‰é¡¹æ˜¯ä¸€ç§æ¯”è¾ƒå¥½çš„è§£å†³æ–¹æ³•ï¼Œè®©ä¸€æ¬¡æ€§ç”Ÿæˆå¤šä¸ªè¯¯å¯¼é€‰å‹æœ‰äº†ä¸€å®šçš„å¯ç”¨æ€§ã€‚<br />
æ–‡ç« çš„ä»£ç å¼€æºï¼Œå¯ä»¥å»<a href="https://github.com/voidful/BDG">github</a>ä¸Šçœ‹è®­ç»ƒç»†èŠ‚å’Œç½‘ç»œç»“æ„ç»†èŠ‚ã€‚</p>

<h1 id="ç¬¬äºŒç¯‡">ç¬¬äºŒç¯‡</h1>
<h2 id="title-1">Title</h2>
<p>Better Distractions: Transformer-based Distractor Generation and Multiple Choice Question Filtering</p>
<h2 id="author-1">Author</h2>
<p>Jeroen Offerijns, Suzan Verberne, Tessa Verhoef</p>
<h2 id="abstract-1">Abstract</h2>
<p>è¿ç”¨GPT2æ¨¡å‹ç”Ÿæˆä¸‰ä¸ªè¯¯å¯¼é€‰é¡¹ï¼ŒåŒæ—¶ç”¨BERTæ¨¡å‹å»å›ç­”è¿™ä¸ªé—®é¢˜ï¼ŒåªæŒ‘é€‰å‡ºå›ç­”æ­£ç¡®çš„é—®é¢˜ã€‚ç›¸å½“äºä½¿ç”¨äº†QAä½œä¸ºä¸€ä¸ªè¿‡æ»¤å™¨(QA filtering)ã€‚</p>
<h2 id="method">Method</h2>
<p>ä½œè€…ä½¿ç”¨äº†Question generation model, distractor generation modelå’Œquestion answer filterï¼Œä½œè€…å°†ä»è¿™ä¸‰æ–¹é¢ä»‹ç»ï¼Œä¸‹å›¾æ˜¯å¤§æ¦‚çš„æµç¨‹å›¾ã€‚</p>

<p><img src="../assets/img/posts/20211130/18.jpg" /></p>

<h3 id="1question-generation">1)question generation</h3>
<ul>
  <li>é¢„è®­ç»ƒæ¨¡å‹ï¼šGPT-2</li>
  <li>æ•°æ®é›†ï¼šEnglish SQuAD</li>
  <li>tokenizerï¼šByte-Pair-Encoding(BPE) tokenizer</li>
  <li>optimizer:Adam</li>
  <li>ä¸‹å›¾å±•ç¤ºäº†QGçš„è¾“å…¥ï¼Œé»‘æ¡†å†…è¢«tokenizeræ ‡è®°ä¸ºç‰¹æ®Šè¯å…ƒ</li>
</ul>

<p><img src="../assets/img/posts/20211130/19.jpg" /></p>

<h3 id="2distractor-generation">2)distractor generation</h3>
<ul>
  <li>é¢„è®­ç»ƒæ¨¡å‹ï¼šGPT-2</li>
  <li>æ•°æ®é›†ï¼šRACE</li>
  <li>tokenizer:BPE<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup></li>
  <li>ä½¿ç”¨äº†repetition penaltyæŠ€æœ¯ï¼Œä¿è¯äº†å°½é‡ä¸ä¼šç”Ÿæˆç›¸ä¼¼çš„textï¼Œå¹¶ä¸”è¿‡æ»¤åˆ°é‚£äº›ä¸å¥½çš„ç”Ÿæˆï¼ˆæ¯”å¦‚ç”Ÿæˆäº†ç©ºå­—ç¬¦ä¸²ï¼‰</li>
  <li>è¾“å…¥ï¼šç»å…¸çš„C(context)ï¼ŒA(answer),Q(question)ï¼Œä¸‹å›¾å±•ç¤ºäº†è¾“å…¥æ ¼å¼</li>
</ul>

<p><img src="../assets/img/posts/20211130/20.jpg" /></p>

<h3 id="3qa-filtering">3)QA filtering</h3>
<ul>
  <li>é¢„è®­ç»ƒæ¨¡å‹ï¼šDistilBERT</li>
  <li>ç½‘ç»œç»“æ„ï¼šCQA<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>è¾“å…¥åˆ°distilbertï¼Œå†è¿æ¥ä¸€ä¸ªdropoutï¼Œå…¨è¿æ¥å±‚å’Œsoftmaxï¼Œæœ€åè¾“å‡ºä¸€ä¸ªç­”æ¡ˆï¼Œå…·ä½“ç»“æ„å¦‚ä¸‹å›¾</li>
</ul>

<p><img src="../assets/img/posts/20211130/21.jpg" /></p>

<h2 id="results">Results</h2>
<h3 id="1quantitative-evaluation">1)quantitative evaluation</h3>
<p>ä¸‹è¡¨ä¸­å±•ç¤ºäº†å’Œä¸Šä¸€ç¯‡è®ºæ–‡ç±»ä¼¼çš„æŒ‡æ ‡,ä¸ç°æœ‰çš„æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼šSEQ2SEQ,HSA<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup>å’ŒCHN<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>ã€‚å¯ä»¥çœ‹å‡ºBLEUæ˜æ˜¾è¦æ¯”ä¹‹å‰æ¨¡å‹è¦å¥½ï¼Œä½†æ˜¯ROUGEæ²¡æœ‰ä¹‹å‰çš„é«˜ã€‚</p>

<p><img src="../assets/img/posts/20211130/22.jpg" /></p>

<h3 id="2question-answering-ability">2)question answering ability</h3>
<p>ç”¨GPT-2æ¨¡å‹ç”Ÿæˆè¯¯å¯¼é€‰é¡¹å†è¾“å…¥åˆ°QAmodelä¸­ï¼Œå…·ä½“ç»“æœè§ä¸‹å›¾ã€‚</p>

<p><img src="../assets/img/posts/20211130/23.jpg" /></p>

<h3 id="3human-evaluation">3)human evaluation</h3>
<p>äººå·¥è¯„ä¼°ï¼Œä»ä¸¤æ–¹é¢è¯„ä¼°distractorç”Ÿæˆçš„å¥½åï¼š</p>
<ul>
  <li><strong>Is the question well-formed and can you understand the meaning?</strong></li>
  <li><strong>If the question is at least understandable, does the answer make sense in relation to the question?</strong>
è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†155ä¸ªæ²¡æœ‰ç»è¿‡QAç­›é€‰å’Œ155ç»è¿‡QAç­›é€‰çš„ï¼Œäº†è§£ä¸€ä¸‹QAè¿‡æ»¤æ¨¡å‹çš„æ•ˆæœã€‚æ•´ä½“æ¥è¯´QAè¿‡æ»¤å™¨è¿˜æ˜¯æœ‰ä¸€ç‚¹æ•ˆæœï¼Œå…·ä½“ç»“æœå¦‚ä¸‹ï¼š</li>
</ul>

<p><img src="../assets/img/posts/20211130/24.jpg" /></p>

<h2 id="conclusion-1">conclusion</h2>
<p>æˆ‘è®¤ä¸ºä½œè€…ä½¿ç”¨çš„DGæ¨¡å‹ä¸»è¦æœ‰ä¸¤å¤§ç‰¹è‰²ï¼Œä¸€ä¸ªæ˜¯ä½¿ç”¨äº†GPT2é¢„è®­ç»ƒæ¨¡å‹ï¼Œç›®å‰ä½¿ç”¨åŸºäºtransformerçš„æ¨¡å‹å·²ç»æˆä¸ºä¸»æµã€‚ç¬¬äºŒä¸ªæ˜¯ä½¿ç”¨äº†QAè¿‡æ»¤å™¨æ¥ç­›é€‰æ‰å›ç­”é”™è¯¯çš„ï¼Œæœ‰ä¸€å®šæå‡ä½†ä¸æ˜¾è‘—ã€‚</p>

<h1 id="ç¬¬ä¸‰ç¯‡">ç¬¬ä¸‰ç¯‡</h1>
<h2 id="title-2">Title</h2>
<p>Generating Distractors for Reading Comprehension Questions from Real Examinations</p>
<h2 id="author-2">Author</h2>
<p>Yifan Gao, Lidong Bing, Piji Li,
Irwin King, Michael R. Lyu</p>
<h2 id="abstract-2">Abstract</h2>
<p>ä¸Šé¢ä¸¤ç¯‡æ–‡çŒ®éƒ½æœ‰æåˆ°è¿™ç¯‡æ–‡ç« ã€‚ä½œè€…ä½¿ç”¨äº†<strong>Hierarchical encoder-decoder framework</strong> with <strong>static</strong> and <strong>dynamic</strong> attention mechanismsæ¥ç”Ÿæˆæœ‰è¯­ä¹‰ä¿¡æ¯çš„è¯¯å¯¼é€‰é¡¹ã€‚ä½¿ç”¨äº†ç¼–ç å™¨-è§£ç å™¨ç»“æ„ç½‘ç»œå’Œé™æ€å’ŒåŠ¨æ€æ³¨æ„åŠ›æœºåˆ¶ã€‚</p>
<h2 id="framework-description-ç½‘ç»œç»“æ„">Framework Description ç½‘ç»œç»“æ„</h2>
<h3 id="1task-definition">1)Task Definition</h3>
<p>è¾“å…¥ï¼šæ–‡ç« ï¼Œé—®é¢˜å’Œç­”æ¡ˆã€‚Pä»£è¡¨æ–‡ç« ï¼Œs<sub>1</sub>,s<sub>2</sub>,s<sub>3</sub>â€¦è¡¨ç¤ºä¸åŒçš„å¥å­ï¼Œqå’Œaåˆ†åˆ«è¡¨ç¤ºé—®é¢˜å’Œç­”æ¡ˆï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯ç”Ÿæˆè¯¯å¯¼é€‰é¡¹$\overline{d}$ã€‚</p>

<p><img src="../assets/img/posts/20211130/25.jpg" /></p>

<h3 id="2framework-overview">2)Framework overview</h3>
<p>ç½‘ç»œç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä¸‹é¢å°†ä»å„ä¸ªç»„æˆéƒ¨åˆ†åˆ†åˆ«ä»‹ç»ï¼š</p>

<p><img src="../assets/img/posts/20211130/26.jpg" /></p>

<h3 id="3hierarchical-encoder">3)Hierarchical encoder</h3>
<ul>
  <li><strong>word embedding</strong>:è¯åµŒå…¥ï¼Œå°†æ¯ä¸ªå¥å­s<sub>i</sub>ä¸­çš„æ¯ä¸ªè¯å…ƒå˜æˆè¯å‘é‡(w<sub>i,1</sub>,w<sub>i,2</sub>,w<sub>i,3</sub>â€¦)</li>
  <li><strong>word encoder</strong>:å°†å¥å­s<sub>i</sub>çš„è¯å‘é‡(w<sub>i,1</sub>,w<sub>i,2</sub>,w<sub>i,3</sub>â€¦)ä½œä¸ºè¾“å…¥ï¼Œç”¨<strong>åŒå‘LSTM</strong>ä½œä¸ºç¼–ç å™¨ï¼Œè·å¾—word-level representation h<sub>i,j</sub><sup>e</sup></li>
</ul>

<p><img src="../assets/img/posts/20211130/27.jpg" /></p>

<ul>
  <li><strong>sentence encoder</strong>:å°†word encoderä¸­æ¯ä¸ªå¥å­æ­£å‘LSTMçš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€å’Œåå‘LSTMçš„æœ€å¼€å§‹çš„éšè—çŠ¶æ€ä½œä¸ºè¾“å…¥åˆ°å¦ä¸€ä¸ªåŒå‘LSTMä¸­è·å¾—<strong>sentence-level representation</strong>(u<sub>1</sub>,u<sub>2</sub>,u<sub>3</sub>â€¦)</li>
</ul>

<h3 id="4static-attention-mechanism">4)static attention mechanism</h3>
<p>ç›®çš„ï¼šç”Ÿæˆçš„è¯¯å¯¼é€‰é¡¹å¿…é¡»å’Œé—®é¢˜Qè¯­ä¹‰ç›¸å…³ï¼Œä½†æ˜¯å’Œç­”æ¡ˆAå¿…é¡»è¯­ä¹‰ä¸ç›¸å…³ã€‚æˆ‘ä»¬ä»(s<sub>1</sub>,s<sub>2</sub>,s<sub>3</sub>â€¦)å­¦ä¹ åˆ°å¥å­çš„æƒé‡åˆ†å¸ƒ(Î³<sub>1</sub>,Î³<sub>2</sub>,Î³<sub>3</sub>â€¦)ï¼Œç„¶åå°†é—®é¢˜qå’Œç­”æ¡ˆaä½œä¸ºqueryã€‚</p>

<h3 id="5encoding-layer">5)encoding layer</h3>
<p>æˆ‘ä»¬å¸Œæœ›æŠŠé—®é¢˜qï¼Œç­”æ¡ˆaå’Œå¥å­séƒ½å˜æˆä¸€æ ·çš„é•¿åº¦çš„å‘é‡è¡¨ç¤ºï¼Œä¹Ÿå°±æ˜¯ä¸Šå›¾ä¸­ç´«è‰²è™šçº¿éƒ¨åˆ†ã€‚å¯¹äºqå’Œaï¼Œæˆ‘ä»¬ç”¨ä¸¤ä¸ªç‹¬ç«‹çš„åŒå‘LSTMæ¥è·å¾—(<strong>a</strong><sub>1</sub>,<strong>a</strong><sub>2</sub>â€¦<strong>a</strong><sub>k</sub>)å’Œ(<strong>q</strong><sub>1</sub>,<strong>q</strong><sub>2</sub>â€¦<strong>q</strong><sub>l</sub>)ï¼Œç„¶åç”¨å¹³å‡æ± åŒ–å±‚å¹³å‡ä¸€ä¸‹ï¼š</p>

<p><img src="../assets/img/posts/20211130/28.jpg" /></p>

<p>å¯¹äºå¥å­sï¼Œæˆ‘ä»¬ä¸ç”¨uè€Œç”¨hï¼š</p>

<p><img src="../assets/img/posts/20211130/29.jpg" /></p>

<h3 id="6matching-layer">6)matching layer</h3>
<p>ç›®çš„ï¼šåŠ é‡ä¸é—®é¢˜qæœ‰å…³çš„å¥å­ï¼Œå‡è½»ä¸ç­”æ¡ˆaæœ‰å…³çš„å¥å­ã€‚o<sub>i</sub>è¡¨ç¤ºä¸åŒå¥å­çš„importance score</p>

<p><img src="../assets/img/posts/20211130/30.jpg" /></p>

<h3 id="7nomalization-layer">7)nomalization layer</h3>
<p>ç›®çš„ï¼šæœ‰äº›é—®é¢˜qå’Œä¸€ä¸¤ä¸ªå¥å­æœ‰å…³ï¼Œè€Œæœ‰äº›é—®é¢˜qå’Œå¾ˆå¤šå¥å­æœ‰å…³ï¼Œæ¯”å¦‚summarizingï¼Œä¸‹é¢çš„Ï„(temperature)å°±æ˜¯è¿™ä¸ªä½œç”¨</p>

<p><img src="../assets/img/posts/20211130/31.jpg" /></p>

<p><img src="../assets/img/posts/20211130/32.jpg" /></p>

<p>ä½œè€…ä»‹ç»static attention mechanismç”¨äº†å¾ˆå¤§ç¯‡å¹…</p>

<h3 id="8distractor-decoder">8)distractor decoder</h3>
<p>è§£ç å™¨ä½¿ç”¨çš„ä¹Ÿæ˜¯LSTMï¼Œä½†æ˜¯å¹¶æ²¡æœ‰ä½¿ç”¨ç¼–ç å™¨çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ä½œä¸ºåˆå§‹çŠ¶æ€ï¼Œè€Œæ˜¯å®šä¹‰äº†ä¸€ä¸ª
<strong>question-based initializer</strong>æ¥è®©ç”Ÿæˆçš„è¯¯å¯¼é€‰é¡¹è¯­æ³•å’Œé—®é¢˜qä¸€è‡´</p>

<h3 id="9question-based-initializer">9)question-based initializer</h3>
<p>å®šä¹‰äº†ä¸€ä¸ªquestion LSTMæ¥ç¼–ç é—®é¢˜qï¼Œä½¿ç”¨æœ€åä¸€å±‚çš„cell stateå’Œhidden stateä½œä¸ºdecoderåˆå§‹çŠ¶æ€ï¼ŒåŒæ—¶è¾“å…¥q<sub>last</sub>ï¼Œè¡¨ç¤ºé—®é¢˜qçš„æœ€åä¸€ä¸ªè¯å…ƒã€‚</p>

<h3 id="10dynamic-hierarchical-attention-mechanism">10)dynamic hierarchical attention mechanism</h3>
<p>å¸¸è§„çš„æ³¨æ„åŠ›æœºåˆ¶å°†ä¸€ç¯‡æ–‡ç« ä½œä¸ºé•¿å¥å­ï¼Œç„¶ådecoderçš„æ¯ä¸€ä¸ªæ—¶é—´æ­¥éƒ½ä¸encoderä¸­æ‰€æœ‰çš„hidden stateè¿›è¡Œæ¯”è¾ƒï¼Œä½†æ˜¯è¿™ç§æ–¹æ³•å¹¶ä¸é€‚åˆç›®å‰çš„æ¨¡å‹ã€‚åŸå› ï¼šé¦–å…ˆLSTMä¸èƒ½å¤„ç†è¿™ä¹ˆé•¿çš„è¾“å…¥ï¼Œå…¶æ¬¡ï¼Œä¸€äº›é—®é¢˜åªä¸éƒ¨åˆ†å¥å­æœ‰å…³ã€‚<br />
ç›®çš„ï¼šæ¯ä¸ªdecoderæ—¶é—´æ­¥åªå…³æ³¨<strong>é‡è¦å¥å­</strong>ï¼Œä½œè€…å°†è¿™ç§æ³¨æ„åŠ›æœºåˆ¶ç§°ä¸ºåŠ¨æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œå› ä¸ºä¸åŒçš„æ—¶é—´æ­¥ï¼Œword-levelå’Œsentence-level æ³¨æ„åŠ›åˆ†å¸ƒéƒ½ä¸åŒã€‚<br />
æ¯ä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥æ˜¯è¯å…ƒd<sub>t-1</sub>å’Œä¸Šä¸€ä¸ªéšè—çŠ¶æ€h<sub>t-1</sub></p>

<p><img src="../assets/img/posts/20211130/33.jpg" /></p>

<p><img src="../assets/img/posts/20211130/34.jpg" /></p>

<p>Î±å’ŒÎ²åˆ†åˆ«è¡¨ç¤ºword-level,sentence-levelæƒé‡ï¼Œæœ€åä½¿ç”¨ä¹‹å‰é™æ€æ³¨æ„åŠ›æœºåˆ¶è·å¾—çš„Î³æ¥è°ƒèŠ‚Î±å’ŒÎ²</p>

<p><img src="../assets/img/posts/20211130/35.jpg" /></p>

<p><img src="../assets/img/posts/20211130/36.jpg" /></p>

<p>è·å¾—ä¸Šä¸‹æ–‡å˜é‡<strong>c</strong><sub>t</sub></p>

<p><img src="../assets/img/posts/20211130/37.jpg" /></p>

<p>è·å¾—attention vector $\tilde{h}$</p>

<p><img src="../assets/img/posts/20211130/38.jpg" /></p>

<h3 id="11training-and-inference">11)training and inference</h3>
<p>æŸå¤±å‡½æ•°ï¼š</p>

<p><img src="../assets/img/posts/20211130/39.jpg" /></p>

<p>ç”Ÿæˆå¤šä¸ªè¯¯å¯¼é€‰é¡¹çš„æ–¹æ³•æ˜¯æŸæœç´¢ï¼Œä½†æ˜¯ç”Ÿæˆçš„è¯¯å¯¼é€‰é¡¹å¾ˆç›¸ä¼¼ï¼Œä½œè€…åšäº†ç›¸åº”çš„å¤„ç†æ–¹æ³•ï¼Œä½†æˆ‘è§‰å¾—æ•ˆæœè¿˜æ˜¯å¾ˆå·®</p>

<h2 id="experimental-setting-å®éªŒè®¾ç½®">experimental setting å®éªŒè®¾ç½®</h2>
<h3 id="1dataset">1)dataset</h3>
<p>RACEæ•°æ®é›†ï¼Œä½œè€…åšäº†ç›¸åº”çš„å¤„ç†ï¼Œå»æ‰äº†å¾ˆå¤šä¸åˆç†çš„å’Œè¯­ä¹‰ä¸ç›¸å…³çš„ï¼Œä½œè€…çš„å¤„ç†æ ‡å‡†æ˜¯ï¼šå¯¹äºè¯¯å¯¼é€‰é¡¹ä¸­çš„è¯å…ƒï¼Œå¦‚æœå®ƒä»¬åœ¨æ–‡ç« ä¸­å‡ºç°çš„æ¬¡æ•°å°äº5æ¬¡ï¼Œé‚£ä¹ˆå°†è¢«ä¿ç•™ï¼ŒåŒæ—¶å»æ‰äº†é‚£äº›éœ€è¦åœ¨å¥å­ä¸­é—´å’Œå¥å­å¼€å§‹å¡«ç©ºçš„é—®é¢˜ã€‚ä¸‹è¡¨å±•ç¤ºäº†å¤„ç†åçš„æ•°æ®é›†çš„ä¸€äº›ä¿¡æ¯ï¼š</p>

<p><img src="../assets/img/posts/20211130/40.jpg" /></p>

<h3 id="2implementation-details-1">2)implementation details</h3>
<p>è¯è¡¨ï¼šä¿ç•™äº†é¢‘ç‡æœ€é«˜çš„50kä¸ªè¯å…ƒï¼ŒåŒæ—¶ä½¿ç”¨GloVeä½œä¸ºè¯åµŒå…¥é¢„è®­ç»ƒæ¨¡å‹ã€‚å…¶ä»–çš„ç»†èŠ‚éƒ½å¯ä»¥åœ¨æ–‡ç« ä¸­çœ‹è§ï¼Œè¿™é‡Œä¸ä¸€ä¸€åˆ—å‡ºäº†ï¼Œä¸»è¦æ˜¯è¶…å‚æ•°çš„è®¾ç½®ã€‚</p>

<h3 id="3baselines-and-ablations">3)baselines and ablations</h3>
<p>ä¸HRED<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>å’Œseq2seqæ¯”è¾ƒ</p>

<h2 id="results-and-analysis-ç»“æœä¸åˆ†æ">results and analysis ç»“æœä¸åˆ†æ</h2>

<p><img src="../assets/img/posts/20211130/41.jpg" /></p>

<p>äººå·¥è¯„ä¼°ï¼š</p>

<p><img src="../assets/img/posts/20211130/42.jpg" /></p>

<p>å¤§è‡´è¿‡ç¨‹æ˜¯è¿™æ ·ï¼šå››ä¸ªè¯¯å¯¼é€‰é¡¹ï¼Œåˆ†åˆ«æ¥è‡ªseq2seqï¼ŒHREDï¼Œä½œè€…çš„æ¨¡å‹å’ŒåŸæœ¬çš„è¯¯å¯¼é€‰é¡¹ï¼Œè®©è‹±è¯­èƒ½åŠ›å¾ˆå¥½çš„äººæ¥é€‰æ‹©æœ€é€‚åˆçš„é€‰é¡¹ï¼Œå¾—å‡ºçš„ç»“æœå¯ä»¥å‘ç°ï¼Œä½œè€…çš„æ¨¡å‹ç”Ÿæˆçš„è¯¯å¯¼é€‰é¡¹æ‹¥æœ‰æœ€å¥½çš„è¯¯å¯¼æ•ˆæœã€‚</p>

<p>ä¸‹å›¾ç›´è§‚å±•ç¤ºäº†static attention distributionï¼š</p>

<p><img src="../assets/img/posts/20211130/43.jpg" /></p>

<h2 id="æˆ‘çš„çœ‹æ³•-1">æˆ‘çš„çœ‹æ³•</h2>
<p>è¿™ç¯‡æ–‡ç« åº”è¯¥æ˜¯ç¬¬ä¸€ä¸ªæå‡ºç”¨å¤„ç†åçš„RACEæ•°æ®é›†æ¥å¤„ç†MCQé—®é¢˜ï¼Œå¤„ç†åçš„RACEæ•°æ®é›†åœ¨åé¢ä¹Ÿæœ‰å¾ˆå¤šæ–‡çŒ®ç”¨åˆ°ï¼Œè¿™ç¯‡æ–‡ç« ä½¿ç”¨äº†seq2seqç½‘ç»œç»“æ„åŒæ—¶ä½¿ç”¨äº†é™æ€å’ŒåŠ¨æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯¹äºç½‘ç»œç»“æ„å’Œæ³¨æ„åŠ›æœºåˆ¶çš„è§£é‡Šéå¸¸å®Œå…¨å’Œè¯¦ç»†ï¼Œè™½ç„¶è¿™ç¯‡æ–‡ç« çš„æ•ˆæœæ”¾åˆ°ç°åœ¨æ¥çœ‹å¯èƒ½ä¸æ˜¯æœ€å¥½äº†ï¼Œä½†æ˜¯å®ƒæå‡ºæ¥çš„è¯„ä¼°æ ‡å‡†å¯èƒ½ä¼šæˆä¸ºä¸€ä¸ªé€šç”¨çš„æ ‡å‡†ã€‚å®ƒçš„æ•°æ®é›†å’Œè®­ç»ƒä»£ç åœ¨<a href="https://github.com/Yifan-Gao/Distractor-Generation-RACE">github</a>ä¸Šä¹Ÿå®Œå…¨å¼€æºã€‚</p>

<h1 id="ç¬¬å››ç¯‡">ç¬¬å››ç¯‡</h1>
<h2 id="title-3">Title</h2>
<p>Co-attention hierarchical network: Generating coherent long distractors for reading comprehension</p>
<h2 id="author-3">Author</h2>
<p>Xiaorui Zhou, Senlin Luo, Yunfang Wu</p>
<h2 id="abstract-3">Abstract</h2>
<p>è¿™ç¯‡æ–‡çŒ®æ˜¯é’ˆå¯¹ä¸Šä¸€ç¯‡Gaoçš„æ–‡ç« (seq2seq)æ‰€ä½œçš„æ”¹è¿›ã€‚æœ¬ç¯‡æ–‡ç« æå‡ºäº†Gaoçš„æ¨¡å‹çš„ä¸¤ä¸ªé—®é¢˜ï¼š1.æ²¡æœ‰å»ºç«‹æ–‡ç« å’Œé—®é¢˜çš„å…³ç³»ï¼Œä»–çš„è§£å†³æ–¹æ³•æ˜¯ä½¿ç”¨<strong>co-attention enhanced hierarchical architecture</strong>æ¥æ•è·æ–‡ç« å’Œé—®é¢˜ä¹‹é—´çš„å…³ç³»ï¼Œè®©è§£ç å™¨ç”Ÿæˆæ›´æœ‰å…³è”çš„è¯¯å¯¼é€‰é¡¹ã€‚2.æ²¡æœ‰åŠ é‡æ•´ç¯‡æ–‡ç« å’Œè¯¯å¯¼é€‰é¡¹çš„å…³ç³»ã€‚ä½œè€…çš„è§£å†³æ€è·¯æ˜¯æ·»åŠ ä¸€ä¸ªé¢å¤–çš„è¯­ä¹‰ç›¸å…³æ€§æŸå¤±å‡½æ•°ï¼Œè®©ç”Ÿæˆçš„è¯¯å¯¼é€‰é¡¹ä¸æ•´ç¯‡æ–‡ç« æ›´æœ‰å…³è”ã€‚</p>
<h2 id="proposed-framework-ç½‘ç»œç»“æ„">Proposed Framework ç½‘ç»œç»“æ„</h2>
<h3 id="1notations-and-task-definition">1)notations and task definition</h3>
<p>article T=(s<sub>1</sub>,s<sub>2</sub>â€¦s<sub>k</sub>)ï¼Œä¸€ç¯‡æ–‡ç« æœ‰kä¸ªå¥å­sï¼ŒåŒæ—¶æ¯ä¸ªå¥å­éƒ½æœ‰ä¸åŒçš„é•¿åº¦lï¼Œs<sub>i</sub>=(w<sub>i,1</sub>,w<sub>i,2</sub>â€¦w<sub>i,l</sub>)ï¼Œæ¯ä¸ªæ–‡ç« æœ‰mä¸ªé—®é¢˜å’Œzä¸ªè¯¯å¯¼é€‰é¡¹ï¼ŒQ=(q<sub>1</sub>,q<sub>2</sub>â€¦q<sub>m</sub>),D=(d<sub>1</sub>,d<sub>2</sub>â€¦d<sub>z</sub>),æˆ‘ä»¬çš„ä»»åŠ¡æ˜¯æ ¹æ®è¾“å…¥çš„Tå’ŒQç”ŸæˆD</p>

<p><img src="../assets/img/posts/20211130/44.jpg" /></p>

<h3 id="2model-overview">2)model overview</h3>
<p>æ•´ä½“ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä¸‹é¢å°†ä»å„ä¸ªéƒ¨åˆ†åˆ†åˆ«ä»‹ç»ï¼š</p>

<p><img src="../assets/img/posts/20211130/45.jpg" /></p>

<h3 id="3encoding-article-and-question">3)encoding article and question</h3>
<p>æ–‡ç« å’Œé—®é¢˜çš„ç¼–ç å™¨ç»“æ„</p>
<ul>
  <li><strong>hierarchical article encoder</strong>
åŒå‘LSTMï¼Œå’Œä¸Šä¸€ç¯‡ç»“æ„å¾ˆåƒï¼Œå¾ˆå¤šéƒ¨åˆ†æˆ‘å°±ç®€å•åˆ—ä¸ªå¼å­ã€‚</li>
</ul>

<p><img src="../assets/img/posts/20211130/46.jpg" /></p>

<p>æ¯ä¸€å¥æœ€åçš„è¯å…ƒæ¥è¡¨ç¤ºæ•´ä¸ªå¥å­</p>

<p><img src="../assets/img/posts/20211130/47.jpg" /></p>

<p>sentence-level encoderï¼š</p>

<p><img src="../assets/img/posts/20211130/48.jpg" /></p>

<p>åŒæ ·ï¼Œç”¨æœ€åä¸€ä¸ªå¥å­æ¥è¡¨ç¤ºæ•´ç¯‡æ–‡ç« </p>

<p><img src="../assets/img/posts/20211130/49.jpg" /></p>

<p>ç”¨<strong>H</strong><sup>*</sup>æ¥ä½œä¸ºsentence-level representation of article,æˆ‘ä»¬æœ‰<strong>H</strong><sub>:t</sub><sup>*</sup>=h<sub>t</sub><sup>s</sup></p>

<p>è¿™æ ·ï¼Œé€šè¿‡ä½¿ç”¨ä¸¤ä¸ªåŒå‘LSTMè·å¾—word-level encodingå’Œsentence-level encoding</p>
<ul>
  <li><strong>question encoder</strong></li>
</ul>

<p><img src="../assets/img/posts/20211130/50.jpg" /></p>

<p>ç”¨<strong>U</strong><sup>*</sup>æ¥ä½œä¸ºword-level representations of question, æˆ‘ä»¬æœ‰<strong>U</strong><sub>:t</sub><sup>*</sup>=h<sub>t</sub><sup>q</sup></p>

<h3 id="4co-attention-between-article-and-question">4)Co-attention between article and question</h3>
<p>Co-attention mechanismå°±æ˜¯ä½¿ç”¨äº†ä¸¤ä¸ªæ–¹å‘çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰ä»articleåˆ°questionçš„ï¼Œä¹Ÿæœ‰questionåˆ°articleçš„ã€‚<br />
ç”¨ä¸€ä¸ªâ€œç›¸ä¼¼â€çŸ©é˜µSè¡¨ç¤ºHå’ŒUçš„å…³ç³»ï¼š</p>

<p><img src="../assets/img/posts/20211130/51.jpg" /></p>

<p>S<sub>i,j</sub>å°±è¡¨ç¤ºç¬¬iä¸ªå¥å­å’Œç¬¬jä¸ªé—®é¢˜è¯å…ƒçš„ç›¸ä¼¼æ€§</p>

<p>æˆ‘ä»¬å¯ä»¥è·å¾—ä¸¤ä¸ªç‰¹æ®Šçš„çŸ©é˜µ<strong>S</strong><sup><strong>Q</strong></sup>å’Œ<strong>S</strong><sup><strong>T</strong></sup></p>

<p><img src="../assets/img/posts/20211130/52.jpg" /></p>

<ul>
  <li>article-to-question attention<br />
$\tilde{U}$<sub>:j</sub> = $\sum$ S<sub>i,j</sub><sup>Q</sup>U<sub>:,i</sub></li>
  <li>question-to-article attention</li>
</ul>

<p><img src="../assets/img/posts/20211130/53.jpg" /></p>

<p>æœ€åï¼Œå°†é—®é¢˜çš„è¯çº§è¡¨ç¤ºHï¼Œä¸¤ä¸ªæ–¹å‘çš„æ³¨æ„åŠ›ç»“æœ$\tilde{U}$å’Œ$\tilde{H}$ç»“åˆä¸€ä¸‹è·å¾—G</p>

<p><img src="../assets/img/posts/20211130/54.jpg" /></p>

<h3 id="5merging-sentence-representation">5)Merging sentence representation</h3>

<p><img src="../assets/img/posts/20211130/55.jpg" /></p>

<p>Zè¡¨ç¤ºfinal representation of sentence-level hidden states</p>

<h3 id="6question-initialization">6)question initialization</h3>
<p>æ¥ä¸‹æ¥å°±è¿›å…¥decoderç¯èŠ‚ï¼Œè¿™é‡Œçš„question initializationå’Œä¸Šç¯‡æ–‡çŒ®å¤„ç†æ–¹æ³•ç›¸åŒ</p>

<h3 id="7hierarchical-attention">7)hierarchical attention</h3>
<p>ä¸åŒæ—¶é—´æ­¥æœ‰ä¸åŒçš„å¥å­ç›¸å…³ï¼Œå’Œä¸Šç¯‡æ–‡çŒ®çš„å¤„ç†æ–¹æ³•åŠ¨æ€æ³¨æ„åŠ›æœºåˆ¶ç›¸åŒã€‚</p>

<p><img src="../assets/img/posts/20211130/56.jpg" /></p>

<p><img src="../assets/img/posts/20211130/57.jpg" /></p>

<p><img src="../assets/img/posts/20211130/58.jpg" /></p>

<p><img src="../assets/img/posts/20211130/59.jpg" /></p>

<h3 id="8semantic-similarity-loss">8)semantic similarity loss</h3>
<p>ç›®çš„ï¼šè·å¾—æ–‡ç« å’Œè¯¯å¯¼é€‰é¡¹çš„å…³ç³»ã€‚è¿˜è®°å¾—ä¹‹å‰å®šä¹‰çš„e<sub>T</sub>å—ï¼Œå®ƒè¡¨ç¤ºæ•´ç¯‡æ–‡ç« ï¼Œé‚£ä¹ˆæˆ‘ä»¬é€šè¿‡ä¸‹é¢çš„å…¬å¼å¯ä»¥è·å¾—distractor representation:</p>

<p><img src="../assets/img/posts/20211130/60.jpg" /></p>

<p>å…¶ä¸­S<sub>M</sub>æ˜¯decoderæœ€åä¸€ä¸ªéšè—çŠ¶æ€ï¼Œé‚£ä¹ˆæˆ‘ä»¬é€šè¿‡cosè®¡ç®—ç›¸ä¼¼å…³ç³»ï¼Œé‚£ä¹ˆæœ€ç»ˆçš„æŸå¤±å‡½æ•°</p>

<p><img src="../assets/img/posts/20211130/61.jpg" /></p>

<h2 id="experimental-settings">Experimental Settings</h2>
<h3 id="1dataset-1">1)dataset</h3>
<p>ä½¿ç”¨äº†ä¸Šç¯‡æ–‡çŒ®å¤„ç†è¿‡çš„RACEæ•°æ®é›†ã€‚</p>

<h3 id="2baselines-and-evaluation-metrics">2)baselines and evaluation metrics</h3>
<p>ä¸seq2seqï¼ŒHREDï¼ŒHCP<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">11</a></sup>ï¼ŒHSA<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">12</a></sup>æ¯”è¾ƒã€‚</p>

<h3 id="3implementation-details">3)implementation details</h3>
<p>ç½‘ç»œè¶…å‚æ•°è®¾ç½®æŠ€å·§ï¼Œä¸å±•å¼€äº†</p>

<h2 id="results-and-analysis-ç»“æœä¸åˆ†æ-1">Results and Analysis ç»“æœä¸åˆ†æ</h2>

<p><img src="../assets/img/posts/20211130/62.jpg" /></p>

<p><img src="../assets/img/posts/20211130/63.jpg" /></p>

<p><img src="../assets/img/posts/20211130/64.jpg" /></p>

<p>ä»‹ç»ä¸€ä¸‹ä¸Šé¢è¿™å¼ è¡¨ï¼Œè¿™å¼ è¡¨æ˜¯äººå·¥è¯„ä¼°çš„ç»“æœï¼Œä»ä¸‰ä¸ªç»´åº¦åˆ†æï¼Œåˆ†åˆ«æ˜¯fluency,coherence,distracting abilityã€‚å¯ä»¥çœ‹å‡ºä½œè€…çš„æ¨¡å‹å¹¶ä¸æ˜¯åœ¨æ‰€æœ‰ç»´åº¦éƒ½æ˜¯æœ€å¥½çš„ã€‚</p>

<p>ä¸‹å›¾æ˜¯æ¡ˆä¾‹åˆ†æï¼š</p>

<p><img src="../assets/img/posts/20211130/65.jpg" /></p>

<h2 id="æˆ‘çš„çœ‹æ³•-2">æˆ‘çš„çœ‹æ³•</h2>
<p>è¿™ç¯‡æ–‡çŒ®æ˜¯åŸºäºä¸Šä¸€ç¯‡æ–‡çŒ®çš„æ–¹æ³•è¿›è¡Œäº†ä¸¤ä¸ªæ”¹è¿›ï¼š1.å…³è”äº†æ•´ç¯‡æ–‡ç« å’Œé—®é¢˜ï¼Œè§£å†³æ–¹æ³•æ˜¯ä½¿ç”¨äº†Co-attention mechanismã€‚2.è®©distractorå’Œarticleè¯­ä¹‰ç›¸å…³ï¼Œæ–¹æ³•æ˜¯å®šä¹‰äº†ç›¸å…³æ€§lossã€‚</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>distractor generation è¯¯å¯¼é€‰é¡¹ç”Ÿæˆï¼Œç®€ç§°DGÂ <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>å½“æˆ‘ä»¬testæ—¶ï¼Œåªéœ€è¦Sequential MLM decoderæ¥é¢„æµ‹ã€‚Â <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>multi-choice reading comprehension (MRC) modelÂ <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>P-MLMÂ <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Answer negativeÂ <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Byte-Pair-EncodingÂ <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>contextï¼Œquestionï¼ŒanswerÂ <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>hierarchical encoder-decoder model with static attentionÂ <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>hierarchical model enhanced with co-attentionÂ <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p>hierarchical encoder-decoderÂ <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p>ç›¸å½“äºHRED+copy,æ˜¯åŸºäºHREDçš„ç½‘ç»œç»“æ„Â <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:12" role="doc-endnote">
      <p>å°±æ˜¯ä¸Šç¯‡æ–‡çŒ®çš„ç½‘ç»œÂ <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET