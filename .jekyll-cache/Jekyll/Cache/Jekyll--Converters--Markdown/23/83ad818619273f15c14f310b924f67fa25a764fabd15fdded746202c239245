I")<h1 id="basic-info">Basic Info</h1>
<ul>
  <li>论文全称: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</li>
  <li>相关链接:
    <ul>
      <li><a href="https://github.com/salesforce/BLIP" target="_blank">Github</a></li>
    </ul>
  </li>
</ul>

<h1 id="introduction">Introduction</h1>
<p>Vision Language Pre-training近些年有不错的进展，但是以往的VLP的预训练任务要么是vision-language understanding，要么是vision-language generation。BLIP在设计预训练任务时，综合考虑了这两方面的预训练任务。BLIP另一大亮点就是对数据的处理，vision-language领域的数据集，人工标注且效果好的数据不多，所以作者提出了一种CapFilt的Bootstrapping方法来过滤掉不好的标注，扩充image-text pair数据。论文主要有两大贡献:</p>
<ol>
  <li>Multimodal mixture of Encoder-Decoder(MED)，一种新的预训练框架，综合考虑了understanding任务和generation任务</li>
  <li>Captioning and Filtering(CapFilt)，一种数据集自举的方法，微调预训练的MED，并分为两个子模块，captioner用来将web获取的图片标注，filter用来将noisy的image text pair去除</li>
</ol>

<h1 id="model-architecture">Model Architecture</h1>
<p>模型的主题架构如下:</p>
<center><img src="../assets/img/posts/20221201/1.jpg" /></center>

<p>作者设计了三种不同的子架构:</p>
<ol>
  <li>unimodal encoder: 上图左侧的两个架构，针对单一的text或者image的encoder，对于text而言就是BERT的encoder，对于image而言就是ViT的encoder，同样用&lt;cls&gt;特殊编码来表示sentence的全局信息</li>
  <li>image-grounded text encoder: 上图中间的架构，针对text而言，不同之处是加上了image encoder后的cross attention，所以是image-grounded。text的开头加上特殊字符&lt;encode&gt;，代表image-text pair的多模态表示</li>
  <li>image-grounded text decoder: 上图最右侧的架构，除了和image encoder的cross-attention外，还用causal self-attention替换了bidirectional self-attention，causal attention具体是什么，可以去看<strong>causal attention for vision-language tasks</strong>这篇论文。特殊字符&lt;decode&gt;用来表示序列的开始，除了self-attention层外，和image-grounded text encoder共享参数</li>
</ol>

<p>MED这种架构的主要作用就是服务于预训练任务:</p>
<ol>
  <li>image-text contrastive loss(ITC): 应该和CLIP的任务类似，对齐特征空间中的image编码表示和text编码表示，属于vision-language understanding任务</li>
  <li>image-text matching loss(ITM): 判断vision和language是否匹配，二分类任务，属于vision-language understanding任务</li>
  <li>language modeling loss(LM): vision-language生成任务，自回归，给定序列的开头和图片的编码，输出完整的caption</li>
</ol>

<h1 id="capfilt">CapFilt</h1>
<p>正如前文所介绍，CapFilt的主要目的是筛除noisy的pair，因为从web上爬取的image-text pair质量太低。CaoFilt利用预训练的MED架构，抽取出两个子模块Captioner和Filter，Captioner就是MED的image-grounded text decoder部分，用来对web的图片进行标注，然后我们就获得了新的pair。Filter就是MED的image-grounded text encoder部分，原本那部分的预训练任务是ITM，用来当作过滤器很合适，这样就可以筛除noisy的pair，从而达到扩充数据集且数据集质量高的效果，下图清晰的介绍了这个过程:</p>
<center><img src="../assets/img/posts/20221201/2.jpg" /></center>

<h1 id="experiments-and-datasets">Experiments and datasets</h1>
<p>CapFilt的效果:</p>
<center><img src="../assets/img/posts/20221201/3.jpg" /></center>

<p>数据集:</p>
<ol>
  <li>两个人工标注数据集: COCO和VG(Visual Genome)</li>
  <li>三个从web爬取的数据集: CC(Conceptual Captions), SBU, LAION</li>
</ol>

<p>生成caption的方法:</p>
<ol>
  <li>nucleus sampling: 选取每个词元时，只要累计的概率超过了一个阈值，那么就作为一种选取方式</li>
  <li>beam search: 文本生成的常用手段，具体来说就是定义一个束宽k，选取每个词元时，都保留概率最高的k个选择，最后选出概率高的输出</li>
</ol>

<p>作者比较了这两种生成caption的方法，认为nucleus sampling会取得更好的结果，但是可能会生成奇怪的caption，如果是为了生成safe的caption，可以用beam search</p>
<center><img src="../assets/img/posts/20221201/4.png" /></center>

<h1 id="downstream-tasks">Downstream tasks</h1>
<ol>
  <li>image-text retrieval: 包含了image-to-text retrieval(TR)和text-to-image retrieval(IR)任务，即图片检索文本和文本检索图片任务，BLIP达到了SOTA，度量指标是Recall@k，在信息检索领域，代表查询图像对应标签在检索标签列表的前topk中出现的次数于真值标签列表个数的比例</li>
</ol>
<center><img src="../assets/img/posts/20221201/5.jpg" /></center>

<ol>
  <li>image captioning: 图片标注，用MED的LM部分实现，指标有CIDEr, SPICE, BLEU@4</li>
</ol>
<center><img src="../assets/img/posts/20221201/6.jpg" /></center>

<ol>
  <li>Visual Question Answering(VQA): 给定问题和图片，生成answer，这里作者把它看成生成任务，图片经过image encoder，问题经过question encoder，然后特殊字符&lt;decode&gt;输入answer encoder生成answer:</li>
</ol>
<center><img src="../assets/img/posts/20221201/7.jpg" /></center>

<ol>
  <li>Natural Language Visual Reasoning(NLVR): 判断一段话是否是在形容一对图片，为了适应这个任务，同样需要对训练框架进行调整:</li>
</ol>
<center><img src="../assets/img/posts/20221201/8.jpg" /></center>

<p>3和4的实验结果:</p>
<center><img src="../assets/img/posts/20221201/9.jpg" /></center>
:ET