I"Lz<!-- TOC -->

<ul>
  <li><a href="#1-论文简介">1. 论文简介</a></li>
  <li><a href="#2-bert-based-deep-neural-networks">2. BERT-Based Deep Neural Networks</a>
    <ul>
      <li><a href="#21-abstract">2.1. Abstract</a></li>
      <li><a href="#22-introduction">2.2. Introduction</a></li>
      <li><a href="#23-related-work">2.3. Related Work</a>
        <ul>
          <li><a href="#231-applications-of-deep-learning-in-asag-tasks">2.3.1. Applications of Deep Learning in ASAG Tasks</a></li>
          <li><a href="#232-bert-model-and-its-application-in-education">2.3.2. BERT Model and Its Application in Education</a></li>
        </ul>
      </li>
      <li><a href="#24-methodology">2.4. Methodology</a>
        <ul>
          <li><a href="#241-task-definition">2.4.1. Task Definition</a></li>
          <li><a href="#242-model">2.4.2. Model</a>
            <ul>
              <li><a href="#2421-bert-layer">2.4.2.1. BERT layer</a></li>
              <li><a href="#2422-semantic-refinement-layer">2.4.2.2. Semantic Refinement Layer</a></li>
              <li><a href="#2423-semantic-fusion-layer">2.4.2.3. Semantic Fusion Layer</a></li>
              <li><a href="#2424-prediction-layer">2.4.2.4. Prediction Layer</a></li>
              <li><a href="#2425-loss-function">2.4.2.5. Loss Function</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#25-experiments">2.5. Experiments</a>
        <ul>
          <li><a href="#251-datasets">2.5.1. Datasets</a></li>
          <li><a href="#252-experimental-settings">2.5.2. Experimental Settings</a></li>
          <li><a href="#253-ablation-studies">2.5.3. Ablation Studies</a></li>
          <li><a href="#254-comparison-with-baseline-systems">2.5.4. Comparison With Baseline Systems</a></li>
        </ul>
      </li>
      <li><a href="#26-discussions">2.6. Discussions</a></li>
      <li><a href="#27-conclusion">2.7. Conclusion</a></li>
    </ul>
  </li>
  <li><a href="#3-semantic-facets">3. Semantic Facets</a>
    <ul>
      <li><a href="#31-abstract">3.1. Abstract</a></li>
      <li><a href="#32-introduction">3.2. Introduction</a></li>
      <li><a href="#33-related-works">3.3. Related Works</a>
        <ul>
          <li><a href="#331-automated-response-evaluation">3.3.1. Automated response evaluation</a></li>
          <li><a href="#332-semantic-similarity-measurement">3.3.2. Semantic similarity measurement</a></li>
        </ul>
      </li>
      <li><a href="#34-patterns-and-indicative-powers-of-facets-matching-states">3.4. Patterns and indicative powers of facets matching states</a>
        <ul>
          <li><a href="#341-materials-and-methods">3.4.1. Materials and methods</a>
            <ul>
              <li><a href="#3411-dataset">3.4.1.1. Dataset</a></li>
            </ul>
          </li>
          <li><a href="#342-results-and-analysis">3.4.2. Results and analysis</a></li>
        </ul>
      </li>
      <li><a href="#35-automatic-extraction-of-facets-matching-features-for-better-prediciton">3.5. Automatic Extraction of Facets Matching Features For Better Prediciton</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h1 id="1-论文简介">1. 论文简介</h1>
<ul>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/9779091" target="_blank">short answer grading model</a></li>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/9860098" target="_blank">semantic facets</a></li>
</ul>

<p>这是两篇关于short-answer assessment的论文，所谓short-answer assessment就是对简答题的答案进行评估(和参考答案对比)，第一篇提出了利用BERT解决这个问题，第二篇提出了改进了评估过程，用多个semantic facets来评估short-answer，这篇博客对这两篇论文进行简单的整理</p>

<h1 id="2-bert-based-deep-neural-networks">2. BERT-Based Deep Neural Networks</h1>
<h2 id="21-abstract">2.1. Abstract</h2>
<p>Automatic short-answer grading(ASAG)，即自动短文本评分任务，是智慧辅导系统的重要组成部分。ASAG目前还存在很多挑战，作者提出了两个主要原因: 1)高精度评分任务需要对answer text有很深的语义理解; 2)ASAG任务的语料一般都很小，不能为深度学习提供足够的训练数据。为了解决这些挑战，作者提出用BERT-based网络来解决ASAG任务的挑战: 1)用预训练模型BERT来encoder答案文本就可以客服语料太小的问题。2)为了生成足够强的语义理解，作者在BERT输出层后加上了一个精炼层(由LSTM和Capsule网络串联组成) 3)作者提出一种triple-hot loss来处理ASAG的回归问题。实验结果表明模型的效果在SemEval-2013和Mohler数据集上表现比SOTA要好。模型在github上<a href="https://github.com/wuhan-1222/ASAG" target="_blank">开源</a></p>

<h2 id="22-introduction">2.2. Introduction</h2>
<p>考试和评估是智慧辅导系统(intelligent tutoring systems, ITSs)的重要组成部分，可以获得学生们的实时知识认知水平，也能为学生们提供个性化的学习方案。多选题是考试的重要组成部分，但是多选题有两个明显的短板: 1)多选题只提供部分选择 2)有些学生的答案可能是蒙出来的。ASAG可以解决上述问题，学生们为简答题提供一个short text，然后ASAG来评估short text是否正确，具体来说评估结果有五类: Correct、Partically correct、Contradictory、Irrelevant、Nondomain。</p>

<p>以往的研究中，Feature engineering是ASAG的主要解决方法，有很多稀疏特征应用于ASAG: token overlap features、syntax and dependency features、knowledge-based features(WordNet)… 但是这种特征工程为基础的方法存在以下问题: 首先，稀疏特征一般需要很多预处理步骤，这些步骤会产生一定的误差，可能会涉及到误差累积和误差传递的后果。此外，缺乏有效地encode文本句的手段</p>

<p>随着deeplearning的发展，出现了很多deep net，比如LSTM-based model、CNN and LSTM-based model、transformer-based model出现在了ASAG任务中。这些模型都从answer text中挖掘语义信息，然后将answer text转化成word enbedding，所以这些方法都是end-to-end的。但是这些方法存在以下问题: 1)学生的答案非常free，也就是说在句子结构、语言风格、段落长度这些方面可能会有很大的区别，所以作者认为需要用更先进的technique去获得text的语义理解。2)由于数据很难标注，所以ASAG任务的数据集语料很小，可能只有few thousand。所以说主要的挑战就是如何在小语料库上训练一个稳定高效的模型</p>

<p>论文的主要贡献:</p>
<ol>
  <li>提出了用预训练模型BERT微调，然后连接一个精炼层的模型表现超过SOTA(在SemEval-2013数据集和Mohler数据集上)</li>
  <li>精炼层由Bi-LSTM和Capsule network(with position information)串联组成，LSTM来抽取全局的context信息，Capsule来抽取局部context信息</li>
  <li>用多头注意力层来连接全局和局部context来生成语义表示</li>
  <li>提出了triple-hot loss策略</li>
</ol>

<h2 id="23-related-work">2.3. Related Work</h2>
<h3 id="231-applications-of-deep-learning-in-asag-tasks">2.3.1. Applications of Deep Learning in ASAG Tasks</h3>
<p>根据deep learning和训练策略的不同，作者将deeplearning在ASAG的应用分为以下三种:</p>
<ol>
  <li>Participator: deep learning参与feature-based方法中</li>
  <li>Contractor: deeplearning独立地在ASAG任务中实现end-to-end</li>
  <li>迁移学习，经典的预训练模型+scaling语料库</li>
</ol>

<p>接下来分别介绍了利用稀疏特征的方法与Deeplearning来来解决ASAG任务:</p>
<ol>
  <li>稀疏特征，也即feature engineering的应用有:
    <ul>
      <li>Marvaniya等人和Saha等人使用预训练的神经网络InferSent对答案文本进行编码，这弥补了标记重叠(token overlap)方法中上下文表示的不足，其中InferSent是使用Bi-LSTM网络的预训练句子嵌入模型</li>
      <li>Tan等人提出了一种将图卷积网络(GCNs)与几种稀疏特征相结合的评分方法。他们首先为答案文本构建了一个无向异构文本图，其中包含句子级节点、单词/bigam级节点和节点之间的边。然后，他们使用两层GCN模型对图结构进行编码，得到图的表示形式。</li>
      <li>Zhang等人使用深度信念网络作为分类器，而不是传统的机器学习，对学生由六个稀疏特征组成的答案表示进行分类</li>
    </ul>
  </li>
  <li>Deeplearning的方法有:
    <ul>
      <li>Kumar等人提出了ASAG的Bi-LSTM框架。他们的框架由三个级联的神经模块组成:分别应用于参考和学生答案的Siamese Bi-LSTMs，使用earth-mover distance(EMD)与LSTMs的隐状态交互的池化层，以及用于输出分数的回归层</li>
      <li>Uto和Uchida将LSTM网络与项目反应理论(item response theory)相结合进行短文本答案评分</li>
      <li>Tulu等人改进了基于LSTM的评分方法，通过引入感觉向量并将池化层替换为曼哈顿距离</li>
      <li>Riordan等人结合CNN和LSTM网络进行短文本答案评分</li>
      <li>Liu等人在一个大型K-12数据集上提出了一个具有多路注意的模型</li>
    </ul>
  </li>
</ol>

<p>上面提到的deeplearning方法需要大语料库支撑的数据集，但是ASAG缺少足够的大语料库，于是出现了用预训练模型来解决ASAG任务，比如ELMo、BERT、GPT、GPT-2，在这些模型中，BERT表现最好</p>

<h3 id="232-bert-model-and-its-application-in-education">2.3.2. BERT Model and Its Application in Education</h3>
<p>BERT吸收了ELMo和GPT的优点，模型如下图所示:</p>
<center><img src="../assets/img/posts/20221010/2.jpg" /></center>
<p>BERTstack了12个transformer块</p>

<p>接下来介绍了BERT在智慧教育领域的应用:</p>
<ol>
  <li>Wang等人提出了分层课程BERT模型，以更好地捕捉每门课程的课程结构质量和语言特征，预测在线教育中教师的绩效</li>
  <li>Khodeir等人将BERT与多层双向GRU相结合，构建了一个紧急分类模型，用于教师快速挑选和响应大规模开放在线课程(MOOC)论坛中最紧急的学生帖子，该模型在三个Stanford MOOC Post数据集上实现了紧急帖子分类，加权F-score分别为91.9%、91.0%和90.0%</li>
  <li>Sung等人利用BERT构建了一个多标签分类模型，用于快速评估学生在探索热力学的过程中的多模态的表征思维</li>
</ol>

<p>关于ASAG的应用有:</p>
<ol>
  <li>Sung等人分析比较了BERT与多种网络结构在short-answer grading的效果</li>
  <li>Leon等人分析比较了BERT、ALBERT、RoBERTa在short-answer grading的效果</li>
</ol>

<h2 id="24-methodology">2.4. Methodology</h2>
<h3 id="241-task-definition">2.4.1. Task Definition</h3>
<p>ASAG问题有两种形式:</p>
<ol>
  <li>回归问题: 连续的分数来评估学生的答案</li>
  <li>分类问题: 将学生的答案分为五类: Correct、Partically correct、Contradictory、Irrelevant、Nondomain</li>
</ol>

<p>作者的做法是用分数来对类别进行分类，比如0-0.5属于类别1，所以问题的本质还是分类问题，那么ASAG的预测类别$y^*$可以表示为:</p>

<p>
\begin{equation}
y^*=\underset{y \in Y}{\operatorname{argmax}}(\operatorname{Pr}(y \mid(q, p)))
\end{equation}
</p>

<p>其中Y表示类别集，Pr()表示预测的概率分布，q是学生答案，p是参考答案</p>

<h3 id="242-model">2.4.2. Model</h3>
<p>作者解释为什么即要用BERT，也要用refinement:</p>
<ol>
  <li>BERT获得word embedding结果，利用了所有词元之间的关系，但是没有考虑顺序和距离，所以需要用Bi-LSTM来生成更精细的全局context，同时弥补BERT时序信息的缺失，然后利用Capsule或者CNN来生成BERT每个隐层的局部信息</li>
  <li>BERT可以获得动态的词嵌入(对比GloVe获得静态的词嵌入)，这样可以获得更丰富的general-purpose knowledge，所以BERT即使在小语料库上也能有不错的效果</li>
  <li>一些研究表明，在BERT上应用经典的神经网络可以在小数据集上获得更好的效果，比如Liao等人结合RoBERTa和CNN来提升情感分析的效果，Yang等人在BERT上应用多头注意力层来添加距离权重在aspect polarity classification上获得更好的效果</li>
</ol>

<p>主题网络模型如下图所示:</p>
<center><img src="../assets/img/posts/20221010/4.jpg" /></center>
<p>接下来从模型的各个板块来分别介绍:</p>

<h4 id="2421-bert-layer">2.4.2.1. BERT layer</h4>
<p>首先BERT layer的参数初始化成BERTbase的参数，微调。BERT layer层的输入是学生和参考答案的token embedding，输出是BERT的隐层</p>

<p>
\begin{equation}
O_{BERT}=BERT(s)=\left\{h_1^b,h_2^b,...,h_n^b\right\}\in \mathbb{R}^{n\times d_b}
\end{equation}
</p>

<h4 id="2422-semantic-refinement-layer">2.4.2.2. Semantic Refinement Layer</h4>
<p>Refinement层由Bi-LSTM和Capsule network(with position information)串联组成，输出结果如下所示:</p>

<p>
\begin{equation}
\begin{aligned}
&amp;\overrightarrow{O_{\mathrm{LSTNS}}}=\overrightarrow{\operatorname{LSTMS}}\left(O_{\text {BERT }}\right)=\left\{\overrightarrow{h_1^L}, \overrightarrow{h_2^L}, \ldots, \overrightarrow{h_n^L}\right\} \in \mathbb{R}^{n \times d_L} \\
&amp;\overleftrightarrow{O_{\mathrm{LSTMs}}}=\overleftarrow{\operatorname{LSTMs}}\left(O_{\mathrm{BERT}}\right)=\left\{\overleftrightarrow{h_1^r}, \overleftrightarrow{h_2^r}, \ldots, \overleftrightarrow{h_n^r}\right\} \in \mathbb{R}^{n \times d_L} \\
&amp;O_{\mathrm{Caps}}=\operatorname{Capsules}\left(O_{\text {BERT }}\right)=\left\{h_1^c, h_2^c, \ldots, h_n^c\right\} \in \mathbb{R}^{n \times d_c}
\end{aligned}
\end{equation}
</p>

<p>输出结果后面都跟了一个层归一化(保证数据分布的稳定，加速收敛)</p>

<p>这里提到了Capsule network，我对Capsule network进行一定的补充: Capsule网络主要想解决卷积神经网络（Convolutional Neural Networks）存在的一些缺陷，比如说信息丢失，视角变化等。Capsule网络结构如下图所示:</p>
<center><img src="../assets/img/posts/20221010/7.jpg" /></center>
<p>以数字图片分类为例，Capsule一共包含3层，2层卷积层和1层全连接层。与普通网络的区别是输出的每个类别都是一个向量，向量的长度表示实体存在的概率大小，向量在空间中的方向表示实体的实例化参数，Capsule网络和CNN还是比较相似的</p>

<h4 id="2423-semantic-fusion-layer">2.4.2.3. Semantic Fusion Layer</h4>
<p>在refinement层后，需要有一个融合层来融合LSTM和Capsule的结果，先用矩阵来stackLSTM、Capsule的结果:</p>

<p>
\begin{equation}
X^{(e)}=\{x_1^{(e)}, x_2^{(e)},...,x_n^{(e)}\}\in \mathbb{R}^{n\times d}
\end{equation}
</p>

<p>其中$x_i^{(e)}=[h_i^L;h_i^r;h_i^c]$，然后再把矩阵X送入多头自注意力层，注意力评分函数是scaled dot-product attention，具体细节如下:</p>

<p>
\begin{equation}
\begin{aligned}
&amp; \text{MultiHead}(Q,K,V)=[\text{head}_1;\text{head}_2;...;\text{head}_h]\omega^R \\
&amp; \text{head}_i=\text{Attention}(Q_i;K_i;V_i)=\text{Attention}(Q\omega ^Q, K\omega ^K, V\omega ^V) \\
&amp; \text{Attention}(Q_i;K_i;V_i)=\text{softmax}(\frac{Q_iK_i^T}{\sqrt{d_K}})V_i \\
 X^{(h)}&amp;=\text{MultiHead}(X^{(e)}, X^{(e)}, X^{(e)}) \\
&amp;=\{x_1^{(h)}, x_2^{(h)},..., x_n^{(h)}\}
\end{aligned}
\end{equation}
</p>

<p>为了让全局context和局部context不互相干扰，作者对多头自注意力层做以下约束:</p>
<ol>
  <li>让LSTM的输出维度和Capsule的输出维度相同，即$d_c=2d_L$</li>
  <li>head数取2</li>
  <li>让局部context和全局context不互相干扰(用参数调整)，如下图所示:</li>
</ol>
<center><img src="../assets/img/posts/20221010/10.jpg" /></center>
<p>最后再连接一个层归一化</p>

<h4 id="2424-prediction-layer">2.4.2.4. Prediction Layer</h4>
<p>预测层，首先用最大池化层获得pair(q,p)的语义表示，其实就是在每个头上选择最大的值:</p>

<p>
\begin{equation}
\begin{aligned}
Z &amp;=\text{Maxpooling}(X^{(h)})={z_1,z_2,...,z_d}\in \mathbb{R}^d \\
z_j &amp;=\text{Max}(x_{1j}^{(h)}, x_{2j}^{(h)},..., x_{nj}^{(h)}), j=1,2,...,d
\end{aligned}
\end{equation}
</p>

<p>然后将语义表示Z输入线性层(加上一个dropout防止overfit)，然后用softmax表示输出的概率分布:</p>

<p>
\begin{equation}
\begin{aligned}
o &amp;=MZ+b \\
p(y\mid Z)&amp;=\frac{exp(o_y)}{\sum_{i}^{d_y}exp(o_i)}
\end{aligned}
\end{equation}
</p>

<h4 id="2425-loss-function">2.4.2.5. Loss Function</h4>
<p>为了适应两种ASAG tasks，作者提出了两种损失函数的策略:</p>
<ul>
  <li>第一种就是常规的交叉熵，分类结果用one-hot编码</li>
</ul>

<p>
\begin{equation}
L(\theta)=-\sum_{i=1}^{|\Omega|}\log \left(p\left(y_i \mid Z_i, \theta\right)\right)
\end{equation}
</p>

<ul>
  <li>第二种就是作者提出用triple-hot编码y，就是在y对应位置的左右也置1，那么损失函数为:</li>
</ul>

<p>
\begin{equation}
\begin{aligned}
L(\theta)=&amp;-\sum_{i=1}^{|\Omega|}\left(\log \left(p\left(y_i^{-1} \mid Z_i, \theta\right)\right)+\log \left(p\left(y_i \mid Z_i, \theta\right)\right)\right.\\
&amp;\left.+\log \left(p\left(y_i^{+1} \mid Z_i, \theta\right)\right)\right)
\end{aligned}
\end{equation}
</p>

<h2 id="25-experiments">2.5. Experiments</h2>
<h3 id="251-datasets">2.5.1. Datasets</h3>
<p>作者在两个ASAG主流数据集上进行评估，分别是SemEval-2013和Mohler数据集</p>
<center><img src="../assets/img/posts/20221010/16.jpg" /></center>

<ol>
  <li>SemEval-2013: 作者使用SemEval-2013中的SciEntsBank语料，SciEntsBank语料包含15个不同科学领域的197个问题和10000个答案，这个语料库是ASAG分类任务的一个benchmark，他包含三种分类类别，分别是two-way(Correct and Incorrect)，three-way (Correct, Contradictory, and Incorrect)，five-way (Correct, Partially correct, Contradictory, Irrelevant, and Non-domain)，为了提供多方面的evaluation，测试数据集分为了三个子集:
    <ul>
      <li>Unseen Answers(UA): 和训练集有相同的题目和参考答案，但是学生的回答不同</li>
      <li>Unseen Questions(UQ): 和训练集的问题不同，但是属于同一个领域</li>
      <li>Unseen Domains(UD): 和训练集的问题不同，且不属于同一个领域</li>
    </ul>
  </li>
</ol>

<p>对于这个数据集，作者用三个性能度量(accuracy, weighted-F1, macro-average F1)来评估两个子任务(three-way, five-way)</p>

<ol>
  <li>Mohler dataset: 数据集<a href="http://web.eecs.umich.edu/?mihalcea/downloads/ ShortAnswerGrading_v2.0.zip" target="_blank">开源</a>。数据集由Mohler团队从University of North Texas的一门计算机科学课程的两个考试和十个测试收集整理。它包含80个问题和2273个学生的答案，每个答案都由两名老师打分(0-5, integer)，由于是平均而来，所以一共由11种分类结果，Mohler数据集同样是ASAG任务的一个benchmark，作者可以将数据集变成了11个类别的分类数据集。</li>
</ol>

<p>由于数据集只有2273个答案对，太小，所以需要对数据集进行扩充，Kumar等人通过把训练集中正确的学生答案作为额外的参考答案，这样就把数据集的答案-问题对扩充到30000对。作者为了避免过拟合，采取了折中的策略，对每个问题只挑选一个学生的正确答案作为额外的参考答案，这样就把2083个答案对扩充至3300个答案对。针对Mohler数据集，作者采用了12折交叉验证的方法，用来评估的性能度量有Cohen’s kappa coefficient(kappa), Pearson correlation coefficient(Pearson’s r), mean absolute error(MAE), root-mean-square error(RMSE)，Mohler的标签只有11类，作者既可以把它当作了回归任务来评估(就是把分类结果用分数表示)，也可以把它当作分类任务来评估，其中kappa系数是分类任务的性能度量，其他的性能度量都是回归任务的性能度量</p>

<h3 id="252-experimental-settings">2.5.2. Experimental Settings</h3>
<p>BERT采用base版本(12层，768个单元，12个head，110M参数)，LSTM的隐层个数设置为200并且在最后一个时间步返回所有的hidden state，Capsule的卷积核个数设置为400，卷积核大小为3，dynamic route设置为3。在融合层，attention head设置为2，每个头400维参数，dropout参数都设置为0.1，使用adam优化器，学习率设置为2e-5，一个小批量64个输入，训练周期为10</p>

<h3 id="253-ablation-studies">2.5.3. Ablation Studies</h3>
<p>为了分析每一层的作用，从六个角度来做ablation studies:</p>
<ol>
  <li>W/O refinement: 无refienment</li>
  <li>W/O multihead: 无multihead</li>
</ol>

<p>以此类推，得到如下结果:</p>
<center><img src="../assets/img/posts/20221010/17.jpg" /></center>

<h3 id="254-comparison-with-baseline-systems">2.5.4. Comparison With Baseline Systems</h3>
<p>与众多模型进行对比，主要的实验结果如下:</p>
<ul>
  <li>Mohler数据集</li>
</ul>
<center><img src="../assets/img/posts/20221010/18.jpg" /></center>

<ul>
  <li>SemEval-2013:</li>
</ul>
<center><img src="../assets/img/posts/20221010/19.jpg" /></center>

<ul>
  <li>PR曲线:</li>
</ul>
<center><img src="../assets/img/posts/20221010/20.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20221010/21.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20221010/22.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20221010/23.jpg" /></center>

<ul>
  <li>AUC值和PR曲线平衡点:</li>
</ul>
<center><img src="../assets/img/posts/20221010/24.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20221010/25.jpg" /></center>

<h2 id="26-discussions">2.6. Discussions</h2>
<p>从Ablation实验结果可以看出，refinement层提升了模型的在Sem-UQ和Mohler数据集上的精度，说明refinement层提高了BERT模型在相同问题领域的泛化性，同时也可以看出LSTM在refinement层也很重要，可以提取更丰富的全局context信息。Capsule的有无也说明了它可以提取局部context信息，同时发现它的效果比一般的CNN要好。Triple-loss的设计也确实提升了模型的性能</p>

<p>接着简单分析了一下表4(Mohler数据集)与表5(SemEval-2013)的结果，其实就是对比了不同模型的性能度量，说哪个模型更好什么的</p>

<p>然后分析PR曲线的结果，作者认为在大多数情况下，模型的PR曲线远高于其他模型的PR曲线，这与表6中模型在所有图中AUC最大的结论是一致的。除此之外，平衡点的值也更高，说明模型性能最好</p>

<p>模型可以应用于智慧教育系统的两个场景:</p>
<ol>
  <li>ITSs领域，即智慧辅导系统，可以让评估变成自动化的</li>
  <li>MOOC线上平台，可以替代老师的手动评估，快速精准地为大量的free-text answer进行评分</li>
</ol>

<h2 id="27-conclusion">2.7. Conclusion</h2>
<p>作者提出了一种新的BERT-based网络结构来解决ASAG问题，进行了大量的实验，得出了一下的结论:</p>
<ol>
  <li>基于词嵌入的网络，如CNN、LSTM、Capsule无法在小数据集上取得很好的结果</li>
  <li>预训练网络BERT可以很好的适配ASAG任务</li>
  <li>利用LSTM和Capsule网络可以进一步挖掘语义信息</li>
</ol>

<p>模型局限性:</p>
<ol>
  <li>在开放领域的问答中，小数据集训练出来的模型无法取得预期的效果，比如Sem-UD</li>
  <li>目前来说，模型无法消除或者替代学生答案中的大量的代词，作者计划在后续通过BERT模型来消除学生答案中的代词来提升模型的性能</li>
</ol>

<h1 id="3-semantic-facets">3. Semantic Facets</h1>
<p>论文全称为Leveraging Semantic Facets for Automatic Assessment of Short Free Text Answers，接下来将逐段阅读并整理论文</p>

<h2 id="31-abstract">3.1. Abstract</h2>
<p>短文本问答能反映出学生对于知识的掌握情况，由于自然语言的复杂性，简答题的自动评估任务仍具有挑战性。现有的自动评估模型的做法是预测答案的分数来评估学生的答案，他们一般不关心参考答案的语义面，这限制了预测的表现。该篇论文的关注点是短文本答案的不同的语义面(semantic facets)，每个语义面对应着需要掌握的知识。利用带有语义面标注的数据集，作者首先展示了语义面状态与答案质量(一个答案的好坏)的对应关系，然后展示了语义面在自动评估答案质量的重要性。作者接着将工作拓展到不包含语义面的数据集上，证明了作者的工作在自动评估短文本答案方面的有效性，这些工作包括语义面提取、预测语义面状态和使用语义面的特征工程。</p>

<p>论文的贡献有:</p>
<ol>
  <li>论文提出的方法提升了短文本答案评估的SOTA的表现</li>
  <li>论文深入研究短文本答案的语义面组成，让短文本评估模型的可解释性更高</li>
</ol>

<h2 id="32-introduction">3.2. Introduction</h2>
<p>评估学生的答案非常重要，在网上学习中，实现手动评估非常困难，加速了关于自动评估的研究。研究着重于学生的短文本答案，与多选题相比，答案更不被定义且不具备结构化，所以自动评估很困难。此外，为了正确回答问题，一个短文本的回答可能传达了学生对知识的更深层次的思考，并且可能包含多个从属的知识。有了语义面之后，一个更详细的评估方法出现了，可以分析学生答案的不同语义部分，而不是简单的给出答案的分数。</p>

<p>最近的研究基本上都采取了黑盒的模式(black box)，即从一端输入学生的答案和参考答案，另一端直接输出答案的分数，这中间发生了什么我们并不知道。虽然说对于评估系统来说，分数很重要，但是参考答案涉及到的多个知识点与学生答案的匹配情况我们却一概不知。为了提升评估任务的表现，作者将关注点从黑盒转移到分解评估的过程。为了简便，作者将参考答案的知识组成称为语义面(Semantic Facets)，给定一段文本，这段本文的语义面是由文本的短语组成的集合</p>

<p>论文的主要实验和工作有两个，分别在SciEntsBank数据集和Beetle数据集上展开</p>
<ol>
  <li>第一个工作数据集是SciEntsBank，每个问题的语义面都标注好了，学生答案与问题的语义面匹配状态(matching state)也给出了，这样我们就可以得到不同评分等级(correct, incorrect…)的答案的语义面匹配状态的分布情况。有了分布情况后，我们便可以回答以下问题: 1)是否能根据学生答案的语义面匹配状态来确定答案的评分? 2)分布情况对自动评估系统是否有帮助? 除此之外，我们还可以构建模型来通过学生的答案和语义面来预测语义面的匹配状态</li>
  <li>为了泛化第一个工作，第二个工作使用的数据集是Bettle数据集，这个数据集既没有语义面的标注，也没有语义面的匹配状态的标注。作者首先提出了一种从参考答案提取语义面的方法: 利用词汇统计(lexical statistics)和语法信息(syntax information)。接着利用第一个工作中训练好的预测语义面的匹配状态的网络来预测这个数据集的语义面匹配状态，然后再利用工作一中发现的pattern来通过学生答案的语义面匹配状态来获取features(后续用于对答案进行评分，所以这一步就是feature engineering)，最后，利用这些feature来预测答案的评分</li>
</ol>

<p>贡献:</p>
<ol>
  <li>部分程度上打开了自动评估模型的black box</li>
  <li>发现了matching state与不同评分等级的对应关系，即发现了pattern，这对于自动和手动评估都有帮助</li>
  <li>提出了一种从参考答案抽取语义面的方法</li>
</ol>

<h2 id="33-related-works">3.3. Related Works</h2>
<p>该小节介绍了论文的两个相关工作: 1)自动评估系统的不同任务及对应方法 2)量化一对文本的语义相似度的方法</p>

<h3 id="331-automated-response-evaluation">3.3.1. Automated response evaluation</h3>
<p>根据自动评估系统目标的不同进行分类:</p>
<ol>
  <li>为了评估学习者的语言使用能力，许多评估系统从语言和语法使用、内容组织等方面评估写作质量，这样的系统有ETS(educational testing services)、E-Rator 、Coh-metrics、AcaWriter</li>
  <li>评估学生答案时要求学生的答案涵盖特定的知识，只有涵盖了最关键的部分，学生才能获得满分。为了这个目标，许多系统训练了一个预测模型，有运用词重叠(word overlapping)，语义和语法相似等特征的模型，也有预训练模型来做embedding的模型</li>
</ol>

<p>论文工作属于第二类</p>

<h3 id="332-semantic-similarity-measurement">3.3.2. Semantic similarity measurement</h3>
<p>量化两段文本的相似度是自然语言处理的基本步骤，短文本自动评估系统通过文段相似度的测量来量化学生答案和参考答案的相似度。测量方法有term matching(术语匹配)技术和涉及外部知识的语义计算(semantic computation)</p>
<ol>
  <li>term matching技术基于真实文本和预测文本的公共单词，基于term matching的方法有BLUE和Rouge。</li>
  <li>term matching有个明显的短板就是无法处理近义词或者同义词，可以用WordNet来解决。除此之外，一个单词或者短语的意思可以被分解成量化的语义块，这样测量起来才是数字化的(非二进制)，这样的方法有LSA(Latent Semantic Analysis)、Word2vec、GloVe。但即使是Word2vec也无法解决一词多义的问题，所以诞生了单词的动态语义嵌入(即考虑了context)，这样的模型有RNN-based ELMo、Bert等等</li>
</ol>

<h2 id="34-patterns-and-indicative-powers-of-facets-matching-states">3.4. Patterns and indicative powers of facets matching states</h2>
<p>也就是intro里提到的第一个工作，着眼于发现state和response type的pattern，然后利用这个pattern来做预测(通过答案的语义面匹配状态来预测答案的评分)</p>

<h3 id="341-materials-and-methods">3.4.1. Materials and methods</h3>
<h4 id="3411-dataset">3.4.1.1. Dataset</h4>
<p>数据集使用的是SciEntsBank的SemEval2013，数据集大约有10000个学生答案和197个问题，学生答案分为五类(5-ways，见表格1)，训练集和测试集分别有4969和5835个样本，同样地，根据问题的不同分为: UA, UQ, UD。数据集中每一个问题都包含语义面的标注</p>

<h3 id="342-results-and-analysis">3.4.2. Results and analysis</h3>

<h2 id="35-automatic-extraction-of-facets-matching-features-for-better-prediciton">3.5. Automatic Extraction of Facets Matching Features For Better Prediciton</h2>
:ET