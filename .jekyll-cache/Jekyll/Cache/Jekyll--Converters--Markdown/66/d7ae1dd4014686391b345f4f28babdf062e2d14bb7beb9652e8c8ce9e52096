I"<h1 id="总体情况">总体情况</h1>
<ul>
  <li>书籍:Python数据挖掘入门与实践</li>
  <li>github_url:https://github.com/LinXueyuanStdio/PythonDataMining</li>
  <li>配套代码和笔记，很适合迅速上手</li>
  <li>这篇博客主要记录一些比较重要的算法</li>
</ul>

<h1 id="第一章-开始数据挖掘之旅">第一章 开始数据挖掘之旅</h1>
<h2 id="11-亲和性分析">1.1 亲和性分析</h2>
<ul>
  <li>亲和性分析根据样本个体（物体）之间的<strong>相似度</strong>，确定它们关系的亲疏。</li>
  <li>例子：商品推荐。</li>
  <li>我们要找出“如果顾客购买了商品X，那么他们可能愿意购买商品Y”这样的规则。简单粗暴的做法是，找出数据集中所有同时购买的两件商品。找出规则后，还需要判断其优劣，我们挑好的规则用。</li>
  <li>规则的优劣有多种判断标准，常用的有支持度(support)和置信度(confidence)</li>
  <li>支持度：数据集中规则应验的次数，统计起来很简单。有时候，还需要对支持度进行规范化，即再除以规则有效前提下的总数量。</li>
  <li>置信度是衡量规则的准确性如何。</li>
</ul>

<h2 id="12-分类">1.2 分类</h2>
<ul>
  <li>根据特征分出类别</li>
  <li>例子：Iris植物分类数据集，通过四个特征分出三个类别</li>
  <li>特征连续值变成离散值</li>
  <li>OneR算法：它根据已有数据中，具有相同特征值的个体最可能属于哪个类别进行分类。比如对于某一个特征值来说，属于A的类别有80个，属于B的类别有20个，那么对于这个特征值来说，取值为1代表为A类别，错误率有20％。给出所有特征值，找出错误率最小的特征值作为判断标准。</li>
</ul>

<h1 id="第二章-用scikit-learn估计器分类">第二章 用scikit-learn估计器分类</h1>
<h2 id="21-scikit-learn">2.1 scikit-learn</h2>
<p>scikit-learn里面已经封装好很多数据挖掘的算法</p>

<p>现介绍数据挖掘框架的搭建方法：</p>

<ul>
  <li>转换器（Transformer）用于数据预处理，数据转换</li>
  <li>流水线（Pipeline）组合数据挖掘流程，方便再次使用（封装）</li>
  <li>估计器（Estimator）用于分类，聚类，回归分析（各种算法对象）
    <ul>
      <li>所有的估计器都有下面2个函数
        <ul>
          <li>fit() 训练
            <ul>
              <li>用法：estimator.fit(X_train, y_train)，</li>
              <li>estimator = KNeighborsClassifier() 是scikit-learn算法对象</li>
              <li>X_train = dataset.data 是numpy数组</li>
              <li>y_train = dataset.target 是numpy数组</li>
            </ul>
          </li>
          <li>predict() 预测
            <ul>
              <li>用法：estimator.predict(X_test)</li>
              <li>estimator = KNeighborsClassifier() 是scikit-learn算法对象</li>
              <li>X_test = dataset.data 是numpy数组</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="22-邻近算法knn">2.2 邻近算法KNN</h2>
<p>邻近算法，或者说K最邻近（KNN，K-NearestNeighbor）分类算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是K个最近的邻居的意思，说的是每个样本都可以用它最接近的K个邻近值来代表。近邻算法就是将数据集合中每一个记录进行分类的方法。</p>

<p>例子：分类，Ionosphere数据集</p>

<h1 id="第三章-用决策树预测获胜球队">第三章 用决策树预测获胜球队</h1>

<h2 id="31-决策树">3.1 决策树</h2>
<p>例子：预测NBA球队获胜情况</p>

<p>决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。</p>

<p>分类树（决策树）是一种十分常用的分类方法。它是一种监督学习，所谓监督学习就是给定一堆样本，每个样本都有一组属性和一个类别，这些类别是事先确定的，那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类。这样的机器学习就被称之为监督学习。</p>

<p>scikit-learn库实现了分类回归树（Classification and Regression Trees，CART）算法并将其作为生成决策树的默认算法，它支持连续型特征和类别型特征。</p>

<h2 id="32-随机森林">3.2 随机森林</h2>
<p>随机森林指的是利用多棵树对样本进行训练并预测的一种分类器。</p>

<p>在机器学习中，随机森林是一个包含多个决策树的分类器， 并且其输出的类别是由个别树输出的类别的众数而定。</p>

<h1 id="第四章-用亲和性分析方法推荐电影">第四章 用亲和性分析方法推荐电影</h1>
<h2 id="41-亲和性分析">4.1 亲和性分析</h2>
<p>亲和性分析就是分析两个样本之间的疏密关系，常用的算法有Apriori，Apriori算法的一大特点是根据最小支持度生成<strong>频繁项集</strong>（frequent itemest），它只从数据集中频繁出现的商品中选取共同出现的商品组成频繁项集。其他亲和性分析算法有Eclat和频繁项集挖掘算法（FP-growth）。</p>

<h2 id="42-apriori算法">4.2 Apriori算法</h2>
<p>Apriori算法主要有两个阶段，第一个阶段是根据最小支持度生成频繁项集，第二个阶段是根据最小置信度选择规则，返回规则。</p>

<p>本章的例子是电影推荐。</p>

<p>第一个阶段，算法会先生成长度较小的项集，再将这个项集作为超集寻找长度较大的项集。</p>

<p>第二个阶段是从频繁项集中抽取关联规则。把其中几部电影作为前提，另一部电影作为结论。组成如下形式的规则：如果用户喜欢前提中的所有电影，那么他们也会喜欢结论中的电影。</p>

<h1 id="第五章-用转换器抽取特征">第五章 用转换器抽取特征</h1>
<h2 id="51-抽取特征">5.1 抽取特征</h2>
<p>抽取数据集的特征是重要的一步，在之前的学习中我们都获得了数据集的特征，但很多没有处理的文本特征并不是很明显，比如一段文本等等。特征值可以分为连续特征，序数特征，类别型特征。</p>

<h2 id="52-特征选择">5.2 特征选择</h2>
<p>通常特征有很多，但我们只想选择其中一部分。<strong>选用干净的数据，选取更具描述性的特征。</strong>判断特征相关性：书中列举的例子是判断一个人的收入能不能超过五万，利用单变量卡方检验(或者皮尔逊相关系数)判断各个特征的相关性，然后给出了三个最好的特征，分别是年龄，资本收入和资本损失。</p>

<h2 id="53-创建特征">5.3 创建特征</h2>
<p>主成分分析算法（Principal Component Analysis，PCA）的目的是找到能用较少信息描述数据集的特征组合。</p>

<h1 id="第六章-使用朴素贝叶斯进行社会媒体挖掘">第六章 使用朴素贝叶斯进行社会媒体挖掘</h1>
<h2 id="61-消歧">6.1 消歧</h2>
<p>本章我们将处理文本，文本通常被称为无结构格式。文本挖掘的一个难点来自于歧义，比如bank一词多义。本章将探讨区别Twitter消息中Python的意思。</p>

<h2 id="62-文本转换器">6.2 文本转换器</h2>
<p>Python中处理文本的库NLTK(Natural Language Toolkit)。据作者说很好用，可以作自然语言处理。N元语法是指由连续的词组成的子序列。</p>

<h2 id="63-朴素贝叶斯">6.3 朴素贝叶斯</h2>
<p>朴素贝叶斯概率模型是以对贝叶斯统计方法的朴素解释为基础。</p>

<p>贝叶斯定理公式如下：</p>

<p>P(A|B) = $\frac{P(B|A)P(A)}{P(B)}}$</p>
:ET