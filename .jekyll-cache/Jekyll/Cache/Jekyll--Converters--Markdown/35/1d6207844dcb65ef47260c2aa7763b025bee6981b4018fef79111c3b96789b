I"¨(<h2 id="æ–‡çŒ®æ•´ç†">æ–‡çŒ®æ•´ç†</h2>
<h3 id="è¦æ±‚">è¦æ±‚</h3>

<p><img src="../assets/img/posts/20211130/requirements.jpg" /></p>

<h3 id="æœé›†åˆ°ç›¸å…³æ–‡çŒ®æ ‡é¢˜å’Œåœ°å€">æœé›†åˆ°ç›¸å…³æ–‡çŒ®æ ‡é¢˜å’Œåœ°å€</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/2010.05384.pdf">A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies</a></li>
  <li><a href="https://arxiv.org/pdf/2010.09598.pdf">Better Distractions: Transformer-based Distractor Generation and Multiple Choice Question Filtering</a></li>
  <li><a href="https://ojs.aaai.org//index.php/AAAI/article/view/4606">Generating Distractors for Reading Comprehension Questions from Real Examinations</a></li>
  <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6522">Co-attention hierarchical network: Generating coherent long distractors for reading comprehension</a></li>
  <li><a href="https://aclanthology.org/2020.coling-main.189.pdf">Automatic Distractor Generation for Multiple Choice Questions in Standard Tests</a></li>
  <li><a href="https://aclanthology.org/W18-0533.pdf">Distractor Generation for Multiple Choice Questions Using Learning to Rank</a></li>
  <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/16559">Knowledge-Driven Distractor Generation for Cloze-style Multiple Choice Questions</a></li>
</ul>

<h1 id="ç¬¬ä¸€ç¯‡">ç¬¬ä¸€ç¯‡</h1>
<h3 id="title">Title</h3>
<p>A BERT-based Distractor Generation Scheme with Multi-tasking and
Negative Answer Training Strategies</p>
<h3 id="author">Author</h3>
<p>Ho-Lam Chung, Ying-Hong Chan, Yao-Chung Fan</p>
<h3 id="abstract">Abstract</h3>
<p>ç°æœ‰çš„DG<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>å±€é™åœ¨åªèƒ½ç”Ÿæˆä¸€ä¸ªè¯¯å¯¼é€‰é¡¹ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆå¤šä¸ªè¯¯å¯¼é€‰é¡¹ï¼Œæ–‡ç« ä¸­æåˆ°ä»–ä»¬å›¢é˜Ÿç”¨multi-taskingå’Œnegative answer trainingæŠ€å·§æ¥ç”Ÿæˆå¤šä¸ªè¯¯å¯¼é€‰é¡¹ï¼Œæ¨¡å‹ç»“æœè¾¾åˆ°äº†å­¦ç•Œé¡¶å°–ã€‚</p>

<h3 id="introduction">Introduction</h3>
<p>DGæ•ˆæœä¸å¥½ï¼Œæ–‡ç« æå‡ºäº†ä¸¤ä¸ªæå‡çš„ç©ºé—´ï¼š</p>
<ol>
  <li>DGè´¨é‡æå‡ï¼š<br />
 BERTæ¨¡å‹æ¥æå‡è¯¯å¯¼é€‰é¡¹è´¨é‡</li>
  <li>å¤šä¸ªè¯¯å¯¼é€‰é¡¹ç”Ÿæˆï¼š
 è¿ç”¨äº†è¦†ç›–çš„æ–¹æ³•æ¥é€‰æ‹©distractorï¼Œè€Œä¸æ˜¯é€‰æ‹©æ¦‚ç‡æœ€é«˜ä½†æ˜¯è¯­ä¹‰å¾ˆç›¸è¿‘çš„distractor</li>
</ol>

<h2 id="bert-distractor-generation">BERT distractor generation</h2>
<h3 id="1bert-based-distractor-generationbdg">1)BERT-based distractor generation(BDG)</h3>
<p>è¾“å…¥ï¼šæ®µè½Pï¼Œç­”æ¡ˆAï¼Œé—®é¢˜Qï¼Œç”¨Cè¡¨ç¤ºè¿™ä¸‰è€…concatenateåçš„ç»“æœã€‚<br />
BDGæ¨¡å‹æ˜¯ä¸€ä¸ªè‡ªå›å½’æ¨¡å‹ï¼Œåœ¨é¢„æµ‹é˜¶æ®µï¼Œæ¯æ¬¡è¾“å…¥Cå’Œä¸Šä¸€æ¬¡é¢„æµ‹çš„è¯å…ƒï¼ŒBDGè¿­ä»£é¢„æµ‹è¯å…ƒï¼Œç›´åˆ°é¢„æµ‹å‡ºç‰¹æ®Šè¯å…ƒ[S]åœæ­¢ã€‚ä¸‹é¢è¿™å¼ å›¾ç®€å•ä»‹ç»äº†è¿™ä¸ªè¿‡ç¨‹ã€‚</p>

<p><img src="../assets/img/posts/20211130/2.jpg" /></p>

<p>ç½‘ç»œç»“æ„ç®€å•ä»‹ç»ï¼šh<sub>[M]</sub>è¡¨ç¤ºbertè¾“å‡ºçš„éšè—çŠ¶æ€ï¼Œéšè—çŠ¶æ€å†è¾“å…¥åˆ°ä¸€ä¸ªå…¨è¿æ¥å±‚ä¸­ç”¨æ¥é¢„æµ‹è¯å…ƒã€‚</p>

<p><img src="../assets/img/posts/20211130/3.jpg" /></p>

<h3 id="2multi-task-with-parallel-mlm">2)Multi-task with Parallel MLM</h3>
<p>MLMå…¨ç§°masked language modelï¼Œé®è”½è¯­è¨€æ¨¡å‹,é€šè¿‡å¹¶è¡ŒBDGå’ŒP-MLMæ¥è®­ç»ƒæ¨¡å‹è®©æ¨¡å‹æœ‰æ›´å¥½çš„æ•ˆæœã€‚</p>

<p><img src="../assets/img/posts/20211130/4.jpg" /></p>

<p>ä¸Šå›¾ä¸­å·¦è¾¹çš„sequential MLMå°±æ˜¯ä¹‹å‰æåˆ°çš„BDGï¼ŒBDGæ¨¡å‹æ˜¯ä¸€ä¸ªè¯æ¥ä¸€ä¸ªè¯çš„é¢„æµ‹ï¼ŒP-MLMæ˜¯å¯¹æ‰€æœ‰çš„masked tokenè¿›è¡Œé¢„æµ‹ï¼Œæœ€åçš„æŸå¤±å‡½æ•°æ˜¯è¿™ä¸¤è€…ç›¸åŠ <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>ï¼Œå…¬å¼å¦‚ä¸‹ï¼š</p>

<p><img src="../assets/img/posts/20211130/5.jpg" /></p>

<p><img src="../assets/img/posts/20211130/6.jpg" /></p>

<p><img src="../assets/img/posts/20211130/7.jpg" /></p>

<p>ä½œè€…å¦‚æ­¤è®¾è®¡çš„æ€è·¯æ˜¯ï¼šBDGå¯èƒ½ä¼šå¿½ç•¥æ•´ä½“è¯­ä¹‰è¯­ä¹‰ä¿¡æ¯ï¼Œä½†æ˜¯ä¼šè¿‡æ‹Ÿåˆå•ä¸ªè¯é¢„æµ‹ã€‚é‚£ä¹ˆå¹¶è¡Œä¸€ä¸ªP-MLMå¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚</p>

<h3 id="3answer-negative-regularization">3)Answer Negative Regularization</h3>
<p>ç›®å‰æœºå™¨é¢„æµ‹çš„distractorå’Œansweræœ‰å¾ˆé«˜çš„ç›¸ä¼¼åº¦ï¼Œä¸‹é¢ä¸€å¼ è¡¨å¯ä»¥å±•ç¤ºç›¸ä¼¼åº¦ã€‚å…¶ä¸­PMè¡¨ç¤ºæœºå™¨ï¼ŒGoldè¡¨ç¤ºäººå·¥ï¼Œä½œè€…å°†è¿™ç±»é—®é¢˜ç§°ä¸ºanswer copying problemã€‚</p>

<p><img src="../assets/img/posts/20211130/8.jpg" /></p>

<p>ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†answer negative lossæ¥è®©æœºå™¨æ›´å¤šçš„é€‰æ‹©ä¸answerä¸åŒçš„è¯æ¥è¡¨ç¤ºæ–°çš„distractorï¼Œå…¬å¼å¦‚ä¸‹ï¼š</p>

<p><img src="../assets/img/posts/20211130/9.jpg" /></p>

<p>å¯ä»¥çœ‹å‡ºBDGçš„lossæ›¿æ¢æˆäº†ANçš„lossï¼Œæ¯ä¸€é¡¹éƒ½å‡å»äº†Answer negative lossã€‚</p>

<h2 id="multiple-distractor-generation">Multiple Distractor Generation</h2>
<h3 id="1selecting-distractors-by-entropy-maximization">1)Selecting Distractors by Entropy Maximization</h3>
<p>é€‰æ‹©è¯­ä¹‰ä¸åŒçš„distractor setã€‚æ–‡ç« å€Ÿé‰´äº†MRC<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>çš„æ–¹æ³•ï¼Œè®©BDGmodelç”Ÿæˆå¾ˆå¤šdistractorç»„æˆ $\hat{D}$ = {$\hat{d}$<sub>1</sub>, $\hat{d}$<sub>2</sub>, $\hat{d}$<sub>3</sub>â€¦}ï¼Œç„¶åæ‰¾å‡ºæœ€å¥½çš„ä¸€ç»„é€‰é¡¹ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ç”±ä¸‰ä¸ªè¯¯å¯¼é€‰é¡¹å’Œä¸€ä¸ªç­”æ¡ˆç»„æˆã€‚é€‰æ‹©çš„ä¸€å¥æ˜¯æœ€å¤§åŒ–ä¸‹é¢è¿™ä¸ªå…¬å¼ï¼š</p>

<p><img src="../assets/img/posts/20211130/10.jpg" /></p>

<h3 id="2bdg-em">2)BDG-EM</h3>
<p>æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸åŒçš„BDGæ¨¡å‹æ¥ç”Ÿæˆä¸åŒçš„è¯¯å¯¼é€‰é¡¹æœ€åç»„åˆï¼Œä¸åŒçš„æ¨¡å‹åŒºåˆ«æ˜¯æœ‰æ²¡æœ‰answer negative/multi-task trainingï¼Œæ¯”å¦‚æˆ‘ä»¬æœ‰è¿™å‡ ä¸ªæ¨¡å‹:$\hat{D}$,$\hat{D}$<sub>PM</sub>,$\hat{D}$<sub>PM+AN</sub>ï¼Œå®ƒä»¬åˆ†åˆ«ä»£è¡¨å«PM<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>å’Œå«AN<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup></p>

<p><img src="../assets/img/posts/20211130/11.jpg" /></p>

<h2 id="performance-evaluation">Performance Evaluation</h2>
<h3 id="1datasets">1)datasets</h3>
<p>RACE,æ²¿ç”¨äº†<a href="https://ojs.aaai.org//index.php/AAAI/article/view/4606">Gao</a>é‚£ç¯‡è®ºæ–‡çš„å¤„ç†,åé¢ä¹Ÿä¼šæ¢³ç†é‚£ç¯‡è®ºæ–‡</p>

<p><img src="../assets/img/posts/20211130/12.jpg" /></p>

<h3 id="2implementation-details">2)implementation details</h3>
<ul>
  <li>tokenizer: wordpiece tokenizer</li>
  <li>framewordk:huggingface trainsformers</li>
  <li>optimizer:adamW(lr:5e-5)</li>
  <li>github_url: <a href="https://github.com/voidful/BDG">BDG</a></li>
</ul>

<h3 id="3compared-methods">3)compared methods</h3>
<p>æ¯”è¾ƒäº†ä¸åŒçš„distractor generation</p>
<ul>
  <li>CO-Attï¼šå‡ºè‡ª<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6522">Zhou</a></li>
  <li>DS-Att: å‡ºè‡ª<a href="https://ojs.aaai.org//index.php/AAAI/article/view/4606">Gao</a></li>
  <li>GPT:baseline</li>
  <li>BDG: æ²¡æœ‰åº”ç”¨P-MLMå’ŒAnswer negative</li>
  <li>BDG<sub>PM</sub></li>
  <li>BDG<sub>AN+PM</sub></li>
</ul>

<h3 id="4token-score-comparison">4)token score comparison</h3>
<p>BLEUå’ŒROUGE(L)ä¸¤ç§åˆ¤æ–­æŒ‡æ ‡</p>

<p><img src="../assets/img/posts/20211130/13.jpg" /></p>

<p>copying problemçš„æ•ˆæœ</p>

<p><img src="../assets/img/posts/20211130/14.jpg" /></p>

<h3 id="5mcq-model-accuracy-comparison">5)MCQ Model Accuracy Comparison</h3>
<p>ä¸å›ç­”ç³»ç»Ÿç›¸ç»“åˆï¼Œå°†ç”Ÿæˆå¥½çš„é€‰é¡¹ï¼ˆä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆä¸‰ä¸ªè¯¯å¯¼é€‰é¡¹ï¼‰æ”¾å…¥MCQ answering modelï¼Œä¸‹é¢æ˜¯å›ç­”æ­£ç¡®ç‡çš„è¡¨æ ¼</p>

<p><img src="../assets/img/posts/20211130/15.jpg" /></p>

<p>å¯ä»¥çœ‹å‡ºä½œè€…çš„æ¨¡å‹é€‰é¡¹çš„è¯¯å¯¼æ€§è¿˜æ˜¯å¾ˆé«˜çš„ã€‚</p>

<h3 id="6parameter-study-on-Î³">6ï¼‰Parameter Study on Î³</h3>
<p>ä¹‹å‰ä½¿ç”¨P-MLMå¹¶è¡Œè®­ç»ƒæ—¶å€™æœ‰ä¸ªæƒé‡å‚æ•°Î³ï¼Œä¸‹è¡¨æ˜¾ç¤ºäº†ä¸åŒÎ³å€¼çš„å½±å“ï¼Œå¯¹äºåªæœ‰PMçš„æ¨¡å‹æ¥è¯´ï¼ŒÎ³=6ï¼Œå¯¹äºæ—¢æœ‰ANå’ŒPMæ¥è¯´ï¼ŒÎ³=7</p>

<p><img src="../assets/img/posts/20211130/16.jpg" /></p>

<h2 id="conclusion">Conclusion</h2>
<p>ç°å­˜çš„DGå¯ä»¥åˆ†ä¸ºcloze-style distractor generationå’Œ reading comprehension distractor generationï¼Œå‰è€…ä¸»è¦æ˜¯word fillingï¼Œåè€…ä¸»è¦çœ‹é‡è¯­ä¹‰ä¿¡æ¯ï¼ŒåŸºäºä¸¤è€…çš„è®¾è®¡å‡ºäº†å¾ˆå¤šæ¨¡å‹ï¼Œç›®å‰æ¥çœ‹è¿˜æ˜¯è€ƒè™‘è¯­ä¹‰ä¿¡æ¯ç”Ÿæˆçš„è¯¯å¯¼é€‰é¡¹æ›´å¥½ã€‚</p>

<p><img src="../assets/img/posts/20211130/17.jpg" /></p>

<h2 id="æˆ‘çš„çœ‹æ³•">æˆ‘çš„çœ‹æ³•</h2>
<p>æ–‡ç« ä¸­çš„æ¨¡å‹æåˆ°äº†ä¸‰ç§æŠ€æœ¯ï¼Œç¬¬ä¸€æ˜¯berté¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨ã€‚ç¬¬äºŒæ˜¯P-MLMçš„å¹¶è¡Œä½¿ç”¨ï¼Œ å®ƒçš„ä½¿ç”¨è®©æ¨¡å‹å¯ä»¥è€ƒè™‘æ®µè½çš„è¯­ä¹‰ä¿¡æ¯ï¼Œé‚£ä¹ˆç”Ÿæˆçš„è¯¯å¯¼é€‰é¡¹æ˜¯sentence-levelè€Œä¸æ˜¯ä¹‹å‰æ¨¡å‹æ‰€ä½¿ç”¨çš„ç±»ä¼¼word-fillingè¿™ç§word-levelã€‚ç¬¬ä¸‰æ˜¯Answer negative lossçš„ä½¿ç”¨ï¼Œå®ƒçš„ä½¿ç”¨ç›¸å½“äºè®©æ¨¡å‹ä¸è¦è€ƒè™‘ä¸æ­£ç¡®ç­”æ¡ˆè¯­ä¹‰å¾ˆæ¥è¿‘çš„è¯¯å¯¼é€‰é¡¹ï¼Œå› ä¸ºç›®å‰å¤§å¤šæ•°DGç”Ÿæˆå¤šä¸ªé€‰é¡¹æ—¶è¯­ä¹‰ä¸æ­£ç¡®ç­”æ¡ˆéƒ½éå¸¸æ¥è¿‘ï¼Œè¿™ä¸å®é™…æƒ…å†µä¸ç¬¦ï¼ŒåŒæ—¶ä¹Ÿèµ·ä¸åˆ°è¯¯å¯¼çš„ä½œç”¨ã€‚  <br />
åŒæ—¶æ–‡ç« æå‡ºäº†ç”Ÿæˆå¤šä¸ªè¯¯å¯¼é€‰é¡¹æ—¶ä½¿ç”¨ä¸åŒæ¨¡å‹ç”Ÿæˆçš„è¯¯å¯¼é€‰é¡¹æ‹¼åœ¨ä¸€èµ·ä½œä¸ºé€‰é¡¹æ˜¯ä¸€ç§æ¯”è¾ƒå¥½çš„è§£å†³æ–¹æ³•ï¼Œè®©ä¸€æ¬¡æ€§ç”Ÿæˆå¤šä¸ªè¯¯å¯¼é€‰å‹æœ‰äº†ä¸€å®šçš„å¯ç”¨æ€§ã€‚<br />
æ–‡ç« çš„ä»£ç å¼€æºï¼Œå¯ä»¥å»<a href="https://github.com/voidful/BDG">github</a>ä¸Šçœ‹è®­ç»ƒç»†èŠ‚å’Œç½‘ç»œç»“æ„ç»†èŠ‚ã€‚</p>

<h1 id="ç¬¬äºŒç¯‡">ç¬¬äºŒç¯‡</h1>
<h3 id="title-1">Title</h3>
<p>Better Distractions: Transformer-based Distractor Generation and Multiple Choice Question Filtering</p>
<h3 id="author-1">Author</h3>
<p>Jeroen Offerijns, Suzan Verberne, Tessa Verhoef</p>
<h3 id="abstract-1">Abstract</h3>

<h3 id="introduction-1">Introduction</h3>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>distractor generation è¯¯å¯¼é€‰é¡¹ç”Ÿæˆï¼Œç®€ç§°DGÂ <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>å½“æˆ‘ä»¬testæ—¶ï¼Œåªéœ€è¦Sequential MLM decoderæ¥é¢„æµ‹ã€‚Â <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>multi-choice reading comprehension (MRC) modelÂ <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>P-MLMÂ <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Answer negativeÂ <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET