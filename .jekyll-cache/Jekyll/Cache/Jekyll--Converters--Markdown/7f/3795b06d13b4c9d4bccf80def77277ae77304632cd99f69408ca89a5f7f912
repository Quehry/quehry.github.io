I"<h1 id="vae">VAE</h1>
<h2 id="ae">AE</h2>
<p>Auto-Encoder自动编码器，比如Seq2seq模型。</p>

<h2 id="vaevariational-auto-encoder">VAE(Variational Auto-Encoder)</h2>
<p>在实际情况中，我们需要在模型的准确率上与隐含向量服从标准正态分布之间做一个权衡，所谓模型的准确率就是指解码器生成的图片与原图片的相似程度。我们可以让网络自己来做这个决定，非常简单，我们只需要将这两者都做一个loss，然后在将他们求和作为总的loss，这样网络就能够自己选择如何才能够使得这个总的loss下降。另外我们要衡量两种分布的相似程度，如何看过之前一片GAN的数学推导，你就知道会有一个东西叫KL-divergence来衡量两种分布的相似程度，这里我们就是用KL-divergence来表示隐含向量与标准正态分布之间差异的loss，另外一个loss仍然使用生成图片与原图片的均方误差来表示。</p>

<h2 id="kl消失">KL消失</h2>
<p>KL消失后，VAE就变成了AE
原因：</p>
<ul>
  <li>KL项本身太容易被优化</li>
  <li>一旦崩塌，Decoder会忽视Z<sub>x</sub></li>
  <li>Z<sub>x</sub>的表示学习依赖于Decoder</li>
</ul>

<h2 id="解决kl消失的思路">解决KL消失的思路</h2>
<p>…</p>

<h1 id="analyze-pretraining-language-model">Analyze Pretraining Language Model</h1>
<h2 id="perspective-of-knowledge">Perspective of knowledge</h2>
<ul>
  <li>Syntacitic/Semantic/lexical 句法，语义，词汇</li>
  <li>重构语法树</li>
  <li>Attention中很多头可能没有用，学到了很多冗余的信息</li>
  <li>Analyze Feed Forward Neural Network</li>
  <li>浅层词汇信息，深层语义信息</li>
  <li>Prompt</li>
</ul>

:ET