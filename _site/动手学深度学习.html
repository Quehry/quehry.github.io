<!DOCTYPE html>
<html lang="en">
  




<head>
	<meta charset="utf-8">
	<title>动手学深度学习 - Quehry</title>
	<link rel="canonical" href="http://localhost:4000/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html">
	<meta name="description" content="d2l note">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:4000"},
  "headline": "动手学深度学习",
  "abstract": "d2l note",
    "keywords": "note",
    "wordcount": "423",
    "image": ["http://localhost:4000/assets/imgposts/20220905/1.jpg"],
  "datePublished": "2022-09-05 00:00:00 +0800",
  "dateModified": "2022-09-05 00:00:00 +0800",
  "author": {
    "@type": "Person",
    "name": "Quehry"},
  "publisher": {
    "@type":  "Organization",
    "logo": {
        "@type": "ImageObject",
        "encodingFormat": "image/png",
        "contentUrl": "http://localhost:4000/assets/img/branding/MVM-symbol-black.png",
        "url": "http://localhost:4000/assets/img/branding/MVM-symbol-black.png"},
    "name" : "Quehry"}
}
</script>
<!-- Open Graph data -->
<meta property="og:url" content="http://localhost:4000/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html"/>
<meta property="og:type" content="article"/>
<meta property="og:title" content="动手学深度学习"/>
<meta property="og:description" content="d2l note"/>
<meta property="og:image" content="http://localhost:4000/assets/imgposts/20220905/1.jpg"/>
<meta property="og:image:alt" content="动手学深度学习"/>
<meta property="og:site_name" content="Quehry" />
<meta property="article:published_time" content="2022-09-05 00:00:00 +0800" />
<meta property="article:modified_time" content="2022-09-05 00:00:00 +0800" />
<meta property="article:tag" content="note" />
<meta property="fb:admins" content="ar.maybach" />
<!-- Schema.org markup for Google -->
<meta itemprop="name" content="动手学深度学习">
<meta itemprop="description" content="d2l note">
<meta itemprop="image" content="http://localhost:4000/assets/imgposts/20220905/1.jpg">
<!-- Twitter Card data -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@QuehryS">
<meta name="twitter:title" content="动手学深度学习">
<meta name="twitter:description" content="d2l note">
<meta name="twitter:creator" content="">
<meta data-rh="true" name="twitter:label1" content="Word count"/>
<meta data-rh="true" name="twitter:data1" content="423"/>
<meta name="twitter:image:src" content="http://localhost:4000/assets/img/posts/20220905/1.jpg">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#311e3e">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#311e3e">
	<!-- Google Fonts -->
	<link rel="preconnect" href="https://fonts.gstatic.com" />
	<style>
/* latin */
@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 400;
  src: url(https://fonts.gstatic.com/s/lora/v17/0QIvMX1D_JOuMwr7I_FMl_E.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin */
@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 600;
  src: url(https://fonts.gstatic.com/s/lora/v17/0QIvMX1D_JOuMwr7I_FMl_E.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin */
@font-face {
  font-family: 'Source Sans Pro';
  font-style: normal;
  font-weight: 200;
  src: url(https://fonts.gstatic.com/s/sourcesanspro/v14/6xKydSBYKcSV-LCoeQqfX1RYOo3i94_wlxdu3cOWxw.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin */
@font-face {
  font-family: 'Source Sans Pro';
  font-style: normal;
  font-weight: 400;
  src: url(https://fonts.gstatic.com/s/sourcesanspro/v14/6xK3dSBYKcSV-LCoeQqfX1RYOo3qOK7lujVj9w.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin */
@font-face {
  font-family: 'Source Sans Pro';
  font-style: normal;
  font-weight: 700;
  src: url(https://fonts.gstatic.com/s/sourcesanspro/v14/6xKydSBYKcSV-LCoeQqfX1RYOo3ig4vwlxdu3cOWxw.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
	</style>
	<!-- <link href="https://fonts.googleapis.com/css?family=Lora:400,600|Source+Sans+Pro:200,400,700" rel="stylesheet"> -->
	<!-- Font Awesome -->
	<link rel="stylesheet" href="./assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="./assets/css/main.css">
	




<link rel="icon" href="./assets/img/favicon/favicon.ico" type="image/x-icon">
<link rel="apple-touch-icon" href="./assets/img/favicon/favicon.ico">
<link rel="apple-touch-icon" sizes="72x72" href="./assets/img/favicon/favicon.ico">
<link rel="apple-touch-icon" sizes="114x114" href="./assets/img/favicon/favicon.ico">
	
<head>
  <body>
    




<section class="hidden">
  <div class="post">
      <a  class="post-list-title" href="./%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html">动手学深度学习</a>
      

  <span class = "post-card-meta">
  
  
    <span class="meta-pre"></span>
  
  
    
      
      <span class="page_meta-date">
        <time datetime="2022-09-05T00:00:00+08:00">September 5, 2022</time>
      </span>
    
    
      <span class="meta-sep"></span>
    
  
  
    
    
    <span class="page_meta-readtime">
      
        2 minute read
      
    </span>
  
  
  </span>

        <div class="post-excerpt">
            <!-- TOC --><ul> <li><a href="#0-简介">0. 简介</a></li> <li><a href="#1-预备知识">1. 预备知识</a></li> <li><a href="#2-线性神经网络">2. 线性神经网络</a></li> <li><a href="#3-多层感知机">3. 多层感知机</a></li> <li><a href="#5-卷积神经网络">5. 卷积神经网络</a></li> <li><a href="#6-现代卷积神经网络">6. 现代卷积神经网络</a></li> <li><a href="#7-循环神经网络">7. 循环神经网络</a></li> <li><a href="#8-现代循环神经网络">8. 现代循环神经网络</a></li> <li><a href="#9-注意力机制">9. 注意力机制</a></li> <li><a href="#13-自然语言处理-预训练">13. 自然语言处理: 预训练</a></li> <li><a href="#14-自然语言处理-应用">14. 自然语言处理: 应用</a></li></ul><!-- /TOC --><h1 id="0-简介">0. 简介</h1><ul> <li>《动手学深度学习》的笔记</li> <li>各种链接: <ul> <li><a href="https://space.bilibili.com/1567748478" target="_blank">bilibili</a></li> <li><a href="https://zh-v2.d2l.ai/index.html" target="_blank">book_zh</a></li> <li><a href="https://d2l.ai/index.html" target="_blank">book_en</a></li> </ul> </li> <li>这本书笼统的介绍了深度学习所需要的各种知识，从线性神经网络开始讲起，然后到CNN，最后到RNN，介绍了CV和NLP领域的较新的网络结构。同时这本书不止有理论内容，每一小节都有代码实践内容，可以边写代码边了解知识，同时bilibili上有李沐老师的网课配合学习，很适合初学者进行学习。</li></ul><h1 id="1-预备知识">1. 预备知识</h1><p>这一章主要介绍了深度学习的一些前置知识，这里对比较重要的点做备注</p><ul> <li>张量(Tensor)包含了一维张量(向量)和二维张量(矩阵)</li> <li>torch中A*B是哈达玛积，表示矩阵元素按元素相乘</li> <li>torch.dot()是点积</li> <li>torch.cat(…, dim=0)表示在行上延伸，比如(3, 4)和(3, 4)变成(6, 4)</li> <li>A.sum(axis=0)表示把每一列的数据都相加，比如(5, 4)变成(4)</li> <li>范数是norm，L1范数为每个元素的绝对值相加，L2范数为元素的平方和开根号，torch中默认L2范数，一般也是L2范数用的最多</li> <li>梯度: 连接多元函数的所有偏导数:</li></ul><center><img src="../assets/img/posts/20220905/2.jpg" /></center><ul> <li>梯度是一个向量</li> <li>常用的梯度计算公式:</li></ul><center><img src="../assets/img/posts/20220905/3.jpg" /></center><ul> <li>torch中自动求导的步骤: <ul> <li>第一步 为x分配内存空间: x.requires_grad_(True)</li> <li>第二步 链式反向传播，希望求哪个函数的梯度，就对那个函数反向传播，比如y.backward()</li> <li>第三步 求x的梯度，x.grad，如果我们需要重新求梯度，需要清零梯度，x.grad.zero_()</li> <li>注意torch中只能对标量输出求梯度，所以常见操作是sum</li> </ul> </li> <li>标量对向量的偏导是向量，向量对向量的偏导是矩阵</li> <li>贝叶斯公式: P(A|B)P(B)=P(B|A)P(A)</li></ul><h1 id="2-线性神经网络">2. 线性神经网络</h1><p>本章主要介绍了线性回归网络和softmax回归网络，接下来是一些笔记</p><ul> <li>随机梯度下降和梯度下降的区别: 梯度下降一般而言是针对所有的样本而言，而随机梯度下降是针对单个样本而言，同样地，小批量随机梯度下降是针对一个批量的样本而言</li> <li>可以调整但是在训练过程中不更新的参数叫做超参数</li> <li>极大似然法: $\theta$是需要估计的值，在写似然函数时只需要把$\theta$看成参数，最大化似然函数即$\theta$的估计值</li>...<a class="read-more" href="./%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html"> read more</a>
        </div>
  </div>
</section>
<div class="flex-container transparent">
  




<header class="main-header">
  <div class="wrapper">
    <div class="header-flex">
      <div class="menu-icon-container">
        <span class="menu-icon"><i class="fa fa-bars" aria-hidden="true"></i></span>
      </div>
      <nav class="main-nav">
        <span class="menu-icon-close"><i class="fa fa-times" aria-hidden="true"></i></span>
        <ul>
          <li>
            <div class="theme-toggle night">
    <input class="night" type="checkbox" id="theme-switch">
    <label class="night" for="theme-switch">
        <div class="toggle night"></div>
        <div class="names night">             
        <p class="light night"><svg class="night" width="20" viewBox="0 0 25 20" fill="currentColor" xmlns="http://www.w3.org/2000/svg">
            <path class="night" d="M12.5 2.49871C11.3401 2.50016 10.2282 2.96156 9.40801 3.78171C8.58785 4.60187 8.12645 5.71383 8.125 6.87371C8.125 7.03947 8.19085 7.19844 8.30806 7.31565C8.42527 7.43286 8.58424 7.49871 8.75 7.49871C8.91576 7.49871 9.07473 7.43286 9.19194 7.31565C9.30915 7.19844 9.375 7.03947 9.375 6.87371C9.37593 6.04519 9.70547 5.25088 10.2913 4.66503C10.8772 4.07918 11.6715 3.74964 12.5 3.74871C12.6658 3.74871 12.8247 3.68286 12.9419 3.56565C13.0592 3.44844 13.125 3.28947 13.125 3.12371C13.125 2.95795 13.0592 2.79898 12.9419 2.68177C12.8247 2.56456 12.6658 2.49871 12.5 2.49871V2.49871ZM12.5 -0.00129131C8.47891 -0.00129131 5.62031 3.26238 5.625 6.88269C5.62487 8.54403 6.22974 10.1486 7.32656 11.3964C8.32891 12.5378 9.29062 14.4007 9.375 14.9987L9.37734 17.9358C9.37744 18.0587 9.41402 18.1787 9.48242 18.2807L10.4395 19.7198C10.4964 19.8055 10.5737 19.8758 10.6644 19.9245C10.7551 19.9731 10.8564 19.9986 10.9594 19.9987H14.0395C14.1426 19.9988 14.2441 19.9734 14.3351 19.9247C14.426 19.8761 14.5034 19.8057 14.5605 19.7198L15.5176 18.28C15.5854 18.1776 15.6219 18.0578 15.6227 17.935L15.625 14.9987C15.7129 14.3846 16.6797 12.5303 17.6734 11.3964C18.5434 10.4028 19.1087 9.17963 19.3015 7.87318C19.4944 6.56673 19.3066 5.23238 18.7608 4.02985C18.215 2.82732 17.3342 1.80757 16.2238 1.09264C15.1135 0.377721 13.8206 -0.00207746 12.5 -0.00129131V-0.00129131ZM14.3727 17.7452L13.7047 18.7487H11.2937L10.6273 17.7452V17.4987H14.3738L14.3727 17.7452ZM14.375 16.2487H10.625L10.6227 14.9987H14.375V16.2487ZM16.7348 10.5725C16.1879 11.1956 15.316 12.4511 14.7594 13.7479H10.243C9.68516 12.4507 8.81328 11.1956 8.26641 10.5725C7.36971 9.5491 6.87599 8.2344 6.87734 6.87371C6.87031 3.8659 9.23594 1.24871 12.5 1.24871C15.602 1.24871 18.125 3.77176 18.125 6.87371C18.1249 8.23456 17.6305 9.54904 16.7336 10.5725H16.7348ZM3.75 6.87371C3.75 6.70795 3.68415 6.54898 3.56694 6.43177C3.44973 6.31456 3.29076 6.24871 3.125 6.24871H0.625C0.45924 6.24871 0.300269 6.31456 0.183058 6.43177C0.065848 6.54898 0 6.70795 0 6.87371C0 7.03947 0.065848 7.19844 0.183058 7.31565C0.300269 7.43286 0.45924 7.49871 0.625 7.49871H3.125C3.29076 7.49871 3.44973 7.43286 3.56694 7.31565C3.68415 7.19844 3.75 7.03947 3.75 6.87371ZM20.625 2.49871C20.7221 2.49849 20.8178 2.4759 20.9047 2.43269L23.4047 1.18269C23.5529 1.10852 23.6657 0.978483 23.718 0.821201C23.7704 0.66392 23.7582 0.492273 23.684 0.344021C23.6473 0.270614 23.5964 0.205161 23.5344 0.151397C23.4724 0.0976336 23.4004 0.0566132 23.3225 0.0306781C23.1652 -0.0217002 22.9936 -0.00945342 22.8453 0.0647243L20.3453 1.31472C20.2194 1.37771 20.1184 1.48136 20.0588 1.60889C19.9991 1.73643 19.9843 1.88037 20.0166 2.0174C20.049 2.15442 20.1267 2.2765 20.2371 2.36386C20.3475 2.45122 20.4842 2.49873 20.625 2.49871ZM24.375 6.24871H21.875C21.7092 6.24871 21.5503 6.31456 21.4331 6.43177C21.3158 6.54898 21.25 6.70795 21.25 6.87371C21.25 7.03947 21.3158 7.19844 21.4331 7.31565C21.5503 7.43286 21.7092 7.49871 21.875 7.49871H24.375C24.5408 7.49871 24.6997 7.43286 24.8169 7.31565C24.9342 7.19844 25 7.03947 25 6.87371C25 6.70795 24.9342 6.54898 24.8169 6.43177C24.6997 6.31456 24.5408 6.24871 24.375 6.24871ZM4.65469 1.31472L2.15469 0.0647243C2.08128 0.0279952 2.00136 0.00608435 1.91948 0.00024269C1.83761 -0.00559897 1.75539 0.004743 1.67751 0.0306781C1.52023 0.0830564 1.39019 0.195769 1.31602 0.344021C1.24184 0.492273 1.22959 0.66392 1.28197 0.821201C1.33435 0.978483 1.44706 1.10852 1.59531 1.18269L4.09531 2.43269C4.18223 2.4759 4.27794 2.49849 4.375 2.49871C4.5158 2.49873 4.65248 2.45122 4.7629 2.36386C4.87332 2.2765 4.951 2.15442 4.98337 2.0174C5.01574 1.88037 5.0009 1.73643 4.94124 1.60889C4.88158 1.48136 4.78061 1.37771 4.65469 1.31472ZM23.4047 12.5647L20.9047 11.3147C20.7564 11.2405 20.5847 11.2283 20.4274 11.2807C20.2701 11.3332 20.14 11.4459 20.0658 11.5942C19.9916 11.7425 19.9794 11.9142 20.0318 12.0715C20.0842 12.2289 20.197 12.3589 20.3453 12.4331L22.8453 13.6831C22.9936 13.7573 23.1653 13.7695 23.3226 13.7171C23.4799 13.6647 23.61 13.5519 23.6842 13.4036C23.7584 13.2553 23.7706 13.0836 23.7182 12.9263C23.6658 12.769 23.553 12.6389 23.4047 12.5647V12.5647ZM4.375 11.2487C4.27794 11.2489 4.18223 11.2715 4.09531 11.3147L1.59531 12.5647C1.44701 12.6389 1.33425 12.769 1.28183 12.9263C1.25588 13.0042 1.24552 13.0864 1.25135 13.1683C1.25719 13.2502 1.27909 13.3302 1.31582 13.4036C1.35255 13.477 1.40338 13.5425 1.46542 13.5963C1.52745 13.6501 1.59947 13.6911 1.67737 13.7171C1.83469 13.7695 2.00638 13.7573 2.15469 13.6831L4.65469 12.4331C4.78083 12.3702 4.88202 12.2666 4.94183 12.1389C5.00164 12.0113 5.01656 11.8672 4.98417 11.7301C4.95178 11.5929 4.87397 11.4707 4.76339 11.3833C4.65281 11.2959 4.51594 11.2485 4.375 11.2487V11.2487Z" /></svg></p>
        <p class="dark night"><svg class="night" width="20" viewBox="0 0 25 21" fill="currentColor" xmlns="http://www.w3.org/2000/svg">
            <path class="night" d="M6.39614 3.72646C7.50591 1.56178 9.72622 0.00900831 12.4782 0.00114817C13.8006 -0.00388798 15.0965 0.375153 16.2101 1.09278C17.3237 1.8104 18.2079 2.83612 18.7564 4.04682C19.3049 5.25751 19.4945 6.60175 19.3024 7.91818C19.1103 9.23461 18.5447 10.4673 17.6735 11.4683C17.5227 11.6416 17.3516 11.859 17.1739 12.1069L9.47856 6.12184C9.65443 5.45016 10.046 4.85578 10.5924 4.43118C11.1387 4.00657 11.8093 3.77554 12.4997 3.77401C12.6654 3.77401 12.8244 3.70776 12.9416 3.58984C13.0588 3.47191 13.1247 3.31197 13.1247 3.1452C13.1247 2.97843 13.0588 2.81849 12.9416 2.70057C12.8244 2.58264 12.6654 2.51639 12.4997 2.51639C11.6212 2.5173 10.7634 2.78383 10.0374 3.28141C9.31146 3.77899 8.75092 4.48463 8.42856 5.30674L6.39614 3.72646ZM6.39614 10.0841C6.64968 10.5817 6.96225 11.0465 7.327 11.4683C7.97231 12.2091 8.98169 13.7568 9.36645 15.0624C9.36645 15.0726 9.36919 15.0828 9.37075 15.093H12.8372L6.39614 10.0841ZM9.37466 16.3502V17.8574C9.37584 18.1045 9.44934 18.3458 9.58599 18.5511L10.2536 19.5607C10.3675 19.7335 10.5221 19.8753 10.7037 19.9734C10.8852 20.0715 11.0881 20.1229 11.2942 20.1231H13.7047C13.9107 20.1231 14.1135 20.0718 14.2951 19.9739C14.4766 19.876 14.6313 19.7345 14.7454 19.5619L15.4129 18.5511C15.5492 18.3451 15.622 18.1033 15.6223 17.8558V17.2581L14.4528 16.3502H9.37466Z"/>
            <path class="night" d="M0.131556 1.2363L0.898352 0.243172C0.948738 0.177883 1.01142 0.123229 1.08282 0.0823368C1.15423 0.0414448 1.23294 0.0151171 1.31446 0.00486006C1.39598 -0.00539702 1.47872 0.000617709 1.55793 0.0225602C1.63714 0.0445026 1.71127 0.0819422 1.77609 0.132737L24.7585 18.0039C24.8894 18.1062 24.9745 18.2567 24.9952 18.4221C25.0158 18.5876 24.9703 18.7545 24.8687 18.8862L24.1015 19.8794C24.0511 19.9446 23.9884 19.9992 23.917 20.0401C23.8457 20.0809 23.767 20.1072 23.6855 20.1175C23.6041 20.1277 23.5214 20.1217 23.4422 20.0998C23.363 20.0779 23.2889 20.0405 23.2241 19.9898L0.241322 2.1186C0.110489 2.01624 0.0254259 1.86578 0.00484145 1.70032C-0.015743 1.53486 0.0298368 1.36795 0.131556 1.2363V1.2363Z"/>
            </svg></p>
        </div>
    </label>
</div>
          </li>
          <li>
            <a href="./">
              <div class="left">
                Home
              </div>  
              <div class="right">
                <svg width="24px" aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><g><rect x="83.534" y="40.929" width="3.997" height="20.071"/></g><path d="M16.466,41.931l33.548-25.123L92.81,48.877l2.396-3.198L50.015,11.814L4.794,45.679l2.396,3.199l5.279-3.954v42.763h75.062  V61h-3.997v22.69H64.598V54.068H35.402V83.69H16.466V41.931z M39.399,58.065h21.202V83.69H39.399V58.065z"/></svg>
              </div>
            </a>
          </li>
          <li>
            <a href="./archive.html">
              <div class="left">
                All Posts
              </div>
              <div class="right">
                <svg width="24px" aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="-3 3 64 64"><g><path d="M60.992,31.985c0-15.979-13-28.978-28.979-28.978c-15.994,0-29.006,12.999-29.006,28.978   c0,15.994,13.012,29.007,29.006,29.007v-2c-14.891,0-27.006-12.115-27.006-27.007c0-14.875,12.115-26.978,27.006-26.978   c14.876,0,26.979,12.103,26.979,26.978c0,8.945-4.479,17.329-11.804,22.338l0.874-10.062l-1.992-0.174l-1.135,13.071l13.042,1.136   l0.174-1.992l-9.183-0.799C56.443,50.079,60.992,41.321,60.992,31.985z"/><polygon points="33.014,12.682 31.014,12.682 31.014,32.398 39.811,41.224 41.227,39.812 33.014,31.572  "/></g></svg>
              </div>
            </a>
          </li>
          <li>
            <a href="./tags.html">
              <div class="left">
                Tags
              </div>
              <div class="right">
                <svg width="24px" aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><g><path d="M75.244,15.066c-2.59,0-5.027,1.012-6.857,2.843c-3.781,3.785-3.778,9.94,0.002,13.724    c1.831,1.833,4.266,2.843,6.857,2.843s5.026-1.01,6.861-2.843c3.781-3.785,3.781-9.943-0.002-13.724    C80.275,16.076,77.838,15.066,75.244,15.066z M78.766,28.252c-1.871,1.869-5.129,1.869-6.996,0c-1.929-1.931-1.931-5.069-0.002-7    c0.934-0.934,2.175-1.448,3.498-1.448c1.322,0,2.564,0.515,3.5,1.448C80.691,23.183,80.691,26.321,78.766,28.252z M94.632,41.027    l0.005-28.872c0-3.745-3.05-6.792-6.792-6.792L58.973,5.368l-1.237-0.004c-1.893,0-4.75,0-6.617,1.869L7.008,51.342    c-1.06,1.059-1.645,2.467-1.645,3.966s0.583,2.908,1.644,3.968l33.717,33.717c1.058,1.06,2.467,1.645,3.966,1.645    s2.908-0.585,3.968-1.645l44.106-44.111c1.893-1.886,1.88-4.604,1.869-7.227L94.632,41.027z M90.022,46.139L45.913,90.25    c-0.654,0.65-1.792,0.652-2.443,0L9.752,56.532c-0.328-0.327-0.507-0.762-0.507-1.225c0-0.462,0.18-0.894,0.507-1.221    L53.861,9.976c0.676-0.674,2.284-0.731,3.874-0.731l1.237,0.004l28.872-0.004c1.604,0,2.909,1.306,2.909,2.911l-0.005,28.872    l0.005,0.642C90.76,43.585,90.769,45.392,90.022,46.139z"/></g></svg>
              </div>
            </a>
          </li>
          <li>
            <a href="./about.html">
              <div class="left">
                About
              </div>
              <div class="right">
                <svg width='24px' aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 846.66 846.66"><g><path d="M351.26 453.22c-276.42,134.06 -224.86,336.22 -224.73,336.8 6.03,25.41 -32.58,34.56 -38.6,9.15 -0.15,-0.65 -55.78,-219.32 218.87,-367.66 -60.98,-39 -100.02,-106.82 -100.02,-182.56 0,-119.6 96.95,-216.55 216.55,-216.55 119.6,0 216.55,96.95 216.55,216.55 0,75.74 -39.04,143.56 -100.02,182.56 274.65,148.34 219.02,367.01 218.87,367.66 -6.02,25.41 -44.63,16.26 -38.6,-9.15 0.13,-0.58 51.69,-202.74 -224.73,-336.8 -22.55,7.96 -46.8,12.29 -72.07,12.29 -25.27,0 -49.52,-4.33 -72.07,-12.29zm72.07 -381.14c-97.68,0 -176.87,79.19 -176.87,176.87 0,97.69 79.19,176.87 176.87,176.87 97.68,0 176.87,-79.18 176.87,-176.87 0,-97.68 -79.19,-176.87 -176.87,-176.87z"/></g></svg>
              </div>
            </a>
          </li>
          <li>
            <a href="./feed.xml">
              <div class="left">
                Atom feed
              </div>
              <div class="right">
                <svg width='24px' aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M80 352c26.467 0 48 21.533 48 48s-21.533 48-48 48-48-21.533-48-48 21.533-48 48-48m0-32c-44.183 0-80 35.817-80 80s35.817 80 80 80 80-35.817 80-80-35.817-80-80-80zm367.996 147.615c-6.448-237.848-198.06-429.164-435.61-435.61C5.609 31.821 0 37.229 0 44.007v8.006c0 6.482 5.146 11.816 11.626 11.994 220.81 6.05 398.319 183.913 404.367 404.367.178 6.48 5.512 11.626 11.994 11.626h8.007c6.778 0 12.185-5.609 12.002-12.385zm-144.245-.05c-6.347-158.132-133.207-284.97-291.316-291.316C5.643 175.976 0 181.45 0 188.247v8.005c0 6.459 5.114 11.72 11.567 11.989 141.134 5.891 254.301 119.079 260.192 260.192.269 6.453 5.531 11.567 11.989 11.567h8.005c6.798 0 12.271-5.643 11.998-12.435z"></path></svg>
              </div>
            </a>
          </li>
        </ul>
      </nav>
      
      
      <div class="logo"><a href="./"><img class="logo" id="logo" src="./assets/img/branding/MVM-logo-full.svg" alt="Quehry"></a></div>
      <div class="search-icon-container">
        <span class="search-icon"><a><i class="fa fa-search" aria-hidden="true"></i></a></span>
      </div>
    </div>
  </div>
</header> <!-- End Header -->

  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="动手学深度学习">
<meta itemprop="description" content="d2l note">
<meta itemprop="datePublished" content="2022-09-05T00:00:00+08:00">

    <div class="page-image">
      <div class="cover-image" style="background: url('./assets/img/posts/20220905/1.jpg') center no-repeat; background-size: cover;"></div>
    </div>
    <div class="wrapper">
      <div class="page-content">
        <div class="header-page">
          <h1 class="page-title">动手学深度学习</h1>
          

  <span class = "post-page-meta">
  
    <p class="page_meta">
  
  
  
    
      
      <span class="page_meta-date">
        <time datetime="2022-09-05T00:00:00+08:00">September 5, 2022</time>
      </span>
    
    
      <span class="meta-sep"></span>
    
  
  
    
    
    <span class="page_meta-readtime">
      
        2 minute read
      
    </span>
  
  
    </p>
  
  </span>

        </div>
        <aside class="sidebar side" id="sidebar">
    



<div class="tag-cloud">
    
        <ul class="tags side">
            
                <li><a href="./tag.html?tag=note" class="tag side">note</a></li>
            
    
        </ul>
</div>
    <div class="share-options side">
    <div class="share-hover side">
        <span class="share-button side"><svg fill="currentColor" width="25" height="25" class="side"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></span>
        <div class="share-icons side" id="sidebar-icons">
            <a class="twitter" href="https://twitter.com/intent/tweet?text=动手学深度学习&url=http://localhost:4000/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html" title="Share on Twitter" rel="nofollow" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
            <a class="facebook" href="https://facebook.com/sharer.php?u=http://localhost:4000/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html" title="Share on Facebook" rel="nofollow" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
            <a class="reddit" href="http://www.reddit.com/submit?url=http://localhost:4000/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html&title=动手学深度学习" title="Submit to Reddit" rel="nofollow" target="_blank"><i class="fa fa-reddit" aria-hidden="true"></i></a>
            <a class="linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html&title=动手学深度学习&summary=d2l note&source=http://localhost:4000/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html" title="Share on LinkedIn" rel="nofollow" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
            <a class="email" href="mailto:?subject=动手学深度学习&body=d2l note%0A%0ARead more here: http://localhost:4000/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html" title="Share via e-mail" rel="nofollow" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
            <a class="copy-link" onclick="copyToClipboard()" title="Copy to clipboard" rel="nofollow" target="_blank"><svg width="20px" fill="currentColor" class="side" viewBox="0 0 18 18"><path d="M16.94 1.1A3.7 3.7 0 0 0 14.3 0c-1 0-1.94.39-2.64 1.1L7.43 5.3c-.91.92-2.09 3.2 0 5.27a.75.75 0 0 0 .82.16c.09-.03.17-.09.24-.15a.74.74 0 0 0 0-1.06c-1.16-1.15-.77-2.39-.02-3.16l4.24-4.22a2.2 2.2 0 0 1 1.58-.65c.6 0 1.16.23 1.58.65.86.87.86 2.29 0 3.16L12.7 8.47a.74.74 0 0 0 1.04 1.05l3.17-3.16a3.73 3.73 0 0 0 0-5.27h.03zM9.54 7.4a.74.74 0 0 0 0 1.06c1.16 1.15.76 2.39 0 3.16l-4.22 4.22c-.42.42-.99.65-1.59.65a2.23 2.23 0 0 1-1.58-3.82l3.17-3.16A.73.73 0 0 0 5.54 9a.78.78 0 0 0-.22-.52.77.77 0 0 0-1.05 0L1.1 11.64A3.72 3.72 0 0 0 3.74 18c1 0 1.94-.39 2.65-1.1l4.23-4.2c.21-.22.94-1.02 1.13-2.2.18-1.12-.2-2.15-1.12-3.07-.27-.27-.78-.27-1.06 0l-.02-.02z" clip-rule="evenodd" fill-rule="evenodd"></path></svg></a>
        </div>
    </div>
    <div class='alert' style='font-size:.6em;color:var(--accent);text-align:center;'></div>
</div>
</aside>

        
        
        <!-- TOC -->

<ul>
  <li><a href="#0-简介">0. 简介</a></li>
  <li><a href="#1-预备知识">1. 预备知识</a></li>
  <li><a href="#2-线性神经网络">2. 线性神经网络</a></li>
  <li><a href="#3-多层感知机">3. 多层感知机</a></li>
  <li><a href="#5-卷积神经网络">5. 卷积神经网络</a></li>
  <li><a href="#6-现代卷积神经网络">6. 现代卷积神经网络</a></li>
  <li><a href="#7-循环神经网络">7. 循环神经网络</a></li>
  <li><a href="#8-现代循环神经网络">8. 现代循环神经网络</a></li>
  <li><a href="#9-注意力机制">9. 注意力机制</a></li>
  <li><a href="#13-自然语言处理-预训练">13. 自然语言处理: 预训练</a></li>
  <li><a href="#14-自然语言处理-应用">14. 自然语言处理: 应用</a></li>
</ul>

<!-- /TOC -->

<h1 id="0-简介">0. 简介</h1>
<ul>
  <li>《动手学深度学习》的笔记</li>
  <li>各种链接:
    <ul>
      <li><a href="https://space.bilibili.com/1567748478" target="_blank">bilibili</a></li>
      <li><a href="https://zh-v2.d2l.ai/index.html" target="_blank">book_zh</a></li>
      <li><a href="https://d2l.ai/index.html" target="_blank">book_en</a></li>
    </ul>
  </li>
  <li>这本书笼统的介绍了深度学习所需要的各种知识，从线性神经网络开始讲起，然后到CNN，最后到RNN，介绍了CV和NLP领域的较新的网络结构。同时这本书不止有理论内容，每一小节都有代码实践内容，可以边写代码边了解知识，同时bilibili上有李沐老师的网课配合学习，很适合初学者进行学习。</li>
</ul>

<h1 id="1-预备知识">1. 预备知识</h1>
<p>这一章主要介绍了深度学习的一些前置知识，这里对比较重要的点做备注</p>
<ul>
  <li>张量(Tensor)包含了一维张量(向量)和二维张量(矩阵)</li>
  <li>torch中A*B是哈达玛积，表示矩阵元素按元素相乘</li>
  <li>torch.dot()是点积</li>
  <li>torch.cat(…, dim=0)表示在行上延伸，比如(3, 4)和(3, 4)变成(6, 4)</li>
  <li>A.sum(axis=0)表示把每一列的数据都相加，比如(5, 4)变成(4)</li>
  <li>范数是norm，L1范数为每个元素的绝对值相加，L2范数为元素的平方和开根号，torch中默认L2范数，一般也是L2范数用的最多</li>
  <li>梯度: 连接多元函数的所有偏导数:</li>
</ul>

<center><img src="../assets/img/posts/20220905/2.jpg" /></center>

<ul>
  <li>梯度是一个向量</li>
  <li>常用的梯度计算公式:</li>
</ul>

<center><img src="../assets/img/posts/20220905/3.jpg" /></center>

<ul>
  <li>torch中自动求导的步骤:
    <ul>
      <li>第一步 为x分配内存空间: x.requires_grad_(True)</li>
      <li>第二步 链式反向传播，希望求哪个函数的梯度，就对那个函数反向传播，比如y.backward()</li>
      <li>第三步 求x的梯度，x.grad，如果我们需要重新求梯度，需要清零梯度，x.grad.zero_()</li>
      <li>注意torch中只能对标量输出求梯度，所以常见操作是sum</li>
    </ul>
  </li>
  <li>标量对向量的偏导是向量，向量对向量的偏导是矩阵</li>
  <li>贝叶斯公式: P(A|B)P(B)=P(B|A)P(A)</li>
</ul>

<h1 id="2-线性神经网络">2. 线性神经网络</h1>
<p>本章主要介绍了线性回归网络和softmax回归网络，接下来是一些笔记</p>
<ul>
  <li>随机梯度下降和梯度下降的区别: 梯度下降一般而言是针对所有的样本而言，而随机梯度下降是针对单个样本而言，同样地，小批量随机梯度下降是针对一个批量的样本而言</li>
  <li>可以调整但是在训练过程中不更新的参数叫做超参数</li>
  <li>极大似然法: $\theta$是需要估计的值，在写似然函数时只需要把$\theta$看成参数，最大化似然函数即$\theta$的估计值</li>
  <li>每个输入与每个输出相连的层成为全连接层</li>
  <li>with torch.no_grad()的作用是让输出结果之后不构建计算图</li>
  <li>本章的训练过程: 计算y的预测值-&gt;计算损失函数-&gt;累加loss并反向传播(记得每个批量在梯度更新前需要清零梯度并反向传播loss)-&gt;更新参数</li>
  <li>训练过程中重要的组成部分: 数据迭代器、损失函数、优化器(updater/trainer)、网络(记得初始化参数)</li>
  <li>softmax为分类服务，softmax本质上是将输出规范成概率数值，方便选取预测概率最大的类作为预测类:</li>
</ul>

<center><img src="../assets/img/posts/20220905/4.jpg" /></center>

<ul>
  <li>分类的标签可以用独热编码定义</li>
  <li>网络模型用nn.sequential()定义</li>
  <li>softmax回归的损失函数可以用极大似然法推出，普通的极大似然法是最大化似然函数，但是在这里我们加上-log就变成了最小化损失函数</li>
  <li>softmax回归的损失函数是交叉熵损失:</li>
</ul>

<center><img src="../assets/img/posts/20220905/5.jpg" /></center>

<h1 id="3-多层感知机">3. 多层感知机</h1>
<p>本小节主要介绍了多层感知机的实现以及面对各种问题的解决方法，比如解决过拟合的权重衰退(weight decay)和暂退法(dropout)，解决梯度爆炸与消失的Xavier初始化。</p>
<ul>
  <li>激活函数的作用是将线性网络变成非线性，常见的有ReLU、Sigmoid、tanh</li>
  <li>ReLU: max(x, 0)</li>
  <li>Sigmoid: $\frac{1}{1+e^{-x}}$</li>
  <li>tanh: $\frac{1-e^{-2x}}{1+e^{-2x}}$</li>
  <li>在torch中可以用@来简单表示矩阵乘法</li>
  <li>用nn.Sequential()来实例化网络时，nn.ReLU()单独算一层</li>
  <li>过拟合问题可以用正则化技术解决，比如权重衰退</li>
  <li>权重衰退就是L2正则化，它在计算损失函数时增加了权重的惩罚项，比如L($\omega$, b)+$\frac{\lambda}{2}$||$\omega$||，其中$\lambda$是超参数</li>
  <li>torch框架中把权重衰退放在优化器的实例化中(torch.optim)，只需要将weight_decay的超参数输入即可</li>
  <li>暂退法(Dropout): 在前向传播中，计算每一内部层的同时注入噪音，就好像在训练过程中丢弃了一些神经元</li>
  <li>中间层活性值:</li>
</ul>

<center><img src="../assets/img/posts/20220905/6.jpg" /></center>

<ul>
  <li>只有在训练过程中才有权重衰退和暂退法</li>
  <li>在torch中简单实现dropout的方法: 在构建net时将nn.Dropout(dropout)加入nn.Sequential()，其中dropout作为丢弃概率输入Dropout中</li>
  <li>网络架构顺序: linear-&gt;relu-&gt;dropout</li>
  <li>torch中实现tensor对tensor求梯度的方法是在backward()里面加入torch.ones_like()</li>
  <li>不正常的参数初始化可能会导致梯度爆炸和梯度消失</li>
  <li>Xavier初始化是解决梯度爆炸和消失的好手段</li>
</ul>

<h1 id="5-卷积神经网络">5. 卷积神经网络</h1>
<p>这章主要介绍了CNN的基础知识，包括卷积计算以及汇聚层和简单的卷积神经网络LeNet</p>
<ul>
  <li>卷积运算即互相关运算，卷积核函数沿着输入矩阵滑动计算，一般的卷积层除了核运算外，还需要加上偏置</li>
</ul>

<center><img src="../assets/img/posts/20220905/7.jpg" /></center>

<ul>
  <li>二维卷积层的输入格式: (批量大小, 通道数, 高, 宽)，卷积层又被称为特征映射(feature map)</li>
  <li>感受野(Receptive Field)的定义是卷积神经网络每一层输出的特征图上的像素点在输入图片上映射的区域大小，也就是一个像素点对应的上一层图像的区域大小</li>
  <li>填充(padding)与步幅(stride):
    <ul>
      <li>填充的作用是在输入图像的边界填充元素(通常为0)，添加$p_h$行与$p_w$列，基本是一半在左一半在右</li>
      <li>一般在定义卷积层nn.Convd()时可以加上填充与步幅</li>
      <li>步幅包括垂直步幅$S_h$与水平步幅$S_w$</li>
      <li>在经过卷积层后，二维图像变成了$[(n_h - k_h + p_h + 1)/S_h, (n_w - k_w + p_w + 1)/S_w]$</li>
    </ul>
  </li>
  <li>多通道输入，只需把各通道输出结果加起来即可</li>
  <li>多通道输出，为每个输出通道创建一个卷积核函数$(c_i, k_h, k_w)$，假设输入通道个数$c_i$，输出通道个数为$c_o$，那么卷积核形状为$(c_o, c_i, k_h, k_w)$</li>
  <li>torch.stack(): 沿一个新维度对输入张量进行连接</li>
  <li>汇聚层(pooling)包括最大汇聚层和平均汇聚层，汇聚层是直接返回输入图像的一个小窗口的最大值或者平均值</li>
  <li>汇聚层没有可学习的参数</li>
  <li>汇聚层同样有填充与步幅，默认情况下步幅与窗口大小相同，nn.MaxPool2d()</li>
  <li>每个卷积块的基本单元是: 卷积层-&gt;激活函数-&gt;汇聚层</li>
  <li>nn.Conv2d(1, 6, kernel_size=5)其中1表示输入通道数，6表示输出通道数</li>
  <li>在CNN的最后都需要连接全连接层来变成预测类别</li>
  <li>在训练过程中如果想好好利用GPU，那么需要将网络的参数与数据集数据传入GPU，具体方法是net.to(device)、X,y.to(device)</li>
  <li>多输入多输出通道的图片示例:</li>
</ul>

<center><img src="../assets/img/posts/20220905/40.jpg" /></center>

<h1 id="6-现代卷积神经网络">6. 现代卷积神经网络</h1>
<p>这一节主要介绍了CNN的各种网络的发展历程，LeNet之后，于2012年出现深度CNN网络AlexNet，之后出现了NiN与VGG，然后是GoogleNet，之后有很大进步的网络是ResNet，直到现在，ResNet用的也很多，残差思想也持续影响后续模型的搭建</p>
<ul>
  <li>促进CV有更深的网络的两大关键因素: 数据与硬件(主要是GPU)</li>
  <li>AlexNet(深度卷积网络)于2012年ImageNet挑战赛上夺冠，第一次学习到的特征超越了手工设计的特征</li>
  <li>相比于LeNet而言，AlexNet更深、激活函数用ReLU、对训练数据进行了增广</li>
  <li>VGG(使用块的网络): VGG块由一系列卷积层(包含ReLU)+汇聚层组成，VGG网络由VGG块组成</li>
  <li>NiN(网络中的网络): NiN块结构是卷积层(含ReLU)+两个1x1卷积层(相当于全连接层)，NiN网络结构是: NiN块+汇聚层+NiN块+汇聚层+…+平均汇聚层</li>
  <li>GoogleNet(含并行连结的网络)是google花费了很多money实验出的网络，特点是参数值特殊，参数以及网络结构都是经过了很多实验得出的结果</li>
  <li>GoogleNet中基本的卷积块被称为Inception块，Inception块的架构如下图所示:</li>
</ul>
<center><img src="../assets/img/posts/20220905/41.jpg" /></center>
<ul>
  <li>GoogleNet架构: 卷积块+Inception块+最大汇聚层+Inception块+最大汇聚层+Inception块+平均汇聚层+全连接层</li>
  <li>批量规范化(Batch Normalization)是一种trick，可加速深层网络的收敛速度</li>
  <li>正则化在深度学习中非常重要</li>
  <li>对于一个批量来说，首先规范化输入(减去其均值并除以标准差)，再应用比例系数与比例偏移，就是对当前批量进行了批量规范化</li>
</ul>
<center><img src="../assets/img/posts/20220905/42.jpg" /></center>
<p>上述式子中x是输入，$\hat{\mu}_B$是这个批量的均值，$\hat{\sigma}_B$是批量的标准差，$\gamma$是比例系数，$\beta$是比例偏移，$\gamma$与$\beta$与x的形状相同，是需要学习的参数</p>
<ul>
  <li>应用于全连接层的BN: $h=\Phi(BN(Wx+b))$</li>
  <li>应用于卷积层的BN: 在每个输出通道的m*p*q个元素上同时执行BN</li>
  <li>可以发现，BN的作用位置为权重层后，激活函数前</li>
  <li>BN在训练和预测时有所不同，在预测时，直接使用模型传入的移动平均所得的均值与方差</li>
  <li>用pytorch架构简单实现BN: nn.BatchNorm2d(通道数)</li>
  <li>直观地说，BN可以使优化更加平滑</li>
  <li>残差网络ResNet于2015年在ImageNet上夺冠</li>
  <li>残差思想: 每个附加层都应该更容易地包含原始函数作为其元素之一，残差块不是为了学习输出f(x)，而是学习输出与输入的差别f(x)-x</li>
  <li>残差块的架构:</li>
</ul>
<center><img src="../assets/img/posts/20220905/43.jpg" /></center>
<ul>
  <li>ResNet的架构: 和GoogleNet很像，就是Inception变成了残差块，同时多了BN</li>
  <li>稠密链接网络DenseNet: 是ResNet的继承，DenseNet的输出是连结，而不是如ResNet那样的简单相加</li>
</ul>
<center><img src="../assets/img/posts/20220905/44.jpg" /></center>

<h1 id="7-循环神经网络">7. 循环神经网络</h1>
<p>这小节主要介绍了文本数据集如何制作，RNN的网络结构与实现</p>
<ul>
  <li>序列数据就是与时间相关的数据</li>
  <li>马尔可夫模型: 用定时间跨度的观测序列预测$x_t$</li>
  <li>$P(x_1,…,x_T)=\prod_{t=1}^TP(x_t|x_{t-1},…,x_{t-\tau})$</li>
  <li>一些名词: 文本序列、词元(token)、词表(vocabulary)、语料(corpus)</li>
  <li>文本预处理过程: 读取数据集成列表-&gt;将列表词元化，变成包含多行的词元列表-&gt;构建词表(词表将词元与数字对应)</li>
  <li>文本预处理中的词元可以是单词，也可以是字符，这里采用字符</li>
  <li>语言模型(language model)的目标是估计联合概率$P(x_1,…,x_T)$</li>
  <li>涉及一个、两个、三个变量的概率公式分别被称为一元语法、二元语法、三元语法</li>
  <li>zip()的作用是将可迭代对象打包成一个个元组，然后返回元组组成的列表</li>
  <li>构建文本序列数据集的两种方法:
    <ul>
      <li>随机采样: 随机选取，特征是原始序列，标签是原始序列右移一位</li>
      <li>顺序分区: 保证每个批量中子序列再原语料中相邻</li>
    </ul>
  </li>
  <li>相比与马尔可夫模型，隐变量模型更能体现过往序列的影响:
  $P(x_t|x_{t-1},…,x_1)=P(x_t|h_{t-1})$</li>
  <li>RNN的示意图以及推导公式:</li>
</ul>

<center><img src="../assets/img/posts/20220905/8.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20220905/9.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20220905/10.jpg" /></center>

<ul>
  <li>循环神经网络中循环的是H(Hidden state)</li>
  <li>度量语言模型的质量的性能度量是困惑度(perplexity): 一个序列中n个词元的交叉熵损失来衡量语言模型的质量</li>
</ul>

<center><img src="../assets/img/posts/20220905/11.jpg" /></center>

<ul>
  <li>最好的情况下，困惑度为1，最差的情况下，困惑度为无穷大</li>
  <li>独热编码将(批量大小, 时间步数)转变成(批量大小, 时间步数, 词表大小)，但为了方便计算，最终转变成(时间步数, 批量大小, 词表大小)</li>
  <li>梯度裁剪的作用是保证梯度不会爆炸</li>
</ul>

<center><img src="../assets/img/posts/20220905/12.jpg" /></center>

<ul>
  <li>RNN的网络结构与之前差别不大，只是在更新梯度前需要进行梯度裁剪</li>
  <li>隐藏状态形状: (隐藏层个数, 批量大小, 隐层参数个数)</li>
  <li>nn.RNN()返回的Y为隐层参数个数，需要再加上全连接层</li>
</ul>

<h1 id="8-现代循环神经网络">8. 现代循环神经网络</h1>
<p>这一章介绍了拥有记忆单元的LSTM模型，以及后续新的NLP任务机器翻译，介绍了数据集处理过程和编码器解码器结构的网络seq2seq，用来处理序列转换任务</p>
<ul>
  <li>长短期记忆网络LSTM(long short term memory)</li>
  <li>LSTM相较于普通的RNN多了很多元素，最主要的设计是记忆单元，它可以影响下一步的隐藏状态:</li>
</ul>

<center><img src="../assets/img/posts/20220905/13.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20220905/14.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20220905/15.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20220905/16.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20220905/17.jpg" /></center>
<p><br /></p>

<ul>
  <li>输入、输出、遗忘门均与$H_{t-1}$和$X_t$有关</li>
  <li>记忆单元C类似于隐状态，时时更新</li>
  <li>总的来说，LSTM中$H_t$与$H_{t-1}$、$X_t$、$C_t$都有关</li>
  <li>RNN的延伸: 多层与双向RNN，其中多层很好理解，就是把单向隐藏层的神经网络变成多层，双向的作用是让序列用到上下文信息，在预测下一个词元的任务中
双向RNN表现不佳，但是在NER中表现很好</li>
  <li>nn.LSTM(num_inputs, num_hiddens, num_layers)</li>
  <li>接下来的内容变成了机器翻译任务(序列转换)</li>
  <li>机器翻译中使用单词级词元化</li>
  <li>机器翻译数据集处理过程: 读取数据集-&gt;词元化列表-&gt;将数据集分割成source(源语言)与target(目标语言)-&gt;序列末端加上&lt;eos&gt;，同时针对长短不一的序列填充&lt;pad&gt;与截断</li>
  <li>处理序列转换任务可以用编码器-解码器结构</li>
  <li>编码器的作用是将长短可变序列变成固定形状的状态，解码器的作用是将固定形状的状态变成长度可变序列</li>
  <li>编码器为解码器输入一个状态，在seq2seq中是编码器编码过程中的隐状态，这个隐状态既作为解码器的初始state，在每个时间步中也作为上下文变量和输入concatenate之后一起输入解码器</li>
  <li>采用嵌入层将词元进行向量化，嵌入层是一个矩阵，(词表大小，特征向量维度)</li>
  <li>编码器与解码器是两个GRU</li>
  <li>permute()可以改变张量维度的位置</li>
  <li>rnn()的输入形状一般为(num_steps, batch_size, embed_size)</li>
  <li>解码器的最后同样需要一个全连接层输出</li>
  <li>解码器的第一个输入为&lt;bos&gt;</li>
  <li>由于序列存在很多&lt;pad&gt;，计算损失时不能计算pad那一部分，可以mask这一部分，所以损失函数需要重新改一下</li>
  <li>在seq2seq训练时，解码器net的输入为cat(&lt;bos&gt;，真实序列少一时间步)，这种训练机制叫做强制教学</li>
  <li>预测的时候解码器net的输入仅为&lt;bos&gt;，用每一步的预测作为下一步的输入</li>
  <li>机器翻译的性能度量为BLEU(bilingual evaluation understudy)，可用来预测输出序列的质量，当预测序列与标签序列完全相同时，BLEU为1，公式如下:</li>
</ul>

<center><img src="../assets/img/posts/20220905/18.jpg" /></center>

<ul>
  <li>编码器的功能主要是为解码器提供上下文变量c和解码器的初始隐状态</li>
</ul>

<h1 id="9-注意力机制">9. 注意力机制</h1>
<p>这章主要介绍了注意力机制，介绍了注意力机制的组成部分，比如查询、键、值、评分函数，后面又介绍了与RNN结合的Bahdanau注意力以及自注意力和多头注意力，最后介绍了transformer</p>
<ul>
  <li>注意力机制的主要成分是查询(query)、键(key)、值(value)，q和k交互形成注意力权重，然后与v相乘得到注意力汇聚结果</li>
</ul>

<center><img src="../assets/img/posts/20220905/19.jpg" /></center>

<ul>
  <li>注意力汇聚结果计算公式:</li>
</ul>

<center><img src="../assets/img/posts/20220905/20.jpg" /></center>

<p>其中x是查询，$x_i$是key，$y_i$是value，$\alpha$的作用是将x与$x_i$之间的关系建模，且权重总和为1，有点像softmax</p>

<ul>
  <li>unsqueeze()的作用是在指定位置添加一个维度，squeeze()的作用是在指定位置删除一个维度，torch.bmm()是批量矩阵乘法</li>
  <li>评分函数a同样是对q和k的关系进行建模，q、k、v都可以是向量，而且长度可以不同</li>
</ul>

<center><img src="../assets/img/posts/20220905/21.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20220905/22.jpg" /></center>
<p><br /></p>

<ul>
  <li>这里介绍了两种评分函数: 加性注意力和缩放点积注意力
    <ul>
      <li>加性注意力: 可以处理长度不同的q与k</li>
    </ul>

    <center><img src="../assets/img/posts/20220905/23.jpg" /></center>

    <ul>
      <li>缩放点积注意力(计算效率高): 要求q与k长度相同</li>
    </ul>

    <center><img src="../assets/img/posts/20220905/24.jpg" /></center>
  </li>
  <li>Bahdanau注意力模型也是编码器解码器结构，与之前的seq2seq不同，这里的上下文变量在解码器的每一步都不相同，上下文变量$c_{t’}$与解码器的上一步隐状态有关，同时在解码器和编码器的输入位置都有嵌入层</li>
</ul>

<center><img src="../assets/img/posts/20220905/25.jpg" /></center>

<ul>
  <li>多头注意力: 对q、k、v使用线性变换得到h组不同的q-k-v来输入h个注意力汇聚层，得到h个输出，这h个输出再线性变换得到最终输出</li>
</ul>

<center><img src="../assets/img/posts/20220905/26.jpg" /></center>

<ul>
  <li>自注意力就是q-k-v都是相同的一组元素</li>
  <li>自注意力无法使用序列的位置信息，可以给输入concatenate一个位置编码，比如X∈$R^{nxd}$表示n个词元的d维嵌入，P∈$R^{nxd}$表示位置嵌入矩阵，那么X+P即输入，位置编码可以基于正弦函数和余弦函数的固定位置编码</li>
</ul>

<center><img src="../assets/img/posts/20220905/27.jpg" /></center>

<ul>
  <li>transformer模型与Bahdanau模型不同，它完全基于注意力机制来构建模型</li>
  <li>transformer每块都由多头注意力和基于位置的前馈神经网络组成，其中还有残差连接，即x+sublayer(x)，再层规范化。在解码器的注意力层中，q是上个解码器层的输出，k和v是编码器输出(每个源序列的位置的编码代表一个键值对)。基于位置的前馈神经网络，简称ffn，即两层MLP</li>
</ul>

<center><img src="../assets/img/posts/20220905/28.jpg" /></center>

<h1 id="13-自然语言处理-预训练">13. 自然语言处理: 预训练</h1>
<p>这一章主要介绍了NLP领域的预训练模型，NLP领域的预训练模型都是encoder，即用文本特征来表示词元(一般都是单词)，首先介绍了word2vec，然后介绍了全局向量的词嵌入，之后介绍了子词嵌入模型fastText与字节对编码(BPE)，之后介绍了BERT(双向Transformer编码器)</p>
<ul>
  <li>在介绍RNN模型时，介绍了用独热向量来表示词元，但是这有个很严重的缺点: 不同词的独热向量的余弦相似度为0。所以接下来会介绍很多词嵌入模型，即用一个词向量来表示单词</li>
  <li>word2vec: 将词映射到固定长度的向量，这里介绍了两种模型: 跳元模型(skip-gram)与连续词袋CBOW
    <ul>
      <li>跳元模型: 假设一个词可以用来在文本序列中生成其周围的词，对于每个索引为i的单词，可以用$u_i$与$v_i$分别表示其作为上下文词和中心词的向量，可以用softmax对生成概率进行建模，对于给定中心词$w_c$，生成上下文词$w_o$的概率为:</li>
    </ul>

    <center><img src="../assets/img/posts/20220905/29.jpg" /></center>

    <p>那么跳元模型的似然函数为(上下文窗口大小为m):</p>

    <center><img src="../assets/img/posts/20220905/30.jpg" /></center>

    <p>然后通过极大似然估计法来训练</p>
    <ul>
      <li>连续词袋: 与跳元模型相反，CBOW是基于上下文词生成中心词，连续词袋模型用$v_i$和$u_i$分别表示一个词的上下文词向量与中心词向量(与跳元模型相反)，同样用softmax建模(上下文词向量相加):</li>
    </ul>

    <center><img src="../assets/img/posts/20220905/31.jpg" /></center>

    <p>连续词袋模型的似然函数:</p>

    <center><img src="../assets/img/posts/20220905/32.jpg" /></center>
  </li>
  <li>由于词表过大，使用softmax来建模的话计算成本过大，可采用两种近似训练办法来优化: 负采样与分层softmax
    <ul>
      <li>负采样建模: 直接用内积加上激活函数来表示概率，负采样即在似然函数中加上负例(从预定义分布中采样噪声词)</li>
    </ul>

    <center><img src="../assets/img/posts/20220905/33.jpg" /></center>

    <ul>
      <li>层序softmax: 用二叉树来表示概率模型，同样使用了激活函数sigmoid，时间复杂度变低</li>
    </ul>
  </li>
  <li>接下来用负采样跳元模型训练(自监督训练)来展示word2vec的效果，数据集用PTB，语料库取自华尔街时报。数据集处理时用到了下采样方法: 高频词有概率被丢弃:</li>
</ul>

<center><img src="../assets/img/posts/20220905/34.jpg" /></center>

<p>上述式子中f($w_i$)是词在整个语料库中出现的比率，t是超参数。这样高频词就不会太影响模型效果，毕竟不太关注类似a和the与其他词共同出现的概率</p>
<ul>
  <li>在下采样与负采样完毕后，一个小批量中第i个样本包括中心词及其$n_i$个上下文词和$m_i$个噪声词，数据集还需要返回mask与label，分别用来遮掩&lt;pad&gt;与标记正例</li>
  <li>word2vec本质上其实是训练一个权重矩阵(词表大小, 嵌入维度)，就是一个nn.Embedding()。跳元模型的前向传播就是计算内积矩阵torch.bmm(embed_v, embed_u)
损失函数是带掩码的交叉熵损失</li>
  <li>跳元模型在预训练完毕后，可以用来找出语义相似的单词，即计算中心词与其余所有词的余弦相似度，计算结果最高的词即为语义最相似的单词</li>
  <li>无论是word2vec的哪个模型，都着眼于中心词与上下文词的关系</li>
  <li>全局向量的词嵌入GloVe: word2vec只考虑了局部的上下文词，GloVe则考虑了全局语料库统计来设计模型，训练GloVe是为了降低以下损失函数:</li>
</ul>

<center><img src="../assets/img/posts/20220905/35.jpg" /></center>

<p>上述式子中，$x_{ij}$是中心词i与上下文词j在一个上下文窗口出现的次数，h($x_{ij}$)是每个损失项的权重，当x小于c时，h(x)=$\frac{x}{c}^\alpha$，当x大于c时h(x)=1，$b_i$与$c_j$是可学习的偏置</p>
<ul>
  <li>子词嵌入模型: 对词的内部结构进行研究(比如dog和dogs的关系)
    <ul>
      <li>fastText模型: 每个中心词由其子词的向量之和表示(子词就是单词的某些连续字符)</li>
    </ul>

    <center><img src="../assets/img/posts/20220905/36.jpg" /></center>

    <p>其余部分与跳元模型相同</p>
    <ul>
      <li>字节对编码(Byte Pair Encoding, BPE): 一种算法，用来提取子词。BPE的本质就是对训练数据集进行统计分析，找出单词的公共符号，这些公共符号将作为划分子词的依据，对于每个单词，都将返回最长的子词划分结果(这样就可以获得任意长度的子词)</li>
    </ul>
  </li>
  <li>从大型语料库中训练的词向量可用于下游的自然语言处理任务，预训练的词向量可应用到词的类比性和相似性任务中，比如GloVe和fastText</li>
  <li>torch.topk(k)的作用是返回列表中的最大值(前k个)</li>
  <li>词相似: 利用余弦相似度</li>
  <li>词类比: a:b::c:d，比如man:woman::son:daughter，词类比任务就是给出a、b、c，找到d，即让d的词向量尽量靠近vec(c)+vec(b)-vec(a)</li>
  <li>word2vec于GloVe都是上下文无关的，即对于一个词元编码，只需要输入该词元即可</li>
  <li>NLP的六种任务: 情感分析、自然语言推断、语义角色标注、共指消解，NER和QA</li>
  <li>GPT的缺点: 自回归，单向</li>
  <li>BERT使用双向Transformer编码器来编码文本，BERT同样是预训练模型，可以基于双向上下文来表示任意词元</li>
  <li>针对不同的上下文任务，BERT需要对架构进行微调</li>
  <li>BERT可输入单个文本，也可以输入文本序列对，当输入单个文本时，BERT的输入序列是&lt;cls&gt;文本&lt;sep&gt;；当输入文本对时，BERT的输入序列是&lt;cls&gt;文本1&lt;sep&gt;文本2&lt;sep&gt;</li>
  <li>在BERT中，有三个嵌入层，分别是词元嵌入(普通的embedding)、段嵌入(两个片段序列的输入，用来区分不同的句子)和位置嵌入(可学习)，之后再把结果输入encoder中</li>
</ul>

<center><img src="../assets/img/posts/20220905/37.jpg" /></center>

<ul>
  <li>BERT通过两个预训练任务来优化双向Transformer编码器，分别是掩蔽语言模型(masked language modeling 即填空)和下一句预测(next sentence predicition)
    <ul>
      <li>掩遮语言模型: 随机掩蔽词元并使用来自双向上下文的词元以自监督的方式预测掩蔽词元。预测阶段使用单隐藏层的MLP来进行预测，输入BERT的编码结果和用于预测词元的位置，然后根据预测词元的位置得到预测词元的编码结果，然后输入mlp中得到预测结果</li>
      <li>下一句预测: 二分类任务，判断文本序列对是否是连续句子，预测时同样使用MLP，输入编码后的&lt;cls&gt;词元</li>
      <li>这两个任务在制作数据集的时候都需要加上一些负例或者噪声</li>
    </ul>
  </li>
  <li>BERT的预训练数据集有很多(针对不同的应用领域，使用不同的数据集进行训练)，最开始使用的是图书语料库和wiki</li>
  <li>BERT本质上就是一个双向Transformer编码器</li>
</ul>

<h1 id="14-自然语言处理-应用">14. 自然语言处理: 应用</h1>
<p>这一章主要介绍了如何将自然语言预处理模型应用到下游任务中，首先是传统的GloVe模型和子词嵌入模型，针对这种预训练模型，需要设计特定的网络结构在适配任务，但是BERT的出现，让下游任务应用更简单，有时候只需要加一个全连接层就行，参数也只需要微调，这一章主要介绍了两个下游任务，分别是情感分析和自然语言推断</p>
<ul>
  <li>情感分析任务(sentiment analysis): 本质上就是文本序列分类任务，数据集采用Imdb的电影评论集(评论+情感)</li>
  <li>处理长短不一的序列时，使用截断与填充来预处理数据集，可以将其变成长短一致的序列</li>
  <li>一般的预训练模型应用于下游任务的方式都是: 预训练模型(词元的文本表示embed)+架构(网络)+应用(各种任务)</li>
</ul>

<center><img src="../assets/img/posts/20220905/38.jpg" /></center>

<ul>
  <li>这里介绍了两种非BERT的架构，分别是用双向LSTM和CNN来处理情感分析任务:
    <ul>
      <li>双向LSTM: 双向LSTM的初始与最终步的隐状态连结起来作为文本序列的表示，然后连接一个全连接层，输出</li>
      <li>CNN: 这里使用了一种名为textCNN的网络架构，把文本序列看成一维图像进行处理，采用一维卷积来获得局部特征</li>
      <li>一维卷积是二维卷积的一个特例，同样是核函数沿着输入滑动。多通道输入的一维互相关等同于单输入通道的二维互相关</li>
    </ul>

    <center><img src="../assets/img/posts/20220905/39.jpg" /></center>
  </li>
  <li>自然语言推断(nature language inference, NLI)任务是文本对分类任务，对文本对进行判断，决定一个句子能否由另一个句子推断出，即假设(hypothesis)能否由前提(premise)推出</li>
  <li>两个句子的三种关系:
    <ul>
      <li>蕴涵(entailment): 假设可以从前提推出</li>
      <li>矛盾(contradiction): 假设的否定可以从前提推出(我感觉本质上就是假设不能由前提推出)</li>
      <li>中性(neutral): 所有其他的情况</li>
    </ul>
  </li>
  <li>NLI使用的数据集是斯坦福自然语言推断数据集(SNLI)</li>
  <li>接下来介绍两种进行NLI的方法，分别是使用注意力机制(包含MLP)和BERT微调:
    <ul>
      <li>使用注意力机制: 利用注意力机制将两个文本序列的词元对齐，然后比较、聚合这些信息，那么本质上就是三个步骤：注意、比较、聚合:
        <ul>
          <li>注意: 与attention机制类似</li>
          <li>比较: 比较软对齐的hypothesis与premise相比较</li>
          <li>聚合: 将比较结果concat之后送入mlp</li>
          <li>这中间涉及到很多mlp层</li>
        </ul>
      </li>
      <li>另一种方法就是利用BERT进行微调，后面会具体介绍</li>
    </ul>
  </li>
  <li>这种使用非BERT的应用都是将嵌入层的权重替换成预训练模型的权重</li>
  <li>BERT可处理的一些下游任务:
    <ul>
      <li>单文本分类(比如情感分析、句子在语法上是否可接受)</li>
      <li>文本对分类/回归(NLI、语义文本相似度)</li>
      <li>词元级任务: 比如文本标注(每个词元经过相同的全连接层后，返回词性标签)、问答(QA, 使用数据集SQuAD，给定段落与问题后，预测用段落的哪个片段进行回答(也即文本片段的开始与结束位置的预测))</li>
    </ul>
  </li>
  <li>加载bert模型时，可以将预训练好的参数直接放到定义好的网络架构中</li>
  <li>之前很多预训练模型在处理下游任务时，都需要为下游任务设定特定的框架，但是BERT却并不需要设置特定的框架，有时只需要添加一个额外的全连接层即可</li>
  <li>微调只更新部分参数</li>
</ul>

        <aside class="sidebar inline" id="post-end">
    



<div class="tag-cloud">
    
        <ul class="tags inline">
            
                <li><a href="./tag.html?tag=note" class="tag inline">note</a></li>
            
    
        </ul>
</div>
    <div class="share-options inline">
    <div class="share-hover inline">
        <span class="share-button inline"><svg fill="currentColor" width="25" height="25" class="inline"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></span>
        <div class="share-icons inline" id="post-end-icons">
            <a class="twitter" href="https://twitter.com/intent/tweet?text=动手学深度学习&url=http://localhost:4000/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html" title="Share on Twitter" rel="nofollow" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
            <a class="facebook" href="https://facebook.com/sharer.php?u=http://localhost:4000/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html" title="Share on Facebook" rel="nofollow" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
            <a class="reddit" href="http://www.reddit.com/submit?url=http://localhost:4000/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html&title=动手学深度学习" title="Submit to Reddit" rel="nofollow" target="_blank"><i class="fa fa-reddit" aria-hidden="true"></i></a>
            <a class="linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html&title=动手学深度学习&summary=d2l note&source=http://localhost:4000/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html" title="Share on LinkedIn" rel="nofollow" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
            <a class="email" href="mailto:?subject=动手学深度学习&body=d2l note%0A%0ARead more here: http://localhost:4000/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html" title="Share via e-mail" rel="nofollow" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
            <a class="copy-link" onclick="copyToClipboard()" title="Copy to clipboard" rel="nofollow" target="_blank"><svg width="20px" fill="currentColor" class="inline" viewBox="0 0 18 18"><path d="M16.94 1.1A3.7 3.7 0 0 0 14.3 0c-1 0-1.94.39-2.64 1.1L7.43 5.3c-.91.92-2.09 3.2 0 5.27a.75.75 0 0 0 .82.16c.09-.03.17-.09.24-.15a.74.74 0 0 0 0-1.06c-1.16-1.15-.77-2.39-.02-3.16l4.24-4.22a2.2 2.2 0 0 1 1.58-.65c.6 0 1.16.23 1.58.65.86.87.86 2.29 0 3.16L12.7 8.47a.74.74 0 0 0 1.04 1.05l3.17-3.16a3.73 3.73 0 0 0 0-5.27h.03zM9.54 7.4a.74.74 0 0 0 0 1.06c1.16 1.15.76 2.39 0 3.16l-4.22 4.22c-.42.42-.99.65-1.59.65a2.23 2.23 0 0 1-1.58-3.82l3.17-3.16A.73.73 0 0 0 5.54 9a.78.78 0 0 0-.22-.52.77.77 0 0 0-1.05 0L1.1 11.64A3.72 3.72 0 0 0 3.74 18c1 0 1.94-.39 2.65-1.1l4.23-4.2c.21-.22.94-1.02 1.13-2.2.18-1.12-.2-2.15-1.12-3.07-.27-.27-.78-.27-1.06 0l-.02-.02z" clip-rule="evenodd" fill-rule="evenodd"></path></svg></a>
        </div>
    </div>
    <div class='alert' style='font-size:.6em;color:var(--accent);text-align:center;'></div>
</div>
</aside>

        <div class="separator"></div>
        



<section class="author-box">
  <div class="narrow-column">
    <a href='https://quehry.github.io'><img src="./assets/img/Dog.jpg" alt="Quehry" class="author-img"></a>
    <ul class="contact-icons">
      
      <li class="twitter"><a class="twitter" href="https://twitter.com/@QuehryS" target="_blank"><i class="fa fa-twitter"></i></a></li>
      
      
      <li class="linkedin"><a class="linkedin" href="https://in.linkedin.com/in/quehry" target="_blank"><i class="fa fa-linkedin"></i></a></li>
      
      
      <li class="github"><a class="github" href="http://github.com/Quehry" target="_blank"><i class="fa fa-github"></i></a></li>
      
      
      <li class="email"><a class="email" href="mailto:quehry@gmail.com"><i class="fa fa-envelope-o"></i></a></li>
      
    </ul>
  </div>
  <div class="author-desc">
    <h3>Quehry</h3>
    <p>Student</p>
  </div>
</section>

        



<div class="recent-box">
  <h2 class="page-subtitle">Recent posts</h2>
  <div class="recent-list">
    
      
        <div class="recent-item">
          
          

          <a href="./GAN.html" class="recent-item-img" style="background: url('./assets/img/posts/20220927/1.jpg') center no-repeat; background-size: cover;">
            <div class="recent-item-title">
              <span>GAN</span>
              

  <span class = "recent-item-meta">
  
  
  
  
    
    
    <p class="page_meta-readtime">
      
        less than 1 minute read
      
    </p>
  
  
  </span>

            </div>
          </a>
        </div>
      
    
      
        <div class="recent-item">
          
          

          <a href="./AutoEncoder-Series.html" class="recent-item-img" style="background: url('./assets/img/posts/20221008/1.jpg') center no-repeat; background-size: cover;">
            <div class="recent-item-title">
              <span>AutoEncoder系列整理</span>
              

  <span class = "recent-item-meta">
  
  
  
  
    
    
    <p class="page_meta-readtime">
      
        less than 1 minute read
      
    </p>
  
  
  </span>

            </div>
          </a>
        </div>
      
    
      
        <div class="recent-item">
          
          

          <a href="./Important-Event.html" class="recent-item-img" style="background: url('./assets/img/posts/20220926/1.jpg') center no-repeat; background-size: cover;">
            <div class="recent-item-title">
              <span>重要时间点及事件</span>
              

  <span class = "recent-item-meta">
  
  
  
  
    
    
    <p class="page_meta-readtime">
      
        less than 1 minute read
      
    </p>
  
  
  </span>

            </div>
          </a>
        </div>
      
    
      
        <div class="recent-item">
          
          

          <a href="./%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB%E4%BC%9A%E7%AC%94%E8%AE%B0.html" class="recent-item-img" style="background: url('./assets/img/posts/20220919/1.jpg') center no-repeat; background-size: cover;">
            <div class="recent-item-title">
              <span>技术分享会笔记(关于text2image)</span>
              

  <span class = "recent-item-meta">
  
  
  
  
    
    
    <p class="page_meta-readtime">
      
        less than 1 minute read
      
    </p>
  
  
  </span>

            </div>
          </a>
        </div>
      
    
  </div>
</div> <!-- End Recent-Box -->

        <div class="newsletter" id="mc_embed_signup">
  <h2 class="page-subtitle">Newsletter</h2>
  <div class="form-container">
    <p>Subscribe here to get our latest updates</p>
    <form action="https://github.us1.list-manage.com/subscribe/post?u=8ece198b3eb260e6838461a60&amp;id=397d90b5f4" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
      <label class="screen-reader-text" for="mce-EMAIL">Email Address</label>
      <div class="newsletter-box" id="mc_embed_signup_scroll">
        <input type="email" name="EMAIL" placeholder="Email address" class="email-input" id="mce-EMAIL" required>
        <input type="submit" value="Subscribe" name="subscribe" class="subscribe-btn" id="mc-embedded-subscribe">
      </div>
    </form>
  </div>
</div> <!-- End Newsletter -->

        
  <section class="comment-area">
    <div class="comment-wrapper">
        
            <div class="row" id="comment-curtain">
                <div class="col-lg-8 col-sm-10 mr-auto ml-auto">
                    <h2 class="page-subtitle">Comments</h2>
                    <div class="comments-trigger" onClick="toggle_comments()">
                        <i class="fa fa-comments"></i>&nbsp;&nbsp;Write a comment ...
                    </div>
                </div>
            </div>
        
      <div id="disqus_thread"></div>
          <script>
            (function() {
                var d = document, s = d.createElement('script');
                s.src = '//amaynez-github-io.disqus.com/embed.js';
                s.setAttribute('data-timestamp', +new Date());
                (d.head || d.body).appendChild(s);
            })();
          </script>
      <noscript>Please enable JavaScript to view the comments.</noscript>
    </div>
  </section> <!-- End Comment Area -->


      </div>
    </div> <!-- End Wrapper -->
  </article>
  <div class="search-box">
  <div class="wrapper">
    <div class="search-grid">
      <form class="search-form">
        <div id="search-container">
          <input type="text" id="search-input" class="search" placeholder="Search">
        </div>
      </form>
      <ul id="results-container" class="results-search"></ul>
      <div class="icon-close-container">
        <span class="search-icon-close"><i class="fa fa-times" aria-hidden="true"></i></span>
      </div>
    </div>
  </div>
</div>

  




<footer class="main-footer">
    <div class="footer-wrapper">
        <div class="logo-symbol">
            <a class="logo-link" title="Quehry" href="./">
                <svg width="45px" height="45px" class="logo-symbol" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid meet" version="1.0" viewBox="0 0 649 649"><g fill="currentColor" stroke="none"><path d="M2938 6380 c-551 -53 -1031 -220 -1478 -514 -306 -201 -588 -470 -810 -771 -276 -374 -475 -850 -559 -1336 -64 -371 -53 -836 29 -1211 155 -712 552 -1347 1130 -1810 450 -360 1021 -589 1640 -659 58 -6 202 -12 320 -12 282 0 503 26 756 87 902 220 1680 841 2079 1661 303 622 402 1333 279 2005 -90 496 -313 984 -634 1390 -111 140 -378 404 -519 513 -456 352 -957 560 -1549 643 -126 17 -558 26 -684 14z m687 -220 c77 -11 189 -32 250 -46 127 -29 345 -94 345 -103 0 -4 -341 -188 -757 -409 l-757 -403 -41 22 c-22 13 -58 26 -79 29 l-40 5 -148 370 c-82 204 -150 377 -153 386 -10 29 325 119 570 153 169 24 174 24 430 21 183 -3 273 -8 380 -25z m-1292 -546 l157 -387 -24 -28 c-13 -16 -30 -44 -36 -64 -6 -19 -17 -35 -23 -35 -7 -1 -382 -54 -834 -119 -453 -64 -823 -115 -823 -111 0 3 43 62 95 132 269 358 584 630 980 844 90 49 321 153 341 154 6 0 81 -174 167 -386z m2026 347 l37 -20 129 -440 128 -440 -35 -39 c-20 -22 -37 -44 -39 -50 -3 -8 -290 13 -929 66 -775 65 -925 80 -928 92 -2 11 223 134 785 432 433 228 794 416 801 417 7 0 30 -8 51 -18z m204 -92 c420 -211 850 -600 1137 -1027 34 -51 60 -95 57 -98 -4 -4 -805 157 -859 172 -10 3 -18 12 -18 21 0 58 -78 133 -137 133 -18 0 -33 2 -33 5 0 3 -52 184 -116 403 -63 218 -117 405 -120 415 -3 9 -3 17 1 17 3 0 42 -19 88 -41z m-913 -894 c494 -41 904 -75 912 -75 7 -1 24 -23 36 -51 l24 -50 -169 -269 -168 -270 -44 0 c-24 0 -63 -9 -86 -21 l-42 -21 -679 402 c-373 222 -688 409 -699 416 -18 13 -18 14 -1 14 10 0 422 -34 916 -75z m-1175 18 l48 -48 -31 -225 c-18 -124 -60 -417 -94 -652 l-62 -428 -44 0 c-24 0 -51 -4 -61 -10 -16 -8 -146 87 -777 566 -418 317 -761 578 -763 580 -3 3 0 9 5 14 8 8 1633 247 1700 249 25 1 42 -9 79 -46z m915 -414 l690 -411 0 -38 0 -37 -147 -46 c-82 -26 -453 -148 -825 -271 l-676 -223 -22 30 -22 29 98 669 c82 557 101 669 114 669 8 0 31 9 50 19 19 11 38 20 43 20 4 1 318 -184 697 -410z m1959 175 c251 -53 457 -97 457 -98 9 -9 123 -247 153 -320 113 -272 189 -599 209 -901 l9 -130 -231 -273 c-224 -265 -232 -273 -264 -270 l-34 3 -418 1013 -419 1013 26 30 c15 16 33 29 41 29 8 0 220 -43 471 -96z m-593 2 c6 -5 804 -1949 802 -1952 -2 -2 -274 272 -605 609 l-600 612 13 32 c20 46 17 82 -9 128 l-23 41 170 271 171 272 40 -6 c22 -3 41 -6 41 -7z m-3341 -611 c402 -307 733 -561 737 -564 31 -26 -38 -31 -947 -65 -516 -19 -941 -33 -944 -30 -2 2 6 77 18 165 49 346 162 693 324 986 44 82 52 91 66 79 9 -7 345 -264 746 -571z m2710 -142 c17 -16 39 -32 48 -38 15 -9 13 -64 -23 -695 -22 -377 -40 -686 -40 -687 0 -2 -8 -3 -19 -3 -10 0 -30 -9 -45 -21 l-27 -21 -342 200 c-188 110 -542 317 -787 460 -245 144 -445 266 -445 273 0 8 312 116 820 285 451 150 822 272 824 273 2 1 18 -11 36 -26z m590 -425 c220 -222 500 -505 623 -629 l223 -225 -17 -31 -18 -30 -585 -82 c-322 -45 -602 -85 -621 -88 -31 -4 -37 -1 -53 27 -11 17 -34 39 -53 47 -19 9 -34 21 -34 27 -1 6 16 316 37 688 l37 678 26 9 c14 5 27 10 30 10 3 1 185 -180 405 -401z m-2587 -114 c-2 -9 -224 -178 -494 -375 l-492 -359 -38 16 c-26 11 -53 14 -84 10 -25 -3 -55 -7 -67 -8 -16 -2 -106 76 -362 316 -204 192 -341 327 -341 338 0 16 10 18 83 19 45 0 431 13 857 28 984 36 942 35 938 15z m59 -88 l22 -23 -104 -467 c-58 -256 -105 -468 -105 -470 0 -2 -14 -6 -32 -10 -17 -3 -47 -18 -65 -32 l-34 -26 -339 122 -338 122 -7 45 c-5 37 -3 47 12 58 10 7 229 169 488 359 258 190 472 346 475 346 3 0 15 -11 27 -24z m1013 -430 c421 -245 769 -452 775 -460 5 -8 7 -18 3 -21 -3 -4 -417 -41 -920 -84 l-913 -77 -16 26 c-9 15 -29 36 -45 47 l-28 21 104 465 105 465 43 7 c27 4 55 17 75 35 18 16 37 28 42 26 6 -2 354 -204 775 -450z m-2307 -276 c-7 -71 -4 -98 10 -126 l15 -30 -206 -248 -206 -248 -27 59 c-140 314 -228 733 -229 1086 l0 138 323 -303 c270 -254 322 -307 320 -328z m5277 366 c-17 -270 -58 -489 -135 -721 -38 -117 -134 -350 -140 -343 -1 2 -35 117 -75 256 l-72 253 26 25 c51 52 58 134 15 189 -21 26 -21 26 -2 48 11 12 102 120 203 240 100 120 184 216 186 214 2 -2 -1 -74 -6 -161z m-642 -487 l21 -41 -456 -782 c-251 -429 -461 -787 -467 -794 -9 -9 -19 -10 -42 -2 l-29 10 -174 676 -174 675 35 37 c20 20 40 51 45 67 l10 30 584 82 c321 44 594 82 605 82 16 1 27 -10 42 -40z m-4015 -145 c301 -109 327 -120 327 -142 0 -12 7 -39 15 -58 l15 -36 -411 -469 c-226 -259 -413 -468 -415 -466 -2 2 1 219 6 483 5 263 10 538 10 610 l0 132 29 12 c16 6 39 23 52 36 13 14 28 23 34 21 5 -2 157 -57 338 -123z m-517 -413 c-3 -262 -9 -527 -12 -589 -8 -134 -2 -135 -120 20 -93 121 -192 278 -272 428 l-61 115 215 257 c181 216 218 255 235 251 l22 -6 -7 -476z m4695 460 c3 -3 43 -133 88 -289 l82 -282 -62 -108 c-75 -130 -175 -278 -265 -392 -140 -176 -514 -528 -595 -558 -13 -5 -72 -13 -130 -17 l-105 -7 -16 40 -16 40 462 796 461 796 45 -6 c25 -4 48 -9 51 -13z m-1736 -69 c-6 -5 -361 -192 -790 -417 -429 -224 -806 -422 -838 -439 -52 -28 -59 -29 -70 -15 -17 22 -78 59 -99 59 -19 0 -16 -14 -68 300 -29 169 -38 246 -30 248 27 9 62 50 74 87 11 32 19 41 43 44 67 8 1683 139 1733 140 30 0 50 -3 45 -7z m90 -62 c10 -11 38 -26 62 -31 38 -10 44 -16 52 -48 90 -346 333 -1280 337 -1292 3 -11 -7 -23 -29 -36 -19 -11 -42 -38 -53 -60 l-19 -40 -1050 297 c-859 244 -1050 301 -1050 315 0 12 239 141 860 465 473 247 863 449 866 449 3 1 14 -8 24 -19z m-2076 -210 c13 0 21 -9 25 -27 10 -47 86 -497 86 -508 0 -5 -13 -19 -29 -29 -35 -24 -57 -59 -66 -108 -4 -22 -14 -39 -24 -42 -143 -41 -861 -236 -870 -236 -7 0 -11 5 -9 11 2 6 190 224 418 484 349 399 416 472 431 464 10 -5 27 -9 38 -9z m1376 -1065 c741 -208 1053 -300 1060 -312 6 -9 24 -32 41 -52 l31 -35 -30 -68 c-34 -77 -25 -71 -205 -139 -310 -116 -653 -180 -984 -183 -97 0 -178 1 -181 4 -89 106 -817 1053 -817 1063 0 18 9 27 23 21 7 -2 485 -137 1062 -299z m-1319 264 c31 -35 81 -51 134 -44 l46 6 380 -487 c208 -268 385 -496 392 -506 13 -17 10 -18 -45 -12 -640 65 -1267 346 -1732 776 l-61 57 42 12 c271 74 805 218 814 218 6 1 19 -9 30 -20z m2814 -563 c0 -5 -223 -138 -278 -165 -24 -13 -46 -20 -49 -18 -2 3 3 19 11 36 12 22 27 33 59 42 27 7 56 26 81 52 35 37 44 41 105 47 36 4 67 8 69 9 1 0 2 -1 2 -3z" transform="translate(0.000000,644.000000) scale(0.100000,-0.100000)"/></g></svg>
            </a>
        </div>
        <div class="copyright">
          <p>2022 &copy; Quehry</p>
        </div>
        <div class="footer-nav">
            <div>
                <a href="./archive.html">
                    Posts
                </a>
            </div>
            <div>
                <a href="./tags.html">
                    Tags
                </a>
            </div>
            <div>
                <a href="./about.html">
                    About
                </a>
            </div>
        </div>
    </div>
</footer> <!-- End Footer -->

</div>

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

<style>
  table {
  margin: auto;
  }
</style>
    <div class="top" title="Top">
      <svg aria-hidden="true" focusable="false" data-prefix="fal" data-icon="angle-up" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="svg-inline--fa fa-angle-up fa-w-8 fa-2x"><path fill="currentColor" d="M136.5 185.1l116 117.8c4.7 4.7 4.7 12.3 0 17l-7.1 7.1c-4.7 4.7-12.3 4.7-17 0L128 224.7 27.6 326.9c-4.7 4.7-12.3 4.7-17 0l-7.1-7.1c-4.7-4.7-4.7-12.3 0-17l116-117.8c4.7-4.6 12.3-4.6 17 .1z" class=""></path></svg>
    </div>
    




<!-- JS -->








<script>
(function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var logo = document.getElementById('logo');
    var nightModeOption = ('auto' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
    storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
    var data = storage.getItem('theme');
    try {
        data = JSON.parse(data ? data : '');
    } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
    }
    return data;
    }

    function handleThemeToggle(nightShift) {
    themeData.nightShift = nightShift;
    saveThemeData(themeData);
    html.dataset.theme = nightShift ? 'dark' : 'light';
    if (nightShift) {
        logo.setAttribute("src", "./assets/img/branding/MVM-logo-full-dark.svg");
    } else {
        logo.setAttribute("src", "./assets/img/branding/MVM-logo-full.svg");
    }
    setTimeout(function() {
        sw.checked = nightShift ? true : false;
    }, 50);
    }

    function autoThemeToggle() {
    // Next time point of theme toggle
    var now = new Date();
    var toggleAt = new Date();
    var hours = now.getHours();
    var nightShift = hours >= 19 || hours <=7;

    if (nightShift) {
        if (hours > 7) {
        toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
    } else {
        toggleAt.setHours(19);
    }

    toggleAt.setMinutes(0);
    toggleAt.setSeconds(0);
    toggleAt.setMilliseconds(0)

    var delay = toggleAt.getTime() - now.getTime();

    // auto toggle theme mode
    setTimeout(function() {
        handleThemeToggle(!nightShift);
    }, delay);

    return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
    };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
    handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
    var data = autoThemeToggle();

    // Toggle theme by local setting
    if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
    } else {
        handleThemeToggle(themeData.nightShift);
    }
    } else if (nightModeOption == 'manual') {
    handleThemeToggle(themeData.nightShift);
    } else {
    var nightShift = themeData.nightShift;
    if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
    }
    handleThemeToggle(nightShift);
    }
})();
</script>

<script src="./assets/js/jekyll-search.js"></script>
<script src="./assets/js/jquery-3.6.0.min.js"></script>







<script src="./assets/js/main.js"></script>
<script>
  SimpleJekyllSearch({
      searchInput: document.getElementById('search-input'),
      resultsContainer: document.getElementById('results-container'),
      json: './search.json',
      searchResultTemplate: '<li><a href="{url}" title="{description}">{title}</a><p>{description}</p></li>',
      noResultsText: 'No results found',
      fuzzy: false,
      exclude: ['Welcome']
    });
</script>




    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R8SZS2YBZK"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R8SZS2YBZK');
</script>
  </body>
</html>
