<!DOCTYPE html>
<html lang="en">
  




<head>
	<meta charset="utf-8">
	<title>短文本自动评估论文阅读整理 - Quehry</title>
	<link rel="canonical" href="http://localhost:4000/Automatic-Short-Answer-Grading.html">
	<meta name="description" content="read and arrange paper about automatic short answer grading(ASAG)">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:4000"},
  "headline": "短文本自动评估论文阅读整理",
  "abstract": "read and arrange paper about automatic short answer grading(ASAG)",
    "keywords": "notes, paper, NLP",
    "wordcount": "3396",
    "image": ["http://localhost:4000/assets/imgposts/20221010/1.jpg"],
  "datePublished": "2022-10-10 00:00:00 +0800",
  "dateModified": "2022-10-10 00:00:00 +0800",
  "author": {
    "@type": "Person",
    "name": "Quehry"},
  "publisher": {
    "@type":  "Organization",
    "logo": {
        "@type": "ImageObject",
        "encodingFormat": "image/png",
        "contentUrl": "http://localhost:4000/assets/img/branding/MVM-symbol-black.png",
        "url": "http://localhost:4000/assets/img/branding/MVM-symbol-black.png"},
    "name" : "Quehry"}
}
</script>
<!-- Open Graph data -->
<meta property="og:url" content="http://localhost:4000/Automatic-Short-Answer-Grading.html"/>
<meta property="og:type" content="article"/>
<meta property="og:title" content="短文本自动评估论文阅读整理"/>
<meta property="og:description" content="read and arrange paper about automatic short answer grading(ASAG)"/>
<meta property="og:image" content="http://localhost:4000/assets/imgposts/20221010/1.jpg"/>
<meta property="og:image:alt" content="短文本自动评估论文阅读整理"/>
<meta property="og:site_name" content="Quehry" />
<meta property="article:published_time" content="2022-10-10 00:00:00 +0800" />
<meta property="article:modified_time" content="2022-10-10 00:00:00 +0800" />
<meta property="article:tag" content="notes, paper, NLP" />
<meta property="fb:admins" content="ar.maybach" />
<!-- Schema.org markup for Google -->
<meta itemprop="name" content="短文本自动评估论文阅读整理">
<meta itemprop="description" content="read and arrange paper about automatic short answer grading(ASAG)">
<meta itemprop="image" content="http://localhost:4000/assets/imgposts/20221010/1.jpg">
<!-- Twitter Card data -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@QuehryS">
<meta name="twitter:title" content="短文本自动评估论文阅读整理">
<meta name="twitter:description" content="read and arrange paper about automatic short answer grading(ASAG)">
<meta name="twitter:creator" content="">
<meta data-rh="true" name="twitter:label1" content="Word count"/>
<meta data-rh="true" name="twitter:data1" content="3396"/>
<meta name="twitter:image:src" content="http://localhost:4000/assets/img/posts/20221010/1.jpg">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#311e3e">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#311e3e">
	<!-- Google Fonts -->
	<link rel="preconnect" href="https://fonts.gstatic.com" />
	<style>
/* latin */
@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 400;
  src: url(https://fonts.gstatic.com/s/lora/v17/0QIvMX1D_JOuMwr7I_FMl_E.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin */
@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 600;
  src: url(https://fonts.gstatic.com/s/lora/v17/0QIvMX1D_JOuMwr7I_FMl_E.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin */
@font-face {
  font-family: 'Source Sans Pro';
  font-style: normal;
  font-weight: 200;
  src: url(https://fonts.gstatic.com/s/sourcesanspro/v14/6xKydSBYKcSV-LCoeQqfX1RYOo3i94_wlxdu3cOWxw.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin */
@font-face {
  font-family: 'Source Sans Pro';
  font-style: normal;
  font-weight: 400;
  src: url(https://fonts.gstatic.com/s/sourcesanspro/v14/6xK3dSBYKcSV-LCoeQqfX1RYOo3qOK7lujVj9w.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin */
@font-face {
  font-family: 'Source Sans Pro';
  font-style: normal;
  font-weight: 700;
  src: url(https://fonts.gstatic.com/s/sourcesanspro/v14/6xKydSBYKcSV-LCoeQqfX1RYOo3ig4vwlxdu3cOWxw.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
	</style>
	<!-- <link href="https://fonts.googleapis.com/css?family=Lora:400,600|Source+Sans+Pro:200,400,700" rel="stylesheet"> -->
	<!-- Font Awesome -->
	<link rel="stylesheet" href="./assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="./assets/css/main.css">
	




<link rel="icon" href="./assets/img/favicon/favicon.ico" type="image/x-icon">
<link rel="apple-touch-icon" href="./assets/img/favicon/favicon.ico">
<link rel="apple-touch-icon" sizes="72x72" href="./assets/img/favicon/favicon.ico">
<link rel="apple-touch-icon" sizes="114x114" href="./assets/img/favicon/favicon.ico">
	
	<link rel="stylesheet" href="./assets/css/highlighter/syntax-base16.monokai.dark.css">
	
<head>
  <body>
    




<section class="hidden">
  <div class="post">
      <a  class="post-list-title" href="./Automatic-Short-Answer-Grading.html">短文本自动评估论文阅读整理</a>
      

  <span class = "post-card-meta">
  
  
    <span class="meta-pre"></span>
  
  
    
      
      <span class="page_meta-date">
        <time datetime="2022-10-10T00:00:00+08:00">October 10, 2022</time>
      </span>
    
    
      <span class="meta-sep"></span>
    
  
  
    
    
    <span class="page_meta-readtime">
      
        16 minute read
      
    </span>
  
  
  </span>

        <div class="post-excerpt">
            <!-- TOC --><ul> <li><a href="#1-论文简介">1. 论文简介</a></li> <li><a href="#2-automatic-short-answer-grading-via-bert-based-deep-neural-networks">2. Automatic Short-Answer Grading via BERT-Based Deep Neural Networks</a> <ul> <li><a href="#21-abstract">2.1. Abstract</a></li> <li><a href="#22-introduction">2.2. Introduction</a></li> <li><a href="#23-related-work">2.3. Related Work</a> <ul> <li><a href="#231-applications-of-deep-learning-in-asag-tasks">2.3.1. Applications of Deep Learning in ASAG Tasks</a></li> <li><a href="#232-bert-model-and-its-application-in-education">2.3.2. BERT Model and Its Application in Education</a></li> </ul> </li> <li><a href="#24-methodology">2.4. Methodology</a> <ul> <li><a href="#241-task-definition">2.4.1. Task Definition</a></li> <li><a href="#242-model">2.4.2. Model</a> <ul> <li><a href="#2421-bert-layer">2.4.2.1. BERT layer</a></li> <li><a href="#2422-semantic-refinement-layer">2.4.2.2. Semantic Refinement Layer</a></li> <li><a href="#2423-semantic-fusion-layer">2.4.2.3. Semantic Fusion Layer</a></li> <li><a href="#2424-prediction-layer">2.4.2.4. Prediction Layer</a></li> <li><a href="#2425-loss-function">2.4.2.5. Loss Function</a></li> </ul> </li> </ul> </li> <li><a href="#25-experiments">2.5. Experiments</a> <ul> <li><a href="#251-datasets">2.5.1. Datasets</a></li> <li><a href="#252-experimental-settings">2.5.2. Experimental Settings</a></li> <li><a href="#253-ablation-studies">2.5.3. Ablation...<a class="read-more" href="./Automatic-Short-Answer-Grading.html"> read more</a>
        </div>
  </div>
</section>
<div class="flex-container transparent">
  




<header class="main-header">
  <div class="wrapper">
    <div class="header-flex">
      <div class="menu-icon-container">
        <span class="menu-icon"><i class="fa fa-bars" aria-hidden="true"></i></span>
      </div>
      <nav class="main-nav">
        <span class="menu-icon-close"><i class="fa fa-times" aria-hidden="true"></i></span>
        <ul>
          <li>
            <div class="theme-toggle night">
    <input class="night" type="checkbox" id="theme-switch">
    <label class="night" for="theme-switch">
        <div class="toggle night"></div>
        <div class="names night">             
        <p class="light night"><svg class="night" width="20" viewBox="0 0 25 20" fill="currentColor" xmlns="http://www.w3.org/2000/svg">
            <path class="night" d="M12.5 2.49871C11.3401 2.50016 10.2282 2.96156 9.40801 3.78171C8.58785 4.60187 8.12645 5.71383 8.125 6.87371C8.125 7.03947 8.19085 7.19844 8.30806 7.31565C8.42527 7.43286 8.58424 7.49871 8.75 7.49871C8.91576 7.49871 9.07473 7.43286 9.19194 7.31565C9.30915 7.19844 9.375 7.03947 9.375 6.87371C9.37593 6.04519 9.70547 5.25088 10.2913 4.66503C10.8772 4.07918 11.6715 3.74964 12.5 3.74871C12.6658 3.74871 12.8247 3.68286 12.9419 3.56565C13.0592 3.44844 13.125 3.28947 13.125 3.12371C13.125 2.95795 13.0592 2.79898 12.9419 2.68177C12.8247 2.56456 12.6658 2.49871 12.5 2.49871V2.49871ZM12.5 -0.00129131C8.47891 -0.00129131 5.62031 3.26238 5.625 6.88269C5.62487 8.54403 6.22974 10.1486 7.32656 11.3964C8.32891 12.5378 9.29062 14.4007 9.375 14.9987L9.37734 17.9358C9.37744 18.0587 9.41402 18.1787 9.48242 18.2807L10.4395 19.7198C10.4964 19.8055 10.5737 19.8758 10.6644 19.9245C10.7551 19.9731 10.8564 19.9986 10.9594 19.9987H14.0395C14.1426 19.9988 14.2441 19.9734 14.3351 19.9247C14.426 19.8761 14.5034 19.8057 14.5605 19.7198L15.5176 18.28C15.5854 18.1776 15.6219 18.0578 15.6227 17.935L15.625 14.9987C15.7129 14.3846 16.6797 12.5303 17.6734 11.3964C18.5434 10.4028 19.1087 9.17963 19.3015 7.87318C19.4944 6.56673 19.3066 5.23238 18.7608 4.02985C18.215 2.82732 17.3342 1.80757 16.2238 1.09264C15.1135 0.377721 13.8206 -0.00207746 12.5 -0.00129131V-0.00129131ZM14.3727 17.7452L13.7047 18.7487H11.2937L10.6273 17.7452V17.4987H14.3738L14.3727 17.7452ZM14.375 16.2487H10.625L10.6227 14.9987H14.375V16.2487ZM16.7348 10.5725C16.1879 11.1956 15.316 12.4511 14.7594 13.7479H10.243C9.68516 12.4507 8.81328 11.1956 8.26641 10.5725C7.36971 9.5491 6.87599 8.2344 6.87734 6.87371C6.87031 3.8659 9.23594 1.24871 12.5 1.24871C15.602 1.24871 18.125 3.77176 18.125 6.87371C18.1249 8.23456 17.6305 9.54904 16.7336 10.5725H16.7348ZM3.75 6.87371C3.75 6.70795 3.68415 6.54898 3.56694 6.43177C3.44973 6.31456 3.29076 6.24871 3.125 6.24871H0.625C0.45924 6.24871 0.300269 6.31456 0.183058 6.43177C0.065848 6.54898 0 6.70795 0 6.87371C0 7.03947 0.065848 7.19844 0.183058 7.31565C0.300269 7.43286 0.45924 7.49871 0.625 7.49871H3.125C3.29076 7.49871 3.44973 7.43286 3.56694 7.31565C3.68415 7.19844 3.75 7.03947 3.75 6.87371ZM20.625 2.49871C20.7221 2.49849 20.8178 2.4759 20.9047 2.43269L23.4047 1.18269C23.5529 1.10852 23.6657 0.978483 23.718 0.821201C23.7704 0.66392 23.7582 0.492273 23.684 0.344021C23.6473 0.270614 23.5964 0.205161 23.5344 0.151397C23.4724 0.0976336 23.4004 0.0566132 23.3225 0.0306781C23.1652 -0.0217002 22.9936 -0.00945342 22.8453 0.0647243L20.3453 1.31472C20.2194 1.37771 20.1184 1.48136 20.0588 1.60889C19.9991 1.73643 19.9843 1.88037 20.0166 2.0174C20.049 2.15442 20.1267 2.2765 20.2371 2.36386C20.3475 2.45122 20.4842 2.49873 20.625 2.49871ZM24.375 6.24871H21.875C21.7092 6.24871 21.5503 6.31456 21.4331 6.43177C21.3158 6.54898 21.25 6.70795 21.25 6.87371C21.25 7.03947 21.3158 7.19844 21.4331 7.31565C21.5503 7.43286 21.7092 7.49871 21.875 7.49871H24.375C24.5408 7.49871 24.6997 7.43286 24.8169 7.31565C24.9342 7.19844 25 7.03947 25 6.87371C25 6.70795 24.9342 6.54898 24.8169 6.43177C24.6997 6.31456 24.5408 6.24871 24.375 6.24871ZM4.65469 1.31472L2.15469 0.0647243C2.08128 0.0279952 2.00136 0.00608435 1.91948 0.00024269C1.83761 -0.00559897 1.75539 0.004743 1.67751 0.0306781C1.52023 0.0830564 1.39019 0.195769 1.31602 0.344021C1.24184 0.492273 1.22959 0.66392 1.28197 0.821201C1.33435 0.978483 1.44706 1.10852 1.59531 1.18269L4.09531 2.43269C4.18223 2.4759 4.27794 2.49849 4.375 2.49871C4.5158 2.49873 4.65248 2.45122 4.7629 2.36386C4.87332 2.2765 4.951 2.15442 4.98337 2.0174C5.01574 1.88037 5.0009 1.73643 4.94124 1.60889C4.88158 1.48136 4.78061 1.37771 4.65469 1.31472ZM23.4047 12.5647L20.9047 11.3147C20.7564 11.2405 20.5847 11.2283 20.4274 11.2807C20.2701 11.3332 20.14 11.4459 20.0658 11.5942C19.9916 11.7425 19.9794 11.9142 20.0318 12.0715C20.0842 12.2289 20.197 12.3589 20.3453 12.4331L22.8453 13.6831C22.9936 13.7573 23.1653 13.7695 23.3226 13.7171C23.4799 13.6647 23.61 13.5519 23.6842 13.4036C23.7584 13.2553 23.7706 13.0836 23.7182 12.9263C23.6658 12.769 23.553 12.6389 23.4047 12.5647V12.5647ZM4.375 11.2487C4.27794 11.2489 4.18223 11.2715 4.09531 11.3147L1.59531 12.5647C1.44701 12.6389 1.33425 12.769 1.28183 12.9263C1.25588 13.0042 1.24552 13.0864 1.25135 13.1683C1.25719 13.2502 1.27909 13.3302 1.31582 13.4036C1.35255 13.477 1.40338 13.5425 1.46542 13.5963C1.52745 13.6501 1.59947 13.6911 1.67737 13.7171C1.83469 13.7695 2.00638 13.7573 2.15469 13.6831L4.65469 12.4331C4.78083 12.3702 4.88202 12.2666 4.94183 12.1389C5.00164 12.0113 5.01656 11.8672 4.98417 11.7301C4.95178 11.5929 4.87397 11.4707 4.76339 11.3833C4.65281 11.2959 4.51594 11.2485 4.375 11.2487V11.2487Z" /></svg></p>
        <p class="dark night"><svg class="night" width="20" viewBox="0 0 25 21" fill="currentColor" xmlns="http://www.w3.org/2000/svg">
            <path class="night" d="M6.39614 3.72646C7.50591 1.56178 9.72622 0.00900831 12.4782 0.00114817C13.8006 -0.00388798 15.0965 0.375153 16.2101 1.09278C17.3237 1.8104 18.2079 2.83612 18.7564 4.04682C19.3049 5.25751 19.4945 6.60175 19.3024 7.91818C19.1103 9.23461 18.5447 10.4673 17.6735 11.4683C17.5227 11.6416 17.3516 11.859 17.1739 12.1069L9.47856 6.12184C9.65443 5.45016 10.046 4.85578 10.5924 4.43118C11.1387 4.00657 11.8093 3.77554 12.4997 3.77401C12.6654 3.77401 12.8244 3.70776 12.9416 3.58984C13.0588 3.47191 13.1247 3.31197 13.1247 3.1452C13.1247 2.97843 13.0588 2.81849 12.9416 2.70057C12.8244 2.58264 12.6654 2.51639 12.4997 2.51639C11.6212 2.5173 10.7634 2.78383 10.0374 3.28141C9.31146 3.77899 8.75092 4.48463 8.42856 5.30674L6.39614 3.72646ZM6.39614 10.0841C6.64968 10.5817 6.96225 11.0465 7.327 11.4683C7.97231 12.2091 8.98169 13.7568 9.36645 15.0624C9.36645 15.0726 9.36919 15.0828 9.37075 15.093H12.8372L6.39614 10.0841ZM9.37466 16.3502V17.8574C9.37584 18.1045 9.44934 18.3458 9.58599 18.5511L10.2536 19.5607C10.3675 19.7335 10.5221 19.8753 10.7037 19.9734C10.8852 20.0715 11.0881 20.1229 11.2942 20.1231H13.7047C13.9107 20.1231 14.1135 20.0718 14.2951 19.9739C14.4766 19.876 14.6313 19.7345 14.7454 19.5619L15.4129 18.5511C15.5492 18.3451 15.622 18.1033 15.6223 17.8558V17.2581L14.4528 16.3502H9.37466Z"/>
            <path class="night" d="M0.131556 1.2363L0.898352 0.243172C0.948738 0.177883 1.01142 0.123229 1.08282 0.0823368C1.15423 0.0414448 1.23294 0.0151171 1.31446 0.00486006C1.39598 -0.00539702 1.47872 0.000617709 1.55793 0.0225602C1.63714 0.0445026 1.71127 0.0819422 1.77609 0.132737L24.7585 18.0039C24.8894 18.1062 24.9745 18.2567 24.9952 18.4221C25.0158 18.5876 24.9703 18.7545 24.8687 18.8862L24.1015 19.8794C24.0511 19.9446 23.9884 19.9992 23.917 20.0401C23.8457 20.0809 23.767 20.1072 23.6855 20.1175C23.6041 20.1277 23.5214 20.1217 23.4422 20.0998C23.363 20.0779 23.2889 20.0405 23.2241 19.9898L0.241322 2.1186C0.110489 2.01624 0.0254259 1.86578 0.00484145 1.70032C-0.015743 1.53486 0.0298368 1.36795 0.131556 1.2363V1.2363Z"/>
            </svg></p>
        </div>
    </label>
</div>
          </li>
          <li>
            <a href="./">
              <div class="left">
                Home
              </div>  
              <div class="right">
                <svg width="24px" aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><g><rect x="83.534" y="40.929" width="3.997" height="20.071"/></g><path d="M16.466,41.931l33.548-25.123L92.81,48.877l2.396-3.198L50.015,11.814L4.794,45.679l2.396,3.199l5.279-3.954v42.763h75.062  V61h-3.997v22.69H64.598V54.068H35.402V83.69H16.466V41.931z M39.399,58.065h21.202V83.69H39.399V58.065z"/></svg>
              </div>
            </a>
          </li>
          <li>
            <a href="./archive.html">
              <div class="left">
                All Posts
              </div>
              <div class="right">
                <svg width="24px" aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="-3 3 64 64"><g><path d="M60.992,31.985c0-15.979-13-28.978-28.979-28.978c-15.994,0-29.006,12.999-29.006,28.978   c0,15.994,13.012,29.007,29.006,29.007v-2c-14.891,0-27.006-12.115-27.006-27.007c0-14.875,12.115-26.978,27.006-26.978   c14.876,0,26.979,12.103,26.979,26.978c0,8.945-4.479,17.329-11.804,22.338l0.874-10.062l-1.992-0.174l-1.135,13.071l13.042,1.136   l0.174-1.992l-9.183-0.799C56.443,50.079,60.992,41.321,60.992,31.985z"/><polygon points="33.014,12.682 31.014,12.682 31.014,32.398 39.811,41.224 41.227,39.812 33.014,31.572  "/></g></svg>
              </div>
            </a>
          </li>
          <li>
            <a href="./tags.html">
              <div class="left">
                Tags
              </div>
              <div class="right">
                <svg width="24px" aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><g><path d="M75.244,15.066c-2.59,0-5.027,1.012-6.857,2.843c-3.781,3.785-3.778,9.94,0.002,13.724    c1.831,1.833,4.266,2.843,6.857,2.843s5.026-1.01,6.861-2.843c3.781-3.785,3.781-9.943-0.002-13.724    C80.275,16.076,77.838,15.066,75.244,15.066z M78.766,28.252c-1.871,1.869-5.129,1.869-6.996,0c-1.929-1.931-1.931-5.069-0.002-7    c0.934-0.934,2.175-1.448,3.498-1.448c1.322,0,2.564,0.515,3.5,1.448C80.691,23.183,80.691,26.321,78.766,28.252z M94.632,41.027    l0.005-28.872c0-3.745-3.05-6.792-6.792-6.792L58.973,5.368l-1.237-0.004c-1.893,0-4.75,0-6.617,1.869L7.008,51.342    c-1.06,1.059-1.645,2.467-1.645,3.966s0.583,2.908,1.644,3.968l33.717,33.717c1.058,1.06,2.467,1.645,3.966,1.645    s2.908-0.585,3.968-1.645l44.106-44.111c1.893-1.886,1.88-4.604,1.869-7.227L94.632,41.027z M90.022,46.139L45.913,90.25    c-0.654,0.65-1.792,0.652-2.443,0L9.752,56.532c-0.328-0.327-0.507-0.762-0.507-1.225c0-0.462,0.18-0.894,0.507-1.221    L53.861,9.976c0.676-0.674,2.284-0.731,3.874-0.731l1.237,0.004l28.872-0.004c1.604,0,2.909,1.306,2.909,2.911l-0.005,28.872    l0.005,0.642C90.76,43.585,90.769,45.392,90.022,46.139z"/></g></svg>
              </div>
            </a>
          </li>
          <li>
            <a href="./about.html">
              <div class="left">
                About
              </div>
              <div class="right">
                <svg width='24px' aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 846.66 846.66"><g><path d="M351.26 453.22c-276.42,134.06 -224.86,336.22 -224.73,336.8 6.03,25.41 -32.58,34.56 -38.6,9.15 -0.15,-0.65 -55.78,-219.32 218.87,-367.66 -60.98,-39 -100.02,-106.82 -100.02,-182.56 0,-119.6 96.95,-216.55 216.55,-216.55 119.6,0 216.55,96.95 216.55,216.55 0,75.74 -39.04,143.56 -100.02,182.56 274.65,148.34 219.02,367.01 218.87,367.66 -6.02,25.41 -44.63,16.26 -38.6,-9.15 0.13,-0.58 51.69,-202.74 -224.73,-336.8 -22.55,7.96 -46.8,12.29 -72.07,12.29 -25.27,0 -49.52,-4.33 -72.07,-12.29zm72.07 -381.14c-97.68,0 -176.87,79.19 -176.87,176.87 0,97.69 79.19,176.87 176.87,176.87 97.68,0 176.87,-79.18 176.87,-176.87 0,-97.68 -79.19,-176.87 -176.87,-176.87z"/></g></svg>
              </div>
            </a>
          </li>
          <li>
            <a href="./feed.xml">
              <div class="left">
                Atom feed
              </div>
              <div class="right">
                <svg width='24px' aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M80 352c26.467 0 48 21.533 48 48s-21.533 48-48 48-48-21.533-48-48 21.533-48 48-48m0-32c-44.183 0-80 35.817-80 80s35.817 80 80 80 80-35.817 80-80-35.817-80-80-80zm367.996 147.615c-6.448-237.848-198.06-429.164-435.61-435.61C5.609 31.821 0 37.229 0 44.007v8.006c0 6.482 5.146 11.816 11.626 11.994 220.81 6.05 398.319 183.913 404.367 404.367.178 6.48 5.512 11.626 11.994 11.626h8.007c6.778 0 12.185-5.609 12.002-12.385zm-144.245-.05c-6.347-158.132-133.207-284.97-291.316-291.316C5.643 175.976 0 181.45 0 188.247v8.005c0 6.459 5.114 11.72 11.567 11.989 141.134 5.891 254.301 119.079 260.192 260.192.269 6.453 5.531 11.567 11.989 11.567h8.005c6.798 0 12.271-5.643 11.998-12.435z"></path></svg>
              </div>
            </a>
          </li>
        </ul>
      </nav>
      
      
      <div class="logo"><a href="./"><img class="logo" id="logo" src="./assets/img/branding/MVM-logo-full.svg" alt="Quehry"></a></div>
      <div class="search-icon-container">
        <span class="search-icon"><a><i class="fa fa-search" aria-hidden="true"></i></a></span>
      </div>
    </div>
  </div>
</header> <!-- End Header -->

  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="短文本自动评估论文阅读整理">
<meta itemprop="description" content="read and arrange paper about automatic short answer grading(ASAG)">
<meta itemprop="datePublished" content="2022-10-10T00:00:00+08:00">

    <div class="page-image">
      <div class="cover-image" style="background: url('./assets/img/posts/20221010/1.jpg') center no-repeat; background-size: cover;"></div>
    </div>
    <div class="wrapper">
      <div class="page-content">
        <div class="header-page">
          <h1 class="page-title">短文本自动评估论文阅读整理</h1>
          

  <span class = "post-page-meta">
  
    <p class="page_meta">
  
  
  
    
      
      <span class="page_meta-date">
        <time datetime="2022-10-10T00:00:00+08:00">October 10, 2022</time>
      </span>
    
    
      <span class="meta-sep"></span>
    
  
  
    
    
    <span class="page_meta-readtime">
      
        16 minute read
      
    </span>
  
  
    </p>
  
  </span>

        </div>
        <aside class="sidebar side" id="sidebar">
    



<div class="tag-cloud">
    
        <ul class="tags side">
            
                <li><a href="./tag.html?tag=notes" class="tag side">notes</a></li>
            
                <li><a href="./tag.html?tag=paper" class="tag side">paper</a></li>
            
                <li><a href="./tag.html?tag=NLP" class="tag side">NLP</a></li>
            
    
        </ul>
</div>
    <div class="share-options side">
    <div class="share-hover side">
        <span class="share-button side"><svg fill="currentColor" width="25" height="25" class="side"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></span>
        <div class="share-icons side" id="sidebar-icons">
            <a class="twitter" href="https://twitter.com/intent/tweet?text=短文本自动评估论文阅读整理&url=http://localhost:4000/Automatic-Short-Answer-Grading.html" title="Share on Twitter" rel="nofollow" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
            <a class="facebook" href="https://facebook.com/sharer.php?u=http://localhost:4000/Automatic-Short-Answer-Grading.html" title="Share on Facebook" rel="nofollow" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
            <a class="reddit" href="http://www.reddit.com/submit?url=http://localhost:4000/Automatic-Short-Answer-Grading.html&title=短文本自动评估论文阅读整理" title="Submit to Reddit" rel="nofollow" target="_blank"><i class="fa fa-reddit" aria-hidden="true"></i></a>
            <a class="linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/Automatic-Short-Answer-Grading.html&title=短文本自动评估论文阅读整理&summary=read and arrange paper about automatic short answer grading(ASAG)&source=http://localhost:4000/Automatic-Short-Answer-Grading.html" title="Share on LinkedIn" rel="nofollow" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
            <a class="email" href="mailto:?subject=短文本自动评估论文阅读整理&body=read and arrange paper about automatic short answer grading(ASAG)%0A%0ARead more here: http://localhost:4000/Automatic-Short-Answer-Grading.html" title="Share via e-mail" rel="nofollow" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
            <a class="copy-link" onclick="copyToClipboard()" title="Copy to clipboard" rel="nofollow" target="_blank"><svg width="20px" fill="currentColor" class="side" viewBox="0 0 18 18"><path d="M16.94 1.1A3.7 3.7 0 0 0 14.3 0c-1 0-1.94.39-2.64 1.1L7.43 5.3c-.91.92-2.09 3.2 0 5.27a.75.75 0 0 0 .82.16c.09-.03.17-.09.24-.15a.74.74 0 0 0 0-1.06c-1.16-1.15-.77-2.39-.02-3.16l4.24-4.22a2.2 2.2 0 0 1 1.58-.65c.6 0 1.16.23 1.58.65.86.87.86 2.29 0 3.16L12.7 8.47a.74.74 0 0 0 1.04 1.05l3.17-3.16a3.73 3.73 0 0 0 0-5.27h.03zM9.54 7.4a.74.74 0 0 0 0 1.06c1.16 1.15.76 2.39 0 3.16l-4.22 4.22c-.42.42-.99.65-1.59.65a2.23 2.23 0 0 1-1.58-3.82l3.17-3.16A.73.73 0 0 0 5.54 9a.78.78 0 0 0-.22-.52.77.77 0 0 0-1.05 0L1.1 11.64A3.72 3.72 0 0 0 3.74 18c1 0 1.94-.39 2.65-1.1l4.23-4.2c.21-.22.94-1.02 1.13-2.2.18-1.12-.2-2.15-1.12-3.07-.27-.27-.78-.27-1.06 0l-.02-.02z" clip-rule="evenodd" fill-rule="evenodd"></path></svg></a>
        </div>
    </div>
    <div class='alert' style='font-size:.6em;color:var(--accent);text-align:center;'></div>
</div>
</aside>

        <aside class="toc">
  <nav class="toc-nav">
    <li class="toc-title">
      <svg aria-hidden="true" focusable="false" data-prefix="fad" data-icon="align-left" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="svg-inline--fa fa-align-left fa-w-14 fa-2x"><g class="fa-group"><path fill="currentColor" d="M12.83 352h262.34A12.82 12.82 0 0 0 288 339.17v-38.34A12.82 12.82 0 0 0 275.17 288H12.83A12.82 12.82 0 0 0 0 300.83v38.34A12.82 12.82 0 0 0 12.83 352zm0-256h262.34A12.82 12.82 0 0 0 288 83.17V44.83A12.82 12.82 0 0 0 275.17 32H12.83A12.82 12.82 0 0 0 0 44.83v38.34A12.82 12.82 0 0 0 12.83 96z" class="fa-secondary"></path><path fill="currentColor" d="M432 160H16a16 16 0 0 0-16 16v32a16 16 0 0 0 16 16h416a16 16 0 0 0 16-16v-32a16 16 0 0 0-16-16zm0 256H16a16 16 0 0 0-16 16v32a16 16 0 0 0 16 16h416a16 16 0 0 0 16-16v-32a16 16 0 0 0-16-16z" class="fa-primary"></path></g></svg>
    </li>
    <ul class="toc-content" id="toc-content"><li class="toc-item-1"><a href="#1-论文简介">1. 论文简介</a></li><li class="toc-item-1"><a href="#2-automatic-short-answer-grading-via-bert-based-deep-neural-networks">2. Automatic Short-Answer Grading via BERT-Based Deep Neural Networks</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#21-abstract">2.1. Abstract</a></li><li class="toc-item-2"><a href="#22-introduction">2.2. Introduction</a></li><li class="toc-item-2"><a href="#23-related-work">2.3. Related Work</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#231-applications-of-deep-learning-in-asag-tasks">2.3.1. Applications of Deep Learning in ASAG Tasks</a></li><li class="toc-item-3"><a href="#232-bert-model-and-its-application-in-education">2.3.2. BERT Model and Its Application in Education</a></li></ul></li><li class="toc-item-2"><a href="#24-methodology">2.4. Methodology</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#241-task-definition">2.4.1. Task Definition</a></li><li class="toc-item-3"><a href="#242-model">2.4.2. Model</a><ul class="toc-submenu-3"><li class="toc-item-4"><a href="#2421-bert-layer">2.4.2.1. BERT layer</a></li><li class="toc-item-4"><a href="#2422-semantic-refinement-layer">2.4.2.2. Semantic Refinement Layer</a></li><li class="toc-item-4"><a href="#2423-semantic-fusion-layer">2.4.2.3. Semantic Fusion Layer</a></li><li class="toc-item-4"><a href="#2424-prediction-layer">2.4.2.4. Prediction Layer</a></li><li class="toc-item-4"><a href="#2425-loss-function">2.4.2.5. Loss Function</a></li></ul></li></ul></li><li class="toc-item-2"><a href="#25-experiments">2.5. Experiments</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#251-datasets">2.5.1. Datasets</a></li><li class="toc-item-3"><a href="#252-experimental-settings">2.5.2. Experimental Settings</a></li><li class="toc-item-3"><a href="#253-ablation-studies">2.5.3. Ablation Studies</a></li><li class="toc-item-3"><a href="#254-comparison-with-baseline-systems">2.5.4. Comparison With Baseline Systems</a></li></ul></li><li class="toc-item-2"><a href="#26-discussions">2.6. Discussions</a></li><li class="toc-item-2"><a href="#27-conclusion">2.7. Conclusion</a></li></ul></li><li class="toc-item-1"><a href="#3-leveraging-semantic-facets-for-automatic-assessment-of-short-free-text-answers">3. Leveraging Semantic Facets for Automatic Assessment of Short Free Text Answers</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#31-abstract">3.1. Abstract</a></li><li class="toc-item-2"><a href="#32-introduction">3.2. Introduction</a></li><li class="toc-item-2"><a href="#33-related-works">3.3. Related Works</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#331-automated-response-evaluation">3.3.1. Automated response evaluation</a></li><li class="toc-item-3"><a href="#332-semantic-similarity-measurement">3.3.2. Semantic similarity measurement</a></li></ul></li><li class="toc-item-2"><a href="#34-patterns-and-indicative-powers-of-facets-matching-states">3.4. Patterns and indicative powers of facets matching states</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#341-materials-and-methods">3.4.1. Materials and methods</a><ul class="toc-submenu-3"><li class="toc-item-4"><a href="#3411-dataset">3.4.1.1. Dataset</a></li><li class="toc-item-4"><a href="#3412-summary-of-facet-matching-states">3.4.1.2. Summary of facet matching states</a></li><li class="toc-item-4"><a href="#3413-answer-quality-prediction-with-facet-matching-states">3.4.1.3. Answer quality prediction with facet matching states</a></li></ul></li><li class="toc-item-3"><a href="#342-results-and-analysis">3.4.2. Results and analysis</a><ul class="toc-submenu-3"><li class="toc-item-4"><a href="#3421-facet-matching-pattern">3.4.2.1. Facet matching pattern</a></li><li class="toc-item-4"><a href="#3422-answer-evaluation-leveraging-facet-matching-states">3.4.2.2. Answer evaluation leveraging facet matching states</a></li></ul></li></ul></li><li class="toc-item-2"><a href="#35-automatic-extraction-of-facets-matching-features-for-better-prediciton">3.5. Automatic Extraction of Facets Matching Features For Better Prediciton</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#351-materials-and-methods">3.5.1. Materials and methods</a><ul class="toc-submenu-3"><li class="toc-item-4"><a href="#3511-dataset">3.5.1.1. Dataset</a></li><li class="toc-item-4"><a href="#3512-automatic-semantic-facet-extraction">3.5.1.2. Automatic semantic facet extraction</a></li><li class="toc-item-4"><a href="#3513-facet-matching-features">3.5.1.3. Facet matching features</a></li><li class="toc-item-4"><a href="#3514-semantic-closeness-features">3.5.1.4. Semantic closeness features</a></li></ul></li><li class="toc-item-3"><a href="#352-results-and-analysis">3.5.2. Results and analysis</a></li></ul></li><li class="toc-item-2"><a href="#36-discussion">3.6. Discussion</a></li><li class="toc-item-2"><a href="#37-conclusion">3.7. Conclusion</a></li><li class="toc-item-2"><a href="#38-小结">3.8. 小结</a></li></ul></li><li class="toc-item-1"><a href="#4-text-to-text-semantic-similarity-for-automatic-short-answer-grading">4. Text-to-Text Semantic Similarity for Automatic Short Answer Grading</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#41-knowledge-based-measures">4.1. Knowledge-based Measures</a></li><li class="toc-item-2"><a href="#42-corpus-based-measures">4.2. Corpus-Based Measures</a></li><li class="toc-item-2"><a href="#43-experiment">4.3. Experiment</a></li><li class="toc-item-2"><a href="#44-lsalatent-semantic-analysis">4.4. LSA(Latent semantic analysis)</a></li></ul></li><li class="toc-item-1"><a href="#5-pre-training-bert-on-domain-for-asag">5. Pre-Training Bert on Domain for ASAG</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#51-usage-of-textbooks">5.1. Usage of Textbooks</a></li><li class="toc-item-2"><a href="#52-usage-of-question-answer-pairs">5.2. Usage of Question-Answer Pairs</a></li><li class="toc-item-2"><a href="#53-微调asag">5.3. 微调ASAG</a></li><li class="toc-item-2"><a href="#54-experiments">5.4. Experiments</a></li></ul></li><li class="toc-item-1"><a href="#6-imporving-short-answer-grading-using-transformer-based-pre-training">6. Imporving Short Answer Grading Using Transformer-Based Pre-training</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#61-数据集">6.1. 数据集</a></li><li class="toc-item-2"><a href="#62-实验">6.2. 实验</a></li></ul></li><li class="toc-item-1"><a href="#7-investigating-transformers-for-automatic-short-answer-grading">7. Investigating Transformers for Automatic Short Answer Grading</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#71-实验">7.1. 实验</a></li><li class="toc-item-2"><a href="#72-结果分析">7.2. 结果分析</a></li></ul></li><li class="toc-item-1"><a href="#8-superlative-model-using-word-cloud-for-short-answers-evaluation-in-elearning">8. Superlative model using word cloud for short answers evaluation in eLearning</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#81-superlative-model">8.1. Superlative Model</a></li><li class="toc-item-2"><a href="#82-词云-word-cloud">8.2. 词云 word cloud</a></li><li class="toc-item-2"><a href="#83-wordnet">8.3. WordNet</a></li></ul></li><li class="toc-item-1"><a href="#9sentence-level-or-token-level-features-for-automatic-short-answer-grading-use-both">9.Sentence Level or Token Level Features for Automatic Short Answer Grading?: Use Both</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#91-proposed-features">9.1. Proposed Features</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#911-sentence-level-features">9.1.1. Sentence Level Features</a></li><li class="toc-item-3"><a href="#912-token-level-features">9.1.2. Token Level Features</a></li></ul></li><li class="toc-item-2"><a href="#92-token-level的消融实验结果">9.2. Token level的消融实验结果</a></li></ul></li><li class="toc-item-1"><a href="#10-an-experimental-study-of-text-preprocessing-techniques-for-asag-in-indonesian">10. An Experimental Study of Text Preprocessing Techniques for ASAG in Indonesian</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#101-introduction">10.1. Introduction</a></li><li class="toc-item-2"><a href="#102-预处理方法">10.2. 预处理方法</a></li><li class="toc-item-2"><a href="#103-research-method">10.3. Research Method</a></li><li class="toc-item-2"><a href="#104-实验结果">10.4. 实验结果</a></li></ul></li><li class="toc-item-1"><a href="#11-feature-engineering-and-ensemble-based-approach-for-improving-automatic-short-answer-grading-performance">11. Feature engineering and ensemble-based approach for improving automatic short-answer grading performance</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#111-introduction">11.1. Introduction</a></li><li class="toc-item-2"><a href="#112-problem-definition">11.2. Problem Definition</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#1121-asag-as-regression-task">11.2.1. ASAG as regression task</a></li><li class="toc-item-3"><a href="#1122-asag-as-classification-task">11.2.2. ASAG as classification task</a></li></ul></li><li class="toc-item-2"><a href="#113-feature-extraction">11.3. Feature extraction</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#1131-semantic-similarity-features">11.3.1. Semantic Similarity Features</a></li><li class="toc-item-3"><a href="#1132-lexical-overlap-features">11.3.2. Lexical Overlap Features</a></li><li class="toc-item-3"><a href="#1133-information-retrieval-measures">11.3.3. Information Retrieval Measures</a></li><li class="toc-item-3"><a href="#1134-topical-similarity-features">11.3.4. Topical Similarity Features</a></li><li class="toc-item-3"><a href="#1135-relevance-feedback-based-features">11.3.5. Relevance Feedback-based Features</a></li><li class="toc-item-3"><a href="#1136-alignment-based-features">11.3.6. Alignment-based Features</a></li></ul></li><li class="toc-item-2"><a href="#114-answer-grading-models">11.4. Answer Grading Models</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#1141-individual-models">11.4.1. Individual models</a></li><li class="toc-item-3"><a href="#1142-ensemble-learning">11.4.2. Ensemble learning</a></li></ul></li><li class="toc-item-2"><a href="#115-evaluation">11.5. Evaluation</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#1151-test-bed">11.5.1. Test Bed</a></li><li class="toc-item-3"><a href="#1152-实验设计">11.5.2. 实验设计</a></li><li class="toc-item-3"><a href="#1153-实验结果">11.5.3. 实验结果</a></li></ul></li><li class="toc-item-2"><a href="#116-总结">11.6. 总结</a></li></ul></li><li class="toc-item-1"><a href="#12-machine-learning-approach-for-automatic-short-answer-grading-a-systematic-review">12. Machine Learning Approach for Automatic Short Answer Grading: A Systematic Review</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#121-nature-of-datasets">12.1. Nature of Datasets</a></li><li class="toc-item-2"><a href="#122-natural-language-processing-techniques">12.2. Natural Language Processing Techniques</a></li><li class="toc-item-2"><a href="#123-machine-learning-algorithms">12.3. Machine Learning Algorithms</a></li><li class="toc-item-2"><a href="#124-features">12.4. Features</a></li><li class="toc-item-2"><a href="#125-systems-evaluation">12.5. Systems’ Evaluation</a></li></ul></li><li class="toc-item-1"><a href="#13-automatic-short-answer-grading-via-multiway-attention-networks">13. Automatic Short Answer Grading via Multiway Attention Networks</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#131-introduction">13.1. Introduction</a></li><li class="toc-item-2"><a href="#132-approach">13.2. Approach</a></li><li class="toc-item-2"><a href="#133-实验结果">13.3. 实验结果</a></li></ul></li><li class="toc-item-1"><a href="#14-automated-short-answer-grading-using-deep-neural-networks-and-item-response-theory">14. Automated Short-Answer Grading Using Deep Neural Networks and Item Response Theory</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#141-introduction">14.1. Introduction</a></li><li class="toc-item-2"><a href="#142-proposed-method">14.2. Proposed Method</a></li><li class="toc-item-2"><a href="#143-实验">14.3. 实验</a></li></ul></li><li class="toc-item-1"><a href="#15-comparative-evaluation-of-pretrained-transfer-learning-models-on-automatic-short-answer-grading">15. Comparative Evaluation of Pretrained Transfer Learning Models on Automatic Short Answer Grading</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#151-introduction">15.1. Introduction</a></li><li class="toc-item-2"><a href="#152-experiment">15.2. Experiment</a></li></ul></li><li class="toc-item-1"><a href="#16-going-deeper-automatic-short-answer-grading-by-combining-student-and-question-models">16. Going deeper: Automatic short-answer grading by combining student and question models</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#161-introduction">16.1. Introduction</a></li><li class="toc-item-2"><a href="#162-state-features">16.2. State features</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#1621-answer-model">16.2.1. Answer model</a></li><li class="toc-item-3"><a href="#1622-question-model">16.2.2. Question model</a></li><li class="toc-item-3"><a href="#1623-student-model">16.2.3. Student model</a></li><li class="toc-item-3"><a href="#1624-composite-feature-space">16.2.4. Composite feature space</a></li></ul></li><li class="toc-item-2"><a href="#163-six-classifiers">16.3. Six Classifiers</a></li><li class="toc-item-2"><a href="#164-data">16.4. Data</a></li><li class="toc-item-2"><a href="#165-实验">16.5. 实验</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#1651-实验设置">16.5.1. 实验设置</a></li><li class="toc-item-3"><a href="#1652-实验结果">16.5.2. 实验结果</a></li></ul></li><li class="toc-item-2"><a href="#166-总结">16.6. 总结</a></li></ul></li><li class="toc-item-1"><a href="#17-automatic-short-answer-grading-with-semspace-sense-vectors-and-malstm">17. Automatic Short Answer Grading With SemSpace Sense Vectors and MaLSTM</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#171-introduction">17.1. Introduction</a></li><li class="toc-item-2"><a href="#172-method">17.2. Method</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#1721-determining-sense-vectors-with-semspace-method">17.2.1. Determining Sense Vectors with Semspace Method</a></li><li class="toc-item-3"><a href="#1722-grading-with-malstm">17.2.2. Grading with MaLSTM</a></li><li class="toc-item-3"><a href="#1723-datasets">17.2.3. Datasets</a></li></ul></li><li class="toc-item-2"><a href="#173-实验">17.3. 实验</a></li></ul></li><li class="toc-item-1"><a href="#18-a-semantic-feature-wise-transformation-relation-network-for-automatic-short-answer-grading">18. A Semantic Feature-Wise Transformation Relation Network for Automatic Short Answer Grading</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#181-introduction">18.1. Introduction</a></li><li class="toc-item-2"><a href="#182-sfrn">18.2. SFRN</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#1821-relation-network">18.2.1. Relation Network</a></li><li class="toc-item-3"><a href="#1822-qra-relation-vectors">18.2.2. QRA relation vectors</a></li><li class="toc-item-3"><a href="#1823-relation-fusion">18.2.3. Relation Fusion</a></li><li class="toc-item-3"><a href="#1824-sfrn-encoder">18.2.4. SFRN Encoder</a></li></ul></li><li class="toc-item-2"><a href="#183-data-augmentation">18.3. Data Augmentation</a></li><li class="toc-item-2"><a href="#184-实验">18.4. 实验</a></li></ul></li><li class="toc-item-1"><a href="#19-an-automatic-short-answer-grading-model-for-semi-open-ended-questions">19. An automatic short-answer grading model for semi-open-ended questions</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#191-introduction">19.1. Introduction</a></li><li class="toc-item-2"><a href="#192-proposed-model">19.2. Proposed model</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#1921-cbow">19.2.1. CBOW</a></li><li class="toc-item-3"><a href="#1922-integration-of-domain-general-information-with-domain-specific-information">19.2.2. Integration of domain-general information with domain-specific information</a></li><li class="toc-item-3"><a href="#1923-lstm-and-classifier">19.2.3. LSTM and classifier</a></li></ul></li></ul></li><li class="toc-item-1"><a href="#20-multi-relational-graph-transformer-for-automatic-short-answer-grading">20. Multi-Relational Graph Transformer for Automatic Short Answer Grading</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#201-introduction">20.1. Introduction</a></li><li class="toc-item-2"><a href="#202-methodology">20.2. Methodology</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#2021-amr-parsing">20.2.1. AMR parsing</a></li><li class="toc-item-3"><a href="#2022-subgraph-preparation-layer">20.2.2. Subgraph Preparation Layer</a></li><li class="toc-item-3"><a href="#2023-preparing-node-and-subgraph-representation">20.2.3. Preparing Node and Subgraph Representation</a></li><li class="toc-item-3"><a href="#2024-graph-matching-layer">20.2.4. Graph Matching Layer</a></li><li class="toc-item-3"><a href="#2025-prediction-layer">20.2.5. Prediction layer</a></li></ul></li><li class="toc-item-2"><a href="#203-experiment-setup">20.3. Experiment setup</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#2031-dataset">20.3.1. Dataset</a></li><li class="toc-item-3"><a href="#2032-data-processing">20.3.2. Data Processing</a></li></ul></li><li class="toc-item-2"><a href="#204-实验">20.4. 实验</a></li></ul></li><li class="toc-item-1"><a href="#21-automatic-short-answer-grading-asag-using-attention-based-deep-learning-model">21. Automatic Short Answer Grading (ASAG) using Attention-Based Deep Learning MODEL</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#211-introduction">21.1. Introduction</a></li><li class="toc-item-2"><a href="#212-method">21.2. Method</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#2121-dataset-preparation">21.2.1. Dataset preparation</a></li><li class="toc-item-3"><a href="#2122-data-preprocessing">21.2.2. Data preprocessing</a></li><li class="toc-item-3"><a href="#2123-bert-grading">21.2.3. BERT grading</a></li><li class="toc-item-3"><a href="#2124-evaluation-metrics">21.2.4. Evaluation metrics</a></li></ul></li></ul></li><li class="toc-item-1"><a href="#22-automated-short-answer-grading-using-deep-learning-a-survey">22. Automated Short Answer Grading Using Deep Learning: A Survey</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#221-introduction">22.1. Introduction</a></li><li class="toc-item-2"><a href="#222-corpora">22.2. Corpora</a></li><li class="toc-item-2"><a href="#223-evaluation-metrics">22.3. Evaluation Metrics</a></li><li class="toc-item-2"><a href="#224-deeplearning-approaches">22.4. Deeplearning Approaches</a></li><li class="toc-item-2"><a href="#225-conclusion">22.5. Conclusion</a></li></ul></li><li class="toc-item-1"><a href="#23-survey-on-automated-short-answer-grading-with-deep-learning-from-word-embeddings-to-transformers">23. Survey on Automated Short Answer Grading with Deep Learning: from Word Embeddings to Transformers</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#231-introduction">23.1. Introduction</a></li><li class="toc-item-2"><a href="#232-historical-perspective">23.2. Historical perspective</a></li><li class="toc-item-2"><a href="#233-benchmark-data-sets-for-short-answer-grading">23.3. Benchmark data sets for short answer grading</a></li><li class="toc-item-2"><a href="#234-hand-engineered-features-and-machine-learning">23.4. Hand-engineered Features and Machine Learning</a><ul class="toc-submenu-2"><li class="toc-item-3"><a href="#2341-lexical-features">23.4.1. Lexical features</a></li><li class="toc-item-3"><a href="#2342-syntactic-features">23.4.2. Syntactic features</a></li><li class="toc-item-3"><a href="#2343-semantic-features">23.4.3. Semantic features</a></li><li class="toc-item-3"><a href="#235-deep-learning-methods">23.5. Deep Learning Methods</a></li><li class="toc-item-3"><a href="#236-discussion">23.6. Discussion</a></li><li class="toc-item-3"><a href="#237-challenges">23.7. Challenges</a></li></ul></li></ul></li><li class="toc-item-1"><a href="#100-todo">100. TODO</a></li></ul>
  </nav>
</aside>

        
        <!-- TOC -->

<ul>
  <li><a href="#1-论文简介">1. 论文简介</a></li>
  <li><a href="#2-automatic-short-answer-grading-via-bert-based-deep-neural-networks">2. Automatic Short-Answer Grading via BERT-Based Deep Neural Networks</a>
    <ul>
      <li><a href="#21-abstract">2.1. Abstract</a></li>
      <li><a href="#22-introduction">2.2. Introduction</a></li>
      <li><a href="#23-related-work">2.3. Related Work</a>
        <ul>
          <li><a href="#231-applications-of-deep-learning-in-asag-tasks">2.3.1. Applications of Deep Learning in ASAG Tasks</a></li>
          <li><a href="#232-bert-model-and-its-application-in-education">2.3.2. BERT Model and Its Application in Education</a></li>
        </ul>
      </li>
      <li><a href="#24-methodology">2.4. Methodology</a>
        <ul>
          <li><a href="#241-task-definition">2.4.1. Task Definition</a></li>
          <li><a href="#242-model">2.4.2. Model</a>
            <ul>
              <li><a href="#2421-bert-layer">2.4.2.1. BERT layer</a></li>
              <li><a href="#2422-semantic-refinement-layer">2.4.2.2. Semantic Refinement Layer</a></li>
              <li><a href="#2423-semantic-fusion-layer">2.4.2.3. Semantic Fusion Layer</a></li>
              <li><a href="#2424-prediction-layer">2.4.2.4. Prediction Layer</a></li>
              <li><a href="#2425-loss-function">2.4.2.5. Loss Function</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#25-experiments">2.5. Experiments</a>
        <ul>
          <li><a href="#251-datasets">2.5.1. Datasets</a></li>
          <li><a href="#252-experimental-settings">2.5.2. Experimental Settings</a></li>
          <li><a href="#253-ablation-studies">2.5.3. Ablation Studies</a></li>
          <li><a href="#254-comparison-with-baseline-systems">2.5.4. Comparison With Baseline Systems</a></li>
        </ul>
      </li>
      <li><a href="#26-discussions">2.6. Discussions</a></li>
      <li><a href="#27-conclusion">2.7. Conclusion</a></li>
    </ul>
  </li>
  <li><a href="#3-leveraging-semantic-facets-for-automatic-assessment-of-short-free-text-answers">3. Leveraging Semantic Facets for Automatic Assessment of Short Free Text Answers</a>
    <ul>
      <li><a href="#31-abstract">3.1. Abstract</a></li>
      <li><a href="#32-introduction">3.2. Introduction</a></li>
      <li><a href="#33-related-works">3.3. Related Works</a>
        <ul>
          <li><a href="#331-automated-response-evaluation">3.3.1. Automated response evaluation</a></li>
          <li><a href="#332-semantic-similarity-measurement">3.3.2. Semantic similarity measurement</a></li>
        </ul>
      </li>
      <li><a href="#34-patterns-and-indicative-powers-of-facets-matching-states">3.4. Patterns and indicative powers of facets matching states</a>
        <ul>
          <li><a href="#341-materials-and-methods">3.4.1. Materials and methods</a>
            <ul>
              <li><a href="#3411-dataset">3.4.1.1. Dataset</a></li>
              <li><a href="#3412-summary-of-facet-matching-states">3.4.1.2. Summary of facet matching states</a></li>
              <li><a href="#3413-answer-quality-prediction-with-facet-matching-states">3.4.1.3. Answer quality prediction with facet matching states</a></li>
            </ul>
          </li>
          <li><a href="#342-results-and-analysis">3.4.2. Results and analysis</a>
            <ul>
              <li><a href="#3421-facet-matching-pattern">3.4.2.1. Facet matching pattern</a></li>
              <li><a href="#3422-answer-evaluation-leveraging-facet-matching-states">3.4.2.2. Answer evaluation leveraging facet matching states</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#35-automatic-extraction-of-facets-matching-features-for-better-prediciton">3.5. Automatic Extraction of Facets Matching Features For Better Prediciton</a>
        <ul>
          <li><a href="#351-materials-and-methods">3.5.1. Materials and methods</a>
            <ul>
              <li><a href="#3511-dataset">3.5.1.1. Dataset</a></li>
              <li><a href="#3512-automatic-semantic-facet-extraction">3.5.1.2. Automatic semantic facet extraction</a></li>
              <li><a href="#3513-facet-matching-features">3.5.1.3. Facet matching features</a></li>
              <li><a href="#3514-semantic-closeness-features">3.5.1.4. Semantic closeness features</a></li>
            </ul>
          </li>
          <li><a href="#352-results-and-analysis">3.5.2. Results and analysis</a></li>
        </ul>
      </li>
      <li><a href="#36-discussion">3.6. Discussion</a></li>
      <li><a href="#37-conclusion">3.7. Conclusion</a></li>
      <li><a href="#38-小结">3.8. 小结</a></li>
    </ul>
  </li>
  <li><a href="#4-text-to-text-semantic-similarity-for-automatic-short-answer-grading">4. Text-to-Text Semantic Similarity for Automatic Short Answer Grading</a>
    <ul>
      <li><a href="#41-knowledge-based-measures">4.1. Knowledge-based Measures</a></li>
      <li><a href="#42-corpus-based-measures">4.2. Corpus-Based Measures</a></li>
      <li><a href="#43-experiment">4.3. Experiment</a></li>
      <li><a href="#44-lsalatent-semantic-analysis">4.4. LSA(Latent semantic analysis)</a></li>
    </ul>
  </li>
  <li><a href="#5-pre-training-bert-on-domain-for-asag">5. Pre-Training Bert on Domain for ASAG</a>
    <ul>
      <li><a href="#51-usage-of-textbooks">5.1. Usage of Textbooks</a></li>
      <li><a href="#52-usage-of-question-answer-pairs">5.2. Usage of Question-Answer Pairs</a></li>
      <li><a href="#53-微调asag">5.3. 微调ASAG</a></li>
      <li><a href="#54-experiments">5.4. Experiments</a></li>
    </ul>
  </li>
  <li><a href="#6-imporving-short-answer-grading-using-transformer-based-pre-training">6. Imporving Short Answer Grading Using Transformer-Based Pre-training</a>
    <ul>
      <li><a href="#61-数据集">6.1. 数据集</a></li>
      <li><a href="#62-实验">6.2. 实验</a></li>
    </ul>
  </li>
  <li><a href="#7-investigating-transformers-for-automatic-short-answer-grading">7. Investigating Transformers for Automatic Short Answer Grading</a>
    <ul>
      <li><a href="#71-实验">7.1. 实验</a></li>
      <li><a href="#72-结果分析">7.2. 结果分析</a></li>
    </ul>
  </li>
  <li><a href="#8-superlative-model-using-word-cloud-for-short-answers-evaluation-in-elearning">8. Superlative model using word cloud for short answers evaluation in eLearning</a>
    <ul>
      <li><a href="#81-superlative-model">8.1. Superlative Model</a></li>
      <li><a href="#82-词云-word-cloud">8.2. 词云 word cloud</a></li>
      <li><a href="#83-wordnet">8.3. WordNet</a></li>
    </ul>
  </li>
  <li><a href="#9sentence-level-or-token-level-features-for-automatic-short-answer-grading-use-both">9.Sentence Level or Token Level Features for Automatic Short Answer Grading?: Use Both</a>
    <ul>
      <li><a href="#91-proposed-features">9.1. Proposed Features</a>
        <ul>
          <li><a href="#911-sentence-level-features">9.1.1. Sentence Level Features</a></li>
          <li><a href="#912-token-level-features">9.1.2. Token Level Features</a></li>
        </ul>
      </li>
      <li><a href="#92-token-level的消融实验结果">9.2. Token level的消融实验结果</a></li>
    </ul>
  </li>
  <li><a href="#10-an-experimental-study-of-text-preprocessing-techniques-for-asag-in-indonesian">10. An Experimental Study of Text Preprocessing Techniques for ASAG in Indonesian</a>
    <ul>
      <li><a href="#101-introduction">10.1. Introduction</a></li>
      <li><a href="#102-预处理方法">10.2. 预处理方法</a></li>
      <li><a href="#103-research-method">10.3. Research Method</a></li>
      <li><a href="#104-实验结果">10.4. 实验结果</a></li>
    </ul>
  </li>
  <li><a href="#11-feature-engineering-and-ensemble-based-approach-for-improving-automatic-short-answer-grading-performance">11. Feature engineering and ensemble-based approach for improving automatic short-answer grading performance</a>
    <ul>
      <li><a href="#111-introduction">11.1. Introduction</a></li>
      <li><a href="#112-problem-definition">11.2. Problem Definition</a>
        <ul>
          <li><a href="#1121-asag-as-regression-task">11.2.1. ASAG as regression task</a></li>
          <li><a href="#1122-asag-as-classification-task">11.2.2. ASAG as classification task</a></li>
        </ul>
      </li>
      <li><a href="#113-feature-extraction">11.3. Feature extraction</a>
        <ul>
          <li><a href="#1131-semantic-similarity-features">11.3.1. Semantic Similarity Features</a></li>
          <li><a href="#1132-lexical-overlap-features">11.3.2. Lexical Overlap Features</a></li>
          <li><a href="#1133-information-retrieval-measures">11.3.3. Information Retrieval Measures</a></li>
          <li><a href="#1134-topical-similarity-features">11.3.4. Topical Similarity Features</a></li>
          <li><a href="#1135-relevance-feedback-based-features">11.3.5. Relevance Feedback-based Features</a></li>
          <li><a href="#1136-alignment-based-features">11.3.6. Alignment-based Features</a></li>
        </ul>
      </li>
      <li><a href="#114-answer-grading-models">11.4. Answer Grading Models</a>
        <ul>
          <li><a href="#1141-individual-models">11.4.1. Individual models</a></li>
          <li><a href="#1142-ensemble-learning">11.4.2. Ensemble learning</a></li>
        </ul>
      </li>
      <li><a href="#115-evaluation">11.5. Evaluation</a>
        <ul>
          <li><a href="#1151-test-bed">11.5.1. Test Bed</a></li>
          <li><a href="#1152-实验设计">11.5.2. 实验设计</a></li>
          <li><a href="#1153-实验结果">11.5.3. 实验结果</a></li>
        </ul>
      </li>
      <li><a href="#116-总结">11.6. 总结</a></li>
    </ul>
  </li>
  <li><a href="#12-machine-learning-approach-for-automatic-short-answer-grading-a-systematic-review">12. Machine Learning Approach for Automatic Short Answer Grading: A Systematic Review</a>
    <ul>
      <li><a href="#121-nature-of-datasets">12.1. Nature of Datasets</a></li>
      <li><a href="#122-natural-language-processing-techniques">12.2. Natural Language Processing Techniques</a></li>
      <li><a href="#123-machine-learning-algorithms">12.3. Machine Learning Algorithms</a></li>
      <li><a href="#124-features">12.4. Features</a></li>
      <li><a href="#125-systems-evaluation">12.5. Systems’ Evaluation</a></li>
    </ul>
  </li>
  <li><a href="#13-automatic-short-answer-grading-via-multiway-attention-networks">13. Automatic Short Answer Grading via Multiway Attention Networks</a>
    <ul>
      <li><a href="#131-introduction">13.1. Introduction</a></li>
      <li><a href="#132-approach">13.2. Approach</a></li>
      <li><a href="#133-实验结果">13.3. 实验结果</a></li>
    </ul>
  </li>
  <li><a href="#14-automated-short-answer-grading-using-deep-neural-networks-and-item-response-theory">14. Automated Short-Answer Grading Using Deep Neural Networks and Item Response Theory</a>
    <ul>
      <li><a href="#141-introduction">14.1. Introduction</a></li>
      <li><a href="#142-proposed-method">14.2. Proposed Method</a></li>
      <li><a href="#143-实验">14.3. 实验</a></li>
    </ul>
  </li>
  <li><a href="#15-comparative-evaluation-of-pretrained-transfer-learning-models-on-automatic-short-answer-grading">15. Comparative Evaluation of Pretrained Transfer Learning Models on Automatic Short Answer Grading</a>
    <ul>
      <li><a href="#151-introduction">15.1. Introduction</a></li>
      <li><a href="#152-experiment">15.2. Experiment</a></li>
    </ul>
  </li>
  <li><a href="#16-going-deeper-automatic-short-answer-grading-by-combining-student-and-question-models">16. Going deeper: Automatic short-answer grading by combining student and question models</a>
    <ul>
      <li><a href="#161-introduction">16.1. Introduction</a></li>
      <li><a href="#162-state-features">16.2. State features</a>
        <ul>
          <li><a href="#1621-answer-model">16.2.1. Answer model</a></li>
          <li><a href="#1622-question-model">16.2.2. Question model</a></li>
          <li><a href="#1623-student-model">16.2.3. Student model</a></li>
          <li><a href="#1624-composite-feature-space">16.2.4. Composite feature space</a></li>
        </ul>
      </li>
      <li><a href="#163-six-classifiers">16.3. Six Classifiers</a></li>
      <li><a href="#164-data">16.4. Data</a></li>
      <li><a href="#165-实验">16.5. 实验</a>
        <ul>
          <li><a href="#1651-实验设置">16.5.1. 实验设置</a></li>
          <li><a href="#1652-实验结果">16.5.2. 实验结果</a></li>
        </ul>
      </li>
      <li><a href="#166-总结">16.6. 总结</a></li>
    </ul>
  </li>
  <li><a href="#17-automatic-short-answer-grading-with-semspace-sense-vectors-and-malstm">17. Automatic Short Answer Grading With SemSpace Sense Vectors and MaLSTM</a>
    <ul>
      <li><a href="#171-introduction">17.1. Introduction</a></li>
      <li><a href="#172-method">17.2. Method</a>
        <ul>
          <li><a href="#1721-determining-sense-vectors-with-semspace-method">17.2.1. Determining Sense Vectors with Semspace Method</a></li>
          <li><a href="#1722-grading-with-malstm">17.2.2. Grading with MaLSTM</a></li>
          <li><a href="#1723-datasets">17.2.3. Datasets</a></li>
        </ul>
      </li>
      <li><a href="#173-实验">17.3. 实验</a></li>
    </ul>
  </li>
  <li><a href="#18-a-semantic-feature-wise-transformation-relation-network-for-automatic-short-answer-grading">18. A Semantic Feature-Wise Transformation Relation Network for Automatic Short Answer Grading</a>
    <ul>
      <li><a href="#181-introduction">18.1. Introduction</a></li>
      <li><a href="#182-sfrn">18.2. SFRN</a>
        <ul>
          <li><a href="#1821-relation-network">18.2.1. Relation Network</a></li>
          <li><a href="#1822-qra-relation-vectors">18.2.2. QRA relation vectors</a></li>
          <li><a href="#1823-relation-fusion">18.2.3. Relation Fusion</a></li>
          <li><a href="#1824-sfrn-encoder">18.2.4. SFRN Encoder</a></li>
        </ul>
      </li>
      <li><a href="#183-data-augmentation">18.3. Data Augmentation</a></li>
      <li><a href="#184-实验">18.4. 实验</a></li>
    </ul>
  </li>
  <li><a href="#19-an-automatic-short-answer-grading-model-for-semi-open-ended-questions">19. An automatic short-answer grading model for semi-open-ended questions</a>
    <ul>
      <li><a href="#191-introduction">19.1. Introduction</a></li>
      <li><a href="#192-proposed-model">19.2. Proposed model</a>
        <ul>
          <li><a href="#1921-cbow">19.2.1. CBOW</a></li>
          <li><a href="#1922-integration-of-domain-general-information-with-domain-specific-information">19.2.2. Integration of domain-general information with domain-specific information</a></li>
          <li><a href="#1923-lstm-and-classifier">19.2.3. LSTM and classifier</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#20-multi-relational-graph-transformer-for-automatic-short-answer-grading">20. Multi-Relational Graph Transformer for Automatic Short Answer Grading</a>
    <ul>
      <li><a href="#201-introduction">20.1. Introduction</a></li>
      <li><a href="#202-methodology">20.2. Methodology</a>
        <ul>
          <li><a href="#2021-amr-parsing">20.2.1. AMR parsing</a></li>
          <li><a href="#2022-subgraph-preparation-layer">20.2.2. Subgraph Preparation Layer</a></li>
          <li><a href="#2023-preparing-node-and-subgraph-representation">20.2.3. Preparing Node and Subgraph Representation</a></li>
          <li><a href="#2024-graph-matching-layer">20.2.4. Graph Matching Layer</a></li>
          <li><a href="#2025-prediction-layer">20.2.5. Prediction layer</a></li>
        </ul>
      </li>
      <li><a href="#203-experiment-setup">20.3. Experiment setup</a>
        <ul>
          <li><a href="#2031-dataset">20.3.1. Dataset</a></li>
          <li><a href="#2032-data-processing">20.3.2. Data Processing</a></li>
        </ul>
      </li>
      <li><a href="#204-实验">20.4. 实验</a></li>
    </ul>
  </li>
  <li><a href="#21-automatic-short-answer-grading-asag-using-attention-based-deep-learning-model">21. Automatic Short Answer Grading (ASAG) using Attention-Based Deep Learning MODEL</a>
    <ul>
      <li><a href="#211-introduction">21.1. Introduction</a></li>
      <li><a href="#212-method">21.2. Method</a>
        <ul>
          <li><a href="#2121-dataset-preparation">21.2.1. Dataset preparation</a></li>
          <li><a href="#2122-data-preprocessing">21.2.2. Data preprocessing</a></li>
          <li><a href="#2123-bert-grading">21.2.3. BERT grading</a></li>
          <li><a href="#2124-evaluation-metrics">21.2.4. Evaluation metrics</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#22-automated-short-answer-grading-using-deep-learning-a-survey">22. Automated Short Answer Grading Using Deep Learning: A Survey</a>
    <ul>
      <li><a href="#221-introduction">22.1. Introduction</a></li>
      <li><a href="#222-corpora">22.2. Corpora</a></li>
      <li><a href="#223-evaluation-metrics">22.3. Evaluation Metrics</a></li>
      <li><a href="#224-deeplearning-approaches">22.4. Deeplearning Approaches</a></li>
      <li><a href="#225-conclusion">22.5. Conclusion</a></li>
    </ul>
  </li>
  <li><a href="#23-survey-on-automated-short-answer-grading-with-deep-learning-from-word-embeddings-to-transformers">23. Survey on Automated Short Answer Grading with Deep Learning: from Word Embeddings to Transformers</a>
    <ul>
      <li><a href="#231-introduction">23.1. Introduction</a></li>
      <li><a href="#232-historical-perspective">23.2. Historical perspective</a></li>
      <li><a href="#233-benchmark-data-sets-for-short-answer-grading">23.3. Benchmark data sets for short answer grading</a></li>
      <li><a href="#234-hand-engineered-features-and-machine-learning">23.4. Hand-engineered Features and Machine Learning</a>
        <ul>
          <li><a href="#2341-lexical-features">23.4.1. Lexical features</a></li>
          <li><a href="#2342-syntactic-features">23.4.2. Syntactic features</a></li>
          <li><a href="#2343-semantic-features">23.4.3. Semantic features</a></li>
          <li><a href="#235-deep-learning-methods">23.5. Deep Learning Methods</a></li>
          <li><a href="#236-discussion">23.6. Discussion</a></li>
          <li><a href="#237-challenges">23.7. Challenges</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#100-todo">100. TODO</a></li>
</ul>

<!-- /TOC -->

<h1 id="1-论文简介">1. 论文简介</h1>
<p>博客主要整理Automatic Short Answer Grading(后续简称ASAG)任务的相关论文，所谓ASAG任务，就是对某个问题的短文本回答进行自动评估，相关论文链接：</p>
<ul>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/9779091" target="_blank">Automatic Short-Answer Grading via BERT-Based Deep Neural Networks</a></li>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/9860098" target="_blank">Leveraging Semantic Facets for Automatic Assessment of Short Free Text Answers</a></li>
  <li><a href="https://aclanthology.org/E09-1065/" target="_blank">Text-to-Text Semantic Similarity for Automatic Short Answer Grading</a></li>
  <li><a href="https://aclanthology.org/D19-1628/" target="_blank">Pre-Training BERT on Domain Resources for Short Answer Grading</a></li>
  <li><a href="https://link.springer.com/chapter/10.1007/978-3-030-23204-7_39" target="_blank">Improving Short Answer Grading Using Transformer-Based Pre-training</a></li>
  <li><a href="https://link.springer.com/chapter/10.1007/978-3-030-52240-7_8" target="_blank">Investigating Transformers for Automatic Short Answer Grading</a></li>
  <li><a href="https://link.springer.com/article/10.1007/s10639-016-9547-0" target="_blank">Superlative model using word cloud for short answers evaluation in eLearning</a></li>
  <li><a href="https://link.springer.com/chapter/10.1007/978-3-319-93843-1_37" target="_blank">Sentence Level or Token Level Features for Automatic Short Answer Grading?: Use Both</a></li>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/8720957" target="_blank">An Experimental Study of Text Preprocessing Techniques for Automatic Short Answer Grading in Indonesian</a></li>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/8636967/" target="_blank">Feature engineering and ensemble-based approach for improving automatic short-answer grading performance</a></li>
  <li><a href="https://link.springer.com/chapter/10.1007/978-3-030-03928-8_31" target="_blank">Machine Learning Approach for Automatic Short Answer Grading: A Systematic Review</a></li>
  <li><a href="https://link.springer.com/chapter/10.1007/978-3-030-23207-8_32" target="_blank">Automatic Short Answer Grading via Multiway Attention Networks</a></li>
  <li><a href="https://link.springer.com/chapter/10.1007/978-3-030-52240-7_61" target="_blank">Automated Short-Answer Grading Using Deep Neural Networks and Item Response Theory</a></li>
  <li><a href="https://arxiv.org/abs/2009.01303" target="_blank">Comparative Evaluation of Pretrained Transfer Learning Models on Automatic Short Answer Grading</a></li>
  <li><a href="https://link.springer.com/article/10.1007/s11257-019-09251-6" target="_blank">Going deeper: Automatic short-answer grading by combining student and question models</a></li>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/9335022" target="_blank">Automatic Short Answer Grading With SemSpace Sense Vectors and MaLSTM</a></li>
  <li><a href="https://aclanthology.org/2021.emnlp-main.487/" target="_blank">A Semantic Feature-Wise Transformation Relation Network for Automatic Short Answer Grading</a></li>
  <li><a href="https://www.tandfonline.com/doi/abs/10.1080/10494820.2019.1648300" target="_blank">An automatic short-answer grading model for semi-open-ended questions</a></li>
  <li><a href="https://aclanthology.org/2022.naacl-main.146/" target="_blank">Multi-Relational Graph Transformer for Automatic Short Answer Grading</a></li>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/10007187" target="_blank">Automatic Short Answer Grading (ASAG) using Attention-Based Deep Learning MODEL</a></li>
  <li><a href="https://link.springer.com/chapter/10.1007/978-3-030-84060-0_5" target="_blank">Automated Short Answer Grading Using Deep Learning: A Survey</a></li>
  <li><a href="https://arxiv.org/abs/2204.03503" target="_blank">Survey on Automated Short Answer Grading with Deep Learning: from Word Embeddings to Transformers</a></li>
</ul>

<h1 id="2-automatic-short-answer-grading-via-bert-based-deep-neural-networks">2. Automatic Short-Answer Grading via BERT-Based Deep Neural Networks</h1>
<h2 id="21-abstract">2.1. Abstract</h2>
<p>Automatic short-answer grading(ASAG)，即自动短文本评分任务，是智慧辅导系统的重要组成部分。ASAG目前还存在很多挑战，作者提出了两个主要原因: 1)高精度评分任务需要对answer text有很深的语义理解; 2)ASAG任务的语料一般都很小，不能为深度学习提供足够的训练数据。为了解决这些挑战，作者提出用BERT-based网络来解决ASAG任务的挑战: 1)用预训练模型BERT来encoder答案文本就可以克服语料太小的问题。2)为了生成足够强的语义理解，作者在BERT输出层后加上了一个精炼层(由LSTM和Capsule网络串联组成) 3)作者提出一种triple-hot loss来处理ASAG的回归问题。实验结果表明模型的效果在SemEval-2013和Mohler数据集上表现比SOTA要好。模型在github上<a href="https://github.com/wuhan-1222/ASAG" target="_blank">开源</a></p>

<h2 id="22-introduction">2.2. Introduction</h2>
<p>考试和评估是智慧辅导系统(intelligent tutoring systems, ITSs)的重要组成部分，可以获得学生们的实时知识认知水平，也能为学生们提供个性化的学习方案。多选题是考试的重要组成部分，但是多选题有两个明显的短板: 1)多选题只提供部分选择 2)有些学生的答案可能是蒙出来的。ASAG可以解决上述问题，学生们为简答题提供一个short text，然后ASAG来评估short text是否正确，具体来说评估结果有五类: Correct、Partically correct、Contradictory、Irrelevant、Nondomain。</p>

<p>以往的研究中，Feature engineering是ASAG的主要解决方法，有很多稀疏特征应用于ASAG: token overlap features、syntax and dependency features、knowledge-based features(WordNet)… 但是这种特征工程为基础的方法存在以下问题: 首先，稀疏特征一般需要很多预处理步骤，这些步骤会产生一定的误差，可能会涉及到误差累积和误差传递的后果。此外，缺乏有效地encode文本句的手段</p>

<p>随着deeplearning的发展，出现了很多deep net，比如LSTM-based model、CNN and LSTM-based model、transformer-based model出现在了ASAG任务中。这些模型都从answer text中挖掘语义信息，然后将answer text转化成word enbedding，所以这些方法都是end-to-end的。但是这些方法存在以下问题: 1)学生的答案非常free，也就是说在句子结构、语言风格、段落长度这些方面可能会有很大的区别，所以作者认为需要用更先进的technique去获得text的语义理解。2)由于数据很难标注，所以ASAG任务的数据集语料很小，可能只有few thousand。所以说主要的挑战就是如何在小语料库上训练一个稳定高效的模型</p>

<p>论文的主要贡献:</p>
<ol>
  <li>提出了用预训练模型BERT微调，然后连接一个精炼层的模型表现超过SOTA(在SemEval-2013数据集和Mohler数据集上)</li>
  <li>精炼层由Bi-LSTM和Capsule network(with position information)串联组成，LSTM来抽取全局的context信息，Capsule来抽取局部context信息</li>
  <li>用多头注意力层来连接全局和局部context来生成语义表示</li>
  <li>提出了triple-hot loss策略</li>
</ol>

<h2 id="23-related-work">2.3. Related Work</h2>
<h3 id="231-applications-of-deep-learning-in-asag-tasks">2.3.1. Applications of Deep Learning in ASAG Tasks</h3>
<p>根据deep learning和训练策略的不同，作者将deeplearning在ASAG的应用分为以下三种:</p>
<ol>
  <li>Participator: deep learning参与feature-based方法中</li>
  <li>Contractor: deeplearning独立地在ASAG任务中实现end-to-end</li>
  <li>迁移学习，经典的预训练模型+scaling语料库</li>
</ol>

<p>接下来分别介绍了利用稀疏特征的方法与Deeplearning来来解决ASAG任务:</p>
<ol>
  <li>稀疏特征，也即feature engineering的应用有:
    <ul>
      <li>Marvaniya等人和Saha等人使用预训练的神经网络InferSent对答案文本进行编码，这弥补了标记重叠(token overlap)方法中上下文表示的不足，其中InferSent是使用Bi-LSTM网络的预训练句子嵌入模型</li>
      <li>Tan等人提出了一种将图卷积网络(GCNs)与几种稀疏特征相结合的评分方法。他们首先为答案文本构建了一个无向异构文本图，其中包含句子级节点、单词/bigam级节点和节点之间的边。然后，他们使用两层GCN模型对图结构进行编码，得到图的表示形式。</li>
      <li>Zhang等人使用深度信念网络作为分类器，而不是传统的机器学习，对学生由六个稀疏特征组成的答案表示进行分类</li>
    </ul>
  </li>
  <li>Deeplearning的方法有:
    <ul>
      <li>Kumar等人提出了ASAG的Bi-LSTM框架。他们的框架由三个级联的神经模块组成:分别应用于参考和学生答案的Siamese Bi-LSTMs，使用earth-mover distance(EMD)与LSTMs的隐状态交互的池化层，以及用于输出分数的回归层</li>
      <li>Uto和Uchida将LSTM网络与项目反应理论(item response theory)相结合进行短文本答案评分</li>
      <li>Tulu等人改进了基于LSTM的评分方法，通过引入感觉向量并将池化层替换为曼哈顿距离</li>
      <li>Riordan等人结合CNN和LSTM网络进行短文本答案评分</li>
      <li>Liu等人在一个大型K-12数据集上提出了一个具有多路注意的模型</li>
    </ul>
  </li>
</ol>

<p>上面提到的deeplearning方法需要大语料库支撑的数据集，但是ASAG缺少足够的大语料库，于是出现了用预训练模型来解决ASAG任务，比如ELMo、BERT、GPT、GPT-2，在这些模型中，BERT表现最好</p>

<h3 id="232-bert-model-and-its-application-in-education">2.3.2. BERT Model and Its Application in Education</h3>
<p>BERT吸收了ELMo和GPT的优点，模型如下图所示:</p>
<center><img src="../assets/img/posts/20221010/2.jpg" /></center>
<p>BERTstack了12个transformer块</p>

<p>接下来介绍了BERT在智慧教育领域的应用:</p>
<ol>
  <li>Wang等人提出了分层课程BERT模型，以更好地捕捉每门课程的课程结构质量和语言特征，预测在线教育中教师的绩效</li>
  <li>Khodeir等人将BERT与多层双向GRU相结合，构建了一个紧急分类模型，用于教师快速挑选和响应大规模开放在线课程(MOOC)论坛中最紧急的学生帖子，该模型在三个Stanford MOOC Post数据集上实现了紧急帖子分类，加权F-score分别为91.9%、91.0%和90.0%</li>
  <li>Sung等人利用BERT构建了一个多标签分类模型，用于快速评估学生在探索热力学的过程中的多模态的表征思维</li>
</ol>

<p>关于ASAG的应用有:</p>
<ol>
  <li>Sung等人分析比较了BERT与多种网络结构在short-answer grading的效果</li>
  <li>Leon等人分析比较了BERT、ALBERT、RoBERTa在short-answer grading的效果</li>
</ol>

<h2 id="24-methodology">2.4. Methodology</h2>
<h3 id="241-task-definition">2.4.1. Task Definition</h3>
<p>ASAG问题有两种形式:</p>
<ol>
  <li>回归问题: 连续的分数来评估学生的答案</li>
  <li>分类问题: 将学生的答案分为五类: Correct、Partically correct、Contradictory、Irrelevant、Nondomain</li>
</ol>

<p>作者的做法是用分数来对类别进行分类，比如0-0.5属于类别1，所以问题的本质还是分类问题，那么ASAG的预测类别$y^*$可以表示为:</p>

<p>
\begin{equation}
y^*=\underset{y \in Y}{\operatorname{argmax}}(\operatorname{Pr}(y \mid(q, p)))
\end{equation}
</p>

<p>其中Y表示类别集，Pr()表示预测的概率分布，q是学生答案，p是参考答案</p>

<h3 id="242-model">2.4.2. Model</h3>
<p>作者解释为什么既要用BERT，也要用refinement:</p>
<ol>
  <li>BERT获得word embedding结果，利用了所有词元之间的关系，但是没有考虑顺序和距离，所以需要用Bi-LSTM来生成更精细的全局context，同时弥补BERT时序信息的缺失，然后利用Capsule或者CNN来生成BERT每个隐层的局部信息</li>
  <li>BERT可以获得动态的词嵌入(对比GloVe获得静态的词嵌入)，这样可以获得更丰富的general-purpose knowledge，所以BERT即使在小语料库上也能有不错的效果</li>
  <li>一些研究表明，在BERT上应用经典的神经网络可以在小数据集上获得更好的效果，比如Liao等人结合RoBERTa和CNN来提升情感分析的效果，Yang等人在BERT上应用多头注意力层来添加距离权重在aspect polarity classification上获得更好的效果</li>
</ol>

<p>主题网络模型如下图所示:</p>
<center><img src="../assets/img/posts/20221010/4.jpg" /></center>
<p>接下来从模型的各个板块来分别介绍:</p>

<h4 id="2421-bert-layer">2.4.2.1. BERT layer</h4>
<p>首先BERT layer的参数初始化成BERTbase的参数，微调。BERT layer层的输入是学生和参考答案的token embedding，输出是BERT的隐层</p>

<p>
\begin{equation}
O_{BERT}=BERT(s)=\left\{h_1^b,h_2^b,...,h_n^b\right\}\in \mathbb{R}^{n\times d_b}
\end{equation}
</p>

<h4 id="2422-semantic-refinement-layer">2.4.2.2. Semantic Refinement Layer</h4>
<p>Refinement层由Bi-LSTM和Capsule network(with position information)串联组成，输出结果如下所示:</p>

<p>
\begin{equation}
\begin{aligned}
&amp;\overrightarrow{O_{\mathrm{LSTNS}}}=\overrightarrow{\operatorname{LSTMS}}\left(O_{\text {BERT }}\right)=\left\{\overrightarrow{h_1^L}, \overrightarrow{h_2^L}, \ldots, \overrightarrow{h_n^L}\right\} \in \mathbb{R}^{n \times d_L} \\
&amp;\overleftrightarrow{O_{\mathrm{LSTMs}}}=\overleftarrow{\operatorname{LSTMs}}\left(O_{\mathrm{BERT}}\right)=\left\{\overleftrightarrow{h_1^r}, \overleftrightarrow{h_2^r}, \ldots, \overleftrightarrow{h_n^r}\right\} \in \mathbb{R}^{n \times d_L} \\
&amp;O_{\mathrm{Caps}}=\operatorname{Capsules}\left(O_{\text {BERT }}\right)=\left\{h_1^c, h_2^c, \ldots, h_n^c\right\} \in \mathbb{R}^{n \times d_c}
\end{aligned}
\end{equation}
</p>

<p>输出结果后面都跟了一个层归一化(保证数据分布的稳定，加速收敛)</p>

<p>这里提到了Capsule network，我对Capsule network进行一定的补充: Capsule网络主要想解决卷积神经网络（Convolutional Neural Networks）存在的一些缺陷，比如说信息丢失，视角变化等。Capsule网络结构如下图所示:</p>
<center><img src="../assets/img/posts/20221010/7.jpg" /></center>
<p>以数字图片分类为例，Capsule一共包含3层，2层卷积层和1层全连接层。与普通网络的区别是输出的每个类别都是一个向量，向量的长度表示实体存在的概率大小，向量在空间中的方向表示实体的实例化参数，Capsule网络和CNN还是比较相似的</p>

<h4 id="2423-semantic-fusion-layer">2.4.2.3. Semantic Fusion Layer</h4>
<p>在refinement层后，需要有一个融合层来融合LSTM和Capsule的结果，先用矩阵来stackLSTM、Capsule的结果:</p>

<p>
\begin{equation}
X^{(e)}=\{x_1^{(e)}, x_2^{(e)},...,x_n^{(e)}\}\in \mathbb{R}^{n\times d}
\end{equation}
</p>

<p>其中$x_i^{(e)}=[h_i^L;h_i^r;h_i^c]$，然后再把矩阵X送入多头自注意力层，注意力评分函数是scaled dot-product attention，具体细节如下:</p>

<p>
\begin{equation}
\begin{aligned}
&amp; \text{MultiHead}(Q,K,V)=[\text{head}_1;\text{head}_2;...;\text{head}_h]\omega^R \\
&amp; \text{head}_i=\text{Attention}(Q_i;K_i;V_i)=\text{Attention}(Q\omega ^Q, K\omega ^K, V\omega ^V) \\
&amp; \text{Attention}(Q_i;K_i;V_i)=\text{softmax}(\frac{Q_iK_i^T}{\sqrt{d_K}})V_i \\
 X^{(h)}&amp;=\text{MultiHead}(X^{(e)}, X^{(e)}, X^{(e)}) \\
&amp;=\{x_1^{(h)}, x_2^{(h)},..., x_n^{(h)}\}
\end{aligned}
\end{equation}
</p>

<p>为了让全局context和局部context不互相干扰，作者对多头自注意力层做以下约束:</p>
<ol>
  <li>让LSTM的输出维度和Capsule的输出维度相同，即$d_c=2d_L$</li>
  <li>head数取2</li>
  <li>让局部context和全局context不互相干扰(用参数调整)，如下图所示:</li>
</ol>
<center><img src="../assets/img/posts/20221010/10.jpg" /></center>
<p>最后再连接一个层归一化</p>

<h4 id="2424-prediction-layer">2.4.2.4. Prediction Layer</h4>
<p>预测层，首先用最大池化层获得pair(q,p)的语义表示，其实就是在每个头上选择最大的值:</p>

<p>
\begin{equation}
\begin{aligned}
Z &amp;=\text{Maxpooling}(X^{(h)})={z_1,z_2,...,z_d}\in \mathbb{R}^d \\
z_j &amp;=\text{Max}(x_{1j}^{(h)}, x_{2j}^{(h)},..., x_{nj}^{(h)}), j=1,2,...,d
\end{aligned}
\end{equation}
</p>

<p>然后将语义表示Z输入线性层(加上一个dropout防止overfit)，然后用softmax表示输出的概率分布:</p>

<p>
\begin{equation}
\begin{aligned}
o &amp;=MZ+b \\
p(y\mid Z)&amp;=\frac{exp(o_y)}{\sum_{i}^{d_y}exp(o_i)}
\end{aligned}
\end{equation}
</p>

<h4 id="2425-loss-function">2.4.2.5. Loss Function</h4>
<p>为了适应两种ASAG tasks，作者提出了两种损失函数的策略:</p>
<ul>
  <li>第一种就是常规的交叉熵，分类结果用one-hot编码</li>
</ul>

<p>
\begin{equation}
L(\theta)=-\sum_{i=1}^{|\Omega|}\log \left(p\left(y_i \mid Z_i, \theta\right)\right)
\end{equation}
</p>

<ul>
  <li>第二种就是作者提出用triple-hot编码y，就是在y对应位置的左右也置1，那么损失函数为:</li>
</ul>

<p>
\begin{equation}
\begin{aligned}
L(\theta)=&amp;-\sum_{i=1}^{|\Omega|}\left(\log \left(p\left(y_i^{-1} \mid Z_i, \theta\right)\right)+\log \left(p\left(y_i \mid Z_i, \theta\right)\right)\right.\\
&amp;\left.+\log \left(p\left(y_i^{+1} \mid Z_i, \theta\right)\right)\right)
\end{aligned}
\end{equation}
</p>

<h2 id="25-experiments">2.5. Experiments</h2>
<h3 id="251-datasets">2.5.1. Datasets</h3>
<p>作者在两个ASAG主流数据集上进行评估，分别是SemEval-2013和Mohler数据集</p>
<center><img src="../assets/img/posts/20221010/16.jpg" /></center>

<ol>
  <li>SemEval-2013: 作者使用SemEval-2013中的SciEntsBank语料，SciEntsBank语料包含15个不同科学领域的197个问题和10000个答案，这个语料库是ASAG分类任务的一个benchmark，他包含三种分类类别，分别是two-way(Correct and Incorrect)，three-way (Correct, Contradictory, and Incorrect)，five-way (Correct, Partially correct, Contradictory, Irrelevant, and Non-domain)，为了提供多方面的evaluation，测试数据集分为了三个子集:
    <ul>
      <li>Unseen Answers(UA): 和训练集有相同的题目和参考答案，但是学生的回答不同</li>
      <li>Unseen Questions(UQ): 和训练集的问题不同，但是属于同一个领域</li>
      <li>Unseen Domains(UD): 和训练集的问题不同，且不属于同一个领域</li>
      <li>对于这个数据集，作者用三个性能度量(accuracy, weighted-F1, macro-average F1)来评估两个子任务(three-way, five-way)</li>
    </ul>
  </li>
  <li>Mohler dataset: 数据集<a href="http://web.eecs.umich.edu/?mihalcea/downloads/ ShortAnswerGrading_v2.0.zip" target="_blank">开源</a>。数据集由Mohler团队从University of North Texas的一门计算机科学课程的两个考试和十个测试收集整理。它包含80个问题和2273个学生的答案，每个答案都由两名老师打分(0-5, integer)，由于是平均而来，所以一共由11种分类结果，Mohler数据集同样是ASAG任务的一个benchmark，作者可以将数据集变成了11个类别的分类数据集。</li>
</ol>

<p>由于数据集只有2273个答案对，太小，所以需要对数据集进行扩充，Kumar等人通过把训练集中正确的学生答案作为额外的参考答案，这样就把数据集的答案-问题对扩充到30000对。作者为了避免过拟合，采取了折中的策略，对每个问题只挑选一个学生的正确答案作为额外的参考答案，这样就把2083个答案对扩充至3300个答案对。针对Mohler数据集，作者采用了12折交叉验证的方法，用来评估的性能度量有Cohen’s kappa coefficient(kappa), Pearson correlation coefficient(Pearson’s r), mean absolute error(MAE), root-mean-square error(RMSE)，Mohler的标签只有11类，作者既可以把它当作了回归任务来评估(就是把分类结果用分数表示)，也可以把它当作分类任务来评估，其中kappa系数是分类任务的性能度量，其他的性能度量都是回归任务的性能度量</p>

<h3 id="252-experimental-settings">2.5.2. Experimental Settings</h3>
<p>BERT采用base版本(12层，768个单元，12个head，110M参数)，LSTM的隐层个数设置为200并且在最后一个时间步返回所有的hidden state，Capsule的卷积核个数设置为400，卷积核大小为3，dynamic route设置为3。在融合层，attention head设置为2，每个头400维参数，dropout参数都设置为0.1，使用adam优化器，学习率设置为2e-5，一个小批量64个输入，训练周期为10</p>

<h3 id="253-ablation-studies">2.5.3. Ablation Studies</h3>
<p>为了分析每一层的作用，从六个角度来做ablation studies:</p>
<ol>
  <li>W/O refinement: 无refienment</li>
  <li>W/O multihead: 无multihead</li>
</ol>

<p>以此类推，得到如下结果:</p>
<center><img src="../assets/img/posts/20221010/17.jpg" /></center>

<h3 id="254-comparison-with-baseline-systems">2.5.4. Comparison With Baseline Systems</h3>
<p>与众多模型进行对比，主要的实验结果如下:</p>
<ul>
  <li>Mohler数据集</li>
</ul>
<center><img src="../assets/img/posts/20221010/18.jpg" /></center>

<ul>
  <li>SemEval-2013:</li>
</ul>
<center><img src="../assets/img/posts/20221010/19.jpg" /></center>

<ul>
  <li>PR曲线:</li>
</ul>
<center><img src="../assets/img/posts/20221010/20.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20221010/21.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20221010/22.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20221010/23.jpg" /></center>

<ul>
  <li>AUC值和PR曲线平衡点:</li>
</ul>
<center><img src="../assets/img/posts/20221010/24.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20221010/25.jpg" /></center>

<h2 id="26-discussions">2.6. Discussions</h2>
<p>从Ablation实验结果可以看出，refinement层提升了模型的在Sem-UQ和Mohler数据集上的精度，说明refinement层提高了BERT模型在相同问题领域的泛化性，同时也可以看出LSTM在refinement层也很重要，可以提取更丰富的全局context信息。Capsule的有无也说明了它可以提取局部context信息，同时发现它的效果比一般的CNN要好。Triple-loss的设计也确实提升了模型的性能</p>

<p>接着简单分析了一下表4(Mohler数据集)与表5(SemEval-2013)的结果，其实就是对比了不同模型的性能度量，说哪个模型更好什么的</p>

<p>然后分析PR曲线的结果，作者认为在大多数情况下，模型的PR曲线远高于其他模型的PR曲线，这与表6中模型在所有图中AUC最大的结论是一致的。除此之外，平衡点的值也更高，说明模型性能最好</p>

<p>模型可以应用于智慧教育系统的两个场景:</p>
<ol>
  <li>ITSs领域，即智慧辅导系统，可以让评估变成自动化的</li>
  <li>MOOC线上平台，可以替代老师的手动评估，快速精准地为大量的free-text answer进行评分</li>
</ol>

<h2 id="27-conclusion">2.7. Conclusion</h2>
<p>作者提出了一种新的BERT-based网络结构来解决ASAG问题，进行了大量的实验，得出了一下的结论:</p>
<ol>
  <li>基于词嵌入的网络，如CNN、LSTM、Capsule无法在小数据集上取得很好的结果</li>
  <li>预训练网络BERT可以很好的适配ASAG任务</li>
  <li>利用LSTM和Capsule网络可以进一步挖掘语义信息</li>
</ol>

<p>模型局限性:</p>
<ol>
  <li>在开放领域的问答中，小数据集训练出来的模型无法取得预期的效果，比如Sem-UD</li>
  <li>目前来说，模型无法消除或者替代学生答案中的大量的代词，作者计划在后续通过BERT模型来消除学生答案中的代词来提升模型的性能</li>
</ol>

<h1 id="3-leveraging-semantic-facets-for-automatic-assessment-of-short-free-text-answers">3. Leveraging Semantic Facets for Automatic Assessment of Short Free Text Answers</h1>
<p>论文全称为Leveraging Semantic Facets for Automatic Assessment of Short Free Text Answers，接下来将逐段阅读并整理论文</p>
<center><img src="../assets/img/posts/20221010/46.jpg" /></center>

<h2 id="31-abstract">3.1. Abstract</h2>
<p>短文本问答能反映出学生对于知识的掌握情况，由于自然语言的复杂性，简答题的自动评估任务仍具有挑战性。现有的自动评估模型的做法是预测答案的分数来评估学生的答案，他们一般不关心参考答案的语义面，这限制了预测的表现。该篇论文的关注点是短文本答案的不同的语义面(semantic facets)，每个语义面对应着需要掌握的知识。利用带有语义面标注的数据集，作者首先展示了语义面状态与答案质量(一个答案的好坏)的对应关系，然后展示了语义面在自动评估答案质量的重要性。作者接着将工作拓展到不包含语义面的数据集上，证明了作者的工作在自动评估短文本答案方面的有效性，这些工作包括语义面提取、预测语义面状态和使用语义面的特征工程。</p>

<p>论文的贡献有:</p>
<ol>
  <li>论文提出的方法提升了短文本答案评估的SOTA的表现</li>
  <li>论文深入研究短文本答案的语义面组成，让短文本评估模型的可解释性更高</li>
</ol>

<h2 id="32-introduction">3.2. Introduction</h2>
<p>评估学生的答案非常重要，在网上学习中，实现手动评估非常困难，加速了关于自动评估的研究。研究着重于学生的短文本答案，与多选题相比，答案更不被定义且不具备结构化，所以自动评估很困难。此外，为了正确回答问题，一个短文本的回答可能传达了学生对知识的更深层次的思考，并且可能包含多个从属的知识。有了语义面之后，一个更详细的评估方法出现了，可以分析学生答案的不同语义部分，而不是简单的给出答案的分数。</p>

<p>最近的研究基本上都采取了黑盒的模式(black box)，即从一端输入学生的答案和参考答案，另一端直接输出答案的分数，这中间发生了什么我们并不知道。虽然说对于评估系统来说，分数很重要，但是参考答案涉及到的多个知识点与学生答案的匹配情况我们却一概不知。为了提升评估任务的表现，作者将关注点从黑盒转移到分解评估的过程。为了简便，作者将参考答案的知识组成称为语义面(Semantic Facets)，给定一段文本，这段本文的语义面是由文本的短语组成的集合</p>

<p>论文的主要实验和工作有两个，分别在SciEntsBank数据集和Beetle数据集上展开</p>
<ol>
  <li>第一个工作数据集是SciEntsBank，每个问题的语义面都标注好了，学生答案与问题的语义面匹配状态(matching state)也给出了，这样我们就可以得到不同评分等级(correct, incorrect…)的答案的语义面匹配状态的分布情况。有了分布情况后，我们便可以回答以下问题: 1)是否能根据学生答案的语义面匹配状态来确定答案的评分? 2)分布情况对自动评估系统是否有帮助? 除此之外，我们还可以构建模型来通过学生的答案和语义面来预测语义面的匹配状态</li>
  <li>为了泛化第一个工作，第二个工作使用的数据集是Bettle数据集，这个数据集既没有语义面的标注，也没有语义面的匹配状态的标注。作者首先提出了一种从参考答案提取语义面的方法: 利用词汇统计(lexical statistics)和语法信息(syntax information)。接着利用第一个工作中训练好的预测语义面的匹配状态的网络来预测这个数据集的语义面匹配状态，然后再利用工作一中发现的pattern来通过学生答案的语义面匹配状态来获取features(后续用于对答案进行评分，所以这一步就是feature engineering)，最后，利用这些feature来预测答案的评分</li>
</ol>

<p>贡献:</p>
<ol>
  <li>部分程度上打开了自动评估模型的black box</li>
  <li>发现了matching state与不同评分等级的对应关系，即发现了pattern，这对于自动和手动评估都有帮助</li>
  <li>提出了一种从参考答案抽取语义面的方法</li>
</ol>

<h2 id="33-related-works">3.3. Related Works</h2>
<p>该小节介绍了论文的两个相关工作: 1)自动评估系统的不同任务及对应方法 2)量化一对文本的语义相似度的方法</p>

<h3 id="331-automated-response-evaluation">3.3.1. Automated response evaluation</h3>
<p>根据自动评估系统目标的不同进行分类:</p>
<ol>
  <li>为了评估学习者的语言使用能力，许多评估系统从语言和语法使用、内容组织等方面评估写作质量，这样的系统有ETS(educational testing services)、E-Rator 、Coh-metrics、AcaWriter</li>
  <li>评估学生答案时要求学生的答案涵盖特定的知识，只有涵盖了最关键的部分，学生才能获得满分。为了这个目标，许多系统训练了一个预测模型，有运用词重叠(word overlapping)，语义和语法相似等特征的模型，也有预训练模型来做embedding的模型</li>
</ol>

<p>论文工作属于第二类</p>

<h3 id="332-semantic-similarity-measurement">3.3.2. Semantic similarity measurement</h3>
<p>量化两段文本的相似度是自然语言处理的基本步骤，短文本自动评估系统通过文段相似度的测量来量化学生答案和参考答案的相似度。测量方法有term matching(术语匹配)技术和涉及外部知识的语义计算(semantic computation)</p>
<ol>
  <li>term matching技术基于真实文本和预测文本的公共单词，基于term matching的方法有BLUE和Rouge。</li>
  <li>term matching有个明显的短板就是无法处理近义词或者同义词，可以用WordNet来解决。除此之外，一个单词或者短语的意思可以被分解成量化的语义块，这样测量起来才是数字化的(非二进制)，这样的方法有LSA(Latent Semantic Analysis)、Word2vec、GloVe。但即使是Word2vec也无法解决一词多义的问题，所以诞生了单词的动态语义嵌入(即考虑了context)，这样的模型有RNN-based ELMo、Bert等等</li>
</ol>

<h2 id="34-patterns-and-indicative-powers-of-facets-matching-states">3.4. Patterns and indicative powers of facets matching states</h2>
<p>也就是intro里提到的第一个工作，着眼于发现state和response type的pattern，然后利用这个pattern来做预测(通过答案的语义面匹配状态来预测答案的评分)</p>

<h3 id="341-materials-and-methods">3.4.1. Materials and methods</h3>
<h4 id="3411-dataset">3.4.1.1. Dataset</h4>
<p>数据集使用的是SciEntsBank，数据集大约有10000个学生答案和197个问题，学生答案分为五类(5-ways，见表格1)，训练集和测试集分别有4969和5835个样本，同样地，根据问题的不同分为: UA, UQ, UD。数据集中每一个问题都包含语义面的标注，正确的学生答案应该cover这些语义面，数据集中同样包含语义面匹配状态的标注，语义面的状态一共有八种，见表格2</p>

<center><img src="../assets/img/posts/20221010/26.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20221010/27.jpg" /></center>

<p>为了更直观地了解样本和语义面及语义面匹配状态，这里举了一个例子:</p>

<center><img src="../assets/img/posts/20221010/28.jpg" /></center>
<center><img src="../assets/img/posts/20221010/29.jpg" /></center>

<p>这个例子的语义面及两个学生答案对应的语义面匹配状态见表格3:</p>

<center><img src="../assets/img/posts/20221010/30.jpg" /></center>

<p>通过表格3我们可以发现语义面关注相关对象的特定属性或状态，这里可以发现相对正确的答案A基本上与所有的语义面都匹配，但是相对不正确的答案B与某些语义面冲突</p>

<h4 id="3412-summary-of-facet-matching-states">3.4.1.2. Summary of facet matching states</h4>
<p>每个语义面只有一个语义面匹配状态，不同问题的语义面数量不同，可以用语义面匹配状态的分布情况来总结一个问题的总体的语义面匹配状态，这样我们就可以比较不同评分等级的答案的匹配状态，比如我们可以比较答案A和B的分布，见表格4:</p>

<center><img src="../assets/img/posts/20221010/31.jpg" /></center>

<h4 id="3413-answer-quality-prediction-with-facet-matching-states">3.4.1.3. Answer quality prediction with facet matching states</h4>
<p>不同答案的语义面匹配状态的分布不同，可以将八种匹配状态的分布权重视为feature，答案的类型作为label，那么就可以进行分类。作者采用Gradient Boosting Tree(GBT)来当作预测模型。这样就可以通过答案的匹配状态分布来预测答案的评分</p>

<h3 id="342-results-and-analysis">3.4.2. Results and analysis</h3>
<h4 id="3421-facet-matching-pattern">3.4.2.1. Facet matching pattern</h4>
<p>数据集总体的语义面匹配状态分布见表格5，利用卡方检测，可以发现答案的类型与语义面匹配状态的权重(即分布)有关</p>

<center><img src="../assets/img/posts/20221010/32.jpg" /></center>

<p>归一化每个匹配状态的权重，得到下图的分布，可以发现不同答案类型的匹配状态分布不同:</p>

<center><img src="../assets/img/posts/20221010/33.jpg" /></center>

<p>观察发现，Correct的答案基本上express语义面，non domain的答案基本上unaddress语义面，不同类型答案的匹配状态分布情况就是一个pattern，用来连接一个答案的匹配状态和类型</p>

<h4 id="3422-answer-evaluation-leveraging-facet-matching-states">3.4.2.2. Answer evaluation leveraging facet matching states</h4>
<p>接下来评估以下GBT模型(通过匹配状态来预测类型)的表现情况，结果见表格6，性能度量是Macro F1</p>

<center><img src="../assets/img/posts/20221010/34.jpg" /></center>

<p>通过表格6可以发现，这个分类任务是具有挑战性的，基本上所有的模型都取得了相对较低的F1值，比较不同模型发现，GBT取得了最好的结果。总的来说，现实任务中不太可能会知道一个问题的语义面或者语义面匹配状态，所以作者将工作拓展到更一般的情况，也就是3.5节将介绍的内容</p>

<h2 id="35-automatic-extraction-of-facets-matching-features-for-better-prediciton">3.5. Automatic Extraction of Facets Matching Features For Better Prediciton</h2>
<p>对于语义面和语义面匹配状态都没有的情况下，作者提出了抽取语义面和预测语义面匹配状态的方法，然后利用特征抽取的方法挖掘出语义面匹配状态分布与pattern的关系，再结合语义相似性来预测答案的类型，这种方法大大提高了预测的表现</p>

<h3 id="351-materials-and-methods">3.5.1. Materials and methods</h3>
<h4 id="3511-dataset">3.5.1.1. Dataset</h4>
<p>数据集使用的是Beetle数据集，56个问题，5000个学生答案，标签有5-way和3-way，训练集和测试集分别有3941和1258个样本，测试集分为UA, UQ</p>

<h4 id="3512-automatic-semantic-facet-extraction">3.5.1.2. Automatic semantic facet extraction</h4>
<p>Beetle数据集的参考答案不包含语义面，但是一个问题会有多个参考答案，他们之间有细微的差别，一个例子:</p>

<p>问题是: Why does measuring voltage help you locate a burned out bulb?</p>

<p>参考答案:</p>
<ul>
  <li>Measuring voltage indicates the place where the electrical state changes due to a damaged bulb</li>
  <li>Measuring voltage indicates the place where theelectrical state changes due to a gap</li>
  <li>Measuring voltage indicates whether two terminals are connected to each other</li>
  <li>Measuring voltage indicates whether two terminals are separated by a gap</li>
</ul>

<p>接下来将介绍作者如何从参考答案集中抽取出语义面:</p>
<ol>
  <li>关键词提取: 从问题和参考答案中抽取出现次数超过两次的词，这些词就是关键词(pivotal word)</li>
  <li>基于语法的expression提取: 从参考答案的语法树(dependency parsing tree)中获取与语法相关的术语来形成更完整的expression，下面这张图展示了例子中四个参考答案的语法树:</li>
</ol>

<center><img src="../assets/img/posts/20221010/35.jpg" /></center>

<p>语法树建立句子中单词的语法关系，从中心词往外伸展，一个句子的中心词就是它的谓语。有两种生成语义面的方法，第一种就是连接关键词和树上与它相邻的词，比如voltage和measuring构成了一个语义面measuring voltage。第二种方法是针对一对关键词，找到它们的最小公共节点构成语义面，比如terminals和gap的最小公共节点是separated，那么它们可以构成语义面terminals separated gap。第一种方法的出发点是一个关键词可能是一个expression的一部分，第二种方法的出发点是两个关键词可以覆盖较大的语义区域，再结合它们的公共节点，便可包含多个语义面，成为一个新的语义面。</p>

<p>通过这两种方法可能会生成意义不明的语义面，但是这也是后续算法需要考虑的部分，增强了算法的泛化性</p>

<h4 id="3513-facet-matching-features">3.5.1.3. Facet matching features</h4>
<p>有了学生答案的语义面后，接下来就需要实现语义面匹配状态的预测。为了实现预测模型，作者用带匹配状态标签的SciEntsBank数据集进行训练。模型的细节在Appendix A中具体展开。模型的结构如下图所示:</p>

<center><img src="../assets/img/posts/20221010/36.jpg" /></center>

<p>模型的左边代表学生答案的输入，模型的右边代表语义面的输入，它们的每个词元都经过Bi-LSTM(embedding选择的是Glove)，然后输出隐状态，然后用注意力机制获得$\tilde{h_i^F}$，查询是语义面的hidden state，K和V是答案的hidden state，注意力权重为:</p>

<p>
\begin{equation}
a_{i,j}=\frac{e^{(h_i^F)^T\cdot h_j^R}}{\sum_{j'=1}^{N_R}e^{(h_i^F)^T\cdot h_{j'}^R}}
\end{equation}
</p>

<p>为了获取facet更全面的信息，作者将$\tilde{h_i^F}$、$h_i^F-\tilde{h_i^F}$、$h_i^F\odot \tilde{h_i^F}$、$h_i^F$连结了起来:</p>

<p>
\begin{equation}
\mathbf{c}_i=\left[\tilde{\mathbf{h}}_i^F ; \mathbf{h}_i^F-\tilde{\mathbf{h}}_i^F ; \mathbf{h}_i^F \odot \tilde{\mathbf{h}}_i^F ; \mathbf{h}_i^F\right]
\end{equation}
</p>

<p>接着对C进行取平均和最大的操作，concat之后输入MLP作预测，MLP这里取了三层，神经元数量分别为64、32和8，激活函数用ReLU:</p>

<p>
\begin{equation}
\begin{aligned}
&amp;\mathbf{c}_{\max }=\max (\mathbf{C}) \\
&amp;\mathbf{c}_{\min }=\operatorname{avg}(\mathbf{C}) \\
&amp;\mathbf{c}_{\mathrm{agg}}=\left[\mathbf{c}_{\max } ; \mathbf{c}_{\mathrm{avg}}\right] \\
&amp;\hat{\mathbf{y}}=\operatorname{softmax}\left(\operatorname{MLP}\left(\mathbf{c}_{\mathrm{agg}}\right)\right)
\end{aligned}
\end{equation}
</p>

<p>模型定义完后，作者比较了三种GBT的效果，GBT-Gold代表GBT在标注好的匹配状态和语义面上训练，GBT-Predict表示GBT在预测的匹配状态和标注好的语义面上训练，GBT-Approx表示GBT在预测的匹配状态和语义面上训练，性能度量同样是Macro F1，数据集用的也是SciEntsBank:</p>

<center><img src="../assets/img/posts/20221010/37.jpg" /></center>

<p>从表格中可以看出，GBT-Predict效果和GBT-Gold的效果差不多，根据预测的语义面匹配状态，我们有:</p>
<ul>
  <li>Aggregated facet matching states: 一个学生答案的预测语义面有很多，预测的语义面匹配状态需要聚合在一起。作者提出了两种聚合方法: 第一种软的是平均了所有语义面的预测匹配状态的概率，第二种硬的做法是针对每个语义面，只保留概率最高的匹配状态，两种方法都可以获得匹配状态的分布</li>
  <li>Pattern matching: 将网络结构的输出aggregate之后，与五种类型的真实匹配状态分布对比后，了解该答案的预测匹配状态分布属于哪一种类型。具体的对比方法是选取KL散度最小的作为该答案的类型，KL散度能测量两个分布的差距，0代表两个分布相似度最高，假如预测的分布为$\tilde{p(x)}$，那么KL散度为:</li>
</ul>

<p>
\begin{equation}
D(p_k(x)\|\tilde{p(x)}=\sum_{x\in C}p_k(x)log\frac{p_k(x)}{\tilde{p(x)}})
\end{equation}
</p>

<ul>
  <li>Confidence of prediction: 除了pattern matching外，匹配状态的confidence level也包含了很多信息。通过aggregate获得语义面的匹配状态分布后，取每个语义面预测概率值最高的概率值作为该语义面的confidence level。那么我们可以计算出Noisy-OR score:</li>
</ul>

<p>
\begin{equation}
Noisy-OR=1-\prod_{i=1}^{N_F}(1-\tilde{p_i})
\end{equation}
</p>

<p>其中$N_F$是语义面的个数，$\tilde{p_i}$就是语义面i的confidence level。如果所有的预测概率都是1，那么Noisy-OR为1，如果所有的预测概率都是0，那么Noisy-OR为0。Noisy-OR衡量了模型中至少有一个预测是合适的可能性有多大。Noisy-OR同样可以对某一个语义面匹配状态进行计算，这样得到的特征更好配合后续的使用</p>

<h4 id="3514-semantic-closeness-features">3.5.1.4. Semantic closeness features</h4>
<p>除了3.5.1.3小节中提到的基于feature的分类方法，基于语义相似度的分类方法也适用。接下来会介绍一些计算参考答案和学生答案语义相似度的方法，如果有多个参考答案，取平均值即可:</p>
<ol>
  <li>Term Matching Features: 用这些指数来计算语义相似度: N-gram overlapping, Rouge, Rouge-1, Rouge-2, Rouge-l, BLUE</li>
  <li>Fixed and dynamic embedding features: Glove, LSA, BERT。对参考答案和学生答案编码后计算相似度</li>
  <li>Semanic entailment features: 使用预训练的text entailment model: Decomposable Attention Model。输入文本对后，模型预测这两段文本的关系，有entailment, contradict, neutral</li>
</ol>

<h3 id="352-results-and-analysis">3.5.2. Results and analysis</h3>
<p>为了评估3.5.1.3.和3.5.1.4.这两节提出的features，可以用GBT来测试。GBT可以看成随机森林，GBT在不同的特征上进行训练，表格7给出了不同特征上训练的结果:</p>

<center><img src="../assets/img/posts/20221010/38.jpg" /></center>

<p>GBT(Full)表示上面提到的所有feature都运用了，包括aggregated matching state、KL散度、Noisy-OR、语义相似度和语义推断(semantic entailment)。可以发现将所有的信息都运用后，GBT(Full)模型是表现最好的模型。然后作者希望了解facet feature是否真的有效，GBT(Facets)就是运用了Facets所有特征的模型，横向对比GBT(Sem.Sim)和GBT(Sem.Entail)，发现facet的表现最好，不止如此，将这些特征全部结合得到的效果会更好。最后对比以下facet三个特征: Nosiy-OR、KL散度、aggregated matching states。发现Noisy-OR的效果最好。</p>

<p>为了更深入地了解facet的三个特征，作者画出了三种特征的importance scores，如下图所示:</p>

<center><img src="../assets/img/posts/20221010/39.jpg" /></center>

<p>importance score表明了数的预测结果相对于特征值的变化有多大，值越高说明该特征对于GBT来说越重要，作者将这些值归一化。观察发现，facet的三个特征中，Noisy-OR最重要，其次是KL散度，最后是匹配状态，这和表格7得出的结果一致。同时也发现语义相似度作为特征来说表现其实已经很好了，比但看facet的任意一个特征都强，但是却不如facet的三个特征加起来使用，变相证明了作者工作的有效性。还有就是虽然Semantic entailment作为特征来说表现不佳，但是也能将模型的整体性能提升一点</p>

<h2 id="36-discussion">3.6. Discussion</h2>
<p>总的来说，作者用两个数据集进行了两个实验，第一个实验首先得到了五种类型的答案的分布情况，然后用GBT实现了通过语义面匹配状态的分布来预测答案的类型。第二个实验首先给出了生产语义面的方法，然后提出了预测语义面匹配状态的网络，有了语义面匹配状态后，就可以得到facet的三个特征: state分布、KL散度和Noisy-OR。实验发现Noisy的效果很好，并且利用confidence进行预测是一种新思路，可能会对后续研究有所帮助。</p>

<p>然后作者分析实验结果发现，模型目前能明显区分出correct和non domain、irrelevant，但是与partially correct、contradictory相比却并不能很好的区分开。作者认为可能是partially和correct在语义上可能并不冲突导致。</p>

<p>接着，作者着眼于强调论文研究的重要性。论文里提出的facet不仅能提高模型的性能，更重要的是它们可以为更详细的反馈提供相关信息，这是教育系统为学习者提供反馈不可缺少的部分。具体来说，facet的出现让评分的可解释性更强。这样我们就可以分析学生答案是哪个知识点没有答对，或者是哪个知识点没有出现在答案中。这样，教育系统就能为学习者提供有效的反馈，可以提供点对点的反馈，比如这个facet为什么没有expressed等等。facet的实用价值高，它除了作为feature外，还可以使答案分析的可解释性更强。</p>

<h2 id="37-conclusion">3.7. Conclusion</h2>
<p>之前的许多研究往往关注评估学生回答的模型的表现，我们将待评估问题的知识点分解为从属的语义单元，称为语义面。每一个语义面都代表知识的某个方面，而它们共同构成参考答案的语义。然后作者实现了两个实验来说明研究的有效性，具体内容在discussion中已经说明。</p>

<p>future work: larger corpora, 优化语义面提取的算法, 预测state的算法使用更新的网络(比如bert及其衍生网络), facet的实用价值(比如提高解释性和用作feedback)</p>

<h2 id="38-小结">3.8. 小结</h2>
<p>facet这篇论文的着重点就是提出了语义面，这篇论文的出发点其实很好想到，就是评估模型的性能可能无法有很大的提升了，那么我将答案分成好几个小部分进行评估是否能提升模型的性能？于是便有了这篇论文，facet可以理解成参考答案的知识点，它是否在学生答案中表达出就是它的states。论文做了两个工作，第一个是直接基于有facet标注和state标注的数据集上进行实验，因为不同问题的facet数量不一样，所以用state的分布来表达可能会更好，首先通过统计得到了不同类型答案的state分布，这为后续KL散度这一特征有帮助。然后使用GBT作为预测模型，通过state的分布来预测答案的类型，得到了较好的效果。第二个实验基于正常的数据集，就是不包含facet和state的标签，这也是比较一般的情况，毕竟标注facet和state非常贵。首先每个问题没有了facet，那么首先就得设计算法来得到问题的facet，作者提出了一种基于Dependency parsing tree得到facet的方法，并在附录B对这个算法进行了评价。得到facet后还是没有state，所以得设计一个网络来得到facet的状态，模型的输入是facet和学生的response，利用LSTM来获得隐状态，并用注意力机制找到facet的近似表示，然后连结了facet的隐状态的各类信息，最后加个MLP得到state。到目前为止，已经有了facet和state，那么就可以得到facet的特征来作预测，相当于特征工程。facet的特征使用到的有: state分布, 与第一个工作中真实分布对比的KL散度, 运用到confidence的Noisy-OR。除此之外，作者还对比了其余的特征，比如语义相似度和semantic entailment(这是已有的工作)。最后将这些特征fusion之后的效果比较好</p>

<h1 id="4-text-to-text-semantic-similarity-for-automatic-short-answer-grading">4. Text-to-Text Semantic Similarity for Automatic Short Answer Grading</h1>
<p>论文题目为Text-to-Text Semantic Similarity for Automatic Short Answer Grading，文章的出发点是利用语义相似度来进行ASAG任务，作者将语义相似度分为了Knowledge-Based Measures和Corpus-Based Measures，其中前者就只考虑词和词的相似性，后者考虑了词表来测量词的相似性关系</p>

<h2 id="41-knowledge-based-measures">4.1. Knowledge-based Measures</h2>
<p>Reference A的每个单词和Student A同词性的每个单词进行语义相似度的测量，找出语义相似度最高的值作为该单词该词性的语义相似度。作者比对了八种不同的语义性测量方法:</p>
<ul>
  <li>shortest path:</li>
</ul>
<p>
\begin{equation}
Sim_{path}=\frac{1}{length}
\end{equation}
</p>

<p>其中length表示两个词的最短路径(通过node-counting的方法)</p>

<ul>
  <li>Leacock &amp; Chodorow</li>
  <li>Lesk</li>
  <li>Wu &amp; Palmer</li>
  <li>Resnik</li>
  <li>Lin</li>
  <li>Jiang &amp; Conrath</li>
  <li>Hirst &amp; St.Onge</li>
</ul>

<h2 id="42-corpus-based-measures">4.2. Corpus-Based Measures</h2>
<p>上面比较词的相似度的方法不考虑整个序列的语法和词表信息，Corpus-Based方法考虑了词表和序列信息，具体来说，作者使用了LSA(latent semantic analysis)和ESA(Explicit semantic analysis)来测量语义相似度</p>

<h2 id="43-experiment">4.3. Experiment</h2>
<p>实验用相关性作为衡量各种语义相似度测量的指标，knowledge-based measures与Wordnet里提供的词的相似性指标进行相关性的测量，LSA与wordnet里提供的Infomap指标进行相关性的测量，ESA则是使用ESA算法进行测量，实验结果如下:</p>

<center><img src="../assets/img/posts/20221010/40.png" /></center>

<h2 id="44-lsalatent-semantic-analysis">4.4. LSA(Latent semantic analysis)</h2>
<p>LSA是分析语义相似度的传统方法，这里对该方法进行简要的介绍。LSA的中文说法是潜在语义分析，是一种无监督学习方法，主要用于文本的话题分析。</p>

<p>首先介绍一下单词向量空间与话题向量空间，单词向量空间就是用一个向量表示一段文本的语义，向量的每一维对应一个单词，其数值为该单词在文本中出现的频数或权值，向量空间的度量，如内积或标准化内积表示文本之间的语义相似度，单词向量空间的优点是模型简单，计算效率高。，局限性在于内积相似度未必能够准确地表达两个文本的语义相似度。</p>

<p>话题向量空间模型就是给定一段文本，用话题空间的一个向量表示该文本，向量的每一个分量对应一个话题，其数值为该话题在该文本中出现的权值。所谓话题，就是指文本所讨论的内容或主题，一段文本一般含有若干个话题，话题由若干个语义相关的单词表示。用两个向量的内积或标准化内积表示两段文本的语义相似度。</p>

<p>潜在语义分析利用矩阵奇异值分解(SVD)，对单词-文本矩阵进行奇异值分解，左矩阵作为话题向量空间，对角矩阵与右矩阵的乘积作为文本在话题向量空间的表示。这样就可以通过单词向量空间得到话题向量空间的向量表示，然后计算不同文本向量的内积就可以得到语义相似度。</p>
<center><img src="../assets/img/posts/20221010/47.png" /></center>

<center><img src="../assets/img/posts/20221010/48.png" /></center>

<h1 id="5-pre-training-bert-on-domain-for-asag">5. Pre-Training Bert on Domain for ASAG</h1>
<p>论文全称为Pre-Training BERT on Domain Resources for Short Answer Grading，同样使用BERT作为主干网络，本篇文章的主要亮点就是利用当前领域的textbooks和QA对BERT进行微调，扩充了预训练的数据集，相当于是一种针对领域的微调方法</p>

<h2 id="51-usage-of-textbooks">5.1. Usage of Textbooks</h2>
<p>使用特定领域的textbook来扩充预训练数据集，将textbooks分为多个段落进行微调</p>

<h2 id="52-usage-of-question-answer-pairs">5.2. Usage of Question-Answer Pairs</h2>
<p>使用正确的Student A和Reference A作为一个pair进行微调，因为BERT本身的任务就有下一句预测，所以相对合理</p>

<h2 id="53-微调asag">5.3. 微调ASAG</h2>
<p>Student A和Reference A作为一个句子对输入BERT，提取&lt;cls&gt;输入一个全连接层进行预测</p>

<h2 id="54-experiments">5.4. Experiments</h2>
<center><img src="../assets/img/posts/20221010/41.jpg" /></center>
<center><img src="../assets/img/posts/20221010/42.jpg" /></center>

<h1 id="6-imporving-short-answer-grading-using-transformer-based-pre-training">6. Imporving Short Answer Grading Using Transformer-Based Pre-training</h1>
<p>论文全称Imporving Short Answer Grading Using Transformer-Based Pre-training，算是第一篇将Bert应用于ASAG任务上的论文，简单的输入Student A和Reference A作为序列对，然后利用&lt;cls&gt;进行分类</p>

<h2 id="61-数据集">6.1. 数据集</h2>
<p>数据集使用了SemEval2013和两个心理学领域的数据集</p>
<center><img src="../assets/img/posts/20221010/44.jpg" /></center>

<h2 id="62-实验">6.2. 实验</h2>
<center><img src="../assets/img/posts/20221010/45.jpg" /></center>

<h1 id="7-investigating-transformers-for-automatic-short-answer-grading">7. Investigating Transformers for Automatic Short Answer Grading</h1>
<p>论文全称Investigating Transformers for Automatic Short Answer Grading，比对了不同的BERT-like架构在ASAG任务上的表现，主要探讨了多语言Transformer的表现、不同Pre-training tasks的表现、knowledge disillation的表现。用wmt2019表现最好的模型做翻译</p>

<h2 id="71-实验">7.1. 实验</h2>
<p>总的实验结果:</p>
<center><img src="../assets/img/posts/20221010/43.jpg" /></center>

<h2 id="72-结果分析">7.2. 结果分析</h2>
<ul>
  <li>大模型能提升ASAG任务的效果吗: 可以</li>
  <li>多语言Transformer的表现如何: 表现一般，XLM模型的表现不好，XLMRoBerta表现和普通的RoBerta差不多</li>
  <li>预训练的时候采用多种语言可以提升模型的泛化能力，在别的语言上也能取得不错的效果</li>
  <li>有更好的预训练任务吗: 实验结果表明，预训练任务MNLI(自然语言蕴含任务)能极大提升ASAG任务的效果</li>
  <li>knowledge distillation表现如何: 虽然说distil的bert性能会下降，但是在节省40%参数的情况下只降低了2%的效果，可以接受</li>
</ul>

<h1 id="8-superlative-model-using-word-cloud-for-short-answers-evaluation-in-elearning">8. Superlative model using word cloud for short answers evaluation in eLearning</h1>
<p>论文的主旨是通过RA和SA生成词云(word cloud)的方法来辅助老师对短文本回答进行评估，生成词云的模型作者命名成superlative model</p>

<h2 id="81-superlative-model">8.1. Superlative Model</h2>
<p>生成词云的大致步骤:</p>
<center><img src="../assets/img/posts/20221010/49.jpg" /></center>

<p>前面的几步可以看出数据预处理的步骤，这里我对其进行简单介绍:</p>
<ol>
  <li>把RA和SA拆成语料，也就是单个的单词</li>
  <li>去掉无用的词，比如冠词、连词、问题包含的单词也可以去掉</li>
  <li>通过wordnet(相当于一本词典)，合并同义词</li>
  <li>替换复数</li>
  <li>生成单词-文本矩阵，行代表单词，纵代表文本，用哈希算法进行存储</li>
  <li>生成词云</li>
</ol>

<p>这里生成词云可以细说一下，主要生成了两种词云，作者对其命名为cohesion word cloud和relative word cloud。前者cohesion word cloud代表RA和SA的共同词组成的词云，后者是非共同词组成的词云，一个例子:</p>
<center><img src="../assets/img/posts/20221010/50.jpg" /></center>

<center><img src="../assets/img/posts/20221010/51.jpg" /></center>

<h2 id="82-词云-word-cloud">8.2. 词云 word cloud</h2>
<p>词云（Word Cloud)又称文字云，是文本数据的视觉表示，由词汇组成类似云的彩色图形，用于展示大量文本数据。每个词的重要性以字体大小或颜色显示。</p>

<h2 id="83-wordnet">8.3. WordNet</h2>
<p>WordNet是一个由普林斯顿大学认识科学实验室在心理学教授乔治·A·米勒的指导下建立和维护的英语字典。由于它包含了语义信息，所以有别于通常意义上的字典。WordNet根据词条的意义将它们分组，每一个具有相同意义的字条组称为一个synset(同义词集合)。WordNet为每一个synset提供了简短，概要的定义，并记录不同synset之间的语义关系。</p>

<p>wordnet可以获得两个单词之间的语义相似度</p>

<h1 id="9sentence-level-or-token-level-features-for-automatic-short-answer-grading-use-both">9.Sentence Level or Token Level Features for Automatic Short Answer Grading?: Use Both</h1>
<h2 id="91-proposed-features">9.1. Proposed Features</h2>
<p>论文结合了hand-crafted feature(token level feature)和sentence-level feature(deep-learning)进行ASAG任务，模型的总览图如下:</p>
<center><img src="../assets/img/posts/20221010/52.jpg" /></center>

<h3 id="911-sentence-level-features">9.1.1. Sentence Level Features</h3>
<p>对于问题，参考答案，学生回答(q,r,a)对而言，首先获得了这三个序列的sentence embedding，使用了InferSent模型，Infersent模型是一个有监督的语句嵌入模型，和sentence2vec有点像。获得qra的语句嵌入后，模型计算了以下的feature:</p>

<p>
\begin{equation}
S_{feat}(q,r,a) = (r * a, |r - a|, r * q, |r - q|, a * q, |a - q|)
\end{equation}
</p>

<h3 id="912-token-level-features">9.1.2. Token Level Features</h3>
<p>首先需要对数据进行预处理，和上一篇论文类似，首先需要去掉RA和SA中的stop words，就是一些没有意义的词，然后需要做question demoting，即去掉问题中出现的单词，接着就可以获得两个bag of words，一个是RA的，一个是SA的。然后就可以根据这两个词袋获得以下features:</p>

<ul>
  <li>Word Overlap，词重叠。取RA中的每一个单词，和SA中的每一个单词计算分数，如果分数超过某一个阈值，那么就认为是overlapping的，分数的计算公式如下:</li>
</ul>

<p>
\begin{equation}
Score(\omega_i,SA) = \mathop{max}_{\omega_j\in SA}Cos(\omega_i, \omega_j), where \quad \omega_i \in RA
\end{equation}
</p>

<p>或者根据wordnet中两个单词属于同一个synset来判断它是否是overlapping，随后计算出Precision/Recall/Precision*Recall来作为features</p>

<ul>
  <li>Histogram of Partial Similarity(HoPS)，HoPS的目标是捕获SA和RA之间的similarity pattern。对于RA中的每个单词$\omega_i$，计算与SA的相似度分数，然后可以得到index I的值:</li>
</ul>

<p>
\begin{equation}
I(\omega_i)=min(\frac{Score(\omega_i, SA)+1}{h},N-1) where \quad h=\frac{2}{N}
\end{equation}
</p>

<ul>
  <li>HoPs with POS tags and Question Types: 这个feature是HoPS的拓展，将RA根据词性分为动词，名称，形容词，副词和其他，然后计算HoPS时，将每个bin分为在RA中和该单词拥有相同词性的单词的个数。Question type就是问题的种类，作者分为了8类，分别是How, What, Why, Who, Which, When, Where, Whom，然后根据问题的类别生成8个二进制的feature</li>
</ul>
<center><img src="../assets/img/posts/20221010/53.jpg" /></center>

<h2 id="92-token-level的消融实验结果">9.2. Token level的消融实验结果</h2>
<center><img src="../assets/img/posts/20221010/54.jpg" /></center>

<h1 id="10-an-experimental-study-of-text-preprocessing-techniques-for-asag-in-indonesian">10. An Experimental Study of Text Preprocessing Techniques for ASAG in Indonesian</h1>
<h2 id="101-introduction">10.1. Introduction</h2>
<p>论文介绍了一些针对于ASAG任务的预处理方法，它使用了印度尼西亚语的问题和答案</p>

<h2 id="102-预处理方法">10.2. 预处理方法</h2>
<p>Burrows et al.总结了文本预处理的五方面技巧，分别是lexical, morphological, semantic, syntactic and surface，所对应的技巧如下图所示:</p>
<center><img src="../assets/img/posts/20221010/55.png" /></center>

<p>然后作者根据以往的研究，总结了应用于ASAG任务的五个预处理技巧:</p>
<ul>
  <li>Case Folding: 将所有字母小写</li>
  <li>Tokenization: 将序列分成词元，在这个过程中可能会丢弃一些字符，比如标点符号</li>
  <li>Punctuation Removal: 在词元化后，会移除所有的标点符号</li>
  <li>Stop Word Removal: 一些common word在序列中可能并没有什么意义，这些词被称作stop word，需要去除</li>
  <li>Stemming: 去掉单词的词缀</li>
</ul>

<h2 id="103-research-method">10.3. Research Method</h2>
<p>在预处理完毕后，计算RA和SA的余弦相似度，作为分数</p>
<center><img src="../assets/img/posts/20221010/56.jpg" /></center>

<p>作者一共进行了两组实验，一组实验是使用了Tokenization和Punctuation的技巧，另一组实验多使用了Case Folding, Stemming, Stopword Removal的技巧，计算学生答案的平均分数和老师答案平均分数的correlation(相关性)和MAE，然后用t-test来判断两组实验是否有区别</p>

<h2 id="104-实验结果">10.4. 实验结果</h2>
<center><img src="../assets/img/posts/20221010/57.png" /></center>
<p>发现没有很大区别</p>

<h1 id="11-feature-engineering-and-ensemble-based-approach-for-improving-automatic-short-answer-grading-performance">11. Feature engineering and ensemble-based approach for improving automatic short-answer grading performance</h1>
<h2 id="111-introduction">11.1. Introduction</h2>
<p>文章整合了ASAG领域的一些feature engineering的技巧，有传统的text similarity和一些新颖的features，比如relevance feedback based features, topic-modelling features, information retrieval motivated feature和Inverse document frequency based overlap feature。对比不同feature的效果，融合不同技巧进行ASAG任务(ensemble)，有点像一篇综述</p>

<h2 id="112-problem-definition">11.2. Problem Definition</h2>
<h3 id="1121-asag-as-regression-task">11.2.1. ASAG as regression task</h3>
<p>输入SA,RA，返回一个分数。回归任务的目标是学习一个回归模型$Y=f(\vec{X}, \vec{\omega})$，其中$\vec{X}$是一个n维的相似向量(similarity vector)，通过n个RA和SA的相似度度量计算得出，模型的目标就是拟合出这些相似性度量的回归系数$\omega$，回归任务的性能度量是均方根误差(RMSE)和皮尔森相关系数$\rho$</p>

<h3 id="1122-asag-as-classification-task">11.2.2. ASAG as classification task</h3>
<p>分类任务的本质其实和回归任务类似，分类模型的目标是计算出当前变量对于每个类别的分数，然后选出分数最高的类别作为分类类别，k是类别:</p>

<p>
\begin{equation}
\begin{aligned}
&amp; score(X_i,k)=\beta_k \cdot X_i  \\
&amp; k^*= \mathop{argmax}_i score(X_i, k)
\end{aligned}
\end{equation}
</p>

<p>分类任务的性能度量是权平均F1值和Macro-average F1值</p>

<h2 id="113-feature-extraction">11.3. Feature extraction</h2>
<p>作者将text similarity feature分为了以下6个类别并对其进行了简单介绍:</p>
<center><img src="../assets/img/posts/20221010/58.jpg" /></center>

<h3 id="1131-semantic-similarity-features">11.3.1. Semantic Similarity Features</h3>
<ul>
  <li>Knowledge-based measures: 使用WordNet查询单词的语义相似度</li>
  <li>Corpus-based features: LSA</li>
  <li>Word-embedding feature: 连续词袋模型(CBOW)和跳元模型(skip-gram)</li>
</ul>

<h3 id="1132-lexical-overlap-features">11.3.2. Lexical Overlap Features</h3>
<p>RA和SA之间的回答会有很多词重叠的部分，可以利用起来作为features:</p>
<center><img src="../assets/img/posts/20221010/59.jpg" /></center>

<ul>
  <li>Word-overlap features: 有Jaccard Similarity Coefiicient, Simple word overlap等</li>
  <li>Summary evaluation measures: ROUGE-N，ROUGE一开始是广泛用于摘要生成的效果评估，具体做法是比较参考摘要和生成摘要共有的gram除以参考摘要的总gram数</li>
</ul>

<h3 id="1133-information-retrieval-measures">11.3.3. Information Retrieval Measures</h3>
<p>TF-IDF可以用于估计RA和SA之间的相关性和相似度，公式为:</p>

<center><img src="../assets/img/posts/20221010/60.jpg" /></center>

<h3 id="1134-topical-similarity-features">11.3.4. Topical Similarity Features</h3>
<ul>
  <li>Latent Dirichlet Allocation(LDA): 假设每段文本都在讨论多个话题的融合，每个话题由文本中出现的单词组成，LDA能处理多义词。每段文本都是由多个话题组成，那么就可以计算出RA和SA之间的话题相似度</li>
  <li>Biterm Topic Model(BTM): 相较于LDA而言，BTM处理短文本的能力更强</li>
</ul>

<h3 id="1135-relevance-feedback-based-features">11.3.5. Relevance Feedback-based Features</h3>
<p>通过学生的回答来更新原始的参考答案，这样可以增强参考答案的词汇量，生成这样的features分为两步，第一步是计算相似度，第二步是更新参考答案。不同features计算相似度的方法都是类似的</p>
<ul>
  <li>similarity computation step: 通过LSA的方法计算出每个学生回答相对于参考答案的余弦相似度，作为similarity</li>
  <li>Top Scorer Dependent Feedback Feature(RF-I): 通过相似度最高的几个学生答案的单词对参考答案进行更新，这样可以重新计算出学生答案和参考答案的余弦相似度，也就是一种根据学生答案对参考答案进行更新的一种反馈机制</li>
  <li>Least Scorer Dependent Feedback Feature(RF-II): 和RF-I类似，但是更新变成了至少P个学生答案</li>
  <li>All Scores Dependent Feedback Feature (RF-III): 全部的学生答案对参考答案进行更新</li>
</ul>

<h3 id="1136-alignment-based-features">11.3.6. Alignment-based Features</h3>
<p>配对学生答案和参考答案的语义相近的单词获得的feature，这里作者没有详细介绍，说可以在word-to-word alignment using word-aligner中找到详细的解释</p>

<h2 id="114-answer-grading-models">11.4. Answer Grading Models</h2>
<p>使用了多个模型进行实验，既用到了单个的模型，也用到了集成学习的方法</p>

<h3 id="1141-individual-models">11.4.1. Individual models</h3>
<ul>
  <li>Regression: 线性回归，支持向量回归，核方法脊回归(Kernel ridge regression)，各种树等等</li>
  <li>Classification: 随机森林</li>
</ul>

<h3 id="1142-ensemble-learning">11.4.2. Ensemble learning</h3>
<p>对于回归任务而言，分为两步:</p>
<ul>
  <li>ensemble generation: 生成单个的回归模型</li>
  <li>ensemble integration: 对单个的基模型进行集成</li>
</ul>

<p>作者对不同的回归子模型使用了名为Stacked Regression的方法，就是对不同的模型进行滑动平均的方式对其进行集成，在基模型不止一种的情况下会有比较好的结果</p>

<h2 id="115-evaluation">11.5. Evaluation</h2>
<h3 id="1151-test-bed">11.5.1. Test Bed</h3>
<p>针对回归任务，使用了University of North Texas数据集(UNT)，针对分类任务，使用了SRA(Subsets of Student Response Analysis)数据集，这个数据集包含两个子集，一个是ScientsBank，一个是Beetle</p>

<h3 id="1152-实验设计">11.5.2. 实验设计</h3>
<p>一共设置了五组实验:</p>
<ul>
  <li>Performance analysis of feature groups: 分析对比了不同feature的效果</li>
  <li>Feature significance tests: 分析feature是否重要</li>
  <li>Optimal feature set selection: 找出最好的feature set</li>
  <li>Ensemble-based regression- University of North Texas dataset: 判断集成学习在回归任务上的表现，先用数据集训练出单个的回归模型SVR,KRR,LR,LASSO,ELAS-TIC,TREE,BAG,BOOST，然后训练一个regressor(MLP)对这些模型预测的分数进行聚合</li>
  <li>消融实验</li>
</ul>

<center><img src="../assets/img/posts/20221010/61.png" /></center>

<center><img src="../assets/img/posts/20221010/62.jpg" /></center>

<h3 id="1153-实验结果">11.5.3. 实验结果</h3>
<center><img src="../assets/img/posts/20221010/63.jpg" /></center>

<center><img src="../assets/img/posts/20221010/64.jpg" /></center>

<center><img src="../assets/img/posts/20221010/65.jpg" /></center>

<h2 id="116-总结">11.6. 总结</h2>
<p>作者提出的stacked-regression模型相较于其他的模型而言表现更好，alignment-based feature, lexical overlapping features, semantic similarity feature都在其中起到了重要的作用，新加上的一些特征也能有效地提升模型的性能</p>

<h1 id="12-machine-learning-approach-for-automatic-short-answer-grading-a-systematic-review">12. Machine Learning Approach for Automatic Short Answer Grading: A Systematic Review</h1>
<p>一篇整合了44篇使用了机器学习方法来解决ASAG的综述，综述的目标是让每一篇论文都回答以下四个问题:</p>
<ul>
  <li>what is the nature of datasets?</li>
  <li>使用了什么机器学习或者自然语言处理的方法？</li>
  <li>选中的feature是什么？</li>
  <li>实现了什么样的结果？</li>
</ul>

<h2 id="121-nature-of-datasets">12.1. Nature of Datasets</h2>
<p>往往是一个问题对应多个回答，问题多的数据集对应的回答会相对少一点，但一般来说只有一个参考答案。用的较多的数据集是Automated Student Assessment Prize(ASAP)和SemEval 2013中的两个数据集SciEntsBank和Beetle，最后还有回归任务使用较多的Texas数据集</p>

<h2 id="122-natural-language-processing-techniques">12.2. Natural Language Processing Techniques</h2>
<p>就是在特征提取的时候使用的一些预处理技巧，比如punctuation, numbers and other symbols removal, acronym expansion(首字母缩略词扩写), sentence segmentation, case normalization(大小写统一)和tokenization</p>

<p>除此之外，还有一些在使用回答的lexical时用到的技巧，有stopword removal, spelling correction and stemming and lemmatization(还原词干，就是去除后缀)</p>

<p>使用syntactic用到的技巧:part of speech tagging(词性)</p>

<p>使用semantic用到的技巧: Wordnet</p>

<h2 id="123-machine-learning-algorithms">12.3. Machine Learning Algorithms</h2>
<p>使用到的机器学习方法有Artificial Neural Networks, Deep Belief Networks, K-Means</p>

<p>最常见的还是将ASAG任务视为分类或回归任务，使用到的机器学习方法有: 支持向量机，决策树，逻辑回归，Ridge Regreesion，朴素贝叶斯，K则最邻近和线性回归。有些文章使用了集成学习的方法: Stacked Generalization, 随机森林，Gradient Boosting Machine，Bagging和Adaptive Boosting</p>

<h2 id="124-features">12.4. Features</h2>
<p>作者将使用到的feature分为了三类: lexical, syntactic and Semantic</p>
<ul>
  <li>Lexical: N-gram(n=1时就是常见的词袋Bag of Words)，用词出现的频率作为权重构建矩阵。ROUGE, BLUE, Word2vec, lexical similarity。还有一些广泛用到的feature有count of words, response’s length, verb counts等等</li>
  <li>Syntactic: phrase ngrams(combination of the main verb and their noun phrase), denpendency ngrams(syntactical relations between words), similarity between RA and SA POS tags</li>
  <li>Semantic: knowledge-based features(WordNet), corpus-based similarity(LSA,ESA…)</li>
</ul>

<h2 id="125-systems-evaluation">12.5. Systems’ Evaluation</h2>
<p>不同数据集，不同模型的evaluation:</p>
<center><img src="../assets/img/posts/20221010/66.jpg" /></center>

<h1 id="13-automatic-short-answer-grading-via-multiway-attention-networks">13. Automatic Short Answer Grading via Multiway Attention Networks</h1>
<h2 id="131-introduction">13.1. Introduction</h2>
<p>ASAG任务的两大困难点: 1. 短文本回答需要有较深的语义理解 2. 问题往往是开放式的，同时涵盖了多个领域</p>

<p>为了解决以上问题，作者提出了用深度神经网络来解决，作者提出了:</p>
<ol>
  <li>一种end-to-end的方式来解决ASAG任务，不需要人为地提取特征</li>
  <li>一种新的框架，可以拟合RA和SA的语义关系</li>
  <li>可以在多领域使用</li>
</ol>

<h2 id="132-approach">13.2. Approach</h2>
<p>总体的模型架构如下:</p>
<center><img src="../assets/img/posts/20221010/67.png" /></center>

<ul>
  <li>Multiway attention: 重点讲一下中间那个cross-attention，每个$h^q_i$都会和其余的每个$h^p_j$计算注意力权重，一共有四组输出，分别代表不同的注意力机制，{a,s,m,d}分别对应addictive, subtractive, multiplicative, dot-product</li>
  <li>Inside Aggregation: 聚合之前的三组结果，使用了Transformer对其进行聚合</li>
  <li>Prediction Layer: 通过self-attention pooling layer把aggregated sequence representation变成一个定长的向量，注意力池化和注意力机制好像没太大区别，作者这里使用的变化公式为:</li>
</ul>

<p>
\begin{equation}
x=softmax(w^z_1tanh(W^z_2Z^T))Z
\end{equation}
</p>

<p>其中$w^z_1$和$W^z_2$是可学习的矩阵，变换后输入MLP得到预测结果，这里作者将其视为二分类任务</p>

<h2 id="133-实验结果">13.3. 实验结果</h2>
<center><img src="../assets/img/posts/20221010/68.png" /></center>

<h1 id="14-automated-short-answer-grading-using-deep-neural-networks-and-item-response-theory">14. Automated Short-Answer Grading Using Deep Neural Networks and Item Response Theory</h1>
<h2 id="141-introduction">14.1. Introduction</h2>
<p>作者提出了一种结合DNN(Deep Neural Networks)和IRT(Item Response Theory)的模型，简单来说就是在提问时加上一些判断正负的客观问题来辅助评分，这种方法可以适用于任何一个DNN-ASAG模型，作者这里采用最标准的LSTM-ASAG模型来演示</p>

<h2 id="142-proposed-method">14.2. Proposed Method</h2>
<center><img src="../assets/img/posts/20221010/69.png" /></center>

<p>模型整体架构如上图所示，首先介绍一下DNN部分，word sequence首先经过look up table layer，这个layer的作用是把单词转换成词元，也就是word embedding representation，接着LSTM layer将其转换成hidden vector，接着经过一个temporal mean layer，将vector输出成一个定长的vector M。</p>

<p>接着介绍一下IRT model，作者使用了一个名为two-parameter logistic IRT model来获得学生的ability $\theta$，学生回答正确的概率公式如下:</p>

<p>
\begin{equation}
(1+exp[-\alpha_i(\theta - \beta_i)])^{-1}
\end{equation}
</p>

<p>其中$\alpha_i$和$\beta_i$分别代表问题的区别系数和难度系数, $\theta$就是学生的ability</p>

<p>concatenate两者的输出后，经过一个MLP进行降维，然后输入线性层输出最终结果</p>

<h2 id="143-实验">14.3. 实验</h2>
<p>实验结果如下所示:</p>
<center><img src="../assets/img/posts/20221010/70.jpg" /></center>

<h1 id="15-comparative-evaluation-of-pretrained-transfer-learning-models-on-automatic-short-answer-grading">15. Comparative Evaluation of Pretrained Transfer Learning Models on Automatic Short Answer Grading</h1>
<h2 id="151-introduction">15.1. Introduction</h2>
<p>作者对比了四种不同的预训练迁移学习模型ELMo，GPT，GPT-2，BERT在ASAG任务上的表现，主要方法就是利用这几种模型的词嵌入做cosine相似，作者对比了RMSE分数，发现ELMo的效果最好</p>
<center><img src="../assets/img/posts/20221010/71.png" /></center>

<h2 id="152-experiment">15.2. Experiment</h2>
<p>实验结果如下:</p>
<center><img src="../assets/img/posts/20221010/72.jpg" /></center>

<p>除了四种预训练模型外，作者还对比了与其他word embedding的模型的区别，并尝试解释其原因</p>

<h1 id="16-going-deeper-automatic-short-answer-grading-by-combining-student-and-question-models">16. Going deeper: Automatic short-answer grading by combining student and question models</h1>
<h2 id="161-introduction">16.1. Introduction</h2>
<p>作者总结了在ASAG任务领域的一些answers-based模型，认为没有考虑到question的作用，于是结合了question model和answers model，并研究了deep belief networks(DBN)在ASAG领域的表现，发现应用question models于传统的answer-based模型能提升其表现，同时发现DBN的效果不错，强于传统的机器学习方法</p>

<h2 id="162-state-features">16.2. State features</h2>
<h3 id="1621-answer-model">16.2.1. Answer model</h3>
<p>每个问题都有对应的参考答案(referred correct answers)，在最开始，answer space只有参考答案，但随着训练过程，不断有学生答案被鉴定为正确并加入到answer space里，通过词袋的方式对answer space进行建模，作者称其为word-answer matrix，横坐标表示单词，纵坐标代表不同的答案，每一行每一列的数值等于这个单词在回答中出现的次数，这个矩阵会动态更新</p>

<p>answer model主要包含了以下6个特征:</p>
<ol>
  <li>length difference: 学生答案和参考答案的句子长度差</li>
  <li>cosine similarity: 通过学生答案和参考答案的tf-idf向量计算余弦相似度，TF-IDF是一种用于资讯检索与资讯探勘的常用加权技术，TF-IDF是一种统计方法，用以评估一个单词对于一个文件集或一个语料库中的其中一份文件的重要程度，单词的重要性会随着它在文件中出现的次数成正比增加，但同时会伴随着它在语料库中出现的频率而成反比下降</li>
  <li>max-matched idf: 衡量了一个学生答案在answer space中词重叠的信息，idf衡量了一个单词能提供的信息</li>
  <li>LSA: LSA用来评估一个学生回答的质量，具体方法就是拿他与其他正确答案进行比较</li>
  <li>Domain-specific text similarity: 在sentence level衡量学生答案和参考答案的相似度，具体做法就是让专家手动的列一个领域词汇表dl，然后就计算学生答案s和参考答案c的相似度:</li>
</ol>

<p>
\begin{equation}
sim_d(s,c)=\sum_{\omega_1 \in sv}\sum_{\omega_2 \in cv} 1_{dl}(\omega_1) \cdot 1_{dl}(\omega_2)
\end{equation}
</p>

<p>其中1代表指标函数，当$\omega_1$在list中时为1，其次sv和cv分别代表matrix中的学生答案向量和参考答案向量</p>

<ol>
  <li>general text similarity: 测量了学生答案和参考答案的整体文本相似度，记为$sim_g(s,c)$，计算它之前，首先需要计算word-level的相似度$sim_w(C_1,C_2)$，公式如下:</li>
</ol>

<p>
\begin{equation}
sim_{\omega}(C_1,C_2)=\frac{2*depth(LCS)}{depth(C_1)+depth(C_2)}
\end{equation}
</p>

<p>上式中$C_1$和$C_2$代表两个concept，depth(·)表示concept沿概念树的边数，所以这里需要借助一个knowledge-based dictionary，这里作者使用了WordNet，LCS全称为Least common subsumer，即最小公共包含，即$C_1$和$C_2$在概念树上的最小公共祖先节点</p>

<p>然后就可以计算$sim_g(s,c)$了，具体公式如下，其中$dl_c$表示领域词列表的补集:</p>

<p>
\begin{equation}
sim_g(s,c)=\sum_{\omega_1 \in sv}\sum_{\omega_2 \in cv} 1_{dl^c}(\omega_1) \cdot 1_{dl^c}(\omega_2) \cdot sim_{\omega}(\omega_1, \omega_2)
\end{equation}
</p>

<p>集合5和6，可以计算出一个归一化的sim(s,c)，就是对5.和6.的公式加权得到，权重分别为0.6和0.4，最后，给定了一个学生答案，学生答案将和answer space中所有的参考答案都计算相似度，然后最后求平均</p>

<h3 id="1622-question-model">16.2.2. Question model</h3>
<p>不同于大多数的ASAG系统是question-specific的，作者提出了一种domain general的ASAG系统，不是针对每个问题都建立一个分类器，而是对所有的问题都建立一个分类器。为了实现这样的目标，需要有一个question model把问题分成一个general feature space，这样才能保证ASAG模型能学习到一些feature。具体而言，作者设计的question model包含两个主要的特征:</p>

<ul>
  <li>Knowledge Components(KCs): 构建一个Q-matrix来表示单个的问题和KCs间的关系，Q-matrix是一个qxk的二维矩阵，q代表问题，k代表KCs。比如说$Q_{jk}$=1代表问题j是KC中k的一个应用。这里作者让专家来设计这个矩阵，一旦有学生回答了一个问题j，就从Q-matrix把第j行拿出来，然后添加到特征向量中</li>
  <li>Question Diffuiculty: 本来KCs的feature只有八个，这个问题难度将作为第九个feature加入Q-matrix中，根据专家的评判以及学生回答这个问题的情况来决定</li>
</ul>

<h3 id="1623-student-model">16.2.3. Student model</h3>
<p>Student model可以定义为收集相关信息的过程，以推断学生当前的认知状态并对其进行表示，以便辅导系统可以访问和使用以提供适应性。Bayesian Knowledge Tracing(BKT)是在ITS领域中应用最广的student model。BKT利用学生与辅导系统的交互的序列信息来更新它对于该学生潜在知识掌握能力的评估。</p>
<center><img src="../assets/img/posts/20221010/73.jpg" /></center>

<p>总的来说，Student model有16个feature，对于KCs的每一列而言，它用BKT来估计学生在每一个KC上的掌握水平</p>

<h3 id="1624-composite-feature-space">16.2.4. Composite feature space</h3>
<p>结合了以下student model和question model的一些特征</p>

<h2 id="163-six-classifiers">16.3. Six Classifiers</h2>
<p>使用了六个分类器来通过前面提到的特征预测学生在某个问题上的回答情况，分别是: 朴素贝叶斯，逻辑回归，决策树，支持向量机，ANN和DBN</p>

<h2 id="164-data">16.4. Data</h2>
<p>为了能有效地利用这些特征，数据集的准备也是别有用心，数据集采自Cordillera，一门教导学生大学物理的课程，它属于能量领域的，KC的特征有动能，重力势能等等。158名学生参与了数据收集的过程: 首先参加背景的调研，然后学习课本和先修材料，然后参加预测试，接着在Cordillera做题，最后参加考试</p>

<h2 id="165-实验">16.5. 实验</h2>
<h3 id="1651-实验设置">16.5.1. 实验设置</h3>
<p>总共设置了三个阶段的实验，如下图所示:</p>
<center><img src="../assets/img/posts/20221010/74.png" /></center>

<h3 id="1652-实验结果">16.5.2. 实验结果</h3>
<center><img src="../assets/img/posts/20221010/75.jpg" /></center>

<center><img src="../assets/img/posts/20221010/76.png" /></center>

<h2 id="166-总结">16.6. 总结</h2>
<p>实验结果表明，Question model和student model的加入能增强answer model的效果，其次就是DBN相较于传统的机器学习方法表现更好</p>

<h1 id="17-automatic-short-answer-grading-with-semspace-sense-vectors-and-malstm">17. Automatic Short Answer Grading With SemSpace Sense Vectors and MaLSTM</h1>
<h2 id="171-introduction">17.1. Introduction</h2>
<p>作者提出了利用Semspace的sense vector输入MaLSTM从而实现ASAG任务，Semspace是一个基于WordNet中synset的一种sense embedding的方法，后续会具体介绍。文章的两大关键组成部分是SemSpace和Manhattan LSTM(MaLSTM)</p>

<h2 id="172-method">17.2. Method</h2>
<p>模型实现分为以下三步:</p>
<ol>
  <li>基于WordNet的同义词集训练SemSpace算法</li>
  <li>根据Word Sense Disambiguation，将数据集分为词元</li>
  <li>训练MaLSTM</li>
</ol>

<h3 id="1721-determining-sense-vectors-with-semspace-method">17.2.1. Determining Sense Vectors with Semspace Method</h3>
<p>Sense Embedding就是根据词的意思来生成词表示，与Word2Vec，Glove，FastText等有区别，后者无法处理多义词。SemSpace是sense embedding的一种方法，作者这里对它进行了一定的改动，在运行Semspace算法时，节点间的关系用欧式距离表示，然后根据WordNet中同义词集的关系(或者说相似度)，调整向量，两个向量之间的相似度通过以下公式计算:</p>

<p>
\begin{equation}
Sim(V_1, V_2)=e^{-\|V_1-V_2\|}
\end{equation}
</p>

<p>其中$V_1$和$V_2$分别表示sense vector的欧式距离，如果两个向量的相似度超过了它们之间的relation weight，那么就把两个向量拉近，反之则拉远。这就是SemSpace算法训练过程中向量位置的变化</p>

<p>预处理步骤:</p>
<center><img src="../assets/img/posts/20221010/77.jpg" /></center>

<p>Sentence a和Sentence B分别代表学生答案和参考答案，那些包含多个WordNet同义词集的单词会通过WSD处理:</p>

<p>
\begin{equation}
C_{WSD}=\mathop{argmin}_{G_j}\sum _i^N \|C_j-P_i\|
\end{equation}
</p>

<p>N是context cluster的同义词集数量，$C_j$表示歧义词的候选同义词集，$P_i$是context cluster的sense vector表示，context cluster就是整个数据集出现最多的单词的集合</p>

<h3 id="1722-grading-with-malstm">17.2.2. Grading with MaLSTM</h3>
<p>MaLSTM被广泛用于句段相似的应用中。模型的主体架构如下图所示:</p>
<center><img src="../assets/img/posts/20221010/78.jpg" /></center>

<p>学生答案和参考答案分别输入不同的LSTM模型中，得到sentence representation(在LSTM的最后一个隐层)，然后计算两个向量之间的Manhattan distance，归一化后将范围控制在0，1之间，得到的结果作为相似度。曼哈顿距离标明两个点在标准坐标系上的绝对轴距总和。</p>
<center><img src="../assets/img/posts/20221010/79.jpg" /></center>

<h3 id="1723-datasets">17.2.3. Datasets</h3>
<p>使用了两个数据集，一个是Mohler数据集，另一个数据集是CU-NLP</p>

<h2 id="173-实验">17.3. 实验</h2>
<p>实验结果:</p>
<center><img src="../assets/img/posts/20221010/80.jpg" /></center>

<center><img src="../assets/img/posts/20221010/81.png" /></center>

<h1 id="18-a-semantic-feature-wise-transformation-relation-network-for-automatic-short-answer-grading">18. A Semantic Feature-Wise Transformation Relation Network for Automatic Short Answer Grading</h1>
<h2 id="181-introduction">18.1. Introduction</h2>
<p>作者提出了一种新的网络模型来解决ASAG任务，名称为Semantic Feature-wise transformation Relation Network(SFRN)。SFRN是一个端到端的模型，有三个组成部分，encoder首先编码QRA对，生成QRA的向量表示。当一个问题存在多个参考答案时，relation network将单个的QRA向量转化成single relation vector，接着一个学习过的feature-wise transformation function融合了所有relation vector。最终，分类器决定每个学生答案对应的分数或者类别。</p>

<p>为了解决数据不足和类别不平衡的问题，作者采用了一个简单的数据增强(data augmentation)的方法，back-translation。ASAG的数据集相对来说小。back-translation能从数据集中已经存在的数据生成新的数据。</p>

<h2 id="182-sfrn">18.2. SFRN</h2>
<h3 id="1821-relation-network">18.2.1. Relation Network</h3>
<p>Relation network，关系网络，最初是用在CV领域，用来学习不同类别的物体的区别，这里作者用来寻找不同text vector之间的关系，Relation Network可以用下面这个公式简单表达:</p>

<p>
\begin{equation}
RN(O)=f_{\phi}(\sum_{i,j}g_\theta(o_i,o_j))
\end{equation}
</p>

<p>公式里O是输入物体的集合，f和g是可以训练的带参函数，g用来学习物体对的关系，返回一个抽象的表示，输入f，f是分类器</p>

<h3 id="1822-qra-relation-vectors">18.2.2. QRA relation vectors</h3>
<p>假设向量化的QRA对为$(q,r_j,a)$，对于一个问题而言，共有n个参考答案。生成relation vector的过程图如下所示:</p>
<center><img src="../assets/img/posts/20221010/82.jpg" /></center>

<p>q,r,a首先concate在一起输入$g_\theta$,g是一个MLP，参数为$\theta$，假设共有n个qra对，那么会生成n个relation vector $l_j$</p>

<h3 id="1823-relation-fusion">18.2.3. Relation Fusion</h3>
<p>接着就需要把这n个关系向量融合在一起，原本的Relation Network会把它们加起来，然后输入到分类器f中，但是一般情况下关系都是二元的，对于ASAG任务而言，qra是三元的，所以作者对其进行了一定的修改。作者使用了Semantic Feature-wise Transformation(SFT)来融合n个关系向量，具体的公式如下:</p>

<p>
\begin{equation}
SFT(C,L,n)=\sum_{j=1}^n(\alpha(c_j)\odot l_j+\beta(c_j))
\end{equation}
</p>

<p>公式中C代表qra set，L表示关系向量set，n表示参考答案的个数，$c_j$是qra pair concatenate的结果，$\alpha$和$\beta$是MLP。</p>

<p>SFT得到的结果输入到分类器中得到最终的分类结果，所以整体上来说SFRN能写成:</p>

<p>
\begin{equation}
SFRN([Q,R,A])=f_\phi(SFT(g_\theta))
\end{equation}
</p>

<h3 id="1824-sfrn-encoder">18.2.4. SFRN Encoder</h3>
<p>剩下还有一部分没有介绍，就是encoder部分。作者实验了不同encoder的效果，baseline model使用了LSTM作为encoder，除此之外，还使用了BERT作为encoder，取BERT最后一个输出层的输出</p>

<h2 id="183-data-augmentation">18.3. Data Augmentation</h2>
<p>鉴于SemEval ASAG数据集只包含有限的数据和严重的数据倾斜问题，所以作者采用了Back-translation的方法来做Data Augmentation。Back-translation就是将数据从原始语言翻译为一个或多个其他语言，然后再把翻译结果从其他语言翻译成原始语言。作者这里使用的语言是汉语和法语。作者使用了EasyNMT网络和谷歌翻译的API来完成这个操作</p>
<center><img src="../assets/img/posts/20221010/83.jpg" /></center>

<p>实验Data Augmentation发现，数据平衡对实验结果影响不大，除非重平衡的类和原始类的数据量差别很大。并且并不是数据扩大的倍数越大越好，如果一个类别和数据量最大的类别的差异有五倍，那么作者会double这个类别</p>

<p>T-test的实验结果</p>
<center><img src="../assets/img/posts/20221010/84.png" /></center>

<p>T-test一般用来检验不同参数设置的实验结果是否有显著的差别</p>

<h2 id="184-实验">18.4. 实验</h2>
<p>和多个其他baseline进行了对比实验，并且分别在Bettle数据集，SciEntsBank数据集和数据增强过的两个数据集上做了对比试验，其中应用了Bert作为encoder的模型表现最好</p>

<h1 id="19-an-automatic-short-answer-grading-model-for-semi-open-ended-questions">19. An automatic short-answer grading model for semi-open-ended questions</h1>
<h2 id="191-introduction">19.1. Introduction</h2>
<p>传统的ASAG通过对比RA和SA的相似度来评估SA的分数，这种方法对于closed-ended question有不错的效果，因为这些问题只有限定数量的RA。但是对于半开放的问题，比如阅读理解题，参考答案相对来说更多更广泛，作者提出了一种基于LSTM的方法实现半开放问题的短文本答案的评估</p>

<h2 id="192-proposed-model">19.2. Proposed model</h2>
<p>结合domain-general information和domain-specific information来解决ASAG任务，前者的信息提取自Wikipedia，后者的信息来源于已标注的学生答案。同时利用LSTM来提取句段信息。模型的大体架构如下图所示:</p>
<center><img src="../assets/img/posts/20221010/85.jpg" /></center>

<h3 id="1921-cbow">19.2.1. CBOW</h3>
<p>Continuous bag-of-words连续词袋模型，简称CBOW，是一种将word embedding的手段，让语义相近的单词在嵌入空间的距离更近，在之前，LSA和LDA也是常见的word embedding的手段。CBOW模型如下图所示:</p>
<center><img src="../assets/img/posts/20221010/86.png" /></center>

<p>输入中心词上下距离c的单词向量，平均下来作为中心词的投影向量，CBOW在学习的过程就是让这个投影向量和它本身向量更相近，和相反意义的单词向量更远，也就是最小化以下损失函数:</p>

<p>
\begin{equation}
loss_{CBOW}=log\prod_{k=1}^{|D|}\{\sigma(v_k^T \theta^k)\prod_{j=1}^{|Neg_k|}[1-\sigma(v_k^T\theta^j)]\}
\end{equation}
</p>

<h3 id="1922-integration-of-domain-general-information-with-domain-specific-information">19.2.2. Integration of domain-general information with domain-specific information</h3>
<p>第一步，用Wikipedia语料库训练CBOW模型，最开始单词向量是随机初始化的，接着CBOW用正确的学生答案的语料进行训练，这时单词的向量来源于第一步训练的结果</p>

<h3 id="1923-lstm-and-classifier">19.2.3. LSTM and classifier</h3>
<p>这里貌似将输入向量经过LSTM之后，加上一个Softmax就直接预测了，但是我觉得这里肯定没有写完整，应该还是经过了一个线性层，输出维度为预测分数的类别数，然后用softmax来得到最终的预测结果</p>

<h1 id="20-multi-relational-graph-transformer-for-automatic-short-answer-grading">20. Multi-Relational Graph Transformer for Automatic Short Answer Grading</h1>
<h2 id="201-introduction">20.1. Introduction</h2>
<p>大多数的ASAG方法使用序列文本来比较RA和SA，忽略了文本的结构性语境。于是作者提出了一种Multi-Relational Graph Transformer，MitiGaTe，在考虑结构化语境的情况下表示词元(词元的嵌入表示)。Abstract Meaning Representation(AMR) graph在解析文本回答后得出，然后被分离成多个subgraphs，每个对应AMR的一种特殊的关系。Graph Transformer用来表示每个词元的嵌入表示(在考虑关系的情况下)，也就是需要利用AMR的subgraph，最终会得到一个subgraph representation。最终，比较RA和SA的subgraph representation，得到最终的分数。</p>

<p>本篇文章的主要贡献:</p>
<ol>
  <li>提出了一种Graph Transformer-based技巧来获取文本的结构信息</li>
  <li>证明了词元的语义表示能提升模型效果</li>
  <li>MitiGate能为学生提供可解释的分数反馈</li>
  <li>提升了Benchmarks的效果</li>
</ol>

<h2 id="202-methodology">20.2. Methodology</h2>
<p>将ASAG任务作以下定义: $A^M= \lbrace w_1^M,w_2^M,.. \rbrace$和$A^S=\lbrace w_1^S,w_2^S,.. \rbrace$分别表示model answer和student answer，text-matching model $f(A^M,A^S)$的作用是计算SA和RA的语义相似度。作者提出了一种graph-based matching model，用来根据输入语句创造graph，首先需要解析每个句段成AMR图，接着从AMR图中准备好subgraphs，接着就可以从每个subgraph创造relation-specific token representation$h_{w,r}$，然后聚合成最终的subgraph representation $g^\phi_{r,M}$和$g^\phi_{r,S}$，比较两者得到最终分数。AMR大体结构图如下所示:</p>
<center><img src="../assets/img/posts/20221010/87.png" /></center>

<h3 id="2021-amr-parsing">20.2.1. AMR parsing</h3>
<p>文本的含义用根指向的图来表示，节点表示概念，边表示概念的关系，比如主语和宾语的关系。AMR捕获有意义的内容，获得抽象的表示。AMR的效果和之前的dependency parser差不多。作者直接使用了AMR model的API来创建每个Answer的AMR graph。</p>

<h3 id="2022-subgraph-preparation-layer">20.2.2. Subgraph Preparation Layer</h3>
<p>根绝边的类别数量来生成subgraph，所有subgraph和原始的graph有相同数量的节点，但是只有这个类别的边被保留。AMR大概有100种不同的relation来捕获语义，如果全部用上会低效率的，作者只保留了ARG1和ARG0，其他全部被划分为other。那么原始的graph会被分成以下部分:</p>

<p>
\begin{equation}
G_{sub}=\lbrace default, A_0, A_1, other \rbrace
\end{equation}
</p>

<h3 id="2023-preparing-node-and-subgraph-representation">20.2.3. Preparing Node and Subgraph Representation</h3>
<p>模型整体架构图如下所示:</p>
<center><img src="../assets/img/posts/20221010/88.png" /></center>

<p>这一步的主要作用是表示之前得到的subgraph。分为两步: 第一步是用Graph Transformer来生成node，接着聚合所有node。</p>

<ul>
  <li>生成node representation的网络是Graph Transformer，其实就是针对节点序列的Transformer，从subgraph输入节点序列$x=(x_1,…,x_n)$，每个节点看成token输入Transformer，与一般的Transformer的区别有两点，一个是只计算该节点相邻节点(包括自身)的注意力权重，第二点是注意力函数有区别，会乘以关系的向量表示$e_{ij}^l$，作者拿最后一层的节点表示$h_{w,r}$来表示特定节点特定关系的表示</li>
  <li>接着对这些node进行相加，得到的结果$g^\phi_{r}$就是这个关系的subgraph representation</li>
</ul>

<p>
\begin{equation}
g^\phi_{r,M}=\frac{\sum_{w\in A_M}h_{w,r}}{\|A_M\|}, \forall r\in G_{sub}
\end{equation}
</p>

<h3 id="2024-graph-matching-layer">20.2.4. Graph Matching Layer</h3>
<p>在获得subgraph representation后，我们就相当于得到了文本的句法信息和语义信息，接着就可以比较RA和SA的距离了:</p>

<p>
\begin{equation}
\begin{aligned}
&amp; D_{r,k}=cosine(w_k^{cos}\odot g^\phi_{r,M},w_k^{cos} \odot g^\phi_{r,S}) \\
&amp; D = [D,D_{r,k}]
\end{aligned}
\end{equation}
</p>

<p>k表示不同方面，$w_k^{cos}$是一个参数向量，负责给不同的方面赋予不同的权重，得到的k个$D_{r,k}$向量concatenate成D</p>

<h3 id="2025-prediction-layer">20.2.5. Prediction layer</h3>
<p>用全连接层作为分数预测层</p>

<h2 id="203-experiment-setup">20.3. Experiment setup</h2>
<h3 id="2031-dataset">20.3.1. Dataset</h3>
<p>使用了Mohler数据集，视为回归任务</p>

<h3 id="2032-data-processing">20.3.2. Data Processing</h3>
<p>在AMR环节，利用GloVe对单词进行embedding</p>

<h2 id="204-实验">20.4. 实验</h2>
<p>实验结果:</p>
<center><img src="../assets/img/posts/20221010/89.jpg" /></center>

<p>feedback可以看下图这个例子:</p>
<center><img src="../assets/img/posts/20221010/90.png" /></center>
<p>其中灰色的节点表示学生答案种遗漏的部分，粉色节点表示学生答案中多余的部分</p>

<h1 id="21-automatic-short-answer-grading-asag-using-attention-based-deep-learning-model">21. Automatic Short Answer Grading (ASAG) using Attention-Based Deep Learning MODEL</h1>
<h2 id="211-introduction">21.1. Introduction</h2>
<p>用BERT来解决ASAG任务</p>

<h2 id="212-method">21.2. Method</h2>
<p>作者将任务步骤分为了三步: 数据准备，数据预处理，评分</p>

<h3 id="2121-dataset-preparation">21.2.1. Dataset preparation</h3>
<p>使用sQuad2.0数据集，数据集包含了10w+的数据</p>

<h3 id="2122-data-preprocessing">21.2.2. Data preprocessing</h3>
<p>用BERT-uncased tokenizer来词元化文本</p>

<h3 id="2123-bert-grading">21.2.3. BERT grading</h3>
<p>用BERT来encode学生答案和参考答案的&lt;cls&gt;词元，然后输入到分类器中进行分类</p>

<h3 id="2124-evaluation-metrics">21.2.4. Evaluation metrics</h3>
<p>kappa系数和混淆矩阵的性能度量，比如召回，精准率等等</p>

<h1 id="22-automated-short-answer-grading-using-deep-learning-a-survey">22. Automated Short Answer Grading Using Deep Learning: A Survey</h1>
<h2 id="221-introduction">22.1. Introduction</h2>
<p>一篇综述，ASAG任务的处理方法可以分为两大类，一类是基于handcrafted features，另一类是基于深度学习的方法。这篇综述整理了ASAG领域的深度学习方法。ASAG解决方法的发展历程:</p>
<center><img src="../assets/img/posts/20221010/91.jpg" /></center>

<p>这篇综述主要回答了以下四个问题:</p>
<ul>
  <li>ASAG领域的数据集</li>
  <li>评价指标</li>
  <li>有哪些深度学习的方法用到了</li>
  <li>结果如何</li>
</ul>

<h2 id="222-corpora">22.2. Corpora</h2>
<p>综述整理了共六个ASAG领域的数据集，其中像ASAP，SemEval-2013和Beetle，ScientsBank都是竞赛的数据集:</p>
<ul>
  <li>ASAP: 来源于Kaggle，共有10686个回答，每个回答都来源于短文，每篇短文大概150词到550词</li>
  <li>Beetle and ScientsBank: Bettle和SciEntsBank都来源于SRA(student response analysis)，都是SemEval-2013的数据集</li>
  <li>Texas</li>
  <li>Cairo</li>
  <li>Powergrading</li>
  <li>Statistics</li>
</ul>

<h2 id="223-evaluation-metrics">22.3. Evaluation Metrics</h2>
<p>根据ASAG任务是分类任务还是回归任务而不同:</p>
<center><img src="../assets/img/posts/20221010/92.png" /></center>

<p>kappa系数是一种衡量分类精度的指标，kappa系数的计算基于混淆矩阵，计算公式如下:</p>

<p>
\begin{equation}
k=\frac{p_0-p_e}{1-p_e}
\end{equation}
</p>

<p>其中$p_0$是分类正确的样本数除以总样本数，$p_e$的计算公式为:</p>

<p>
\begin{equation}
p_e=\frac{a_1*b_1+a_2*b_2+..+a_c*b_c}{n*n}
\end{equation}
</p>

<p>其中每一类真实样本个数分别为$a_1,a_2,…$，而预测出来的每一个类样本的个数分别为$b_1,b_2,…$，总样本数为n，kappa的计算结果为-1到1，但通常kappa落在0-1之间，越高代表分类精度越高</p>

<p>QWK就是quadratic weighted kappa，二次加权kappa，在多级分类的深度学习评价中经常使用，就是加权的kappa</p>

<h2 id="224-deeplearning-approaches">22.4. Deeplearning Approaches</h2>
<p>主要用到的深度学习的方法分为了三大类，一类是基于LSTM及其变种，一种是基于Attention机制，最后一类就是Transformer-based，后续将逐个介绍，首先看一张近年来应用于ASAG的深度学习重要的文章:</p>
<center><img src="../assets/img/posts/20221010/93.jpg" /></center>

<p>基于深度学习的方法及其性能:</p>
<center><img src="../assets/img/posts/20221010/94.jpg" /></center>

<ul>
  <li>LSTM: Siamese Bi-LSTM, Bi-LSTM</li>
  <li>Attention</li>
  <li>Transformer</li>
</ul>

<p>下表展示了作者收集的文章的模型及其效果和缺陷:</p>
<center><img src="../assets/img/posts/20221010/95.png" /></center>

<h2 id="225-conclusion">22.5. Conclusion</h2>
<p>总的来说，用深度学习的方法往往相较于传统的特征工程的方法要省时且效果更好。这些方法中，涉及到注意力机制的模型往往表现更好。更多的Transformer方法被提出，比如BERT，XLNet，T5等等，更多的tokenization的方法提出，比如BPE,Word-Piece Encoding和Sentence-Piece Encoding</p>

<h1 id="23-survey-on-automated-short-answer-grading-with-deep-learning-from-word-embeddings-to-transformers">23. Survey on Automated Short Answer Grading with Deep Learning: from Word Embeddings to Transformers</h1>
<h2 id="231-introduction">23.1. Introduction</h2>
<p>一篇关于Deeplearning在ASAG领域的综述，作者首先介绍了hand-engineering features到表征学习的转化，然后介绍了深度学习方法，主要分为了三大类: word embedding，sequential models，attention-based methods</p>

<p>作者根据文本的表征方法将方法分为两类:</p>
<ul>
  <li>hand-engineered features with classifiers，我称其为特征工程的方法，具体来说根据特征的不同分为: lexical, syntactic, semantic</li>
  <li>deep learning methods，端到端，具体来说分为了word embedding, sequential models, attention-based methods</li>
</ul>
<center><img src="../assets/img/posts/20221010/97.jpg" /></center>

<h2 id="232-historical-perspective">23.2. Historical perspective</h2>
<p>按照大概的时间发展来说，研究者们首先使用concept mapping(概念映射)的方法来比较学生答案和参考答案，随着information retrieval的发展，从学生答案提取出feature与参考答案进行直接的比较。这个阶段的方法并没有考虑语义信息，纯粹的考虑句段的分析，基于语法树。接着，利用大型词典WordNet，语义信息开始被考虑。</p>

<h2 id="233-benchmark-data-sets-for-short-answer-grading">23.3. Benchmark data sets for short answer grading</h2>
<p>聚焦于这四个广泛使用的数据集: SciEntsBank, Beetle, Texas, ASAP-SAS，关于这四个数据集的总览信息如下表所示:</p>
<center><img src="../assets/img/posts/20221010/96.png" /></center>

<ul>
  <li>SciEntsBank and Beetle: 都来源于SemEval 2013 challenge，总有三类的标签，分别是2分类(correct, incorrect)，3分类(correct contradictory, incorrect)和5分类(non-domain, correct, partially correct incomplete. contradictory, irrelevant answer)。除此之外，数据集还有三个子集，分别代表评估系统可能遇到的情况，分别是unseen answers, unseen questions, unseen domains</li>
  <li>University of North Texas data set</li>
  <li>ASAP-SAS: 全称叫做Automated Student Assessment Prize Short Answer Scoring，来源于Kaggle competition in 2013</li>
</ul>

<h2 id="234-hand-engineered-features-and-machine-learning">23.4. Hand-engineered Features and Machine Learning</h2>
<p>特征工程的使用到的所有方法列于下表:</p>
<center><img src="../assets/img/posts/20221010/98.png" /></center>
<p>不同的方法在benchmark数据集取得的效果如下表所示:</p>
<center><img src="../assets/img/posts/20221010/99.png" /></center>

<h3 id="2341-lexical-features">23.4.1. Lexical features</h3>
<p>词汇特征，从单词的角度出发，早期用到的词汇特征有word-overlap，通过余弦计算方法得到的词重叠特征取得的效果最好。随着overlap技术的发展，也有像sentence overlap这样的方法出现。</p>

<h3 id="2342-syntactic-features">23.4.2. Syntactic features</h3>
<p>语法特征，常用的有语法树(parse tree)和词性标签(POS tags)</p>

<h3 id="2343-semantic-features">23.4.3. Semantic features</h3>
<p>LSA,ESA和WordNet</p>

<h3 id="235-deep-learning-methods">23.5. Deep Learning Methods</h3>
<p>这里作者将深度学习应用于ASAG的方法分为了三大类，分别是word embedding，sequential models和attention-based models。这和NLP和文本表征方法的进步相关，word embedding就是将单词和句段映射到隐空间中，能够捕获句段的语义信息，常见的有word2vec，glove等。sequential models，常见的有RNN和LSTM，能获得一段文本的序列信息，语义信息，重点是能获得长序列文本中各单词之间的关系。attention-based models同样能获得文本的序列信息和语义信息，通过注意力机制获得单词之间的关系。</p>

<p>下表展示了各模型用到的方法:</p>
<center><img src="../assets/img/posts/20221010/100.png" /></center>

<center><img src="../assets/img/posts/20221010/101.jpg" /></center>

<p>各模型的在benchmark数据集的表现:</p>
<center><img src="../assets/img/posts/20221010/102.png" /></center>

<center><img src="../assets/img/posts/20221010/103.png" /></center>

<h3 id="236-discussion">23.6. Discussion</h3>
<p>ASAG的methods发展的大趋势是从hand-engineered text features到deeplearning。一些适用于多种NLP任务的大模型并不能在这个下游任务上表现良好，这可能与ASAG任务本身的稀疏性和domain difference有关，稀疏性在自然语言处理领域有多种含义，比如data sparsity指的就是数据中存在多个零数据，远多于数据集中的非零数据，这里指的是ASAG的数据集非常sparse</p>

<p>attention-based model在NLP领域取得了很好的结果，但是在ASAG任务上，单一个finetuned transformer model并不能取得最好的效果，作者猜测它不能解开短文本中丰富的语义信息</p>

<h3 id="237-challenges">23.7. Challenges</h3>
<ul>
  <li>semantic understanding: 现存的模型并不能有效地拟合短文本的语义信息，这与短文本本身的性质有关，短文本的信息用一种精简的方式蕴含在一段很短的文本中</li>
  <li>linguistic variations: 一个问题的回答词汇可能不止一种，回答问题的句子结构可能也不止一种，这给ASAG带来了很大的挑战，模型需要考虑不同的近义词汇或者不同的语法结构</li>
  <li>Details of questions and reference answers: 一般来说，参考答案相对来说简短，可能不包含足够的细节，除此之外，需要针对不同类型的问题和开放式问题进行评估</li>
  <li>Generalization across domains and answers: 字面意思</li>
</ul>

<h1 id="100-todo">100. TODO</h1>
<ul>
  <li>看GBT, GPT, ELMo</li>
  <li>深入了解一下ASAG用特征工程解决的思路</li>
  <li>想想改进方向</li>
  <li>代码</li>
</ul>

        <aside class="sidebar inline" id="post-end">
    



<div class="tag-cloud">
    
        <ul class="tags inline">
            
                <li><a href="./tag.html?tag=notes" class="tag inline">notes</a></li>
            
                <li><a href="./tag.html?tag=paper" class="tag inline">paper</a></li>
            
                <li><a href="./tag.html?tag=NLP" class="tag inline">NLP</a></li>
            
    
        </ul>
</div>
    <div class="share-options inline">
    <div class="share-hover inline">
        <span class="share-button inline"><svg fill="currentColor" width="25" height="25" class="inline"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></span>
        <div class="share-icons inline" id="post-end-icons">
            <a class="twitter" href="https://twitter.com/intent/tweet?text=短文本自动评估论文阅读整理&url=http://localhost:4000/Automatic-Short-Answer-Grading.html" title="Share on Twitter" rel="nofollow" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
            <a class="facebook" href="https://facebook.com/sharer.php?u=http://localhost:4000/Automatic-Short-Answer-Grading.html" title="Share on Facebook" rel="nofollow" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
            <a class="reddit" href="http://www.reddit.com/submit?url=http://localhost:4000/Automatic-Short-Answer-Grading.html&title=短文本自动评估论文阅读整理" title="Submit to Reddit" rel="nofollow" target="_blank"><i class="fa fa-reddit" aria-hidden="true"></i></a>
            <a class="linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/Automatic-Short-Answer-Grading.html&title=短文本自动评估论文阅读整理&summary=read and arrange paper about automatic short answer grading(ASAG)&source=http://localhost:4000/Automatic-Short-Answer-Grading.html" title="Share on LinkedIn" rel="nofollow" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
            <a class="email" href="mailto:?subject=短文本自动评估论文阅读整理&body=read and arrange paper about automatic short answer grading(ASAG)%0A%0ARead more here: http://localhost:4000/Automatic-Short-Answer-Grading.html" title="Share via e-mail" rel="nofollow" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
            <a class="copy-link" onclick="copyToClipboard()" title="Copy to clipboard" rel="nofollow" target="_blank"><svg width="20px" fill="currentColor" class="inline" viewBox="0 0 18 18"><path d="M16.94 1.1A3.7 3.7 0 0 0 14.3 0c-1 0-1.94.39-2.64 1.1L7.43 5.3c-.91.92-2.09 3.2 0 5.27a.75.75 0 0 0 .82.16c.09-.03.17-.09.24-.15a.74.74 0 0 0 0-1.06c-1.16-1.15-.77-2.39-.02-3.16l4.24-4.22a2.2 2.2 0 0 1 1.58-.65c.6 0 1.16.23 1.58.65.86.87.86 2.29 0 3.16L12.7 8.47a.74.74 0 0 0 1.04 1.05l3.17-3.16a3.73 3.73 0 0 0 0-5.27h.03zM9.54 7.4a.74.74 0 0 0 0 1.06c1.16 1.15.76 2.39 0 3.16l-4.22 4.22c-.42.42-.99.65-1.59.65a2.23 2.23 0 0 1-1.58-3.82l3.17-3.16A.73.73 0 0 0 5.54 9a.78.78 0 0 0-.22-.52.77.77 0 0 0-1.05 0L1.1 11.64A3.72 3.72 0 0 0 3.74 18c1 0 1.94-.39 2.65-1.1l4.23-4.2c.21-.22.94-1.02 1.13-2.2.18-1.12-.2-2.15-1.12-3.07-.27-.27-.78-.27-1.06 0l-.02-.02z" clip-rule="evenodd" fill-rule="evenodd"></path></svg></a>
        </div>
    </div>
    <div class='alert' style='font-size:.6em;color:var(--accent);text-align:center;'></div>
</div>
</aside>

        <div class="separator"></div>
        



<section class="author-box">
  <div class="narrow-column">
    <a href='https://quehry.github.io'><img src="./assets/img/Dog.jpg" alt="Quehry" class="author-img"></a>
    <ul class="contact-icons">
      
      <li class="twitter"><a class="twitter" href="https://twitter.com/@QuehryS" target="_blank"><i class="fa fa-twitter"></i></a></li>
      
      
      <li class="linkedin"><a class="linkedin" href="https://in.linkedin.com/in/quehry" target="_blank"><i class="fa fa-linkedin"></i></a></li>
      
      
      <li class="github"><a class="github" href="http://github.com/Quehry" target="_blank"><i class="fa fa-github"></i></a></li>
      
      
      <li class="email"><a class="email" href="mailto:quehry@gmail.com"><i class="fa fa-envelope-o"></i></a></li>
      
    </ul>
  </div>
  <div class="author-desc">
    <h3>Quehry</h3>
    <p>Student</p>
  </div>
</section>

        



<div class="recent-box">
  <h2 class="page-subtitle">Recent posts</h2>
  <div class="recent-list">
    
      
        <div class="recent-item">
          
          

          <a href="./%E7%A0%94%E4%B8%80%E4%B8%8A%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93.html" class="recent-item-img" style="background: url('./assets/img/posts/20221205/cover.jpg') center no-repeat; background-size: cover;">
            <div class="recent-item-title">
              <span>研一上课程总结</span>
              

  <span class = "recent-item-meta">
  
  
  
  
    
    
    <p class="page_meta-readtime">
      
        1 minute read
      
    </p>
  
  
  </span>

            </div>
          </a>
        </div>
      
    
      
        <div class="recent-item">
          
          

          <a href="./BLIP.html" class="recent-item-img" style="background: url('./assets/img/posts/20221201/cover.jpg') center no-repeat; background-size: cover;">
            <div class="recent-item-title">
              <span>BLIP</span>
              

  <span class = "recent-item-meta">
  
  
  
  
    
    
    <p class="page_meta-readtime">
      
        less than 1 minute read
      
    </p>
  
  
  </span>

            </div>
          </a>
        </div>
      
    
      
        <div class="recent-item">
          
          

          <a href="./DreamBooth.html" class="recent-item-img" style="background: url('./assets/img/posts/20221128/1.jpg') center no-repeat; background-size: cover;">
            <div class="recent-item-title">
              <span>DreamBooth</span>
              

  <span class = "recent-item-meta">
  
  
  
  
    
    
    <p class="page_meta-readtime">
      
        less than 1 minute read
      
    </p>
  
  
  </span>

            </div>
          </a>
        </div>
      
    
      
        <div class="recent-item">
          
          

          <a href="./Object-Oriented-Programming.html" class="recent-item-img" style="background: url('./assets/img/posts/20221120/1.jpg') center no-repeat; background-size: cover;">
            <div class="recent-item-title">
              <span>C++面向对象程序设计</span>
              

  <span class = "recent-item-meta">
  
  
  
  
    
    
    <p class="page_meta-readtime">
      
        2 minute read
      
    </p>
  
  
  </span>

            </div>
          </a>
        </div>
      
    
  </div>
</div> <!-- End Recent-Box -->

        <div class="newsletter" id="mc_embed_signup">
  <h2 class="page-subtitle">Newsletter</h2>
  <div class="form-container">
    <p>Subscribe here to get our latest updates</p>
    <form action="https://github.us1.list-manage.com/subscribe/post?u=8ece198b3eb260e6838461a60&amp;id=397d90b5f4" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
      <label class="screen-reader-text" for="mce-EMAIL">Email Address</label>
      <div class="newsletter-box" id="mc_embed_signup_scroll">
        <input type="email" name="EMAIL" placeholder="Email address" class="email-input" id="mce-EMAIL" required>
        <input type="submit" value="Subscribe" name="subscribe" class="subscribe-btn" id="mc-embedded-subscribe">
      </div>
    </form>
  </div>
</div> <!-- End Newsletter -->

        
  <section class="comment-area">
    <div class="comment-wrapper">
        
            <div class="row" id="comment-curtain">
                <div class="col-lg-8 col-sm-10 mr-auto ml-auto">
                    <h2 class="page-subtitle">Comments</h2>
                    <div class="comments-trigger" onClick="toggle_comments()">
                        <i class="fa fa-comments"></i>&nbsp;&nbsp;Write a comment ...
                    </div>
                </div>
            </div>
        
      <div id="disqus_thread"></div>
          <script>
            (function() {
                var d = document, s = d.createElement('script');
                s.src = '//amaynez-github-io.disqus.com/embed.js';
                s.setAttribute('data-timestamp', +new Date());
                (d.head || d.body).appendChild(s);
            })();
          </script>
      <noscript>Please enable JavaScript to view the comments.</noscript>
    </div>
  </section> <!-- End Comment Area -->


      </div>
    </div> <!-- End Wrapper -->
  </article>
  <div class="search-box">
  <div class="wrapper">
    <div class="search-grid">
      <form class="search-form">
        <div id="search-container">
          <input type="text" id="search-input" class="search" placeholder="Search">
        </div>
      </form>
      <ul id="results-container" class="results-search"></ul>
      <div class="icon-close-container">
        <span class="search-icon-close"><i class="fa fa-times" aria-hidden="true"></i></span>
      </div>
    </div>
  </div>
</div>

  




<footer class="main-footer">
    <div class="footer-wrapper">
        <div class="logo-symbol">
            <a class="logo-link" title="Quehry" href="./">
                <svg width="45px" height="45px" class="logo-symbol" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid meet" version="1.0" viewBox="0 0 649 649"><g fill="currentColor" stroke="none"><path d="M2938 6380 c-551 -53 -1031 -220 -1478 -514 -306 -201 -588 -470 -810 -771 -276 -374 -475 -850 -559 -1336 -64 -371 -53 -836 29 -1211 155 -712 552 -1347 1130 -1810 450 -360 1021 -589 1640 -659 58 -6 202 -12 320 -12 282 0 503 26 756 87 902 220 1680 841 2079 1661 303 622 402 1333 279 2005 -90 496 -313 984 -634 1390 -111 140 -378 404 -519 513 -456 352 -957 560 -1549 643 -126 17 -558 26 -684 14z m687 -220 c77 -11 189 -32 250 -46 127 -29 345 -94 345 -103 0 -4 -341 -188 -757 -409 l-757 -403 -41 22 c-22 13 -58 26 -79 29 l-40 5 -148 370 c-82 204 -150 377 -153 386 -10 29 325 119 570 153 169 24 174 24 430 21 183 -3 273 -8 380 -25z m-1292 -546 l157 -387 -24 -28 c-13 -16 -30 -44 -36 -64 -6 -19 -17 -35 -23 -35 -7 -1 -382 -54 -834 -119 -453 -64 -823 -115 -823 -111 0 3 43 62 95 132 269 358 584 630 980 844 90 49 321 153 341 154 6 0 81 -174 167 -386z m2026 347 l37 -20 129 -440 128 -440 -35 -39 c-20 -22 -37 -44 -39 -50 -3 -8 -290 13 -929 66 -775 65 -925 80 -928 92 -2 11 223 134 785 432 433 228 794 416 801 417 7 0 30 -8 51 -18z m204 -92 c420 -211 850 -600 1137 -1027 34 -51 60 -95 57 -98 -4 -4 -805 157 -859 172 -10 3 -18 12 -18 21 0 58 -78 133 -137 133 -18 0 -33 2 -33 5 0 3 -52 184 -116 403 -63 218 -117 405 -120 415 -3 9 -3 17 1 17 3 0 42 -19 88 -41z m-913 -894 c494 -41 904 -75 912 -75 7 -1 24 -23 36 -51 l24 -50 -169 -269 -168 -270 -44 0 c-24 0 -63 -9 -86 -21 l-42 -21 -679 402 c-373 222 -688 409 -699 416 -18 13 -18 14 -1 14 10 0 422 -34 916 -75z m-1175 18 l48 -48 -31 -225 c-18 -124 -60 -417 -94 -652 l-62 -428 -44 0 c-24 0 -51 -4 -61 -10 -16 -8 -146 87 -777 566 -418 317 -761 578 -763 580 -3 3 0 9 5 14 8 8 1633 247 1700 249 25 1 42 -9 79 -46z m915 -414 l690 -411 0 -38 0 -37 -147 -46 c-82 -26 -453 -148 -825 -271 l-676 -223 -22 30 -22 29 98 669 c82 557 101 669 114 669 8 0 31 9 50 19 19 11 38 20 43 20 4 1 318 -184 697 -410z m1959 175 c251 -53 457 -97 457 -98 9 -9 123 -247 153 -320 113 -272 189 -599 209 -901 l9 -130 -231 -273 c-224 -265 -232 -273 -264 -270 l-34 3 -418 1013 -419 1013 26 30 c15 16 33 29 41 29 8 0 220 -43 471 -96z m-593 2 c6 -5 804 -1949 802 -1952 -2 -2 -274 272 -605 609 l-600 612 13 32 c20 46 17 82 -9 128 l-23 41 170 271 171 272 40 -6 c22 -3 41 -6 41 -7z m-3341 -611 c402 -307 733 -561 737 -564 31 -26 -38 -31 -947 -65 -516 -19 -941 -33 -944 -30 -2 2 6 77 18 165 49 346 162 693 324 986 44 82 52 91 66 79 9 -7 345 -264 746 -571z m2710 -142 c17 -16 39 -32 48 -38 15 -9 13 -64 -23 -695 -22 -377 -40 -686 -40 -687 0 -2 -8 -3 -19 -3 -10 0 -30 -9 -45 -21 l-27 -21 -342 200 c-188 110 -542 317 -787 460 -245 144 -445 266 -445 273 0 8 312 116 820 285 451 150 822 272 824 273 2 1 18 -11 36 -26z m590 -425 c220 -222 500 -505 623 -629 l223 -225 -17 -31 -18 -30 -585 -82 c-322 -45 -602 -85 -621 -88 -31 -4 -37 -1 -53 27 -11 17 -34 39 -53 47 -19 9 -34 21 -34 27 -1 6 16 316 37 688 l37 678 26 9 c14 5 27 10 30 10 3 1 185 -180 405 -401z m-2587 -114 c-2 -9 -224 -178 -494 -375 l-492 -359 -38 16 c-26 11 -53 14 -84 10 -25 -3 -55 -7 -67 -8 -16 -2 -106 76 -362 316 -204 192 -341 327 -341 338 0 16 10 18 83 19 45 0 431 13 857 28 984 36 942 35 938 15z m59 -88 l22 -23 -104 -467 c-58 -256 -105 -468 -105 -470 0 -2 -14 -6 -32 -10 -17 -3 -47 -18 -65 -32 l-34 -26 -339 122 -338 122 -7 45 c-5 37 -3 47 12 58 10 7 229 169 488 359 258 190 472 346 475 346 3 0 15 -11 27 -24z m1013 -430 c421 -245 769 -452 775 -460 5 -8 7 -18 3 -21 -3 -4 -417 -41 -920 -84 l-913 -77 -16 26 c-9 15 -29 36 -45 47 l-28 21 104 465 105 465 43 7 c27 4 55 17 75 35 18 16 37 28 42 26 6 -2 354 -204 775 -450z m-2307 -276 c-7 -71 -4 -98 10 -126 l15 -30 -206 -248 -206 -248 -27 59 c-140 314 -228 733 -229 1086 l0 138 323 -303 c270 -254 322 -307 320 -328z m5277 366 c-17 -270 -58 -489 -135 -721 -38 -117 -134 -350 -140 -343 -1 2 -35 117 -75 256 l-72 253 26 25 c51 52 58 134 15 189 -21 26 -21 26 -2 48 11 12 102 120 203 240 100 120 184 216 186 214 2 -2 -1 -74 -6 -161z m-642 -487 l21 -41 -456 -782 c-251 -429 -461 -787 -467 -794 -9 -9 -19 -10 -42 -2 l-29 10 -174 676 -174 675 35 37 c20 20 40 51 45 67 l10 30 584 82 c321 44 594 82 605 82 16 1 27 -10 42 -40z m-4015 -145 c301 -109 327 -120 327 -142 0 -12 7 -39 15 -58 l15 -36 -411 -469 c-226 -259 -413 -468 -415 -466 -2 2 1 219 6 483 5 263 10 538 10 610 l0 132 29 12 c16 6 39 23 52 36 13 14 28 23 34 21 5 -2 157 -57 338 -123z m-517 -413 c-3 -262 -9 -527 -12 -589 -8 -134 -2 -135 -120 20 -93 121 -192 278 -272 428 l-61 115 215 257 c181 216 218 255 235 251 l22 -6 -7 -476z m4695 460 c3 -3 43 -133 88 -289 l82 -282 -62 -108 c-75 -130 -175 -278 -265 -392 -140 -176 -514 -528 -595 -558 -13 -5 -72 -13 -130 -17 l-105 -7 -16 40 -16 40 462 796 461 796 45 -6 c25 -4 48 -9 51 -13z m-1736 -69 c-6 -5 -361 -192 -790 -417 -429 -224 -806 -422 -838 -439 -52 -28 -59 -29 -70 -15 -17 22 -78 59 -99 59 -19 0 -16 -14 -68 300 -29 169 -38 246 -30 248 27 9 62 50 74 87 11 32 19 41 43 44 67 8 1683 139 1733 140 30 0 50 -3 45 -7z m90 -62 c10 -11 38 -26 62 -31 38 -10 44 -16 52 -48 90 -346 333 -1280 337 -1292 3 -11 -7 -23 -29 -36 -19 -11 -42 -38 -53 -60 l-19 -40 -1050 297 c-859 244 -1050 301 -1050 315 0 12 239 141 860 465 473 247 863 449 866 449 3 1 14 -8 24 -19z m-2076 -210 c13 0 21 -9 25 -27 10 -47 86 -497 86 -508 0 -5 -13 -19 -29 -29 -35 -24 -57 -59 -66 -108 -4 -22 -14 -39 -24 -42 -143 -41 -861 -236 -870 -236 -7 0 -11 5 -9 11 2 6 190 224 418 484 349 399 416 472 431 464 10 -5 27 -9 38 -9z m1376 -1065 c741 -208 1053 -300 1060 -312 6 -9 24 -32 41 -52 l31 -35 -30 -68 c-34 -77 -25 -71 -205 -139 -310 -116 -653 -180 -984 -183 -97 0 -178 1 -181 4 -89 106 -817 1053 -817 1063 0 18 9 27 23 21 7 -2 485 -137 1062 -299z m-1319 264 c31 -35 81 -51 134 -44 l46 6 380 -487 c208 -268 385 -496 392 -506 13 -17 10 -18 -45 -12 -640 65 -1267 346 -1732 776 l-61 57 42 12 c271 74 805 218 814 218 6 1 19 -9 30 -20z m2814 -563 c0 -5 -223 -138 -278 -165 -24 -13 -46 -20 -49 -18 -2 3 3 19 11 36 12 22 27 33 59 42 27 7 56 26 81 52 35 37 44 41 105 47 36 4 67 8 69 9 1 0 2 -1 2 -3z" transform="translate(0.000000,644.000000) scale(0.100000,-0.100000)"/></g></svg>
            </a>
        </div>
        <div class="copyright">
          <p>2023 &copy; Quehry</p>
        </div>
        <div class="footer-nav">
            <div>
                <a href="./archive.html">
                    Posts
                </a>
            </div>
            <div>
                <a href="./tags.html">
                    Tags
                </a>
            </div>
            <div>
                <a href="./about.html">
                    About
                </a>
            </div>
        </div>
    </div>
</footer> <!-- End Footer -->

</div>


    <div class="top" title="Top">
      <svg aria-hidden="true" focusable="false" data-prefix="fal" data-icon="angle-up" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="svg-inline--fa fa-angle-up fa-w-8 fa-2x"><path fill="currentColor" d="M136.5 185.1l116 117.8c4.7 4.7 4.7 12.3 0 17l-7.1 7.1c-4.7 4.7-12.3 4.7-17 0L128 224.7 27.6 326.9c-4.7 4.7-12.3 4.7-17 0l-7.1-7.1c-4.7-4.7-4.7-12.3 0-17l116-117.8c4.7-4.6 12.3-4.6 17 .1z" class=""></path></svg>
    </div>
    




<!-- JS -->








<script>
(function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var logo = document.getElementById('logo');
    var nightModeOption = ('manual' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
    storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
    var data = storage.getItem('theme');
    try {
        data = JSON.parse(data ? data : '');
    } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
    }
    return data;
    }

    function handleThemeToggle(nightShift) {
    themeData.nightShift = nightShift;
    saveThemeData(themeData);
    html.dataset.theme = nightShift ? 'dark' : 'light';
    if (nightShift) {
        logo.setAttribute("src", "./assets/img/branding/MVM-logo-full-dark.svg");
    } else {
        logo.setAttribute("src", "./assets/img/branding/MVM-logo-full.svg");
    }
    setTimeout(function() {
        sw.checked = nightShift ? true : false;
    }, 50);
    }

    function autoThemeToggle() {
    // Next time point of theme toggle
    var now = new Date();
    var toggleAt = new Date();
    var hours = now.getHours();
    var nightShift = hours >= 19 || hours <=7;

    if (nightShift) {
        if (hours > 7) {
        toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
    } else {
        toggleAt.setHours(19);
    }

    toggleAt.setMinutes(0);
    toggleAt.setSeconds(0);
    toggleAt.setMilliseconds(0)

    var delay = toggleAt.getTime() - now.getTime();

    // auto toggle theme mode
    setTimeout(function() {
        handleThemeToggle(!nightShift);
    }, delay);

    return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
    };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
    handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
    var data = autoThemeToggle();

    // Toggle theme by local setting
    if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
    } else {
        handleThemeToggle(themeData.nightShift);
    }
    } else if (nightModeOption == 'manual') {
    handleThemeToggle(themeData.nightShift);
    } else {
    var nightShift = themeData.nightShift;
    if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
    }
    handleThemeToggle(nightShift);
    }
})();
</script>

<script src="./assets/js/jekyll-search.js"></script>
<script src="./assets/js/jquery-3.6.0.min.js"></script>


  <script>
    function toggle_comments(){
        document.getElementById('comment-curtain').classList.toggle('hide')
        document.getElementById('disqus_thread').classList.toggle('show')
    }

    function copyToClipboard() {
      navigator.clipboard.writeText('http://localhost:4000/Automatic-Short-Answer-Grading.html').then(function() {
      alerts = document.getElementsByClassName('alert')
      for (i=0; i < alerts.length; i++){
        alerts[i].innerHTML='\u00ABlink copied\u00BB';
        setTimeout((function(i){ return function(){alerts[i].innerHTML='';}})(i), 1600 );
      };
      }, function() {
        prompt("Unable to copy, please use this link:", "http://localhost:4000/Automatic-Short-Answer-Grading.html");
      });
    }

    $(function () {
      if (document.getElementById('comment-curtain') == null){
        document.getElementById('disqus_thread').classList.toggle('show')
      }

      var tweetTags = document.getElementsByTagName("tweet");

      if (tweetTags != null){
        for (i=0; i<tweetTags.length; i++){
          tweetA = document.createElement("a")
          tweetA.href = 'https://twitter.com/share?text='
                       + encodeURIComponent(tweetTags[i].textContent)
                       + '&via=QuehryS&url='
                       + window.location.href;
          tweetA.target = "_blank";
          tweetA.className = 'twitter';
          tweetSpanText = document.createElement('span');
          tweetSpanText.className = 'tweetText';
          tweetSpanText.appendChild(document.createTextNode(tweetTags[i].textContent));
          tweetSpanIcon = document.createElement('span');
          tweetSpanIcon.className = 'tweetIcon';
          tweetSpanIcon.appendChild(document.createTextNode("click to tweet"));
          tweetI = document.createElement("i");
          tweetI.className = 'fa fa-twitter';
          tweetSpanIcon.appendChild(tweetI);
          tweetA.appendChild(tweetSpanText);
          tweetA.appendChild(tweetSpanIcon);
          tweetTags[i].textContent = "";
          tweetTags[i].appendChild(tweetA);
        }
      }

    });

  </script>
  <!-- Mailchimp linking -->
  <script id="mcjs">!function(c,h,i,m,p){m=c.createElement(h),p=c.getElementsByTagName(h)[0],m.async=1,m.src=i,p.parentNode.insertBefore(m,p)}(document,"script","https://chimpstatic.com/mcjs-connected/js/users/8ece198b3eb260e6838461a60/d20d9fb9aad962399025da52e.js");</script>



  <script src="https://cdn.jsdelivr.net/gh/cferdinandi/gumshoe@5.1.1/dist/gumshoe.polyfills.min.js"></script>
  <script>
    var spy = new Gumshoe("#toc-content a", {
      navClass:"active",
      contentClass:"underline",
      nested:0,
      nestedClass:"active",
      offset:20,
      reflow:1,
      events:1
    });

    var coll = document.getElementsByClassName("toc-item-1");
    var i;
    var chevron_up = "<svg aria-hidden=\"true\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 256 512\"><path fill=\"currentColor\" d=\"M136.5 185.1l116 117.8c4.7 4.7 4.7 12.3 0 17l-7.1 7.1c-4.7 4.7-12.3 4.7-17 0L128 224.7 27.6 326.9c-4.7 4.7-12.3 4.7-17 0l-7.1-7.1c-4.7-4.7-4.7-12.3 0-17l116-117.8c4.7-4.6 12.3-4.6 17 .1z\"></path></svg>"
    var chevron_down = "<svg aria-hidden=\"true\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 256 512\"><path fill=\"currentColor\" d=\"M119.5 326.9L3.5 209.1c-4.7-4.7-4.7-12.3 0-17l7.1-7.1c4.7-4.7 12.3-4.7 17 0L128 287.3l100.4-102.2c4.7-4.7 12.3-4.7 17 0l7.1 7.1c4.7 4.7 4.7 12.3 0 17L136.5 327c-4.7 4.6-12.3 4.6-17-.1z\"></path></svg>"
    for (i = 0; i < coll.length; i++) {
      if (coll[i].childElementCount > 1) {
        sign = document.createElement('div');
        sign.className = "toc-sign";
        sign.innerHTML = chevron_down;
        coll[i].insertBefore(sign, coll[i].childNodes[0].nextSibling);
        coll[i].addEventListener("click", function() {
          var content = this.lastElementChild;
          if (content.style.maxHeight){
            content.style.maxHeight = null;
            this.firstElementChild.nextSibling.innerHTML = chevron_down;
          } else {
            content.style.maxHeight = content.scrollHeight + "px";
            this.firstElementChild.nextSibling.innerHTML = chevron_up;
          }
        });
      }
    }
  </script>



  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "CommonHTML": { linebreaks: { automatic: true } }
    });
  </script>
  <script src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>


<script src="./assets/js/main.js"></script>
<script>
  SimpleJekyllSearch({
      searchInput: document.getElementById('search-input'),
      resultsContainer: document.getElementById('results-container'),
      json: './search.json',
      searchResultTemplate: '<li><a href="{url}" title="{description}">{title}</a><p>{description}</p></li>',
      noResultsText: 'No results found',
      fuzzy: false,
      exclude: ['Welcome']
    });
</script>




    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R8SZS2YBZK"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R8SZS2YBZK');
</script>
  </body>
</html>
