<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-02-23T22:45:09+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Quehry</title><subtitle>Artificial Intelligence trends and concepts made easy.</subtitle><author><name>Quehry</name></author><entry><title type="html">机器学习</title><link href="http://localhost:4000/Machine-Learning.html" rel="alternate" type="text/html" title="机器学习" /><published>2021-12-22T00:00:00+08:00</published><updated>2021-12-22T00:00:00+08:00</updated><id>http://localhost:4000/Machine-Learning</id><content type="html" xml:base="http://localhost:4000/Machine-Learning.html"><![CDATA[<h1 id="目录">目录</h1>

<!-- TOC -->

<ul>
  <li><a href="#目录">目录</a></li>
  <li><a href="#1-第1章-绪论">1. 第1章 绪论</a></li>
  <li><a href="#2-第2章-模型评估与选择">2. 第2章 模型评估与选择</a>
    <ul>
      <li><a href="#21-思维导图">2.1. 思维导图</a></li>
      <li><a href="#22-经验误差与过拟合">2.2. 经验误差与过拟合</a></li>
      <li><a href="#23-评估方法">2.3. 评估方法</a>
        <ul>
          <li><a href="#231-留出法">2.3.1. 留出法</a></li>
          <li><a href="#232-交叉验证法">2.3.2. 交叉验证法</a></li>
          <li><a href="#233-自助法">2.3.3. 自助法</a></li>
        </ul>
      </li>
      <li><a href="#24-性能度量">2.4. 性能度量</a>
        <ul>
          <li><a href="#241-错误率与精度">2.4.1. 错误率与精度</a></li>
          <li><a href="#242-查准率查全率与f1">2.4.2. 查准率、查全率与F1</a></li>
          <li><a href="#243-roc与auc">2.4.3. ROC与AUC</a></li>
          <li><a href="#244-代价敏感错误率与代价曲线">2.4.4. 代价敏感错误率与代价曲线</a></li>
        </ul>
      </li>
      <li><a href="#25-比较检验">2.5. 比较检验</a>
        <ul>
          <li><a href="#251-假设检验">2.5.1. 假设检验</a></li>
          <li><a href="#252-交叉验证t检验">2.5.2. 交叉验证t检验</a></li>
          <li><a href="#253-mcnemar检验">2.5.3. McNemar检验</a></li>
          <li><a href="#254-friedman检验与nemenyi后续检验">2.5.4. Friedman检验与Nemenyi后续检验</a></li>
        </ul>
      </li>
      <li><a href="#26-偏差与方差">2.6. 偏差与方差</a></li>
    </ul>
  </li>
  <li><a href="#3-第3章-线性模型">3. 第3章 线性模型</a>
    <ul>
      <li><a href="#31-思维导图">3.1. 思维导图</a></li>
      <li><a href="#32-基本形式">3.2. 基本形式</a></li>
      <li><a href="#33-线性回归">3.3. 线性回归</a></li>
      <li><a href="#34-对数几率回归">3.4. 对数几率回归</a></li>
      <li><a href="#35-线性判别分析">3.5. 线性判别分析</a></li>
      <li><a href="#36-多分类学习">3.6. 多分类学习</a></li>
      <li><a href="#37-类别不平衡问题">3.7. 类别不平衡问题</a></li>
    </ul>
  </li>
  <li><a href="#4-第4章-决策树">4. 第4章 决策树</a>
    <ul>
      <li><a href="#41-思维导图">4.1. 思维导图</a>
        <ul>
          <li><a href="#411-章节导图">4.1.1. 章节导图</a></li>
          <li><a href="#412-如何生成一棵决策树">4.1.2. 如何生成一棵决策树</a></li>
        </ul>
      </li>
      <li><a href="#42-基本流程">4.2. 基本流程</a></li>
      <li><a href="#43-划分选择">4.3. 划分选择</a>
        <ul>
          <li><a href="#431-信息增益">4.3.1. 信息增益</a></li>
          <li><a href="#432-增益率">4.3.2. 增益率</a></li>
          <li><a href="#433-基尼指数">4.3.3. 基尼指数</a></li>
        </ul>
      </li>
      <li><a href="#44-剪枝处理">4.4. 剪枝处理</a>
        <ul>
          <li><a href="#441-预剪枝">4.4.1. 预剪枝</a></li>
          <li><a href="#442-后剪枝">4.4.2. 后剪枝</a></li>
        </ul>
      </li>
      <li><a href="#45-连续与缺失值">4.5. 连续与缺失值</a>
        <ul>
          <li><a href="#451-连续值处理">4.5.1. 连续值处理</a></li>
          <li><a href="#452-缺失值处理">4.5.2. 缺失值处理</a></li>
        </ul>
      </li>
      <li><a href="#46-多变量决策树">4.6. 多变量决策树</a></li>
      <li><a href="#47-阅读材料">4.7. 阅读材料</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h1 id="1-第1章-绪论">1. 第1章 绪论</h1>

<h1 id="2-第2章-模型评估与选择">2. 第2章 模型评估与选择</h1>

<h2 id="21-思维导图">2.1. 思维导图</h2>

<center><img src="../assets/img/posts/20211222/16.jpg" /></center>

<h2 id="22-经验误差与过拟合">2.2. 经验误差与过拟合</h2>
<p><strong>定义：</strong></p>
<ul>
  <li><strong>错误率(error rate)</strong>: 如果m个样本中有a个样本分类错误，则错误率E=a/m。</li>
  <li><strong>精度(accuracy)</strong>：精度=1-错误率。</li>
  <li><strong>训练误差</strong>：学习器在训练集上的误差称为训练误差或者经验误差。</li>
  <li><strong>泛化误差(generalization error)</strong>：学习器在新样本上的误差称为泛化误差。</li>
  <li><strong>过拟合(overfitting)</strong>：当学习器把训练样本学得太好了的时候，很可能把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这就会导致泛化性能下降。</li>
  <li><strong>欠拟合(underfitting)</strong>：对训练样本的一般性质尚未学好。</li>
</ul>

<h2 id="23-评估方法">2.3. 评估方法</h2>
<p>通常我们可通过实验测试来对学习器的泛化误差进行评估而做出选择，于是我们需要一个<strong>测试集(testing set)</strong>，然后以测试集上的测试误差作为泛化误差的近似。下面介绍一些常见的处理数据集D的方法(数据集D-&gt;训练集S+测试集T)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<h3 id="231-留出法">2.3.1. 留出法</h3>
<ul>
  <li>留出发(hold-out)直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T。在S上训练出模型后，用T来评估其测试误差。</li>
  <li>以采样的角度来看待数据集划分的过程，则保留类别比例的采样方式通常称为<strong>分层采样</strong>。比如1000个样本，50%的正例，50%负例，以7：3划分数据集，那么训练集包含350个正例和350个负例就是分层采样。</li>
  <li>在使用留出法评估结果时，一般要采用若干次随即划分、重复进行实验评估后取平均值作为留出法的评估结果。</li>
  <li>常用做法时将大约2/3~4/5的样本用于训练。</li>
</ul>

<h3 id="232-交叉验证法">2.3.2. 交叉验证法</h3>
<ul>
  <li><strong>交叉验证法(cross validation)</strong>先将数据集D划分为k个大小相似的互斥子集，每个子集D<sub>i</sub>都尽可能保持数据分布的一致性。然后每次用k-1个自己的并集作为训练集，余下的那个子集作为测试集，这样就可以获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。</li>
  <li>交叉验证也称为k折交叉验证。k的常见取值有10、5、20。</li>
  <li><strong>留一法</strong>就是k=m，其中数据集D有m个样本。</li>
</ul>

<h3 id="233-自助法">2.3.3. 自助法</h3>
<ul>
  <li><strong>自助法(bootstrapping)</strong>：给定包含m个样本的数据集D，每次随机从D中挑选一个样本，将其拷贝放入D’, 然后再将该样本放回最初的数据集D中，这个过程重复执行m次后，我们获得了包含m个样本的数据集D’。我们将D’作为训练集，D\D’作为测试集。</li>
  <li>不难发现大概有36.8%(m趋于无限大时)的样本再m次采样中始终不被采到。</li>
</ul>

<center>$\lim\limits_{m\rightarrow\infty}(1-\frac{1}{m})^m = \frac{1}{e} ≈ 0.368$</center>

<ul>
  <li>缺点：自助法产生的数据集改变了初始数据集的分布，这样会引入估计偏差。因此，在初始数据量足够时，留出法和交叉验证法更常用一些。</li>
</ul>

<h2 id="24-性能度量">2.4. 性能度量</h2>
<ul>
  <li>对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是<strong>性能度量(performance measure)</strong>。</li>
  <li>在预测任务中，给定D = {(x<sub>1</sub>,y<sub>1</sub>), (x<sub>2</sub>,y<sub>2</sub>)…(x<sub>m</sub>,y<sub>m</sub>)}, 其中y<sub>i</sub>是x<sub>i</sub>的真实标记。学习器f。</li>
  <li><strong>均方误差(mean squared error)</strong>：回归任务最常用的性能度量是均方误差：</li>
</ul>

<center>$E(f;D)=\frac{1}{m}\sum_1^m(f(x_i)-y_i)^2$</center>

<p>接下来我将介绍<strong>分类任务</strong>中常用的性能度量</p>

<h3 id="241-错误率与精度">2.4.1. 错误率与精度</h3>
<p>本章开头提到了错误率和精度，这是分类任务中最常用的两种性能度量，既适用于二分类任务，也适用于多分类任务。</p>

<h3 id="242-查准率查全率与f1">2.4.2. 查准率、查全率与F1</h3>

<ul>
  <li>针对二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例(true positive)、假正例(false positive)、真反例(true negative)、假反例(false negative)，分别记为TP、FP、TN、FN。</li>
  <li>混淆矩阵(confusion matrix)</li>
</ul>

<table>
  <tbody>
    <tr>
      <td>真\预</td>
      <td>正例</td>
      <td>反例</td>
    </tr>
    <tr>
      <td>正例</td>
      <td>TP</td>
      <td>FN</td>
    </tr>
    <tr>
      <td>反例</td>
      <td>FP</td>
      <td>TN</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>查准率(precision)，记为P，它表示选择的好瓜中有多少是真正的好瓜</li>
</ul>

<center>$P=\frac{TP}{TP+FP}$</center>

<ul>
  <li>查全率(recall)，记为R，它表示好瓜中有多少被选出来了</li>
</ul>

<center>$R=\frac{TP}{TP+FN}$</center>

<ul>
  <li>
    <p>一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。</p>
  </li>
  <li>
    <p>P-R曲线：在很多情形下，我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器认为最可能是正例的样本，排在最后的则是学习器认为最不可能是正例的样本，按此顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率、查准率，也得到了P-R图。</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/2.jpg" /></center>

<ul>
  <li>
    <p>如果一个学习器的P-R曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者。如果两者曲线相交，则可以通过比较平衡点(break-even-point)来比较，越大越好。</p>
  </li>
  <li>
    <p>F1度量：F1综合考虑了查准率和查全率，是他们的调和平均</p>
  </li>
</ul>

<center>$F1=\frac{2*P*R}{P+R}$</center>

<ul>
  <li>F1的一般形式：有时候我们希望赋予查准率和查重率不同的权重：</li>
</ul>

<center>$F_\beta=\frac{(1+\beta^2)*P*R}{\beta^2*P+R}$</center>

<p>其中β大于1表示查全率有更大影响</p>

<ul>
  <li>
    <p>有时候我们有多个二分类混淆矩阵，我们希望在这n个混淆矩阵上综合考虑查准率和查全率，那么我们有以下的度量</p>
  </li>
  <li>
    <p>宏查准率macro-P，宏查全率macro-R，宏F1：</p>
  </li>
</ul>

<center>$macro-P=\frac{1}{n}\sum_1^nP_i$</center>

<center>$macro-R=\frac{1}{n}\sum_1^nR_i$</center>

<ul>
  <li>微查准率micro-P，微查全率micro-P，微F1：</li>
</ul>

<p>对TP、FP、TN、FN进行平均</p>

<center>$micro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}$</center>

<center>$micro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}$</center>

<h3 id="243-roc与auc">2.4.3. ROC与AUC</h3>
<ul>
  <li>
    <p>很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为反类。</p>
  </li>
  <li>
    <p>ROC曲线是从排序角度来出发研究学习器的泛化性能的工具。ROC的全称是Receiver Operating Characteristic。</p>
  </li>
  <li>
    <p>与P-R曲线类似，我们根据学习器的预测结果进行排序，按此顺序逐个把样本作为正例进行预测，计算出真正例率TPR(true positive rate)与假正例率FPR(false positive rate)并以它们分别为横纵坐标画出ROC曲线</p>
  </li>
</ul>

<center>$TPR=\frac{TP}{TP+FN}$</center>

<center>$FPR=\frac{FP}{FP+TN}$</center>

<center><img src="../assets/img/posts/20211222/3.jpg" /></center>

<ul>
  <li>
    <p>同样的，如果一个学习器的ROC曲线被另一个学习器完全包住，则认为后者的性能优于前者。若两个曲线相交，则比较曲线下的面积AUC(area under curve)</p>
  </li>
  <li>
    <p>形式化的看，AUC考虑的是样本预测的排序质量，那么我们可以定义一个排序损失l<sub>rank</sub></p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/4.jpg" /></center>

<center>AUC=1-l<sub>rank</sub></center>

<h3 id="244-代价敏感错误率与代价曲线">2.4.4. 代价敏感错误率与代价曲线</h3>
<ul>
  <li>有时候将正例误判断为负例与将负例误判断为正例所带来的后果不一样，我们赋予它们非均等代价</li>
  <li>代价矩阵(cost matrix)</li>
</ul>

<table>
  <tbody>
    <tr>
      <td>真\预</td>
      <td>第0类</td>
      <td>第1类</td>
    </tr>
    <tr>
      <td>第0类</td>
      <td>0</td>
      <td>cost<sub>01</sub></td>
    </tr>
    <tr>
      <td>第1类</td>
      <td>cost<sub>10</sub></td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<p>其中cost<sub>ij</sub>表示将第i类样本预测为第j类样本的代价</p>

<ul>
  <li>代价敏感错误率</li>
</ul>

<center><img src="../assets/img/posts/20211222/5.jpg" /></center>

<ul>
  <li>在非均等代价下，ROC曲线不能直接反映出学习器的期望总代价，而代价曲线则可达到此目的，横轴是正例概率代价，纵轴是归一化代价</li>
</ul>

<center><img src="../assets/img/posts/20211222/6.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/7.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/8.jpg" /></center>

<h2 id="25-比较检验">2.5. 比较检验</h2>
<p>我们有了实验评估方法与性能度量，是否就可以对学习器的性能进行评估比较了呢？实际上，机器学习中性能比较这件事比大家想象的要复杂很多，我们需要考虑多种因素的影响，下面介绍一些对学习器性能进行比较的方法，本小节默认以错误率作为性能度量。(但我觉得似乎同一个数据集下就可以比较)</p>

<h3 id="251-假设检验">2.5.1. 假设检验</h3>
<ul>
  <li>假设检验中的假设是对学习器泛化错误率分布的某种猜想或者判断，例如“ε=ε<sub>0</sub>”这样的假设</li>
  <li>现实任务中我们并不知道学习器的泛化错误率，我们只能获知其测试错误率$\hat{\epsilon}$，但直观上，两者接近的可能性比较大，因此可根据测试错误率估推出泛化错误率的分布。</li>
  <li>泛化错误率为$\epsilon$的学习器被测得测试错误率为$\hat{\epsilon}$的概率：</li>
</ul>

<center><img src="../assets/img/posts/20211222/9.jpg" /></center>

<ul>
  <li>我们发现$\epsilon$符合二项分布</li>
</ul>

<center><img src="../assets/img/posts/20211222/10.jpg" /></center>

<ul>
  <li>
    <p>二项检验：我们可以使用<strong>二项检验(binomial test)</strong>来对“$\epsilon$&lt;0.3”这样的假设进行检验，即在$\alpha$显著度下，$1-\alpha$置信度下判断假设是否成立。</p>
  </li>
  <li>
    <p>t检验：我们也可以用t检验(t-test)来检验。</p>
  </li>
  <li>
    <p>上面介绍的都是针对单个学习器泛化性能的假设进行检验</p>
  </li>
</ul>

<h3 id="252-交叉验证t检验">2.5.2. 交叉验证t检验</h3>

<ul>
  <li>
    <p>对于两个学习器A和B，若我们使用k折交叉验证法得到的测试错误率分别是$\epsilon_1^A$, $\epsilon_2^A$…$\epsilon_k^A$和$\epsilon_1^B$, $\epsilon_2^B$…$\epsilon_k^B$。其中$\epsilon_i^A$和$\epsilon_i^B$是在相同第i折训练集上得到的结果。则可以用k折交叉验证“成对t检验”来进行比较检验。</p>
  </li>
  <li>
    <p>我们这里的基本思想是若两个学习器的性能相同，则它们使用相同的训练测试集得到的错误率应该相同，即$\epsilon_i^A=\epsilon_1^B$</p>
  </li>
  <li>
    <p>$\Delta_i$ = $\epsilon_i^A$ - $\epsilon_i^B$，然后对$\Delta$进行分析</p>
  </li>
</ul>

<h3 id="253-mcnemar检验">2.5.3. McNemar检验</h3>
<ul>
  <li>对于二分类问题，使用留出法不仅可以估计出学习器A和B的测试错误率，还可获得两学习器分类结果的差别，即两者都正确、都错误…的样本数</li>
</ul>

<center><img src="../assets/img/posts/20211222/11.jpg" /></center>

<ul>
  <li>若我们假设两学习器性能相同，则应有e<sub>01</sub>=e<sub>10</sub>，那么变量|e<sub>01</sub>-e<sub>10</sub>|应该服从正态分布/卡方分布，然后用McNemar检验</li>
</ul>

<h3 id="254-friedman检验与nemenyi后续检验">2.5.4. Friedman检验与Nemenyi后续检验</h3>

<ul>
  <li>
    <p>交叉验证t检验与McNemar检验都是在一个数据集上比较两个算法的性能，但是很多时候，我们会在一组数据集上比较多个算法。</p>
  </li>
  <li>
    <p>当有多个算法参与比较时，一种做法是在每个数据集上本别列出两两比较的结果。另一种方法则是基于算法排列的<strong>Friedman检验</strong></p>
  </li>
  <li>
    <p>假定我们用D<sub>1</sub>, D<sub>2</sub>, D<sub>3</sub>, D<sub>4</sub>四个数据集对算法A、B、C进行比较。首先，使用留出法或交叉验证法得到每个算法在每个数据集上的测试结果。然后在每个数据集上根据测试性能由好到坏排序，并赋予序值1,2,…若算法的测试性能相同，则平分序值。</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/12.jpg" /></center>

<ul>
  <li>
    <p>然后使用Fredman检验来判断这些算法是否性能都相同，若相同，那么它们的平均序值应当相同。r<sub>i</sub>表示第i个算法的平均序值，那么它的均值和方差应该满足…</p>
  </li>
  <li>
    <p>若“所有算法的性能相同”这个假设被拒绝，则说明算法的性能显著不同，这时需要后续检验(post-hoc test)来进一步区分各算法，常用的有Nemenyi后续检验</p>
  </li>
  <li>
    <p>Nemenyi检验计算出平均序值差别的临界值域</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/13.jpg" /></center>

<ul>
  <li>在表中找到k=3时q<sub>0.05</sub>=2.344，根据公式计算出临界值域CD=1.657，由表中的平均序值可知A与B算法的差距，以及算法B与C的差距均未超过临界值域，而算法A与C的差距超过临界值域，因此检验结果认为算法A与C的性能显著不同，而算法A与B以及算法B与C的性能没有显著差别。</li>
</ul>

<center><img src="../assets/img/posts/20211222/14.jpg" /></center>

<h2 id="26-偏差与方差">2.6. 偏差与方差</h2>

<ul>
  <li>
    <p>对学习算法除了通过实验估计其泛化性能，人们往往还希望了解它“为什么”具有这样的性能。偏差-方差分解是解释学习算法泛化性能的一种重要工具</p>
  </li>
  <li><strong>偏差</strong>度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力</li>
  <li><strong>方差</strong>度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响</li>
  <li>
    <p><strong>噪声</strong>则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度</p>
  </li>
  <li>一般来说，偏置与方差是有冲突的，也就是偏置大的方差小，偏置小的方差大</li>
</ul>

<center><img src="../assets/img/posts/20211222/15.jpg" /></center>

<h1 id="3-第3章-线性模型">3. 第3章 线性模型</h1>
<h2 id="31-思维导图">3.1. 思维导图</h2>

<center><img src="../assets/img/posts/20211222/42.jpg" /></center>

<h2 id="32-基本形式">3.2. 基本形式</h2>

<ul>
  <li>
    <p>给定由d个属性描述的示例$x=(x_1;x_2;…x_d)$，这是一个列向量，其中$x_i$是$x$在第i个属性上的取值。</p>
  </li>
  <li>
    <p><strong>线性模型(linear model)</strong>试图学得一个通过属性线性组合来进行预测的函数：</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/17.jpg" /></center>

<ul>
  <li>向量形式：</li>
</ul>

<center>$f(x)=\omega^Tx+b$</center>

<p>其中$\omega=(\omega_1;\omega_2…\omega_d)$</p>

<ul>
  <li>
    <p>当$\omega$和b学得后，模型就得以确定</p>
  </li>
  <li>
    <p>线性模型的优点：形式简单，易于建模，良好的可解释性(comprehensibility)</p>
  </li>
</ul>

<h2 id="33-线性回归">3.3. 线性回归</h2>
<ul>
  <li>
    <p>对离散属性的处理：1.若属性值存在“序”的关系，可通过连续化将其转化为连续值，比如高、矮变成1、0；2.若不存在序的关系，可转化为k维向量。</p>
  </li>
  <li>
    <p>均方误差是回归任务中最常用的性能度量，试图让均方误差最小化：</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/18.jpg" /></center>

<ul>
  <li>
    <p>均方误差有非常好的几何意义，它对应了欧氏距离。基于均方误差最小化来进行模型求解的方法称为<strong>最小二乘法(least square method)</strong>。在线性回归中，最小二乘法就是试图找到一条直线，使得样本到直线上的欧氏距离之和最小</p>
  </li>
  <li>
    <p>首先观察一个属性值的情况。求解$\omega$和$b$使得均方误差最小化的过程，称为线性回归的最小二乘“参数估计”，我们令均方误差分别对$\omega$和$b$求导令其为零，可以得到最优解的<strong>闭式解(closed-form)</strong>,即解析解</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/19.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/20.jpg" /></center>

<ul>
  <li>更一般的情况是d个属性，称其为“多元线性回归”，同样的步骤，只是$\omega$变成向量形式，自变量写成m*(d+1)的矩阵形式，m对应了m个样本，d+1对应了d个属性和偏置。同样的求导为0然后得出$\hat{\omega}$最优解的闭式解，其中$\hat{\omega}=(\omega;b)$。当$X^TX$为满秩矩阵<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>或正定矩阵时，有唯一的解：</li>
</ul>

<center><img src="../assets/img/posts/20211222/21.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/22.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/23.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/24.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/25.jpg" /></center>

<ul>
  <li>
    <p>然而现实任务中往往不是满秩矩阵，例如在许多任务中我们会遇到大量的变量其数目甚至超过样例数。那么我们会求出$\hat{\omega}=(\omega;b)$的多个解，它们都能使均方误差最小化。选择哪一个解作为输出与学习算法的归纳偏好决定，常见的做法是<strong>引入正则化(regularization)</strong></p>
  </li>
  <li>
    <p><strong>广义线性模型(generalized linear model)</strong>:</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/26.jpg" /></center>

<center>$g(y)=\omega^Tx+b$</center>

<p>其中g()单调可微，被称为联系函数。比如当g()=ln()时称为对数线性回归</p>

<h2 id="34-对数几率回归">3.4. 对数几率回归</h2>
<ul>
  <li>
    <p>上一节讨论的是线性回归进行回归学习。那么如果我们要做的是分类任务应该怎么改变？我们只需要找到一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。</p>
  </li>
  <li>
    <p>考虑二分类问题，那么我们可以用单位阶跃函数/Heaviside函数联系y与z，其中y是真是标记，z是预测值，$z=\omega^Tx+b$</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/27.jpg" /></center>

<ul>
  <li>
    <p>但是它有个问题就是不连续，所以我们希望找到能在一定程度上近似它的替代函数。</p>
  </li>
  <li>
    <p><strong>对数几率函数(logistic function)</strong>就是一个替代函数：</p>
  </li>
</ul>

<center>$y=\frac{1}{1+e^{-z}}$</center>

<center><img src="../assets/img/posts/20211222/28.jpg" /></center>

<ul>
  <li>那么这个式子可以转化为：可以把y视为样本x作为正例的可能性，则1-y是其反例的可能性，两者的比值称为几率(odds)：</li>
</ul>

<center><img src="../assets/img/posts/20211222/29.jpg" /></center>

<ul>
  <li>若将y视为类后验概率估计。则式子可以重写为：</li>
</ul>

<center><img src="../assets/img/posts/20211222/30.jpg" /></center>

<ul>
  <li>接下来我们可以通过<strong>极大似然法(maximum likelihood method)</strong>来估计$\omega$和$b$。给定数据集，对数似然函数<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>为：</li>
</ul>

<center><img src="../assets/img/posts/20211222/31.jpg" /></center>

<p>即每个样本属于其真实标记的概率越大越好。</p>

<ul>
  <li>推导过程：</li>
</ul>

<center><img src="../assets/img/posts/20211222/32.jpg" /></center>

<p>上面有个式子应该有问题，(3.26)应该是</p>

<center>$p(y_i|x_i;\omega,b) = p_1(\hat{x_i};\beta)^{y_i}p_0(\hat{x_i};\beta)^{1-y_i}$</center>

<p>因为$\beta$是高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法如<strong>梯度下降法(gradient descent method)</strong>和牛顿法都可以求得最优解</p>

<h2 id="35-线性判别分析">3.5. 线性判别分析</h2>

<ul>
  <li>
    <p>线性判别分析(Linear Discriminant Analysis，简称LDA)是一种经典的线性学习方法。</p>
  </li>
  <li>
    <p>LDA的思想非常朴素: 给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离;在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/33.jpg" /></center>

<ul>
  <li>令$X_i$、$\mu_i$、$\Sigma_i$分别表示第i类示例的集合、均值向量<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>、协方差矩阵<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>。</li>
</ul>

<ul>
  <li>欲使同类样例的投影点尽可能接近，可以让同类样例投影点的协方差尽可能小，即$\omega^T\Sigma_0\omega+\omega^T\Sigma_1\omega$尽可能小;而欲使异类样例的投影点尽可能远离，可以让类中心之间的距离尽可能大，即$||\omega^T\mu_0-\omega^T\mu_1||$尽可能大:</li>
</ul>

<center><img src="../assets/img/posts/20211222/34.jpg" /></center>

<ul>
  <li>剩余推导过程：</li>
</ul>

<center><img src="../assets/img/posts/20211222/35.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/36.jpg" /></center>

<ul>
  <li>值得一提的是，LDA可从贝时斯决策理论的角度来阐释，并可证明，当两类数据同先验、满足高斯分布且协方差相等时，LDA可达到最优分类。</li>
</ul>

<h2 id="36-多分类学习">3.6. 多分类学习</h2>
<ul>
  <li>
    <p>现实中常遇到多分类学习任务，有些二分类学习方法可直接推广到多分类，但在更多情形下，我们是基于一些基本策略，利用二分类学习器来解决多分类问题。</p>
  </li>
  <li>
    <p>不失一般性，考虑N个类别$C_1$、$C_2$…$C_N$，多分类学习的基本思路是”<strong>拆解法</strong>”，即将多分类任务拆为若干个二分类任务求解。具体来说，先对问题进行拆分，然后为拆出的每个二分类任务训练一个分类器;在测试时，对这些分类器的预测结果进行集成以获得最终的多分类结果。</p>
  </li>
  <li>
    <p>这里我们着重介绍如何拆分，最经典的拆分策略有三种：一对一(One vs One)、一对其余(One vs Rest)、多对多(Many vs Many)</p>
  </li>
  <li>
    <p>一对一：将这N个类别两两配对，从而产生 N(N-1)/2个二分类任务。在测试阶段，新样本将同时提交给所有分类器，于是我们将得到N(N-1)/2个分类结果，最终结果可通过投票产生:即把被预测得最
多的类别作为最终分类结果。</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/37.jpg" /></center>

<ul>
  <li>一对其余：OvR则是每次将一个类的样例作为正例，所有其他类的样例作为反例来训练N个分类器。在测试时若仅有一个分类器预测为正类，则对应的类别标记作为最终分类结果。若有多个分类器预测为正类，则通常考虑各分类器的预测置信度，选择置信度最大的类别标记作为分类结果。</li>
</ul>

<center><img src="../assets/img/posts/20211222/38.jpg" /></center>

<ul>
  <li>
    <p>多对多MvM是每次将若干个类作为正类，若干个其他类作为反类。这里我们介绍一种最常用的MvM技术：<strong>纠错输出码(ECOC)</strong></p>
  </li>
  <li>
    <p>ECOC是将编码的思想引入类别拆分，主要分为两步：</p>
  </li>
</ul>

<p>  1.编码： 对N个类别做M次划分，每次划分将一部分类别划为正类，一部分划为反类，从而形成一个二分类训练集;这样一共产生M个训练集，可训练出M个分类器</p>

<p>  2.解码：:M个分类器分别对测试样本进行预测，这些预测标记组成一个编码.将这个预测编码与每个类别各自的编码进行比较，返回其中距离最小的类别作为最终预测结果。</p>

<ul>
  <li>类别划分通过<strong>编码矩阵</strong>指定。编码矩阵有多种形式，常见的有二元码和三元码，前者将每个类别分别指定为正类和反类，后者在正、反类之外，还可指定“停用类”</li>
</ul>

<center><img src="../assets/img/posts/20211222/39.jpg" /></center>

<h2 id="37-类别不平衡问题">3.7. 类别不平衡问题</h2>

<ul>
  <li>
    <p>前面介绍的分类学习方法都有一个共同的基本假设：即不同类别的训练样例数目相当。如果不同类别的样例数差别很大，会对学习过程造成困扰。</p>
  </li>
  <li>
    <p><strong>类别不平衡(class-imbalance)</strong>就是指分类任务中不同类别的训练样例数目差别很大的情况。</p>
  </li>
  <li>
    <p><strong>再缩放(rescaling)</strong>是类别不平衡中的一个基本策略：比如在最简单的二分类问题中，我们假设y大于0.5为正例，y小于0.5为负例，但是在类别不平衡时，我们可以改变阈值来达到再平衡：</p>
  </li>
</ul>

<p>  将</p>

<center><img src="../assets/img/posts/20211222/40.jpg" /></center>

<p>  变成</p>

<center><img src="../assets/img/posts/20211222/41.jpg" /></center>

<ul>
  <li>现有的解决类别不平衡的技术大体上有三类做法(这里我们均假设正例样本少):</li>
</ul>

<p>  1.第一类是直接对训练集里的反类样例进行”欠采样” (undersampling)，即去除一些反例使得正、反例数日接近，然后再进行学习;</p>

<p>  2.第二类是对训练集里的正类样例进行”过采样” (oversampling)，即增加一些正例使得正、反例数目接近，然后再进行学习;</p>

<p>  3.第三类则是直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将上面的公式(改变阈值)嵌入到其决策过程中，称为”阔值移动” (threshold-moving)</p>

<ul>
  <li>需注意的是，过采样法不能简单地对初始正例样本进行重复来样，否则会招致严重的过拟合；过采样法的代表性算法SMOTE是通过对训练集里的正例进行插值来产生额外的正例.另一方面，欠采样法若随机丢弃反例可能丢失一些重要信息;欠采样法的代表性算法EasyEnsemble则是利用集成学习机制，将反例划分为若干个集合供不同学习器使用，这样对每个学习器来看都进行了欠采样，但在全局来看却不会丢失重要信息。</li>
</ul>

<h1 id="4-第4章-决策树">4. 第4章 决策树</h1>
<h2 id="41-思维导图">4.1. 思维导图</h2>
<h3 id="411-章节导图">4.1.1. 章节导图</h3>

<center><img src="../assets/img/posts/20211222/61.jpg" /></center>

<h3 id="412-如何生成一棵决策树">4.1.2. 如何生成一棵决策树</h3>

<center><img src="../assets/img/posts/20211222/62.jpg" /></center>

<h2 id="42-基本流程">4.2. 基本流程</h2>
<ul>
  <li>决策树是基于树结构来进行决策，其中包含一个根结点，多个内部结点和多个叶结点</li>
  <li>叶结点对应决策结果，其他每个结点都对应一个属性测试</li>
  <li>每个结点包含的样本集合根据属性测试被划分到子结点中，那么根结点包含样本全集</li>
  <li>决策树学习的目的是为了产生一棵泛化能力强的决策树</li>
  <li>决策树的生成是一个递归过程，下面这张图展示了递归的过程：</li>
</ul>

<center><img src="../assets/img/posts/20211222/43.jpg" /></center>

<p>对于每个结点，首先判断该结点的样本集是否属于同一个类别C，如果是，则将该结点标记为C类叶结点。再判断该结点的样本集的属性值是否完全相同(或者是否为空集)，如果是，则将该结点标记为D类叶结点，其中D类是这些样本中最多的类别。如果该结点即不是<strong>同属于一个类别</strong>也不是<strong>属性值取值相同</strong>，那么则需要继续划分，选择一个最优的划分属性$a_*$,创建新的分支，对于每个分支结点首先判断是否为空，如果为空则判定为E类叶结点，其中E类是父结点中类别最多的类。如果子结点不为空则递归。</p>

<h2 id="43-划分选择">4.3. 划分选择</h2>
<p>可以发现生成决策树最关键的步骤就是选择最优划分属性，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的<strong>纯度</strong>越来越高。我们有很多指标来确定选择哪一个属性作为最优划分选择，下面将分别介绍：</p>

<h3 id="431-信息增益">4.3.1. 信息增益</h3>
<ul>
  <li><strong>信息熵(information entropy)</strong>是度量样本集合纯度最常用的一种指标。下面是信息熵的定义公式：</li>
</ul>

<center><img src="../assets/img/posts/20211222/44.jpg" /></center>

<p>  其中$p_k$表示第k类样本在样本集D中所占比例，信息熵越小表示D的纯度越高。</p>

<ul>
  <li>假设离散属性a有V个可能的取值${a^1,a^2…a^V}$,那么我们可以计算出在使用a作为划分属性前后的信息熵差别，也就是<strong>信息增益(information gain)</strong>：</li>
</ul>

<center><img src="../assets/img/posts/20211222/45.jpg" /></center>

<p>  对每一个子结点$D^v$都赋予权重同时相加。</p>

<ul>
  <li>
    <p>著名的ID3决策树学习算法就是以信息增益作为准则来选择划分属性，我们希望找到信息增益最大的属性。</p>
  </li>
  <li>
    <p>书上使用信息增益划分的例子：</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/46.jpg" /></center>

<h3 id="432-增益率">4.3.2. 增益率</h3>
<ul>
  <li>
    <p>信息增益对可取值数目较多的属性有所偏好，比如我们使用编号这一属性来划分，每一个编号都只有一个样本，那么信息增益肯定增大了，但是决策树的泛化能力显然下降了。</p>
  </li>
  <li>
    <p><strong>增益率(gain ratio)</strong>，我们通过对信息增益除以IV来平衡属性数目带来的影响，增益率的定义如下：</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/47.jpg" /></center>

<p>  IV(intrinsic value)的定义如下：</p>

<center><img src="../assets/img/posts/20211222/48.jpg" /></center>

<p>  IV是属性a的固有值，属性a可取的数值数目越多，那么IV就越大</p>

<ul>
  <li>但是增益率也有问题，那就是对于可取值数目较少的属性有偏好，所以著名的C4.5决策树算法并不是直接使用增益率，而是先从候选划分属性中找出信息增益高于平均水平的属性，然后再从中选择增益率最高的属性</li>
</ul>

<h3 id="433-基尼指数">4.3.3. 基尼指数</h3>
<ul>
  <li>CART决策树使用基尼指数来选择划分属性</li>
  <li>数据集D的纯度定义如下</li>
</ul>

<center><img src="../assets/img/posts/20211222/49.jpg" /></center>

<p>  直观来说，Gini反映了从数据集随便抽取两个样本，它们类别不一致概率</p>

<ul>
  <li><strong>基尼指数</strong>定义如下：</li>
</ul>

<center><img src="../assets/img/posts/20211222/50.jpg" /></center>

<p>  很明显，我们希望基尼指数越小越好，所以我们选择基尼指数最小的属性最为最优划分属性。</p>

<h2 id="44-剪枝处理">4.4. 剪枝处理</h2>
<ul>
  <li>不难发现，上面对于属性的划分很容易过拟合，所以针对过拟合现象，决策树选择<strong>剪枝(pruning)</strong>来对付过拟合</li>
  <li>剪枝就是去掉一些分支来降低过拟合的风险，剪枝可以分为预剪枝和后剪枝</li>
  <li><strong>预剪枝(prepruning)</strong>:在决策树生成的过程中，对每个结点在划分前进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分</li>
  <li><strong>后剪枝(postpruning)</strong>:从训练集生成了一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能的提升，则将该子树替换为叶结点。</li>
</ul>

<h3 id="441-预剪枝">4.4.1. 预剪枝</h3>
<ul>
  <li>如何判断决策树泛化性能？可以使用留出法预留一部分数据用作验证集进行性能评估，性能度量可以用之前介绍的那些，本小节使用精度作为性能度量</li>
  <li>预剪枝生成的决策树：</li>
</ul>

<center><img src="../assets/img/posts/20211222/51.jpg" /></center>

<ul>
  <li>可以发现预剪枝显著减少了分支的数量，这样可以减少决策树的训练时间开销。但是这样也有一个问题，就是有些分支的当前划分虽然不能提升泛化性能，但是后续划分却有可能导致性能显著提高，这样就带来了欠拟合的风险</li>
</ul>

<h3 id="442-后剪枝">4.4.2. 后剪枝</h3>
<ul>
  <li>首先生成决策树，然后对每个结点进行评估是否需要剪枝</li>
</ul>

<center><img src="../assets/img/posts/20211222/52.jpg" /></center>

<ul>
  <li>虽然后剪枝决策树的欠拟合风险小，泛化性能也往往优于预剪枝决策树，但是后剪枝的时间开销大</li>
</ul>

<h2 id="45-连续与缺失值">4.5. 连续与缺失值</h2>
<h3 id="451-连续值处理">4.5.1. 连续值处理</h3>
<ul>
  <li>到目前为止仅讨论了基于离散属性来生成决策树，但是现实学习任务中通常会遇到连续属性</li>
  <li>很明显连续属性不能根据连续属性的可取值来对结点进行划分，我们需要用到<strong>连续属性离散化</strong>的技术，最简单的策略是<strong>二分法</strong></li>
  <li>给定样本集D和和连续属性a，假定a在D上出现了n个不同的取值，将这些值从小到大排序，然后每次选择每两个数的中位数t作为划分点，那么连续值就可以当作离散值来处理了，分出的子样本集分别记作$D_t^+$和$D_t^-$</li>
</ul>

<center><img src="../assets/img/posts/20211222/53.jpg" /></center>

<ul>
  <li>划分结果：</li>
</ul>

<center><img src="../assets/img/posts/20211222/54.jpg" /></center>

<ul>
  <li>需要注意的是：<strong>连续属性在划分后并不会被丢失，后续划分仍然可以使用</strong></li>
</ul>

<h3 id="452-缺失值处理">4.5.2. 缺失值处理</h3>
<ul>
  <li>在实际数据中一般都有很多缺失值，所以我们需要考虑如何对含有缺失值的数据进行学习</li>
  <li>给几个定义：$\tilde{D}$表示D中属性a上没有缺失值的样本子集，假设属性值a可取值{$a^1$,$a^2$…$a^V$}, $\tilde{D}^v$表示$\tilde{D}$中属性值a取值为$a^v$的子集，$\tilde{D}_k$表示样本子集，我们为每个样本赋予权重$\omega_x$(决策树开始阶段，根结点中权重初始化为1)并定义：</li>
</ul>

<center><img src="../assets/img/posts/20211222/55.jpg" /></center>

<ul>
  <li>
    <p>直观地看，对属性a，$\rho$表示无缺失值样本所占比例，$\tilde{p}_k$表示无缺失样本中第k类样本所占的比例，$\tilde{r}_v$表示无缺失值样本在属性上取值为$a^v$所占的比例</p>
  </li>
  <li>
    <p>那么我们可以将信息增益的公式推广为</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/56.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/57.jpg" /></center>

<ul>
  <li>
    <p>那么对于那些在该属性上缺失的值如何处理呢？分两种情况：，若样本$x$在划分属性$a$上的取值己知, 则将$x$划入与其取值对应的子结点，且样本权值在于结点中保持为$\omega_x$, 若样本$x$在划分属性$a$上的取值未知，则将$x$同时划入所有子结点, 且样本权值在与属性值$a^v$对应的子结点中调整为$\tilde{r}_v*\omega_x$，直观地看，这就是让同一个样本以不同的概率划入到不同的子结点中去。</p>
  </li>
  <li>
    <p>C4.5就是使用了上述的解决方法</p>
  </li>
</ul>

<h2 id="46-多变量决策树">4.6. 多变量决策树</h2>
<ul>
  <li>若我们把每个属性视为坐标空间中的一个坐标轴，则d个属性描述的样本就对应了d维空间中的一个数据点，对样本分类则意味着在这个坐标空间中寻找不同类样本之间的分类边界，决策树生成的分类边界有个明显的特点：<strong>轴平行</strong>，即它的分类边界由若干个与坐标轴平行的分段组成</li>
</ul>

<center><img src="../assets/img/posts/20211222/58.jpg" /></center>

<ul>
  <li>这样的决策树由于要进行大量的属性测试，预测时间开销会很大，所以我们希望使用如下图红线所示的斜划分。<strong>多变量决策树</strong>就是能实现这样斜划分甚至更复杂划分的决策树</li>
</ul>

<center><img src="../assets/img/posts/20211222/59.jpg" /></center>

<ul>
  <li>以实现斜划分的决策树为例，非叶结点不再是仅对某一个属性，而是对属性的线性组合进行测试</li>
</ul>

<center><img src="../assets/img/posts/20211222/60.jpg" /></center>

<h2 id="47-阅读材料">4.7. 阅读材料</h2>
<ul>
  <li>
    <p>多变量决策树算法主要有OC1，还有一些算法试图在决策树的叶结点上嵌入神经网络，比如感知机树在每个叶结点上训练一个感知机</p>
  </li>
  <li>
    <p>有些决策树学习算法可进行<strong>“增量学习”(incrementallearning)</strong>，即在接收到新样本后可对己学得的模型进行调整，而不用完全重新学习。主要机制是通过调整分支路径上的划分属性次序来对树进行部分重构，代表性算法ID4、ID5R、ITI等。增量学习可有效地降低每次接收到新样本后的训练时间开销，但多步增量学习后的模型会与基于全部数据训练而得的模型有较大差别。</p>
  </li>
</ul>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>注意这里的测试集其实是验证集，我们通常把实际情况中遇到的数据集称为测试集。 <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>满秩矩阵指方阵的秩等于矩阵的行数/列数，满秩矩阵有逆矩阵且对于y=Xb有唯一的解 <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>统计学中，似然函数是一种关于统计模型参数的函数。给定输出x时，关于参数θ的似然函数L(θ|x)（在数值上）等于给定参数θ后变量X的概率：L(θ|x)=P(X=x|θ)。 <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>随机变量的期望组成的向量称为期望向量或者均值向量 <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>协方差矩阵的每个元素是各个向量元素之间的协方差。协方差就是Covariance <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[《机器学习》周志华读书笔记]]></summary></entry><entry><title type="html">组会记录</title><link href="http://localhost:4000/1221%E7%BB%84%E4%BC%9A.html" rel="alternate" type="text/html" title="组会记录" /><published>2021-12-21T00:00:00+08:00</published><updated>2021-12-21T00:00:00+08:00</updated><id>http://localhost:4000/1221%E7%BB%84%E4%BC%9A</id><content type="html" xml:base="http://localhost:4000/1221%E7%BB%84%E4%BC%9A.html"><![CDATA[<h1 id="vae">VAE</h1>
<h2 id="ae">AE</h2>
<p>Auto-Encoder自动编码器，比如Seq2seq模型。</p>

<h2 id="vaevariational-auto-encoder">VAE(Variational Auto-Encoder)</h2>
<p>在实际情况中，我们需要在模型的准确率上与隐含向量服从标准正态分布之间做一个权衡，所谓模型的准确率就是指解码器生成的图片与原图片的相似程度。我们可以让网络自己来做这个决定，非常简单，我们只需要将这两者都做一个loss，然后在将他们求和作为总的loss，这样网络就能够自己选择如何才能够使得这个总的loss下降。另外我们要衡量两种分布的相似程度，如何看过之前一片GAN的数学推导，你就知道会有一个东西叫KL-divergence来衡量两种分布的相似程度，这里我们就是用KL-divergence来表示隐含向量与标准正态分布之间差异的loss，另外一个loss仍然使用生成图片与原图片的均方误差来表示。</p>

<h2 id="kl消失">KL消失</h2>
<p>KL消失后，VAE就变成了AE
原因：</p>
<ul>
  <li>KL项本身太容易被优化</li>
  <li>一旦崩塌，Decoder会忽视Z<sub>x</sub></li>
  <li>Z<sub>x</sub>的表示学习依赖于Decoder</li>
</ul>

<h2 id="解决kl消失的思路">解决KL消失的思路</h2>
<p>…</p>

<h1 id="analyze-pretraining-language-model">Analyze Pretraining Language Model</h1>
<h2 id="perspective-of-knowledge">Perspective of knowledge</h2>
<ul>
  <li>Syntacitic/Semantic/lexical 句法，语义，词汇</li>
  <li>重构语法树</li>
  <li>Attention中很多头可能没有用，学到了很多冗余的信息</li>
  <li>Analyze Feed Forward Neural Network</li>
  <li>浅层词汇信息，深层语义信息</li>
  <li>Prompt</li>
</ul>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[组会笔记]]></summary></entry><entry><title type="html">制作类RACE数据集</title><link href="http://localhost:4000/RACElike-datasets.html" rel="alternate" type="text/html" title="制作类RACE数据集" /><published>2021-12-21T00:00:00+08:00</published><updated>2021-12-21T00:00:00+08:00</updated><id>http://localhost:4000/RACElike-datasets</id><content type="html" xml:base="http://localhost:4000/RACElike-datasets.html"><![CDATA[<h1 id="目录">目录</h1>
<!-- TOC -->

<ul>
  <li><a href="#目录">目录</a></li>
  <li><a href="#race">RACE</a>
    <ul>
      <li><a href="#简介">简介</a></li>
      <li><a href="#race数据集格式">RACE数据集格式</a></li>
      <li><a href="#race数据集分布">RACE数据集分布</a></li>
      <li><a href="#race数据集中的长度">RACE数据集中的长度</a></li>
      <li><a href="#race数据集中的问题的统计信息">RACE数据集中的问题的统计信息</a></li>
    </ul>
  </li>
  <li><a href="#gaorace">GaoRACE</a>
    <ul>
      <li><a href="#gao他们对于race数据集的处理">Gao他们对于RACE数据集的处理</a></li>
      <li><a href="#gao处理后的race数据集统计信息">Gao处理后的RACE数据集统计信息</a></li>
      <li><a href="#gao处理后的数据集格式">Gao处理后的数据集格式</a>
        <ul>
          <li><a href="#预处理">预处理</a></li>
          <li><a href="#updated">updated</a></li>
          <li><a href="#预处理代码">预处理代码</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#mrc-阅读理解数据集">MRC 阅读理解数据集</a>
    <ul>
      <li><a href="#简介-1">简介</a></li>
      <li><a href="#title">Title</a></li>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#table-一张十分完整的表格">Table 一张十分完整的表格</a></li>
      <li><a href="#值得关注的地方">值得关注的地方</a></li>
    </ul>
  </li>
  <li><a href="#自制数据集">自制数据集</a>
    <ul>
      <li><a href="#大型题库">大型题库</a></li>
      <li><a href="#方法">方法</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->
<h1 id="race">RACE</h1>
<h2 id="简介">简介</h2>
<p>RACE数据集包含了中国初高中阅读理解题目，最初发布在2017年，一共含有28k短文和100k个问题，最开始发布的目的是为了<strong>阅读理解</strong>任务。它的特点是包含了很多需要推理的问题。</p>

<ul>
  <li>原RACE数据集<a href="http://www.cs.cmu.edu/~glai1/data/race/">地址</a></li>
  <li>下载地址<a href="http://www.cs.cmu.edu/~glai1/data/race/RACE.tar.gz">url</a></li>
  <li>论文地址：<a href="https://arxiv.org/abs/1704.04683">RACE: Large-scale ReAding Comprehension Dataset From Examinations</a></li>
</ul>

<h2 id="race数据集格式">RACE数据集格式</h2>
<p>Each passage is a JSON file. The JSON file contains following fields:</p>

<ol>
  <li>article: A string, which is the passage. 文章</li>
  <li>questions: A string list. Each string is a query. We have two types of questions. First one is an interrogative sentence. Another one has a placeholder, which is represented by _. 四个问题题干</li>
  <li>options: A list of the options list. Each options list contains 4 strings, which are the candidate option. 四个题目的四个选项</li>
  <li>answers: A list contains the golden label of each query.四个题目的正确答案</li>
  <li>id: Each passage has a unique id in this dataset.</li>
</ol>

<h2 id="race数据集分布">RACE数据集分布</h2>

<p><img src="../assets/img/posts/20211221/3.jpg" /></p>

<p>RACE-M表示初中题目，RACE-H表示高中题目</p>

<h2 id="race数据集中的长度">RACE数据集中的长度</h2>

<p><img src="../assets/img/posts/20211221/4.jpg" /></p>

<h2 id="race数据集中的问题的统计信息">RACE数据集中的问题的统计信息</h2>

<p><img src="../assets/img/posts/20211221/5.jpg" /></p>

<h1 id="gaorace">GaoRACE</h1>
<h2 id="gao他们对于race数据集的处理">Gao他们对于RACE数据集的处理</h2>
<ul>
  <li>去掉了那些误导选项和文章语义不相关的数据</li>
  <li>去掉了那些需要<code class="language-plaintext highlighter-rouge">world knowledge</code>生成的选项</li>
  <li>github<a href="https://github.com/Yifan-Gao/Distractor-Generation-RACE">url</a>,上面有预处理RACE数据集的代码</li>
</ul>

<h2 id="gao处理后的race数据集统计信息">Gao处理后的RACE数据集统计信息</h2>

<p><img src="../assets/img/posts/20211221/7.jpg" /></p>

<h2 id="gao处理后的数据集格式">Gao处理后的数据集格式</h2>

<h3 id="预处理">预处理</h3>

<p>首先把数据集规整到一个json文件里，分为dev,test,train三个json文件。</p>

<p>每一行包含以下信息：</p>

<p>article, sent(sentence), question(问题有两种，一种是疑问句，一种是填空), answer_text, answer, id, word_overlap_score, word_overlap_count, article_id, question_id, distractor_id.</p>

<p>那么一个问题会有2-3个误导选项，一篇文章又会有3-4个问题。相比于原本的数据集多了word-overlap指标，word-overlap就是词重叠率，交集比上并集。</p>

<h3 id="updated">updated</h3>
<p>updated数据集和original数据集格式类似，少了overlap，内容上去掉了一些语义不相关的题目。</p>

<h3 id="预处理代码">预处理代码</h3>
<p>利用torchtext框架预处理文本，流程大概如下：</p>
<ul>
  <li>定义Field：声明如何处理数据 定义</li>
  <li>Dataset：得到数据集，此时数据集里每一个样本是一个 经过 Field声明的预处理 预处理后的 wordlist</li>
  <li>建立vocab：在这一步建立词汇表，词向量(word embeddings)</li>
  <li>构造迭代器：构造迭代器，用来分批次训练模型</li>
</ul>

<p>Gao说有去掉一些语义不相关的误导选项，但是在代码中并没有看见这步操作？？</p>

<p><img src="../assets/img/posts/20211221/8.jpg" /></p>

<h1 id="mrc-阅读理解数据集">MRC 阅读理解数据集</h1>

<h2 id="简介-1">简介</h2>
<p>发现了一篇很好的综述，里面涵盖了2021年之前用到的所有MRC数据集。现在对这篇综述简单介绍一下</p>

<h2 id="title">Title</h2>
<p>English Machine Reading Comprehension Datasets: A Survey</p>

<h2 id="abstract">Abstract</h2>
<p>文献收集了60个英语阅读理解数据集，分别从不同维度进行比较，包括size, vocabulary, data source, method of creation, human performance level, first question word。调研发现维基百科是最多的数据来源，同时也发现了缺少很多why,when,where问题。</p>

<h2 id="table-一张十分完整的表格">Table 一张十分完整的表格</h2>

<p><img src="../assets/img/posts/20211221/44.jpg" /></p>

<p>首先我简单解释以下这个表格，这个表格一个收录了18个Multiple Choice Datasets,也就是说这18个数据集都着眼于多选题。</p>
<ul>
  <li>第一列是数据集的名称。</li>
  <li>第二列表示数据集中问题的个数(size)。</li>
  <li>第三列表示数据集中文章的来源，其中ER表示education resource, AG表示automatically generated即自动生成,CRW表示crowdsourcing。</li>
  <li>第四列表示答案的来源(answer)，其中UG表示user generated。</li>
  <li>第五列LB表示leader board available，即是否有排行榜，带*表示排行榜在<a href="https://paperswithcode.com/">网站</a>上发布。</li>
  <li>第六列表示人在该数据集上的表现。</li>
  <li>第七列表示该数据集是否有被解决，也就是说是否有比较好的模型能在该数据集上表现良好。</li>
  <li>第八列表示问题第一个单词出现最频繁的是哪个？比如what,how,which这样的单词。</li>
  <li>第九列PAD表示是否开源。</li>
</ul>

<h2 id="值得关注的地方">值得关注的地方</h2>
<p>这么多数据集中，来源于考试题目的有RACE,RACE-C,DREAM,ReClor,这些数据集的收集方法可以借鉴。</p>

<h1 id="自制数据集">自制数据集</h1>
<h2 id="大型题库">大型题库</h2>
<p>泸江，星火英语…</p>
<h2 id="方法">方法</h2>
<p>Python爬取网页</p>]]></content><author><name>Quehry</name></author><category term="work" /><summary type="html"><![CDATA[帮助学长制作RACE数据集]]></summary></entry><entry><title type="html">计算机图形学</title><link href="http://localhost:4000/Computer_Graphics.html" rel="alternate" type="text/html" title="计算机图形学" /><published>2021-12-21T00:00:00+08:00</published><updated>2021-12-21T00:00:00+08:00</updated><id>http://localhost:4000/Computer_Graphics</id><content type="html" xml:base="http://localhost:4000/Computer_Graphics.html"><![CDATA[<!-- TOC -->

<ul>
  <li><a href="#1-lecture-01-overview-of-computer-graphics">1. Lecture 01 Overview of Computer Graphics</a>
    <ul>
      <li><a href="#11-课程情况">1.1. 课程情况</a></li>
      <li><a href="#12-什么是好的画面">1.2. 什么是好的画面</a></li>
      <li><a href="#13-应用场景">1.3. 应用场景</a></li>
      <li><a href="#14-rasterization-光栅化">1.4. Rasterization 光栅化</a></li>
      <li><a href="#15-计算机视觉">1.5. 计算机视觉</a></li>
      <li><a href="#16-推荐书籍">1.6. 推荐书籍</a></li>
    </ul>
  </li>
  <li><a href="#2-lecture-02-review-of-linear-algebra">2. Lecture 02 Review of Linear Algebra</a>
    <ul>
      <li><a href="#21-图形学依赖学科">2.1. 图形学依赖学科</a></li>
      <li><a href="#22-向量">2.2. 向量</a></li>
      <li><a href="#23-矩阵">2.3. 矩阵</a></li>
    </ul>
  </li>
  <li><a href="#3-lecture-03-transformation">3. Lecture 03 Transformation</a>
    <ul>
      <li><a href="#31-why-transformation-为什么要变换">3.1. why transformation 为什么要变换</a></li>
      <li><a href="#32-d变换">3.2. D变换</a></li>
      <li><a href="#33-齐次坐标-homogeneous-coordinate">3.3. 齐次坐标 homogeneous coordinate</a></li>
    </ul>
  </li>
  <li><a href="#4-lecture-04-transformation-cont">4. Lecture 04 Transformation Cont.</a>
    <ul>
      <li><a href="#41-d-transformations">4.1. D Transformations</a></li>
      <li><a href="#42-view-transformation-视图变换">4.2. view transformation 视图变换</a></li>
      <li><a href="#43-projection-transformation-投影变换">4.3. projection transformation 投影变换</a></li>
    </ul>
  </li>
  <li><a href="#5-lecture05-rasterization-1triangles">5. Lecture05 Rasterization 1(Triangles)</a>
    <ul>
      <li><a href="#51-perspective-projection-透视投影">5.1. Perspective Projection 透视投影</a></li>
      <li><a href="#52-canonical-cube-to-screen-光栅化">5.2. Canonical Cube to Screen 光栅化</a></li>
      <li><a href="#53-different-raster-displays-不同的成像设备">5.3. Different Raster Displays 不同的成像设备</a></li>
      <li><a href="#54-三角形光栅化">5.4. 三角形光栅化</a></li>
    </ul>
  </li>
  <li><a href="#6-lecture-06-rasterization-2antialiasing-and-z-buffering">6. Lecture 06 Rasterization 2(Antialiasing and Z-Buffering)</a>
    <ul>
      <li><a href="#61-sampling-采样原理">6.1. sampling 采样原理</a></li>
      <li><a href="#62-frequency-domaine-信号处理频率">6.2. Frequency domaine 信号处理频率</a></li>
      <li><a href="#63-antialiasing-反走样抗锯齿">6.3. antialiasing 反走样/抗锯齿</a></li>
      <li><a href="#64-antialiasing-today-目前反走样的方法">6.4. antialiasing today 目前反走样的方法</a></li>
    </ul>
  </li>
  <li><a href="#7-lecture-07-shadingillumination-shading-and-graphics-pipeline">7. Lecture 07 Shading(Illumination, Shading, and Graphics Pipeline)</a>
    <ul>
      <li><a href="#71-painters-algorithm-画家算法">7.1. Painter’s Algorithm 画家算法</a></li>
      <li><a href="#72-z-buffer-深度缓存">7.2. Z-buffer 深度缓存</a></li>
      <li><a href="#73-目前为止学到了什么">7.3. 目前为止学到了什么</a></li>
      <li><a href="#74-shading-着色">7.4. shading 着色</a></li>
    </ul>
  </li>
  <li><a href="#8-shading-2shading-pipeline-texture-mapping">8. Shading 2(Shading, Pipeline, Texture Mapping)</a>
    <ul>
      <li><a href="#81-specular-term-高光项">8.1. Specular Term 高光项</a></li>
      <li><a href="#82-ambient-term-环境项">8.2. Ambient Term 环境项</a></li>
      <li><a href="#83-shading-frequencies-着色频率">8.3. Shading Frequencies 着色频率</a></li>
      <li><a href="#84-graphics-pipeline-图像管线实时渲染管线">8.4. Graphics Pipeline 图像管线/实时渲染管线</a></li>
      <li><a href="#85-texture-mapping-纹理映射">8.5. Texture Mapping 纹理映射</a></li>
    </ul>
  </li>
  <li><a href="#9-lecture-09-shading-3-texture-mapping">9. Lecture 09 Shading 3 (Texture Mapping)</a>
    <ul>
      <li><a href="#91-barycentric-coordinates重心坐标系">9.1. Barycentric Coordinates重心坐标系</a></li>
      <li><a href="#92-interpolate-插值">9.2. Interpolate 插值</a></li>
      <li><a href="#93-simple-texture-mapping-简单的纹理映射模型">9.3. Simple Texture Mapping 简单的纹理映射模型</a></li>
      <li><a href="#94-texture-magnification-纹理放大">9.4. Texture Magnification 纹理放大</a></li>
      <li><a href="#95-point-sampling-textures">9.5. Point Sampling Textures</a></li>
      <li><a href="#96-mipmap-范围查询">9.6. Mipmap 范围查询</a></li>
    </ul>
  </li>
  <li><a href="#10-lecture-10-geomrtry-1introduction">10. Lecture 10 Geomrtry 1(introduction)</a>
    <ul>
      <li><a href="#101-纹理的应用">10.1. 纹理的应用</a>
        <ul>
          <li><a href="#1011-environment-map-环境光映射">10.1.1. Environment Map 环境光映射</a></li>
          <li><a href="#1012-spherical-environment-map-球形环境光映射">10.1.2. Spherical Environment Map 球形环境光映射</a></li>
          <li><a href="#1013-纹理凹凸贴图bump-mapping">10.1.3. 纹理凹凸贴图bump mapping</a></li>
          <li><a href="#1014-位移贴图-displacement-mapping">10.1.4. 位移贴图 displacement mapping</a></li>
          <li><a href="#1015-三维纹理">10.1.5. 三维纹理</a></li>
        </ul>
      </li>
      <li><a href="#102-几何">10.2. 几何</a>
        <ul>
          <li><a href="#1021-分类">10.2.1. 分类</a></li>
          <li><a href="#1022-隐式几何">10.2.2. 隐式几何</a></li>
          <li><a href="#1023-显式几何">10.2.3. 显式几何</a></li>
          <li><a href="#1024-隐式的表达方式">10.2.4. 隐式的表达方式</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#11-lecture-11-geometry-2curves-and-surfaces">11. Lecture 11 Geometry 2(Curves and Surfaces)</a>
    <ul>
      <li><a href="#111-显式几何的表示方法">11.1. 显式几何的表示方法</a>
        <ul>
          <li><a href="#1111-point-cloud-点云">11.1.1. Point Cloud 点云</a></li>
          <li><a href="#1112-polygone-mesh">11.1.2. Polygone Mesh</a></li>
          <li><a href="#1113-一个例子">11.1.3. 一个例子</a></li>
        </ul>
      </li>
      <li><a href="#112-curves-曲线">11.2. Curves 曲线</a>
        <ul>
          <li><a href="#1121-贝塞尔曲线">11.2.1. 贝塞尔曲线</a></li>
          <li><a href="#1122-如何画一条贝塞尔曲线">11.2.2. 如何画一条贝塞尔曲线</a></li>
          <li><a href="#1123-piecewise-bézier-curves-逐段的贝塞尔曲线">11.2.3. Piecewise Bézier Curves 逐段的贝塞尔曲线</a></li>
          <li><a href="#1124-spline-样条">11.2.4. Spline 样条</a></li>
        </ul>
      </li>
      <li><a href="#113-曲面">11.3. 曲面</a>
        <ul>
          <li><a href="#1131-贝塞尔曲面">11.3.1. 贝塞尔曲面</a></li>
          <li><a href="#1132-曲面细分">11.3.2. 曲面细分</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#12-lecture-12-geometry-3">12. Lecture 12 Geometry 3</a>
    <ul>
      <li><a href="#121-mesh-subdivisionupsampling-网格细分">12.1. Mesh Subdivision(upsampling) 网格细分</a></li>
      <li><a href="#122-mesh-simplification-网格简化">12.2. Mesh Simplification 网格简化</a></li>
      <li><a href="#123-阴影-shadow-mapping">12.3. 阴影 Shadow mapping</a></li>
    </ul>
  </li>
  <li><a href="#13-lecture-13-ray-tracing-1">13. Lecture 13 Ray Tracing 1</a>
    <ul>
      <li><a href="#131-why-ray-tracing">13.1. Why ray tracing</a></li>
      <li><a href="#132-light-rays">13.2. Light Rays</a></li>
      <li><a href="#133-ray-casting-光线投射">13.3. Ray Casting 光线投射</a></li>
      <li><a href="#134-recursive-ray-tracing-递归光线追踪">13.4. Recursive Ray Tracing 递归光线追踪</a></li>
      <li><a href="#135-ray-surface-interaction-光线和表面相交">13.5. Ray-Surface interaction 光线和表面相交</a>
        <ul>
          <li><a href="#1351-ray-equation">13.5.1. Ray Equation</a></li>
          <li><a href="#1352-与圆相交的交点">13.5.2. 与圆相交的交点</a></li>
          <li><a href="#1353-intersection-with-implicit-surface">13.5.3. intersection with implicit surface</a></li>
          <li><a href="#1354-intersection-with-triangle-mesh">13.5.4. intersection with triangle mesh</a></li>
          <li><a href="#1355-accelerating-ray-surface-intersection">13.5.5. accelerating ray-surface intersection</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#14-lecture-14-ray-tracing-2">14. Lecture 14 Ray Tracing 2</a>
    <ul>
      <li><a href="#141-uniform-spatial-partitions-grids">14.1. Uniform Spatial Partitions (Grids)</a></li>
      <li><a href="#142-spatial-partitions-空间划分">14.2. Spatial Partitions 空间划分</a>
        <ul>
          <li><a href="#1421-一些划分示例">14.2.1. 一些划分示例</a></li>
          <li><a href="#1422-kd-tree">14.2.2. KD-Tree</a></li>
        </ul>
      </li>
      <li><a href="#143-object-partitions-物体划分">14.3. Object Partitions 物体划分</a>
        <ul>
          <li><a href="#1431-bounding-volume-hierarchybvh">14.3.1. Bounding Volume Hierarchy(BVH)</a></li>
          <li><a href="#1432-building-bvh">14.3.2. Building BVH</a></li>
          <li><a href="#1433-与空间划分的对比">14.3.3. 与空间划分的对比</a></li>
        </ul>
      </li>
      <li><a href="#144-whitted-style">14.4. Whitted style</a></li>
      <li><a href="#145-radiometry-辐射度量学">14.5. Radiometry 辐射度量学</a>
        <ul>
          <li><a href="#1451-一些物理量">14.5.1. 一些物理量</a></li>
          <li><a href="#1452-radiant-energy-and-flux">14.5.2. Radiant Energy and Flux</a></li>
          <li><a href="#1453-radiant-intensity">14.5.3. Radiant Intensity</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#15-lecture-15-ray-tracing">15. Lecture 15 Ray Tracing</a>
    <ul>
      <li><a href="#151-radiometry-cont-辐射度量学">15.1. Radiometry cont. 辐射度量学</a>
        <ul>
          <li><a href="#1511-继续上节课的内容">15.1.1. 继续上节课的内容</a></li>
          <li><a href="#1512-irradiance">15.1.2. Irradiance</a></li>
          <li><a href="#1513-radiance">15.1.3. Radiance</a></li>
        </ul>
      </li>
      <li><a href="#152-bidirectional-reflectance-distribution-function-brdf">15.2. Bidirectional Reflectance Distribution Function (BRDF)</a></li>
      <li><a href="#153-rendering-equation-渲染方程">15.3. Rendering Equation 渲染方程</a>
        <ul>
          <li><a href="#1531-如何理解渲染方程">15.3.1. 如何理解渲染方程</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#16-lecture-16-ray-tracing-4">16. Lecture 16 Ray Tracing 4</a>
    <ul>
      <li><a href="#161-monte-carlo-integration-蒙特卡洛积分">16.1. Monte Carlo Integration 蒙特卡洛积分</a></li>
      <li><a href="#162-path-tracing-路径追踪">16.2. Path Tracing 路径追踪</a>
        <ul>
          <li><a href="#1621-解渲染方程">16.2.1. 解渲染方程</a></li>
          <li><a href="#1622-最终的代码">16.2.2. 最终的代码</a></li>
        </ul>
      </li>
      <li><a href="#163-路径追踪">16.3. 路径追踪</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h1 id="1-lecture-01-overview-of-computer-graphics">1. Lecture 01 Overview of Computer Graphics</h1>
<h2 id="11-课程情况">1.1. 课程情况</h2>
<ul>
  <li>授课老师：闫令琪</li>
  <li>授课形式：网课（B站）</li>
</ul>

<h2 id="12-什么是好的画面">1.2. 什么是好的画面</h2>
<p>画面<strong>亮</strong></p>
<h2 id="13-应用场景">1.3. 应用场景</h2>
<p>电影，游戏，动画，设计，可视化，虚拟现实，增强现实，模拟，GUI图形用户接口。</p>

<p>电影中里程碑：阿凡达，大量应用面部捕捉技术。</p>
<h2 id="14-rasterization-光栅化">1.4. Rasterization 光栅化</h2>
<p>实时，FPS&gt;30</p>

<p>离线, FPS&lt;30</p>
<h2 id="15-计算机视觉">1.5. 计算机视觉</h2>
<p>计算机图形学离不开计算机视觉，但是视觉一般是对图像的处理。</p>

<h2 id="16-推荐书籍">1.6. 推荐书籍</h2>
<p>Tiger虎书</p>

<h1 id="2-lecture-02-review-of-linear-algebra">2. Lecture 02 Review of Linear Algebra</h1>
<h2 id="21-图形学依赖学科">2.1. 图形学依赖学科</h2>
<p>Optics, Mechanics, Linear algebra, statics, Singal processing, numerical analysis数值分析</p>

<h2 id="22-向量">2.2. 向量</h2>

<p>向量的定义</p>

<p><img src="../assets/img/posts/20211221/9.jpg" /></p>

<p>单位向量</p>

<p><img src="../assets/img/posts/20211221/10.jpg" /></p>

<p>向量计算，向量加法</p>

<p><img src="../assets/img/posts/20211221/11.jpg" /></p>

<p>用笛卡尔坐标系表示向量</p>

<p><img src="../assets/img/posts/20211221/12.jpg" /></p>

<p>向量乘法，点乘和叉乘，点乘在笛卡尔坐标系中就是对应元素相乘。</p>

<p>在图形学中，点乘是为了寻找两个向量的夹角(夹角可以判断两个向量方向的接近程度)，或者获得一个向量在另一个向量的投影，还可以获得向量的分解。</p>

<p><img src="../assets/img/posts/20211221/13.jpg" /></p>

<p>叉乘，叉积结果垂直于这两个向量所在的平面，满足右手定则。向量的叉乘可以写成矩阵形式。</p>

<p>在图形学中的应用：判断左右关系，比如a^b&gt;0，说明b在a的左边。还可以判断内外，比如判断一个点是否在一个三角形内。</p>

<p><img src="../assets/img/posts/20211221/14.jpg" /></p>

<p>坐标系的定义，右手坐标系</p>

<p><img src="../assets/img/posts/20211221/15.jpg" /></p>

<h2 id="23-矩阵">2.3. 矩阵</h2>

<p>矩阵定义</p>

<p><img src="../assets/img/posts/20211221/16.jpg" /></p>

<p>矩阵乘法</p>

<p><img src="../assets/img/posts/20211221/17.jpg" /></p>

<p>矩阵乘法没有交换律，但是有结合律</p>

<p>矩阵转置，矩阵的逆</p>

<p>向量的点乘和叉乘都可以写成矩阵乘法形式</p>

<p><img src="../assets/img/posts/20211221/18.jpg" /></p>

<h1 id="3-lecture-03-transformation">3. Lecture 03 Transformation</h1>

<h2 id="31-why-transformation-为什么要变换">3.1. why transformation 为什么要变换</h2>
<p>viewing: 3D to 2D projection</p>

<h2 id="32-d变换">3.2. D变换</h2>
<ul>
  <li>缩放 scale transform</li>
</ul>

<p><img src="../assets/img/posts/20211221/19.jpg" /></p>

<ul>
  <li>非均匀缩放 scale(non-uniform)</li>
</ul>

<p><img src="../assets/img/posts/20211221/20.jpg" /></p>

<ul>
  <li>翻转 reflection matrix</li>
</ul>

<p><img src="../assets/img/posts/20211221/21.jpg" /></p>

<ul>
  <li>切变 shear matrix</li>
</ul>

<p>竖直方向上没有变化，水平方向上发生了变化</p>

<p><img src="../assets/img/posts/20211221/22.jpg" /></p>

<ul>
  <li>旋转 Rotate</li>
</ul>

<p>旋转默认绕零点逆时针旋转</p>

<p><img src="../assets/img/posts/20211221/23.jpg" /></p>

<p>二维旋转矩阵R</p>

<p>上述所有的变化都可以写成x$\prime$=Mx，也就是线性变换</p>

<h2 id="33-齐次坐标-homogeneous-coordinate">3.3. 齐次坐标 homogeneous coordinate</h2>

<ul>
  <li>
    <p>为什么要引入齐次坐标，因为对于简单的平移操作并不能写成线性变换的形式，但是人们也不想认为平移是一种特殊的变换，所以引入齐次坐标</p>
  </li>
  <li>
    <p>齐次坐标</p>
  </li>
</ul>

<p>注意点和向量的表示方法不同</p>

<p><img src="../assets/img/posts/20211221/24.jpg" /></p>

<ul>
  <li>仿射变换 affine transformations</li>
</ul>

<p><img src="../assets/img/posts/20211221/25.jpg" /></p>

<ul>
  <li>2D Transformations</li>
</ul>

<p><img src="../assets/img/posts/20211221/26.jpg" /></p>

<ul>
  <li>
    <p>逆变换就是乘以逆矩阵</p>
  </li>
  <li>
    <p>复杂的变换都是简单的变换的组合，变换的组合顺序很重要</p>
  </li>
  <li>
    <p>绕着某一个点（非原点）旋转的分解</p>
  </li>
</ul>

<p><img src="../assets/img/posts/20211221/27.jpg" /></p>

<h1 id="4-lecture-04-transformation-cont">4. Lecture 04 Transformation Cont.</h1>

<h2 id="41-d-transformations">4.1. D Transformations</h2>

<ul>
  <li>齐次坐标</li>
</ul>

<p>对于w不等于1，每一个坐标除以w</p>

<p><img src="../assets/img/posts/20211221/28.jpg" /></p>

<ul>
  <li>正交矩阵</li>
</ul>

<p>一个矩阵的逆等于矩阵的转置，旋转矩阵就是一个正交矩阵</p>

<ul>
  <li>仿射变换（旋转+平移）</li>
</ul>

<p>仿射变换是先进行旋转再进行平移</p>

<p><img src="../assets/img/posts/20211221/29.jpg" /></p>

<ul>
  <li>矩阵表示（缩放，平移）</li>
</ul>

<p><img src="../assets/img/posts/20211221/30.jpg" /></p>

<ul>
  <li>旋转</li>
</ul>

<p>绕着某一个轴旋转</p>

<p><img src="../assets/img/posts/20211221/31.jpg" /></p>

<p>一般的旋转（分解成三个坐标轴的旋转）</p>

<p><img src="../assets/img/posts/20211221/32.jpg" /></p>

<p>Rodrigues’ Rotation Formula, 用向量n表示旋转轴，最终推出这个公式</p>

<p><img src="../assets/img/posts/20211221/33.jpg" /></p>

<h2 id="42-view-transformation-视图变换">4.2. view transformation 视图变换</h2>

<ul>
  <li>
    <p>观测变换viewing，包括了视图变化和投影变化</p>
  </li>
  <li>
    <p>MVP变换(model-&gt;view-&gt;projection)</p>
  </li>
</ul>

<p><img src="../assets/img/posts/20211221/34.jpg" /></p>

<ul>
  <li>view transformation(不等于viewing) 视图变换</li>
</ul>

<p>视图变换是把相机放到标准位置上，located at origin, look at -Z</p>

<p><img src="../assets/img/posts/20211221/35.jpg" /></p>

<p>利用逆变换，先平移再旋转</p>

<p><img src="../assets/img/posts/20211221/36.jpg" /></p>

<p>一般把model和view变换统称为view transformation</p>

<h2 id="43-projection-transformation-投影变换">4.3. projection transformation 投影变换</h2>
<ul>
  <li>orthographic vs perspectiive projection</li>
</ul>

<p><img src="../assets/img/posts/20211221/37.jpg" /></p>

<ul>
  <li>orthographic projection 正交投影</li>
</ul>

<p><img src="../assets/img/posts/20211221/38.jpg" /></p>

<p>平移，缩放（不考虑旋转）</p>

<p><img src="../assets/img/posts/20211221/39.jpg" /></p>

<ul>
  <li>perspective projection 透视投影</li>
</ul>

<p>满足近大远小</p>

<p>透视投影就是先把物体挤压成立方体，然后对立方体进行正交投影</p>

<p><img src="../assets/img/posts/20211221/41.jpg" /></p>

<p><img src="../assets/img/posts/20211221/40.jpg" /></p>

<p><img src="../assets/img/posts/20211221/42.jpg" /></p>

<p><img src="../assets/img/posts/20211221/43.jpg" /></p>

<h1 id="5-lecture05-rasterization-1triangles">5. Lecture05 Rasterization 1(Triangles)</h1>

<h2 id="51-perspective-projection-透视投影">5.1. Perspective Projection 透视投影</h2>
<ul>
  <li>首先是对上节课的透视投影的一些补充, 其中l=left, r=right, b=bottom, t=top, n=near, f=far，这些量可以描述视锥Frustum</li>
</ul>

<center><img src="../assets/img/posts/20211221/45.jpg" /></center>

<ul>
  <li>视锥Frustum的描述还可以用fovY(field of view)垂直视角和aspect ratio宽高比</li>
</ul>

<center><img src="../assets/img/posts/20211221/46.jpg" /></center>

<h2 id="52-canonical-cube-to-screen-光栅化">5.2. Canonical Cube to Screen 光栅化</h2>
<ul>
  <li>
    <p>把物体的数学描述以及与物体相关的颜色信息转换为屏幕上用于对应位置的像素及用于填充像素的颜色，这个过程称为光栅化。</p>
  </li>
  <li>
    <p>屏幕是最常见的光栅设备，每一个像素都是一个小方块，像素是最小的单位，一个像素的颜色可以用rgb三种颜色表示</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/47.jpg" /></center>

<ul>
  <li>屏幕空间screen space</li>
</ul>

<center><img src="../assets/img/posts/20211221/48.jpg" /></center>

<ul>
  <li>把之前投影后的小方块变成屏幕空间</li>
</ul>

<center><img src="../assets/img/posts/20211221/49.jpg" /></center>

<center><img src="../assets/img/posts/20211221/50.jpg" /></center>

<h2 id="53-different-raster-displays-不同的成像设备">5.3. Different Raster Displays 不同的成像设备</h2>
<ul>
  <li>
    <p>Oscilloscope 示波器</p>
  </li>
  <li>
    <p>Cathode Ray Tube 阴极射线管成像原理。早期电视屏幕就是这样实现成像，扫描成像。</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/51.jpg" /></center>

<ul>
  <li>Frame Buffer: Memory for a Raster Display 内存中的一块区域存储图像信息。</li>
</ul>

<center><img src="../assets/img/posts/20211221/52.jpg" /></center>

<ul>
  <li>LCD(liquid crystal display)液晶显示器，光的波动性原理。</li>
</ul>

<center><img src="../assets/img/posts/20211221/53.jpg" /></center>

<ul>
  <li>LED发光二极管</li>
</ul>

<center><img src="../assets/img/posts/20211221/54.jpg" /></center>

<h2 id="54-三角形光栅化">5.4. 三角形光栅化</h2>
<ul>
  <li>三角形是最基本的多边形，有很多好的性质。</li>
</ul>

<center><img src="../assets/img/posts/20211221/55.jpg" /></center>

<ul>
  <li>sampling 采样。三角形离散化。</li>
</ul>

<center><img src="../assets/img/posts/20211221/56.jpg" /></center>

<center><img src="../assets/img/posts/20211221/57.jpg" /></center>

<p>在不同的像素中心，确定是0还是1,表示在三角形里还是外</p>

<center><img src="../assets/img/posts/20211221/58.jpg" /></center>

<ul>
  <li>如何判断点和三角形关系，利用叉积，边界上的点自己定义。</li>
</ul>

<center><img src="../assets/img/posts/20211221/59.jpg" /></center>

<center><img src="../assets/img/posts/20211221/60.jpg" /></center>

<ul>
  <li>jaggies锯齿，走样aliasing</li>
</ul>

<center><img src="../assets/img/posts/20211221/61.jpg" /></center>

<center><img src="../assets/img/posts/20211221/62.jpg" /></center>

<h1 id="6-lecture-06-rasterization-2antialiasing-and-z-buffering">6. Lecture 06 Rasterization 2(Antialiasing and Z-Buffering)</h1>

<h2 id="61-sampling-采样原理">6.1. sampling 采样原理</h2>
<ul>
  <li>视频就是对时间进行采样</li>
  <li>采样的artifact(瑕疵)：锯齿，摩尔纹，轮胎效应(在时间上采样)</li>
</ul>

<center><img src="../assets/img/posts/20211221/63.jpg" /></center>

<ul>
  <li>反走样采样：可以对原始的图像进行滤波(模糊处理)然后再采样。</li>
</ul>

<center><img src="../assets/img/posts/20211221/64.jpg" /></center>

<ul>
  <li>采样速度跟不上信号变化的速度就会走样(aliasing)</li>
</ul>

<h2 id="62-frequency-domaine-信号处理频率">6.2. Frequency domaine 信号处理频率</h2>
<ul>
  <li>傅里叶变换：所有的周期函数都可以写成不同平吕的正弦函数的组合。傅里叶变换就是频域和时域/空间域的变换</li>
</ul>

<center><img src="../assets/img/posts/20211221/66.jpg" /></center>

<ul>
  <li>走样的原因(时域)：高频信号欠采样，高频信号和低频信号在某一采样速度下没有差别，就会产生走样</li>
</ul>

<center><img src="../assets/img/posts/20211221/65.jpg" /></center>

<center><img src="../assets/img/posts/20211221/67.jpg" /></center>

<ul>
  <li>
    <p>滤波：抹掉特定的频率。比如高通滤波(过滤到低频信号)</p>
  </li>
  <li>
    <p>卷积：图形学上的简化定义，见下图</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/68.jpg" /></center>

<ul>
  <li>卷积定律：时域上的卷积等于频域上的乘积</li>
</ul>

<center><img src="../assets/img/posts/20211221/69.jpg" /></center>

<ul>
  <li>采样：重复频域上的内容</li>
</ul>

<center><img src="../assets/img/posts/20211221/70.jpg" /></center>

<ul>
  <li>走样在频率上的解释：采样频率小会让频域上发生重叠</li>
</ul>

<center><img src="../assets/img/posts/20211221/71.jpg" /></center>

<h2 id="63-antialiasing-反走样抗锯齿">6.3. antialiasing 反走样/抗锯齿</h2>

<ul>
  <li>
    <p>第一种解决方法：增加采样率，相当于增加了频域上的两个信号的距离</p>
  </li>
  <li>
    <p>第二种解决方法：反走样。即先对信号进行滤波再采样</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/72.jpg" /></center>

<ul>
  <li>比如对于之前三角形的问题</li>
</ul>

<center><img src="../assets/img/posts/20211221/73.jpg" /></center>

<ul>
  <li>但是这种反走样的方法比较复杂，有一种更简单的近似方法(对滤波这一步的近似)：supersampling，就是在对每个像素点变成更多的小点</li>
</ul>

<center><img src="../assets/img/posts/20211221/74.jpg" /></center>

<h2 id="64-antialiasing-today-目前反走样的方法">6.4. antialiasing today 目前反走样的方法</h2>
<p>介绍了两种新的抗锯齿的操作：FXAA和TAA。FXAA的做法是把边界找到然后对边界进行处理。</p>

<center><img src="../assets/img/posts/20211221/75.jpg" /></center>

<h1 id="7-lecture-07-shadingillumination-shading-and-graphics-pipeline">7. Lecture 07 Shading(Illumination, Shading, and Graphics Pipeline)</h1>

<h2 id="71-painters-algorithm-画家算法">7.1. Painter’s Algorithm 画家算法</h2>
<ul>
  <li>首先画出远处的物体，然后再画近处的物体。画近处的物体再覆盖远处的物体。</li>
  <li>需要定义深度信息，根据深度信息排序</li>
</ul>

<h2 id="72-z-buffer-深度缓存">7.2. Z-buffer 深度缓存</h2>
<ul>
  <li>对每个像素都有最小的z值，除了一个frame buffer储存颜色信息外，还需要z-buffer储存深度信息。</li>
</ul>

<center><img src="../assets/img/posts/20211221/76.jpg" /></center>

<center><img src="../assets/img/posts/20211221/77.jpg" /></center>

<ul>
  <li>
    <p>假设每个像素最开始的时候深度为无限远</p>
  </li>
  <li>
    <p>特点是在像素维度进行操作</p>
  </li>
</ul>

<h2 id="73-目前为止学到了什么">7.3. 目前为止学到了什么</h2>

<center><img src="../assets/img/posts/20211221/78.jpg" /></center>

<h2 id="74-shading-着色">7.4. shading 着色</h2>
<ul>
  <li>
    <p>着色：对不同物体应用不同的材质</p>
  </li>
  <li>
    <p>一个简单的着色模型(Blinn-Phong Reflection model)</p>
  </li>
  <li>
    <p>局部着色，不考虑阴影</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/79.jpg" /></center>

<ul>
  <li>diffuse reflection 漫反射，一个物体有多亮与接收到多少光的能量有关。点光源的能量随距离缩减。在点光源的光线到达物体表面时被物体接受多少能量又与光线和法线的夹角的cos值有关，也就是说直射时接受的能量最大(相同距离)。漫反射表示不论观测角度在哪，你观测到的亮度应该是一样的。</li>
</ul>

<center><img src="../assets/img/posts/20211221/80.jpg" /></center>

<h1 id="8-shading-2shading-pipeline-texture-mapping">8. Shading 2(Shading, Pipeline, Texture Mapping)</h1>
<h2 id="81-specular-term-高光项">8.1. Specular Term 高光项</h2>
<ul>
  <li>着色包括三部分：漫反射，高光，环境光</li>
  <li>高光就是观测方向和镜面反射方向相同，即半程向量是否和法向量接近</li>
</ul>

<center><img src="../assets/img/posts/20211221/81.jpg" /></center>

<ul>
  <li>通常高光都是白色的</li>
</ul>

<h2 id="82-ambient-term-环境项">8.2. Ambient Term 环境项</h2>
<ul>
  <li>
    <p>环境光就是一些其他物体反射的光照亮背光物体</p>
  </li>
  <li>
    <p>这里介绍非常简化的模型</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/82.jpg" /></center>

<ul>
  <li>最终结果</li>
</ul>

<center><img src="../assets/img/posts/20211221/83.jpg" /></center>

<h2 id="83-shading-frequencies-着色频率">8.3. Shading Frequencies 着色频率</h2>
<ul>
  <li>
    <p>之前介绍的着色是应用在着色点，对应在屏幕空间是如何的呢？</p>
  </li>
  <li>
    <p>第一种：Shading ecah triangle 对每个三角形着色</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/84.jpg" /></center>

<ul>
  <li>第二种：shading each vertex 对顶点着色，然后插值</li>
</ul>

<center><img src="../assets/img/posts/20211221/85.jpg" /></center>

<ul>
  <li>第三种：shading each pixel 对每个像素点着色</li>
</ul>

<center><img src="../assets/img/posts/20211221/86.jpg" /></center>

<ul>
  <li>如何定义顶点的法向量呢？对周围的面的法向量求平均</li>
</ul>

<center><img src="../assets/img/posts/20211221/87.jpg" /></center>

<ul>
  <li>如何定义像素的法向量？</li>
</ul>

<center><img src="../assets/img/posts/20211221/88.jpg" /></center>

<h2 id="84-graphics-pipeline-图像管线实时渲染管线">8.4. Graphics Pipeline 图像管线/实时渲染管线</h2>
<ul>
  <li>一个实时渲染的流程/流水线</li>
</ul>

<center><img src="../assets/img/posts/20211221/89.jpg" /></center>

<ul>
  <li>现代的GPU允许写入顶点着色部分与片段着色部分的代码</li>
</ul>

<h2 id="85-texture-mapping-纹理映射">8.5. Texture Mapping 纹理映射</h2>
<ul>
  <li>
    <p>希望在物体的不同位置定义不同的属性，比如漫反射系数等等</p>
  </li>
  <li>
    <p>3维物体的表现都是一个平面</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/90.jpg" /></center>

<ul>
  <li>纹理映射就是对于一个平面定义不同的属性，有一个映射关系</li>
</ul>

<center><img src="../assets/img/posts/20211221/91.jpg" /></center>

<ul>
  <li>纹理也有坐标系</li>
</ul>

<center><img src="../assets/img/posts/20211221/92.jpg" /></center>

<h1 id="9-lecture-09-shading-3-texture-mapping">9. Lecture 09 Shading 3 (Texture Mapping)</h1>

<h2 id="91-barycentric-coordinates重心坐标系">9.1. Barycentric Coordinates重心坐标系</h2>

<center><img src="../assets/img/posts/20211221/93.jpg" /></center>

<h2 id="92-interpolate-插值">9.2. Interpolate 插值</h2>
<ul>
  <li>重心坐标系插值</li>
</ul>

<center><img src="../assets/img/posts/20211221/94.jpg" /></center>

<h2 id="93-simple-texture-mapping-简单的纹理映射模型">9.3. Simple Texture Mapping 简单的纹理映射模型</h2>

<center><img src="../assets/img/posts/20211221/95.jpg" /></center>

<h2 id="94-texture-magnification-纹理放大">9.4. Texture Magnification 纹理放大</h2>

<center><img src="../assets/img/posts/20211221/96.jpg" /></center>

<h2 id="95-point-sampling-textures">9.5. Point Sampling Textures</h2>
<ul>
  <li>就是走样问题</li>
</ul>

<center><img src="../assets/img/posts/20211221/97.jpg" /></center>

<h2 id="96-mipmap-范围查询">9.6. Mipmap 范围查询</h2>
<ul>
  <li>生成不同分辨率的图片</li>
</ul>

<center><img src="../assets/img/posts/20211221/98.jpg" /></center>

<ul>
  <li>
    <p>任何一个像素可以映射到纹理区域的一个点，mipmap可以让像素点快速查阅，因为他又很多层，不同的纹理区域的面积对应不同的层</p>
  </li>
  <li>
    <p>mipmap也不是最好的方法，只是一种折中的办法</p>
  </li>
  <li>
    <p>anisotropic filtering 各向异性过滤</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/99.jpg" /></center>

<h1 id="10-lecture-10-geomrtry-1introduction">10. Lecture 10 Geomrtry 1(introduction)</h1>
<h2 id="101-纹理的应用">10.1. 纹理的应用</h2>
<h3 id="1011-environment-map-环境光映射">10.1.1. Environment Map 环境光映射</h3>
<ul>
  <li>纹理可以用来映射环境光</li>
</ul>

<center><img src="../assets/img/posts/20211221/100.jpg" /></center>

<ul>
  <li>假设环境光来自无限远</li>
</ul>

<h3 id="1012-spherical-environment-map-球形环境光映射">10.1.2. Spherical Environment Map 球形环境光映射</h3>
<ul>
  <li>将环境光信息存在球上</li>
</ul>

<center><img src="../assets/img/posts/20211221/101.jpg" /></center>

<ul>
  <li>但是在边缘部分会有扭曲，解决方法有环境光存在正方体上</li>
</ul>

<h3 id="1013-纹理凹凸贴图bump-mapping">10.1.3. 纹理凹凸贴图bump mapping</h3>
<ul>
  <li>
    <p>纹理不仅可以表示颜色，还可以应用一个复杂的纹理来定义高度，也就改变了法线的方向</p>
  </li>
  <li>
    <p>凹凸贴图只增加表面细节，不添加新的三角形</p>
  </li>
</ul>

<h3 id="1014-位移贴图-displacement-mapping">10.1.4. 位移贴图 displacement mapping</h3>
<ul>
  <li>和凹凸贴图很像，但是移动了顶点</li>
</ul>

<center><img src="../assets/img/posts/20211221/102.jpg" /></center>

<h3 id="1015-三维纹理">10.1.5. 三维纹理</h3>
<ul>
  <li>
    <p>定义了空间中任意一个点的纹理坐标</p>
  </li>
  <li>
    <p>广泛应用于体积渲染</p>
  </li>
</ul>

<h2 id="102-几何">10.2. 几何</h2>
<h3 id="1021-分类">10.2.1. 分类</h3>
<ul>
  <li>隐式几何</li>
  <li>显式几何</li>
</ul>

<h3 id="1022-隐式几何">10.2.2. 隐式几何</h3>
<ul>
  <li>不给出点的具体坐标，而是给出点的坐标关系，比如$x^2+y^2+z^2=1$</li>
  <li>推广到一般形式, $f(x,y,z)=0$</li>
  <li>缺点：不直观，不好采样</li>
  <li>优点：可以很容易的判断点在不在几何体内</li>
</ul>

<h3 id="1023-显式几何">10.2.3. 显式几何</h3>
<ul>
  <li>直接给出或者参数映射的方式给出</li>
</ul>

<center><img src="../assets/img/posts/20211221/103.jpg" /></center>

<ul>
  <li>优点：采样方便，直观</li>
  <li>缺点：不好判断点是否在几何体内还是外</li>
</ul>

<h3 id="1024-隐式的表达方式">10.2.4. 隐式的表达方式</h3>
<ul>
  <li>公式定义</li>
</ul>

<center><img src="../assets/img/posts/20211221/104.jpg" /></center>

<ul>
  <li>通过几何体的布尔组合，目前有很多建模软件就是这么表示的</li>
</ul>

<center><img src="../assets/img/posts/20211221/105.jpg" /></center>

<ul>
  <li>距离函数定义，SDF有向距离场</li>
</ul>

<center><img src="../assets/img/posts/20211221/106.jpg" /></center>

<h1 id="11-lecture-11-geometry-2curves-and-surfaces">11. Lecture 11 Geometry 2(Curves and Surfaces)</h1>
<h2 id="111-显式几何的表示方法">11.1. 显式几何的表示方法</h2>

<h3 id="1111-point-cloud-点云">11.1.1. Point Cloud 点云</h3>
<ul>
  <li>点的集合</li>
  <li>优点：可以表示任何几何体</li>
</ul>

<h3 id="1112-polygone-mesh">11.1.2. Polygone Mesh</h3>
<ul>
  <li>使用顶点和图形表示(三角形，正方形)</li>
</ul>

<h3 id="1113-一个例子">11.1.3. 一个例子</h3>

<center><img src="../assets/img/posts/20211221/107.jpg" /></center>

<p>里面定义了顶点坐标，法线，纹理坐标和哪几个点组成一个三角形</p>

<h2 id="112-curves-曲线">11.2. Curves 曲线</h2>
<h3 id="1121-贝塞尔曲线">11.2.1. 贝塞尔曲线</h3>
<ul>
  <li>用一系列控制点定义曲线</li>
</ul>

<center><img src="../assets/img/posts/20211221/108.jpg" /></center>

<ul>
  <li>曲线不一定要经过控制点</li>
</ul>

<h3 id="1122-如何画一条贝塞尔曲线">11.2.2. 如何画一条贝塞尔曲线</h3>
<ul>
  <li>Casteljau Algorithm：这个算法的核心是画出每个时间t的点的位置(递归)</li>
</ul>

<center><img src="../assets/img/posts/20211221/109.jpg" /></center>

<p>其中$b_0^2$就是时间t的点的位置</p>

<ul>
  <li>大致流程</li>
</ul>

<center><img src="../assets/img/posts/20211221/110.jpg" /></center>

<ul>
  <li>代数形式</li>
</ul>

<center><img src="../assets/img/posts/20211221/111.jpg" /></center>

<ul>
  <li>生成的曲线只能在控制点的凸包内</li>
</ul>

<h3 id="1123-piecewise-bézier-curves-逐段的贝塞尔曲线">11.2.3. Piecewise Bézier Curves 逐段的贝塞尔曲线</h3>

<ul>
  <li>每四个控制点定义一条贝塞尔曲线</li>
</ul>

<center><img src="../assets/img/posts/20211221/112.jpg" /></center>

<ul>
  <li>C0连续(点连续)，C1连续(切线连续)</li>
</ul>

<h3 id="1124-spline-样条">11.2.4. Spline 样条</h3>
<ul>
  <li>样条是用一系列的点画出线条</li>
</ul>

<center><img src="../assets/img/posts/20211221/113.jpg" /></center>

<h2 id="113-曲面">11.3. 曲面</h2>
<h3 id="1131-贝塞尔曲面">11.3.1. 贝塞尔曲面</h3>
<ul>
  <li>使用贝塞尔曲线生成贝塞尔曲面</li>
</ul>

<center><img src="../assets/img/posts/20211221/114.jpg" /></center>

<ul>
  <li>竖直方向生成四条曲线，然后对于t来说四个点再作为控制前生成曲线</li>
</ul>

<center><img src="../assets/img/posts/20211221/115.jpg" /></center>

<h3 id="1132-曲面细分">11.3.2. 曲面细分</h3>
<ul>
  <li>使用很多三角形网格来表示曲面</li>
</ul>

<center><img src="../assets/img/posts/20211221/116.jpg" /></center>

<h1 id="12-lecture-12-geometry-3">12. Lecture 12 Geometry 3</h1>
<h2 id="121-mesh-subdivisionupsampling-网格细分">12.1. Mesh Subdivision(upsampling) 网格细分</h2>
<ul>
  <li>引入更多三角形，微调它们的位置</li>
  <li>Loop Subdivision：第一步增加三角形的数量，第二部调整三角形的位置</li>
</ul>

<center><img src="../assets/img/posts/20211221/117.jpg" /></center>

<ul>
  <li>Loop细分规则：</li>
</ul>

<center><img src="../assets/img/posts/20211221/118.jpg" /></center>

<ul>
  <li>另一种细分规则：Catmull-Clark Subdivision</li>
</ul>

<p>奇异点是这个点的度不是4的点(就是连接的边数不等于4)</p>

<center><img src="../assets/img/posts/20211221/119.jpg" /></center>

<center><img src="../assets/img/posts/20211221/120.jpg" /></center>

<ul>
  <li>这种细分方法可以用于任何面</li>
</ul>

<h2 id="122-mesh-simplification-网格简化">12.2. Mesh Simplification 网格简化</h2>
<ul>
  <li>
    <p>基本思路是为了减少网格数目但是保持它的基本形状</p>
  </li>
  <li>
    <p>一种方法：Collapsing an edge 边坍缩。删除一些点</p>
  </li>
  <li>
    <p>判断标准：quadric error metrics 二次误差度量</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/121.jpg" /></center>

<ul>
  <li>实际效果</li>
</ul>

<center><img src="../assets/img/posts/20211221/122.jpg" /></center>

<h2 id="123-阴影-shadow-mapping">12.3. 阴影 Shadow mapping</h2>
<ul>
  <li>
    <p>光栅化着色的时候是局部的，但是有时候会有问题，比如有东西挡在shading point和光源之间时，所以需要在这种情况下生成阴影</p>
  </li>
  <li>
    <p>光栅化生成阴影的方法叫做shadow mapping</p>
  </li>
  <li>
    <p>shadow mapping 的两步</p>
  </li>
  <li>
    <p>第一步：从光源出发，看向shading point，记录能看见的点的深度</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/123.jpg" /></center>

<ul>
  <li>第二步：从摄像机出发，看向shading point，如果看见的点的深度和光源所看见的深度相同，那么这个点不在阴影内，否则，它在阴影内。</li>
</ul>

<center><img src="../assets/img/posts/20211221/124.jpg" /></center>

<ul>
  <li>具体的例子：</li>
</ul>

<center><img src="../assets/img/posts/20211221/125.jpg" /></center>

<ul>
  <li>问题：走样，阴影分辨率，只能做硬阴影(hard shadow)…</li>
</ul>

<center><img src="../assets/img/posts/20211221/126.jpg" /></center>

<h1 id="13-lecture-13-ray-tracing-1">13. Lecture 13 Ray Tracing 1</h1>
<h2 id="131-why-ray-tracing">13.1. Why ray tracing</h2>
<ul>
  <li>光栅化的缺点：无法表示全局的光照、毛玻璃效果无法很好表示、阴影处理不算好</li>
  <li>光纤追踪很精准但是比较慢，经常做离线(电影制作)</li>
</ul>

<h2 id="132-light-rays">13.2. Light Rays</h2>
<ul>
  <li>光线沿直线传播</li>
  <li>光线不会交叉</li>
  <li>光线是不断折回然后打到人眼</li>
  <li>光路可逆性</li>
</ul>

<h2 id="133-ray-casting-光线投射">13.3. Ray Casting 光线投射</h2>
<ul>
  <li>从眼睛到像素点出发，到虚拟世界，再到光源(Local)</li>
</ul>

<center><img src="../assets/img/posts/20211221/127.jpg" /></center>

<ul>
  <li>从眼睛到像素点到虚拟世界的线叫做eye ray</li>
</ul>

<h2 id="134-recursive-ray-tracing-递归光线追踪">13.4. Recursive Ray Tracing 递归光线追踪</h2>
<ul>
  <li>如果在shading point 处可以折射，能量损失，则继续折射然后对每个点都算着色值</li>
</ul>

<center><img src="../assets/img/posts/20211221/128.jpg" /></center>

<ul>
  <li>对每个点都要计算是否处在阴影中</li>
</ul>

<h2 id="135-ray-surface-interaction-光线和表面相交">13.5. Ray-Surface interaction 光线和表面相交</h2>
<h3 id="1351-ray-equation">13.5.1. Ray Equation</h3>

<center><img src="../assets/img/posts/20211221/129.jpg" /></center>

<h3 id="1352-与圆相交的交点">13.5.2. 与圆相交的交点</h3>

<center><img src="../assets/img/posts/20211221/130.jpg" /></center>

<ul>
  <li>一个交点就是相切，两个交点就是相交</li>
</ul>

<h3 id="1353-intersection-with-implicit-surface">13.5.3. intersection with implicit surface</h3>
<ul>
  <li>与隐式表面相交</li>
</ul>

<center><img src="../assets/img/posts/20211221/131.jpg" /></center>

<h3 id="1354-intersection-with-triangle-mesh">13.5.4. intersection with triangle mesh</h3>
<ul>
  <li>
    <p>也就是与显式表面(三角形网格)相交</p>
  </li>
  <li>
    <p>第一种想法就是光线与每个三角形进行计算，但这样计算量太大</p>
  </li>
  <li>
    <p>第二种想法是光线与三角形所在的平面相交，然后判断交点是不是在三角形内</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/132.jpg" /></center>

<ul>
  <li>如何定义平面？一个点+法线</li>
</ul>

<center><img src="../assets/img/posts/20211221/133.jpg" /></center>

<ul>
  <li>
    <p>然后将光线方程带入平面方程中，就可以得出光线与平面的交点</p>
  </li>
  <li>
    <p>如何简化判断交点与三角形的位置关系？MT算法：</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/134.jpg" /></center>

<p>这个算法的核心就是利用重心坐标系：解出重心坐标后，如果它们都为正，那么点在三角形内</p>

<h3 id="1355-accelerating-ray-surface-intersection">13.5.5. accelerating ray-surface intersection</h3>
<ul>
  <li>
    <p>加速交点(一般指与三角形网格的交点)计算过程</p>
  </li>
  <li>
    <p>bounding volume 包围盒</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/135.jpg" /></center>

<p>引入包围盒的思路是：如果光线与包围盒都不相交，那么肯定不会与里面的几何体有交点</p>

<ul>
  <li>包围盒由三个对面的交集</li>
</ul>

<center><img src="../assets/img/posts/20211221/136.jpg" /></center>

<p>轴对齐包围盒(就是对面与坐标轴平行)axis-aligned bounding box</p>

<ul>
  <li>先考虑二维的情况Ray intersection with aabb</li>
</ul>

<center><img src="../assets/img/posts/20211221/137.jpg" /></center>

<p>找到最大的时间和最小的时间</p>

<ul>
  <li>三维：对于三组对面，计算$t_{min}$和$t_{max}$，然后找到$t_{enter}$和$t_{exit}$。那么我们就知道了进入的时间和出去的时间，如果进去的时间小于出去的时间，那么光线进入了aabb，表示光线在盒子里呆过一段时间</li>
</ul>

<center><img src="../assets/img/posts/20211221/138.jpg" /></center>

<ul>
  <li>还要要保证进入的时间和出去的时间都要大于0</li>
</ul>

<h1 id="14-lecture-14-ray-tracing-2">14. Lecture 14 Ray Tracing 2</h1>
<h2 id="141-uniform-spatial-partitions-grids">14.1. Uniform Spatial Partitions (Grids)</h2>
<ul>
  <li>继续上节课的加速计算话题</li>
  <li>一种加速方法：生成grid</li>
</ul>

<center><img src="../assets/img/posts/20211221/139.jpg" /></center>

<p>找到aabb后，创建网格，存储aabb内几何体</p>

<ul>
  <li>然后光线沿着这些小格子相交</li>
</ul>

<center><img src="../assets/img/posts/20211221/140.jpg" /></center>

<h2 id="142-spatial-partitions-空间划分">14.2. Spatial Partitions 空间划分</h2>
<h3 id="1421-一些划分示例">14.2.1. 一些划分示例</h3>

<center><img src="../assets/img/posts/20211221/141.jpg" /></center>

<p>八叉树Oct-Tree，KD-Tree，BSP-Tree</p>

<h3 id="1422-kd-tree">14.2.2. KD-Tree</h3>

<center><img src="../assets/img/posts/20211221/142.jpg" /></center>

<ul>
  <li>
    <p>每次划分都沿着坐标轴移动，对于中间的结点都有子节点，只存储叶子结点的数据</p>
  </li>
  <li>
    <p>缺点：一个物体可能存在在多个叶子节点里</p>
  </li>
</ul>

<h2 id="143-object-partitions-物体划分">14.3. Object Partitions 物体划分</h2>
<h3 id="1431-bounding-volume-hierarchybvh">14.3.1. Bounding Volume Hierarchy(BVH)</h3>
<ul>
  <li>这种方法是目前图形学中使用较多的方法</li>
</ul>

<center><img src="../assets/img/posts/20211221/143.jpg" /></center>

<ul>
  <li>
    <p>沿着物体不断细分出bbox</p>
  </li>
  <li>
    <p>bvh的缺点：两部分bbox可能相交</p>
  </li>
</ul>

<h3 id="1432-building-bvh">14.3.2. Building BVH</h3>
<ul>
  <li>如何划分结点？选择一个维度进行划分，每次找最长的结点进行细分，细分的结点在中位数，当结点处图形较少，则停止</li>
</ul>

<center><img src="../assets/img/posts/20211221/144.jpg" /></center>

<h3 id="1433-与空间划分的对比">14.3.3. 与空间划分的对比</h3>

<center><img src="../assets/img/posts/20211221/145.jpg" /></center>

<h2 id="144-whitted-style">14.4. Whitted style</h2>
<ul>
  <li>到目前为止，已经讲了国内光线追踪会讲的内容。也就是讲完了Whitted style光线追踪</li>
</ul>

<h2 id="145-radiometry-辐射度量学">14.5. Radiometry 辐射度量学</h2>
<h3 id="1451-一些物理量">14.5.1. 一些物理量</h3>
<ul>
  <li>new terms: radiant flux, intensity, irradiance, radiance</li>
</ul>

<h3 id="1452-radiant-energy-and-flux">14.5.2. Radiant Energy and Flux</h3>
<ul>
  <li>randiant flux就是单位时间能量/功率</li>
</ul>

<center><img src="../assets/img/posts/20211221/146.jpg" /></center>

<h3 id="1453-radiant-intensity">14.5.3. Radiant Intensity</h3>
<ul>
  <li>辐射强度就是单位立体角(solid angle)的功率</li>
</ul>

<center><img src="../assets/img/posts/20211221/147.jpg" /></center>

<ul>
  <li>那么立体角是什么呢？立体角就是二维空间的角在三维空间的沿伸，就是球面面积除以半径的平方</li>
</ul>

<center><img src="../assets/img/posts/20211221/148.jpg" /></center>

<h1 id="15-lecture-15-ray-tracing">15. Lecture 15 Ray Tracing</h1>
<h2 id="151-radiometry-cont-辐射度量学">15.1. Radiometry cont. 辐射度量学</h2>
<h3 id="1511-继续上节课的内容">15.1.1. 继续上节课的内容</h3>

<ul>
  <li>微分立体角，就是球坐标系上对$\theta$和$\phi$的微分</li>
</ul>

<center><img src="../assets/img/posts/20211221/149.jpg" /></center>

<h3 id="1512-irradiance">15.1.2. Irradiance</h3>
<ul>
  <li>单位面积的功率</li>
</ul>

<center><img src="../assets/img/posts/20211221/150.jpg" /></center>

<ul>
  <li>面积是投影的面积</li>
</ul>

<h3 id="1513-radiance">15.1.3. Radiance</h3>
<ul>
  <li>randiance就是单位投影面积单位立体角的功率</li>
</ul>

<center><img src="../assets/img/posts/20211221/151.jpg" /></center>

<ul>
  <li>irradiance和radiance的区别：irradiance是某一个面积上接受的能量，而radiance是某一个面积某一个角度上接受的能量</li>
</ul>

<center><img src="../assets/img/posts/20211221/152.jpg" /></center>

<h2 id="152-bidirectional-reflectance-distribution-function-brdf">15.2. Bidirectional Reflectance Distribution Function (BRDF)</h2>
<ul>
  <li>
    <p>双向反射分布方程BRDF是描述光线传播的方程</p>
  </li>
  <li>
    <p>某一个方向$\omega_i$的光线打到某一个表面然后被吸收同时从另一个方向$\omega_r$反射出去</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/153.jpg" /></center>

<ul>
  <li>反射方程</li>
</ul>

<center><img src="../assets/img/posts/20211221/154.jpg" /></center>

<ul>
  <li>
    <p>观察某一个物体的反射光线不止从光源有光线，还有其他物体反射的光</p>
  </li>
  <li>
    <p>渲染方程Rendering Equation</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/155.jpg" /></center>

<ul>
  <li>渲染方程两部分组成，一部分是自身发光，另一部分是接受的光线的反射光线(半球上每个方向)</li>
</ul>

<h2 id="153-rendering-equation-渲染方程">15.3. Rendering Equation 渲染方程</h2>

<h3 id="1531-如何理解渲染方程">15.3.1. 如何理解渲染方程</h3>
<ul>
  <li>
    <p>反射的光线由两个个部分组成：自身的emission和从各个方向的反射光</p>
  </li>
  <li>
    <p>如何考虑物体反射的光？把物体看作一个光源，也就是看作一个递归的过程</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/156.jpg" /></center>

<ul>
  <li>通过数学式子简化渲染方程：</li>
</ul>

<center><img src="../assets/img/posts/20211221/157.jpg" /></center>

<ul>
  <li>然后通过逆矩阵可以解出L</li>
</ul>

<center><img src="../assets/img/posts/20211221/158.jpg" /></center>

<ul>
  <li>
    <p>光线弹射一次叫做直接光照、弹射两次及以上叫做间接光照</p>
  </li>
  <li>
    <p>那么就可以发现与光栅化的区别</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/159.jpg" /></center>

<ul>
  <li>在多次弹射后场景会趋于一个固定的亮度</li>
</ul>

<h1 id="16-lecture-16-ray-tracing-4">16. Lecture 16 Ray Tracing 4</h1>
<h2 id="161-monte-carlo-integration-蒙特卡洛积分">16.1. Monte Carlo Integration 蒙特卡洛积分</h2>
<ul>
  <li>有些函数不太好用解析式写出来</li>
  <li>蒙特卡洛积分就是数值积分的方法</li>
</ul>

<center><img src="../assets/img/posts/20211221/160.jpg" /></center>

<ul>
  <li>就是采样值除以采样密度</li>
</ul>

<center><img src="../assets/img/posts/20211221/161.jpg" /></center>

<h2 id="162-path-tracing-路径追踪">16.2. Path Tracing 路径追踪</h2>
<ul>
  <li>与whitted sytle的区别：whitted sytle没有考虑全局光照</li>
</ul>

<h3 id="1621-解渲染方程">16.2.1. 解渲染方程</h3>
<ul>
  <li>考虑一个简单的模型，只有直接光照</li>
</ul>

<center><img src="../assets/img/posts/20211221/162.jpg" /></center>

<ul>
  <li>每一个$\omega_i$都看作采样，那么可以应用蒙特卡洛积分</li>
</ul>

<center><img src="../assets/img/posts/20211221/163.jpg" /></center>

<ul>
  <li>应用全局光照，将物体反射面也看做光源，做一个递归</li>
</ul>

<center><img src="../assets/img/posts/20211221/164.jpg" /></center>

<ul>
  <li>
    <p>但是这样会出现一个问题，那就是爆炸，如果我取多个X，那么弹射很多次后就会爆炸</p>
  </li>
  <li>
    <p>解决方法，对每个点只取一个方向，也就是N=1，所以它叫做路径追踪</p>
  </li>
  <li>
    <p>这样噪声会比较大，但是从每个像素点有多个路径，所以还是可以接受</p>
  </li>
  <li>
    <p>第二个问题是递归不会停止？解决方法：俄罗斯轮盘赌，即在某一个程度停止递归</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/165.jpg" /></center>

<ul>
  <li>那么我们可以设定一个概率P来决定每个点是否打出一条光线，同时保证期望不变</li>
</ul>

<center><img src="../assets/img/posts/20211221/166.jpg" /></center>

<ul>
  <li>
    <p>到目前为止已经是一个正确的path tracing的渲染方法，但是这样效率比较低</p>
  </li>
  <li>
    <p>效率低的原因：每个点打到或者打不到光源是随机的，也就是说浪费了很多光线</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/167.jpg" /></center>

<ul>
  <li>可以在光源上采样，这样没有光线会浪费，渲染方程就需要写成在光源上采样</li>
</ul>

<center><img src="../assets/img/posts/20211221/168.jpg" /></center>

<ul>
  <li>那么我们就可以将渲染方程分为两部分，一部分是光源直接光照，方法使用上面提到的在光源上采样，另一部分是间接光照，保持不变</li>
</ul>

<h3 id="1622-最终的代码">16.2.2. 最终的代码</h3>

<center><img src="../assets/img/posts/20211221/169.jpg" /></center>

<ul>
  <li>但还有一个小问题，就是中间有物体遮挡，需要添加一个判断</li>
</ul>

<center><img src="../assets/img/posts/20211221/170.jpg" /></center>

<h2 id="163-路径追踪">16.3. 路径追踪</h2>
<ul>
  <li>在之前，ray tracing主要指whitted-style ray tracing</li>
  <li>但现在，只要设计了光线传播方法，就是ray tracing，路径追踪只是其中的一个方法</li>
</ul>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[Games 101 introduction to computer graphics 课程笔记]]></summary></entry><entry><title type="html">推荐系统</title><link href="http://localhost:4000/Recommender_system.html" rel="alternate" type="text/html" title="推荐系统" /><published>2021-12-16T00:00:00+08:00</published><updated>2021-12-16T00:00:00+08:00</updated><id>http://localhost:4000/Recommender_system</id><content type="html" xml:base="http://localhost:4000/Recommender_system.html"><![CDATA[<!-- TOC -->

<ul>
  <li><a href="#1-推荐系统总览">1. 推荐系统总览</a>
    <ul>
      <li><a href="#11-协同过滤-collaborative-filtering">1.1. 协同过滤 Collaborative Filtering</a></li>
      <li><a href="#12-显式反馈和隐式反馈">1.2. 显式反馈和隐式反馈</a></li>
      <li><a href="#13-推荐任务">1.3. 推荐任务</a></li>
    </ul>
  </li>
  <li><a href="#2-矩阵分解-matrix-factorization">2. 矩阵分解 Matrix Factorization</a></li>
  <li><a href="#3-autorec">3. AutoRec</a>
    <ul>
      <li><a href="#31-overview">3.1. overview</a></li>
      <li><a href="#32-formula">3.2. formula</a></li>
    </ul>
  </li>
  <li><a href="#4-personalized-ranking-for-recommender-system">4. Personalized Ranking for Recommender System</a>
    <ul>
      <li><a href="#41-overview">4.1. overview</a></li>
      <li><a href="#42-bayesian-personalized-ranking-loss-贝叶斯损失">4.2. Bayesian Personalized Ranking loss 贝叶斯损失</a></li>
      <li><a href="#43-hinge-loss">4.3. Hinge Loss</a></li>
    </ul>
  </li>
  <li><a href="#5-neural-collaborative-filtering-for-personalized-ranking-使用协同过滤网络个性化排序">5. Neural Collaborative Filtering for Personalized Ranking 使用协同过滤网络个性化排序</a>
    <ul>
      <li><a href="#51-the-neumf-model">5.1. The NeuMF model</a></li>
      <li><a href="#52-evaluator">5.2. Evaluator</a></li>
      <li><a href="#53-代码">5.3. 代码</a></li>
    </ul>
  </li>
  <li><a href="#6-sequence-aware-recommender-systems">6. Sequence-Aware Recommender Systems</a>
    <ul>
      <li><a href="#61-model-architectures">6.1. Model Architectures</a></li>
      <li><a href="#62-negative-sampling-负采样">6.2. Negative Sampling 负采样</a></li>
    </ul>
  </li>
  <li><a href="#7-feature-rich-recommender-systems">7. Feature-Rich Recommender Systems</a></li>
  <li><a href="#8-factorization-machines-因子分解机">8. Factorization Machines 因子分解机</a>
    <ul>
      <li><a href="#81-2-way-factorization-machines">8.1. 2-Way Factorization Machines</a></li>
      <li><a href="#82-an-efficient-optimization-citerion">8.2. An Efficient Optimization Citerion</a></li>
    </ul>
  </li>
  <li><a href="#9-deep-factorization-machines-深度因子分解机deeofm">9. Deep Factorization Machines 深度因子分解机DeeoFM</a>
    <ul>
      <li><a href="#91-model-architectures-模型架构">9.1. Model Architectures 模型架构</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h2 id="1-推荐系统总览">1. 推荐系统总览</h2>
<h3 id="11-协同过滤-collaborative-filtering">1.1. 协同过滤 Collaborative Filtering</h3>
<p>协同过滤最早出现在1992年Tapestry system，“人们相互协作，相互帮助，执行过滤程序，以处理大量的电子邮件和张贴到新闻组的信息。”现在协同过滤的概念更加广泛，从广义上讲，它是利用涉及多个用户、代理和数据源之间协作的技术来过滤<strong>信息或模式</strong>的过程。</p>

<p>协同过滤模型可以分为:1.memory-based CF; 2.model-based CF. 其中Memory-based CF又可以分为item-based和user-based CF。model-based CF有矩阵分解模型。</p>

<p>总的来说，协同过滤就是利用用户-物品的数据来预测和推荐。</p>

<h3 id="12-显式反馈和隐式反馈">1.2. 显式反馈和隐式反馈</h3>
<p>为了学习用户的偏好，系统需要收集用户的反馈feedback。反馈可以分为显式和隐式。</p>

<p>显式反馈就是需要用户主动提供兴趣偏好。比如点赞、点踩。</p>

<p>隐式反馈则是间接反映用户的喜好，比如购物历史记录，浏览记录，观看记录甚至是鼠标移动。</p>

<h3 id="13-推荐任务">1.3. 推荐任务</h3>
<p>电影推荐、新闻推荐、评分预测rating prediction task、top-n reommendation。如果使用了时间戳信息，那么我们构建了sequence-aware recommendation。针对新用户推荐新物品称为cold-start recommendation冷启动推荐。</p>

<h2 id="2-矩阵分解-matrix-factorization">2. 矩阵分解 Matrix Factorization</h2>
<p>The Matrix Factorization Model矩阵分解模型</p>

<p>R是user-item矩阵，行数是用户数量，列数是物品数量,那么R∈R<sup>mxn</sup>。P是user latent matrix，P∈R<sup>mxk</sup>，Q是item latent matrix，Q∈R<sup>nxk</sup></p>

<p>矩阵分解就是把R分解成P和Q，那么预测的评分就是：</p>

<p><img src="../assets/img/posts/20211216/2.jpg" /></p>

<p>但是上面这个式子没有考虑偏置，我们会有下面这个完整的式子：</p>

<p><img src="../assets/img/posts/20211216/3.jpg" /></p>

<p>那么<strong>目标函数</strong>可以定义为：</p>

<p><img src="../assets/img/posts/20211216/4.jpg" /></p>

<p>右边那一串是正则项，为了避免过拟合</p>

<p>下面这张图值观的展示了矩阵分解过程：</p>

<p><img src="../assets/img/posts/20211216/5.jpg" /></p>

<h2 id="3-autorec">3. AutoRec</h2>
<h3 id="31-overview">3.1. overview</h3>
<p>使用autoencoder预测评分，上小节介绍的矩阵分解模型是线性模型，它不能捕捉复杂的非线性关系，比如用户的偏好。这一小节介绍一个非线性协同过滤神经网络模型AutoRec。</p>

<p>AutoRec是基于自编码器的结构，自编码器是一种特殊的神经网络架构，他的输入和输出的架构是相同的，自编码器通过无监督学习来训练获取输入数据在较低维度的表达，在神经网络的后段，这些低纬度的信息再次被重构回高维的数据表达。</p>

<p>所以AutoRec的架构也是输入层，隐藏层和重构输出层。它的目的是输入一个只有部分兴趣矩阵，输出一个完整的兴趣矩阵。</p>

<p>AutoRec可以分为user-based 和 item-based</p>

<h3 id="32-formula">3.2. formula</h3>
<p>针对item-based：</p>

<p>$R_{*i}$表示兴趣矩阵的第i列，不知道的项填为0。那么神经网络的构架可以定义为：</p>

<center><img src="../assets/img/posts/20211216/6.jpg" /></center>

<p>h()表示最终的输出，输出一个完整的兴趣矩阵，那么误差定义为：</p>

<center><img src="../assets/img/posts/20211216/7.jpg" /></center>

<h2 id="4-personalized-ranking-for-recommender-system">4. Personalized Ranking for Recommender System</h2>
<h3 id="41-overview">4.1. overview</h3>
<p>在上一节中，我们用到了显式反馈，同时模型只在能观察到的评分上训练。那么这种模型有两个缺点：第一个是很多的反馈并不是显式的。第二个是没有观察到的评分被完全忽略了。</p>

<p>个性化推荐可以分为:1.pointwise;2.pairwise;3.listwise。Pointwise表示每次预测单个偏好，pairwise则是预测出一系列的偏好然后进行排序，listwise则是预测所有的item并进行排序。</p>

<h3 id="42-bayesian-personalized-ranking-loss-贝叶斯损失">4.2. Bayesian Personalized Ranking loss 贝叶斯损失</h3>
<ul>
  <li>
    <p>贝叶斯损失是一种pairwise个性化推荐损失。它被广泛应用于多种推荐系统中。它假设用户相对于无观察项，更加喜欢positive item</p>
  </li>
  <li>
    <p>训练集格式是(u, i, j)表示用户u喜欢i超过j，BPR希望最大化下面这个后验概率：</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211216/8.jpg" /></center>

<p>其中$\Theta$表示推荐系统的参数，$&gt;_u$表示用户u对所有item的排序。</p>

<center><img src="../assets/img/posts/20211216/9.jpg" /></center>

<h3 id="43-hinge-loss">4.3. Hinge Loss</h3>
<ul>
  <li>数学表达式</li>
</ul>

<center><img src="../assets/img/posts/20211216/10.jpg" /></center>

<p>其中m表示安全系数，它的目的是让不喜欢的项离喜欢的项更远。它和贝叶斯都是为了优化positive sample和negative sample之间的距离。</p>

<h2 id="5-neural-collaborative-filtering-for-personalized-ranking-使用协同过滤网络个性化排序">5. Neural Collaborative Filtering for Personalized Ranking 使用协同过滤网络个性化排序</h2>
<p>本小节重新将目光聚集到隐式反馈中，介绍协同过滤推荐系统NeuMF。NeuMF利用隐式反馈，它由两个子结构组成，分别是generalized matrix factorization(GMF)和MLP。不同于评分的预测如AutoRec，它将生成一系列的推荐，它根据用户是否看过这场电影来区分为正例和反例</p>

<h3 id="51-the-neumf-model">5.1. The NeuMF model</h3>
<p>NeuMF的网络结构由两部分组成。</p>

<ul>
  <li>一部分是GMF，也就是matrix factorization的类似形式，输入用户向量$p_u$和物品向量$q_i$，返回x</li>
</ul>

<center><img src="../assets/img/posts/20211216/12.jpg" /></center>

<ul>
  <li>另一部分是MLP，输入和GMF一样，但是用不同的字母表示，具体公式如下：</li>
</ul>

<center><img src="../assets/img/posts/20211216/13.jpg" /></center>

<ul>
  <li>最后对这两个子结构concatenate一下，就是最终的输出</li>
</ul>

<center><img src="../assets/img/posts/20211216/14.jpg" /></center>

<ul>
  <li>大体的网络结构如下</li>
</ul>

<center><img src="../assets/img/posts/20211216/11.jpg" /></center>

<h3 id="52-evaluator">5.2. Evaluator</h3>
<p>有两个性能度量指标</p>

<ul>
  <li>hit rate at given cutting off l，记作Hit@l</li>
</ul>

<center><img src="../assets/img/posts/20211216/15.jpg" /></center>

<p>这个式子的主题思路是判断推荐的物品是否在top l中，m表示用户的数量，$rank_{u,g_u}$表示对于用户u和物品$g_u$的排名，1表示指标函数</p>

<ul>
  <li>AUC，即ROC曲线下的面积，也是模型泛化能力的一个指标</li>
</ul>

<center><img src="../assets/img/posts/20211216/16.jpg" /></center>

<p>其中$S_u$表示模型对于u的推荐物品集，I表示item set，AUC越大越好</p>

<h3 id="53-代码">5.3. 代码</h3>
<p>网络结构就是上面介绍的那样，net的输出是用户和物品匹配出的一个推荐值(我的想法)。在进行训练的时候，会给出正例物品(即用户有过评分的物品)和反例物品(用户没有评分，也就是没有看过)分别与用户得到一个推荐值，然后利用上一小节介绍的贝叶斯损失来优化(让评分过的物品有更高的推荐值)，然后最终我们希望返回一系列的推荐物品，这些推荐物品都是没有负例物品，然后根据推荐值进行排序。性能指标是hit或者auc。hit的思想是让真实评分的物品在推荐列表中。</p>

<h2 id="6-sequence-aware-recommender-systems">6. Sequence-Aware Recommender Systems</h2>
<p>之前的模型都没有考虑时序信息，这小节的Caser模型将会考虑用户的时序信息。</p>
<h3 id="61-model-architectures">6.1. Model Architectures</h3>
<p>模型的输入$E^{(u,t)}$表示用户u的近期L个评价的物品，Caser模型有横向和纵向的卷积层，输入矩阵分别与卷积层作用后，结果concatenate变成$z$，$z$再和用户的一般信息结合，也就是$z$和$p_u$concatenate最终输出$\hat{y}_{uit}$，其中$p_u$表示用户u的item信息</p>

<center><img src="../assets/img/posts/20211216/17.jpg" /></center>

<h3 id="62-negative-sampling-负采样">6.2. Negative Sampling 负采样</h3>
<p>我们需要对数据集进行重新处理，比如一个人喜欢9部电影，同时我们的L=5，那么我们将最近的一部电影留出来作为test，其余的都作为训练集，可以划分出3个训练集。同时我们也需要进行负采样(采样没有评分的item)</p>

<h2 id="7-feature-rich-recommender-systems">7. Feature-Rich Recommender Systems</h2>
<p>之前的模型大都用到了用户物品的交互矩阵，但是很少有用到一些额外的信息，比如物品的特征，用户的简介，发生交互的背景等等…利用这些信息可以获得用户的兴趣特征。本节提出了一个新的任务CTR(click-through rate)，也就是点击率任务，对象可以是广告、电影等等。</p>

<h2 id="8-factorization-machines-因子分解机">8. Factorization Machines 因子分解机</h2>
<p>Factorization machines(FM)是一个监督算法，可用于分类，回归和排名任务。它有两个优点：1.它能处理稀疏的数据；2.它能减少时间复杂度和线性复杂度</p>

<h3 id="81-2-way-factorization-machines">8.1. 2-Way Factorization Machines</h3>
<p>$x$表示样本的特征值，而$y$表示它的标签值，即click/non-click。第二项表示线性项，第三项表示矩阵分解项</p>

<center><img src="../assets/img/posts/20211216/18.jpg" /></center>

<h3 id="82-an-efficient-optimization-citerion">8.2. An Efficient Optimization Citerion</h3>
<p>上面式子的第三项时间复杂度太高，我们可以简化一下</p>

<center><img src="../assets/img/posts/20211216/19.jpg" /></center>

<h2 id="9-deep-factorization-machines-深度因子分解机deeofm">9. Deep Factorization Machines 深度因子分解机DeeoFM</h2>
<p>上小节提到的因子分解机用到的都是线性模型(单线性和双线性)，这种模型在真实数据表现并不好。这里我们就可以结合因子分解机和深度神经网络，比如我们这小节即将介绍的DeepFM。</p>

<h3 id="91-model-architectures-模型架构">9.1. Model Architectures 模型架构</h3>
<p>DeepFM由两部分组成，FM component和deep component，FM部分和上小节提到的2-way FM做法一样，主要是处理低纬度特征，而deep部分用到的MLP来处理高维度和非线性。这两部分使用相同的输入/嵌入层然后它们的结果整合成最终的预测。模型结构如下图：</p>

<center><img src="../assets/img/posts/20211216/20.jpg" /></center>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[d2l推荐系统笔记]]></summary></entry><entry><title type="html">Robotics</title><link href="http://localhost:4000/Robotics.html" rel="alternate" type="text/html" title="Robotics" /><published>2021-12-13T00:00:00+08:00</published><updated>2021-12-13T00:00:00+08:00</updated><id>http://localhost:4000/Robotics</id><content type="html" xml:base="http://localhost:4000/Robotics.html"><![CDATA[<h1 id="报告">报告</h1>
<h2 id="报告内容">报告内容</h2>
<p>用solidworks设计一个至少三个自由度的机械臂，并且描述它的动能，移动能力等等。提交的是截图，源文件等等。</p>
<h2 id="报告格式">报告格式</h2>
<ol>
  <li>标题，下面有姓名学号电话等等</li>
  <li>摘要</li>
  <li>正文</li>
</ol>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[《机器人》课程随堂笔记]]></summary></entry><entry><title type="html">数据挖掘</title><link href="http://localhost:4000/datamining.html" rel="alternate" type="text/html" title="数据挖掘" /><published>2021-12-10T00:00:00+08:00</published><updated>2021-12-10T00:00:00+08:00</updated><id>http://localhost:4000/datamining</id><content type="html" xml:base="http://localhost:4000/datamining.html"><![CDATA[<!-- TOC -->

<ul>
  <li><a href="#总体情况">总体情况</a>
    <ul>
      <li><a href="#第一章-开始数据挖掘之旅">第一章 开始数据挖掘之旅</a>
        <ul>
          <li><a href="#11-亲和性分析">1.1 亲和性分析</a></li>
          <li><a href="#12-分类">1.2 分类</a></li>
        </ul>
      </li>
      <li><a href="#第二章-用scikit-learn估计器分类">第二章 用scikit-learn估计器分类</a>
        <ul>
          <li><a href="#21-scikit-learn">2.1 scikit-learn</a></li>
          <li><a href="#22-邻近算法knn">2.2 邻近算法KNN</a></li>
        </ul>
      </li>
      <li><a href="#第三章-用决策树预测获胜球队">第三章 用决策树预测获胜球队</a>
        <ul>
          <li><a href="#31-决策树">3.1 决策树</a></li>
          <li><a href="#32-随机森林">3.2 随机森林</a></li>
        </ul>
      </li>
      <li><a href="#第四章-用亲和性分析方法推荐电影">第四章 用亲和性分析方法推荐电影</a>
        <ul>
          <li><a href="#41-亲和性分析">4.1 亲和性分析</a></li>
          <li><a href="#42-apriori算法">4.2 Apriori算法</a></li>
        </ul>
      </li>
      <li><a href="#第五章-用转换器抽取特征">第五章 用转换器抽取特征</a>
        <ul>
          <li><a href="#51-抽取特征">5.1 抽取特征</a></li>
          <li><a href="#52-特征选择">5.2 特征选择</a></li>
          <li><a href="#53-创建特征">5.3 创建特征</a></li>
        </ul>
      </li>
      <li><a href="#第六章-使用朴素贝叶斯进行社会媒体挖掘">第六章 使用朴素贝叶斯进行社会媒体挖掘</a>
        <ul>
          <li><a href="#61-消歧">6.1 消歧</a></li>
          <li><a href="#62-文本转换器">6.2 文本转换器</a></li>
          <li><a href="#63-朴素贝叶斯">6.3 朴素贝叶斯</a></li>
          <li><a href="#64-f1值">6.4 F1值</a></li>
        </ul>
      </li>
      <li><a href="#第九章-作者归属问题">第九章 作者归属问题</a>
        <ul>
          <li><a href="#91-作者归属">9.1 作者归属</a></li>
          <li><a href="#92-支持向量机">9.2 支持向量机</a></li>
          <li><a href="#93-基础svm的局限性">9.3 基础SVM的局限性</a></li>
        </ul>
      </li>
      <li><a href="#第十章-新闻语料分类">第十章 新闻语料分类</a>
        <ul>
          <li><a href="#101-新闻语料聚类">10.1 新闻语料聚类</a></li>
          <li><a href="#102-k-means算法">10.2 K-means算法</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h1 id="总体情况">总体情况</h1>
<ul>
  <li>书籍:Python数据挖掘入门与实践</li>
  <li>github_url:https://github.com/LinXueyuanStdio/PythonDataMining</li>
  <li>配套代码和笔记，很适合迅速上手</li>
  <li>这篇博客主要记录一些比较重要的算法</li>
</ul>

<h2 id="第一章-开始数据挖掘之旅">第一章 开始数据挖掘之旅</h2>
<h3 id="11-亲和性分析">1.1 亲和性分析</h3>
<ul>
  <li>亲和性分析根据样本个体（物体）之间的<strong>相似度</strong>，确定它们关系的亲疏。</li>
  <li>例子：商品推荐。</li>
  <li>我们要找出“如果顾客购买了商品X，那么他们可能愿意购买商品Y”这样的规则。简单粗暴的做法是，找出数据集中所有同时购买的两件商品。找出规则后，还需要判断其优劣，我们挑好的规则用。</li>
  <li>规则的优劣有多种判断标准，常用的有支持度(support)和置信度(confidence)</li>
  <li>支持度：数据集中规则应验的次数，统计起来很简单。有时候，还需要对支持度进行规范化，即再除以规则有效前提下的总数量。</li>
  <li>置信度是衡量规则的准确性如何。</li>
</ul>

<h3 id="12-分类">1.2 分类</h3>
<ul>
  <li>根据特征分出类别</li>
  <li>例子：Iris植物分类数据集，通过四个特征分出三个类别</li>
  <li>特征连续值变成离散值</li>
  <li>OneR算法：它根据已有数据中，具有相同特征值的个体最可能属于哪个类别进行分类。比如对于某一个特征值来说，属于A的类别有80个，属于B的类别有20个，那么对于这个特征值来说，取值为1代表为A类别，错误率有20％。给出所有特征值，找出错误率最小的特征值作为判断标准。</li>
</ul>

<h2 id="第二章-用scikit-learn估计器分类">第二章 用scikit-learn估计器分类</h2>
<h3 id="21-scikit-learn">2.1 scikit-learn</h3>
<p>scikit-learn里面已经封装好很多数据挖掘的算法</p>

<p>现介绍数据挖掘框架的搭建方法：</p>

<ul>
  <li>转换器（Transformer）用于数据预处理，数据转换</li>
  <li>流水线（Pipeline）组合数据挖掘流程，方便再次使用（封装）</li>
  <li>估计器（Estimator）用于分类，聚类，回归分析（各种算法对象）
    <ul>
      <li>所有的估计器都有下面2个函数
        <ul>
          <li>fit() 训练
            <ul>
              <li>用法：estimator.fit(X_train, y_train)，</li>
              <li>estimator = KNeighborsClassifier() 是scikit-learn算法对象</li>
              <li>X_train = dataset.data 是numpy数组</li>
              <li>y_train = dataset.target 是numpy数组</li>
            </ul>
          </li>
          <li>predict() 预测
            <ul>
              <li>用法：estimator.predict(X_test)</li>
              <li>estimator = KNeighborsClassifier() 是scikit-learn算法对象</li>
              <li>X_test = dataset.data 是numpy数组</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="22-邻近算法knn">2.2 邻近算法KNN</h3>
<p>邻近算法，或者说K最邻近（KNN，K-NearestNeighbor）分类算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是K个最近的邻居的意思，说的是每个样本都可以用它最接近的K个邻近值来代表。近邻算法就是将数据集合中每一个记录进行分类的方法。</p>

<p>例子：分类，Ionosphere数据集</p>

<h2 id="第三章-用决策树预测获胜球队">第三章 用决策树预测获胜球队</h2>

<h3 id="31-决策树">3.1 决策树</h3>
<p>例子：预测NBA球队获胜情况</p>

<p>决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。</p>

<p>分类树（决策树）是一种十分常用的分类方法。它是一种监督学习，所谓监督学习就是给定一堆样本，每个样本都有一组属性和一个类别，这些类别是事先确定的，那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类。这样的机器学习就被称之为监督学习。</p>

<p>scikit-learn库实现了分类回归树（Classification and Regression Trees，CART）算法并将其作为生成决策树的默认算法，它支持连续型特征和类别型特征。</p>

<h3 id="32-随机森林">3.2 随机森林</h3>
<p>随机森林指的是利用多棵树对样本进行训练并预测的一种分类器。</p>

<p>在机器学习中，随机森林是一个包含多个决策树的分类器， 并且其输出的类别是由个别树输出的类别的众数而定。</p>

<h2 id="第四章-用亲和性分析方法推荐电影">第四章 用亲和性分析方法推荐电影</h2>
<h3 id="41-亲和性分析">4.1 亲和性分析</h3>
<p>亲和性分析就是分析两个样本之间的疏密关系，常用的算法有Apriori，Apriori算法的一大特点是根据最小支持度生成<strong>频繁项集</strong>（frequent itemest），它只从数据集中频繁出现的商品中选取共同出现的商品组成频繁项集。其他亲和性分析算法有Eclat和频繁项集挖掘算法（FP-growth）。</p>

<h3 id="42-apriori算法">4.2 Apriori算法</h3>
<p>Apriori算法主要有两个阶段，第一个阶段是根据最小支持度生成频繁项集，第二个阶段是根据最小置信度选择规则，返回规则。</p>

<p>本章的例子是电影推荐。</p>

<p>第一个阶段，算法会先生成长度较小的项集，再将这个项集作为超集寻找长度较大的项集。</p>

<p>第二个阶段是从频繁项集中抽取关联规则。把其中几部电影作为前提，另一部电影作为结论。组成如下形式的规则：如果用户喜欢前提中的所有电影，那么他们也会喜欢结论中的电影。</p>

<h2 id="第五章-用转换器抽取特征">第五章 用转换器抽取特征</h2>
<h3 id="51-抽取特征">5.1 抽取特征</h3>
<p>抽取数据集的特征是重要的一步，在之前的学习中我们都获得了数据集的特征，但很多没有处理的文本特征并不是很明显，比如一段文本等等。特征值可以分为连续特征，序数特征，类别型特征。</p>

<h3 id="52-特征选择">5.2 特征选择</h3>
<p>通常特征有很多，但我们只想选择其中一部分。<strong>选用干净的数据，选取更具描述性的特征。</strong>判断特征相关性：书中列举的例子是判断一个人的收入能不能超过五万，利用单变量卡方检验(或者皮尔逊相关系数)判断各个特征的相关性，然后给出了三个最好的特征，分别是年龄，资本收入和资本损失。</p>

<h3 id="53-创建特征">5.3 创建特征</h3>
<p>主成分分析算法（Principal Component Analysis，PCA）的目的是找到能用较少信息描述数据集的特征组合。</p>

<h2 id="第六章-使用朴素贝叶斯进行社会媒体挖掘">第六章 使用朴素贝叶斯进行社会媒体挖掘</h2>
<h3 id="61-消歧">6.1 消歧</h3>
<p>本章我们将处理文本，文本通常被称为无结构格式。文本挖掘的一个难点来自于歧义，比如bank一词多义。本章将探讨区别Twitter消息中Python的意思。</p>

<h3 id="62-文本转换器">6.2 文本转换器</h3>
<p>Python中处理文本的库NLTK(Natural Language Toolkit)。据作者说很好用，可以作自然语言处理。N元语法是指由连续的词组成的子序列。</p>

<h3 id="63-朴素贝叶斯">6.3 朴素贝叶斯</h3>
<p>朴素贝叶斯概率模型是以对贝叶斯统计方法的朴素解释为基础。</p>

<p>贝叶斯定理公式如下：</p>

<p>$ P(A|B) = \frac {P(B|A)P(A)}{P(B)} $</p>

<p>贝叶斯公式可以用它来计算个体属于给定类别的概率。朴素贝叶斯算法假定了各个特征之间相互独立，那么我们计算文档D属于类别C的概率为P(D|C)=P(D1|C)*P(D2|C)…P(Dn|C)。贝叶斯分类器是输入数据来更新贝叶斯的先验概率和后验概率，输入贝叶斯模型后，返回不同类别中概率的最大值。</p>

<p>示例：</p>

<blockquote>
  <p>举例说明下计算过程，假如数据集中有以下一条用二值特征表示的数据：[1, 0, 0, 1]。<br />
训练集中有75%的数据属于类别0，25%属于类别1，且每个特征属于每个类别的似然度如下。<br />
类别0：[0.3, 0.4, 0.4, 0.7] <br />
类别1：[0.7, 0.3, 0.4, 0.9] <br />
拿类别0中特征1的似然度举例子，上面这两行数据可以这样理解：类别0中有30%的数据，特征1的值为1。<br />
我们来计算一下这条数据属于类别0的概率。类别为0时，P(C=0) = 0.75。<br />
朴素贝叶斯算法用不到P(D)，因此我们不用计算它。我们来看下计算过程。<br />
P(D|C=0) = P(D1|C=0) x P(D2|C=0) x P(D3|C=0) x P(D4|C=0)<br />
= 0.3 x 0.6 x 0.6 x 0.7 <br />
= 0.0756 <br />
现在，我们就可以计算该条数据从属于每个类别的概率。需要提醒的是，我们没有计算P(D)，因此，计算结果不是实际的概率。由于两次都不计算P(D)，结果具有可比较性，能够区分出大小就足够了。来看下计算结果。<br />
P(C=0|D) = P(C=0) P(D|C=0) <br />
= 0.75 * 0.0756 <br />
= 0.0567</p>
</blockquote>

<h3 id="64-f1值">6.4 F1值</h3>
<p>F1值是一种评价指标。F1值是以每个类别为基础进行定义的，包括两大概念：准确率（precision）和召回率（recall）。准确率是指预测结果属于某一类的个体，实际属于该类的比例。召回率是指被正确预测为某个类别的个体数量与数据集中该类别个体总量的比例。F1值是准确率和召回率的调和平均数。</p>

<h2 id="第九章-作者归属问题">第九章 作者归属问题</h2>
<h3 id="91-作者归属">9.1 作者归属</h3>
<p>作者归属（authorship attribution）是作者分析的一个细分领域，研究目标是从一组可能的作者中找到文档真正的主人。利用功能词进行分类，功能词是指本身含义很少，但是是组成句子必不可少的部分。</p>

<h3 id="92-支持向量机">9.2 支持向量机</h3>
<p>支持向量机（SVM）分类算法背后的思想很简单，它是一种二类分类器（扩展后可用来对多个类别进行分类）。假如我们有两个类别的数据，而这两个类别恰好能被一条线分开，线上所有点为一类，线下所有点属于另一类。SVM要做的就是找到这条线，用它来做预测，跟线性回归原理很像。</p>

<p>下图中有三条线，那么哪一条线的分类效果最好呢？直觉告诉我们从左下到右上的这一条线效果最好，因为每一个点到这条线的距离最远，那么寻找这条线就变成了最优化问题。</p>

<p><img src="../assets/img/posts/20211210/2.jpg" /></p>

<p>对于多种类别的分类问题，我们创建多个SVM分类器，其中每个SVM分类器还是二分类。连接多个分类器的方法有很多，比如说我们可以将每个类别创建一对多分类器。把训练数据分为两个类别——属于特定类别的数据和其他所有类别数据。对新数据进行分类时，从这些类别中找出最匹配的。</p>

<h3 id="93-基础svm的局限性">9.3 基础SVM的局限性</h3>
<p>最基础的SVM只能区分线性可分的两种类别，如果数据线性不可分，就需要将其置入更高维的空间中，加入更多伪特征直到数据线性可分。寻找最佳分隔线时往往需要计算个体之间的内积。我们把内核函数定义为数据集中两个个体函数的点积。</p>

<p>常用的内核函数有几种。线性内核最简单，它无外乎两个个体的特征向量的点积、带权重的特征和偏置项。多项式内核提高点积的阶数（比如2）。此外，还有高斯内核（rbf）、Sigmoind内核。</p>

<h2 id="第十章-新闻语料分类">第十章 新闻语料分类</h2>
<h3 id="101-新闻语料聚类">10.1 新闻语料聚类</h3>
<p>之前我们研究的都是监督学习，在已经知道类别的情况下进行分类。本章着眼于无监督学习，聚类。</p>

<h3 id="102-k-means算法">10.2 K-means算法</h3>
<p>k-means聚类算法迭代寻找最能够代表数据的聚类质心点。算法开始时使用从训练数据中随机选取的几个数据点作为质心点。k-means中的k表示寻找多少个质心点，同时也是算法将会找到的簇的数量。例如，把k设置为3，数据集所有数据将会被分成3个簇。</p>

<p>k-means算法分为两个步骤：为每一个数据点分配簇标签，更新各簇的质心点。k-means算法会重复上述两个步骤；每次更新质心点时，所有质心点将会小范围移动。这会
轻微改变每个数据点在簇内的位置，从而引发下一次迭代时质心点的变动。这个过程会重复执行直到条件不再满足时为止。通常是在迭代一定次数后，或者当质心点的整体移动量很小时，就可以终止算法的运行。有时可以等算法自行终止运行，这表明簇已经相当稳定——数据点所属的簇不再变动，质心点也不再改变时。</p>

<p><img src="../assets/img/posts/20211210/3.jpg" /></p>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[《Python数据挖掘入门与实践》笔记]]></summary></entry><entry><title type="html">RACE数据集相关文献</title><link href="http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html" rel="alternate" type="text/html" title="RACE数据集相关文献" /><published>2021-11-30T00:00:00+08:00</published><updated>2021-11-30T00:00:00+08:00</updated><id>http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE</id><content type="html" xml:base="http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html"><![CDATA[<h1 id="目录"><strong>目录</strong></h1>

<ul>
  <li><a href="#目录"><strong>目录</strong></a></li>
  <li><a href="#文献整理">文献整理</a>
    <ul>
      <li><a href="#要求">要求</a></li>
      <li><a href="#搜集到相关文献标题和地址">搜集到相关文献标题和地址</a></li>
    </ul>
  </li>
  <li><a href="#第一篇">第一篇</a>
    <ul>
      <li><a href="#title">Title</a></li>
      <li><a href="#author">Author</a></li>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#introduction">Introduction</a></li>
      <li><a href="#bert-distractor-generation">BERT distractor generation</a>
        <ul>
          <li><a href="#1bert-based-distractor-generationbdg">1)BERT-based distractor generation(BDG)</a></li>
          <li><a href="#2multi-task-with-parallel-mlm">2)Multi-task with Parallel MLM</a></li>
          <li><a href="#3answer-negative-regularization">3)Answer Negative Regularization</a></li>
        </ul>
      </li>
      <li><a href="#multiple-distractor-generation">Multiple Distractor Generation</a>
        <ul>
          <li><a href="#1selecting-distractors-by-entropy-maximization">1)Selecting Distractors by Entropy Maximization</a></li>
          <li><a href="#2bdg-em">2)BDG-EM</a></li>
        </ul>
      </li>
      <li><a href="#performance-evaluation">Performance Evaluation</a>
        <ul>
          <li><a href="#1datasets">1)datasets</a></li>
          <li><a href="#2implementation-details">2)implementation details</a></li>
          <li><a href="#3compared-methods">3)compared methods</a></li>
          <li><a href="#4token-score-comparison">4)token score comparison</a></li>
          <li><a href="#5mcq-model-accuracy-comparison">5)MCQ Model Accuracy Comparison</a></li>
          <li><a href="#6parameter-study-on-γ">6）Parameter Study on γ</a></li>
        </ul>
      </li>
      <li><a href="#conclusion">Conclusion</a></li>
      <li><a href="#我的看法">我的看法</a></li>
    </ul>
  </li>
  <li><a href="#第二篇">第二篇</a>
    <ul>
      <li><a href="#title-1">Title</a></li>
      <li><a href="#author-1">Author</a></li>
      <li><a href="#abstract-1">Abstract</a></li>
      <li><a href="#method">Method</a>
        <ul>
          <li><a href="#1question-generation">1)question generation</a></li>
          <li><a href="#2distractor-generation">2)distractor generation</a></li>
          <li><a href="#3qa-filtering">3)QA filtering</a></li>
        </ul>
      </li>
      <li><a href="#results">Results</a>
        <ul>
          <li><a href="#1quantitative-evaluation">1)quantitative evaluation</a></li>
          <li><a href="#2question-answering-ability">2)question answering ability</a></li>
          <li><a href="#3human-evaluation">3)human evaluation</a></li>
        </ul>
      </li>
      <li><a href="#conclusion">conclusion</a></li>
    </ul>
  </li>
  <li><a href="#第三篇">第三篇</a>
    <ul>
      <li><a href="#title-2">Title</a></li>
      <li><a href="#author-2">Author</a></li>
      <li><a href="#abstract-2">Abstract</a></li>
      <li><a href="#framework-description-网络结构">Framework Description 网络结构</a>
        <ul>
          <li><a href="#1task-definition">1)Task Definition</a></li>
          <li><a href="#2framework-overview">2)Framework overview</a></li>
          <li><a href="#3hierarchical-encoder">3)Hierarchical encoder</a></li>
          <li><a href="#4static-attention-mechanism">4)static attention mechanism</a></li>
          <li><a href="#5encoding-layer">5)encoding layer</a></li>
          <li><a href="#6matching-layer">6)matching layer</a></li>
          <li><a href="#7nomalization-layer">7)nomalization layer</a></li>
          <li><a href="#8distractor-decoder">8)distractor decoder</a></li>
          <li><a href="#9question-based-initializer">9)question-based initializer</a></li>
          <li><a href="#10dynamic-hierarchical-attention-mechanism">10)dynamic hierarchical attention mechanism</a></li>
          <li><a href="#11training-and-inference">11)training and inference</a></li>
        </ul>
      </li>
      <li><a href="#experimental-setting-实验设置">experimental setting 实验设置</a>
        <ul>
          <li><a href="#1dataset">1)dataset</a></li>
          <li><a href="#2implementation-details-1">2)implementation details</a></li>
          <li><a href="#3baselines-and-ablations">3)baselines and ablations</a></li>
        </ul>
      </li>
      <li><a href="#results-and-analysis-结果与分析">results and analysis 结果与分析</a></li>
      <li><a href="#我的看法-1">我的看法</a></li>
    </ul>
  </li>
  <li><a href="#第四篇">第四篇</a>
    <ul>
      <li><a href="#title-3">Title</a></li>
      <li><a href="#author-3">Author</a></li>
      <li><a href="#abstract-3">Abstract</a></li>
      <li><a href="#proposed-framework-网络结构">Proposed Framework 网络结构</a>
        <ul>
          <li><a href="#1notations-and-task-definition">1)notations and task definition</a></li>
          <li><a href="#2model-overview">2)model overview</a></li>
          <li><a href="#3encoding-article-and-question">3)encoding article and question</a></li>
          <li><a href="#4co-attention-between-article-and-question">4)Co-attention between article and question</a></li>
          <li><a href="#5merging-sentence-representation">5)Merging sentence representation</a></li>
          <li><a href="#6question-initialization">6)question initialization</a></li>
          <li><a href="#7hierarchical-attention">7)hierarchical attention</a></li>
          <li><a href="#8semantic-similarity-loss">8)semantic similarity loss</a></li>
        </ul>
      </li>
      <li><a href="#experimental-settings">Experimental Settings</a>
        <ul>
          <li><a href="#1dataset-1">1)dataset</a></li>
          <li><a href="#2baselines-and-evaluation-metrics">2)baselines and evaluation metrics</a></li>
          <li><a href="#3implementation-details">3)implementation details</a></li>
        </ul>
      </li>
      <li><a href="#results-and-analysis-结果与分析">Results and Analysis 结果与分析</a></li>
      <li><a href="#我的看法-2">我的看法</a></li>
    </ul>
  </li>
  <li><a href="#补充">补充</a>
    <ul>
      <li><a href="#race数据集简介">RACE数据集简介</a></li>
      <li><a href="#bleu">BLEU</a></li>
      <li><a href="#rouge">ROUGE</a></li>
    </ul>
  </li>
</ul>

<h1 id="文献整理">文献整理</h1>

<h2 id="要求">要求</h2>

<p><img src="../assets/img/posts/20211130/requirements.jpg" /></p>

<h2 id="搜集到相关文献标题和地址">搜集到相关文献标题和地址</h2>
<ul>
  <li><a href="https://arxiv.org/pdf/2010.05384.pdf">A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies</a></li>
  <li><a href="https://arxiv.org/pdf/2010.09598.pdf">Better Distractions: Transformer-based Distractor Generation and Multiple Choice Question Filtering</a></li>
  <li><a href="https://ojs.aaai.org//index.php/AAAI/article/view/4606">Generating Distractors for Reading Comprehension Questions from Real Examinations</a></li>
  <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6522">Co-attention hierarchical network: Generating coherent long distractors for reading comprehension</a></li>
  <li><a href="https://aclanthology.org/2020.coling-main.189.pdf">Automatic Distractor Generation for Multiple Choice Questions in Standard Tests</a></li>
  <li><a href="https://aclanthology.org/W18-0533.pdf">Distractor Generation for Multiple Choice Questions Using Learning to Rank</a></li>
  <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/16559">Knowledge-Driven Distractor Generation for Cloze-style Multiple Choice Questions</a></li>
</ul>

<h1 id="第一篇">第一篇</h1>
<h2 id="title">Title</h2>
<p>A BERT-based Distractor Generation Scheme with Multi-tasking and
Negative Answer Training Strategies</p>
<h2 id="author">Author</h2>
<p>Ho-Lam Chung, Ying-Hong Chan, Yao-Chung Fan</p>
<h2 id="abstract">Abstract</h2>
<p>现有的DG<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>局限在只能生成一个误导选项，我们需要生成多个误导选项，文章中提到他们团队用multi-tasking和negative answer training技巧来生成多个误导选项，模型结果达到了学界顶尖。</p>

<h2 id="introduction">Introduction</h2>
<p>DG效果不好，文章提出了两个提升的空间：</p>
<ol>
  <li>DG质量提升：<br />
 BERT模型来提升误导选项质量</li>
  <li>多个误导选项生成：
 运用了覆盖的方法来选择distractor，而不是选择概率最高但是语义很相近的distractor</li>
</ol>

<h2 id="bert-distractor-generation">BERT distractor generation</h2>
<h3 id="1bert-based-distractor-generationbdg">1)BERT-based distractor generation(BDG)</h3>
<p>输入：段落P，答案A，问题Q，用C表示这三者concatenate后的结果。<br />
BDG模型是一个自回归模型，在预测阶段，每次输入C和上一次预测的词元，BDG迭代预测词元，直到预测出特殊词元[S]停止。下面这张图简单介绍了这个过程。</p>

<p><img src="../assets/img/posts/20211130/2.jpg" /></p>

<p>网络结构简单介绍：h<sub>[M]</sub>表示bert输出的隐藏状态，隐藏状态再输入到一个全连接层中用来预测词元。</p>

<p><img src="../assets/img/posts/20211130/3.jpg" /></p>

<h3 id="2multi-task-with-parallel-mlm">2)Multi-task with Parallel MLM</h3>
<p>MLM全称masked language model，遮蔽语言模型,通过并行BDG和P-MLM来训练模型让模型有更好的效果。</p>

<p><img src="../assets/img/posts/20211130/4.jpg" /></p>

<p>上图中左边的sequential MLM就是之前提到的BDG，BDG模型是一个词接一个词的预测，P-MLM是对所有的masked token进行预测，最后的损失函数是这两者相加<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>，公式如下：</p>

<p><img src="../assets/img/posts/20211130/5.jpg" /></p>

<p><img src="../assets/img/posts/20211130/6.jpg" /></p>

<p><img src="../assets/img/posts/20211130/7.jpg" /></p>

<p>作者如此设计的思路是：BDG可能会忽略整体语义语义信息，但是会过拟合单个词预测。那么并行一个P-MLM可以防止过拟合。</p>

<h3 id="3answer-negative-regularization">3)Answer Negative Regularization</h3>
<p>目前机器预测的distractor和answer有很高的相似度，下面一张表可以展示相似度。其中PM表示机器，Gold表示人工，作者将这类问题称为answer copying problem。</p>

<p><img src="../assets/img/posts/20211130/8.jpg" /></p>

<p>为了解决这个问题，作者提出了answer negative loss来让机器更多的选择与answer不同的词来表示新的distractor，公式如下：</p>

<p><img src="../assets/img/posts/20211130/9.jpg" /></p>

<p>可以看出BDG的loss替换成了AN的loss，每一项都减去了Answer negative loss。</p>

<h2 id="multiple-distractor-generation">Multiple Distractor Generation</h2>
<h3 id="1selecting-distractors-by-entropy-maximization">1)Selecting Distractors by Entropy Maximization</h3>
<p>选择语义不同的distractor set。文章借鉴了MRC<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>的方法，让BDGmodel生成很多distractor组成 $\hat{D}$ = {$\hat{d}$<sub>1</sub>, $\hat{d}$<sub>2</sub>, $\hat{d}$<sub>3</sub>…}，然后找出最好的一组选项，一般情况下由三个误导选项和一个答案组成。选择的一句是最大化下面这个公式：</p>

<p><img src="../assets/img/posts/20211130/10.jpg" /></p>

<h3 id="2bdg-em">2)BDG-EM</h3>
<p>我们可以通过不同的BDG模型来生成不同的误导选项最后组合，不同的模型区别是有没有answer negative/multi-task training，比如我们有这几个模型:$\hat{D}$,$\hat{D}$<sub>PM</sub>,$\hat{D}$<sub>PM+AN</sub>，它们分别代表含PM<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>和含AN<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup></p>

<p><img src="../assets/img/posts/20211130/11.jpg" /></p>

<h2 id="performance-evaluation">Performance Evaluation</h2>
<h3 id="1datasets">1)datasets</h3>
<p>RACE,沿用了<a href="https://ojs.aaai.org//index.php/AAAI/article/view/4606">Gao</a>那篇论文的处理,后面也会梳理那篇论文</p>

<p><img src="../assets/img/posts/20211130/12.jpg" /></p>

<h3 id="2implementation-details">2)implementation details</h3>
<ul>
  <li>tokenizer: wordpiece tokenizer</li>
  <li>framewordk:huggingface trainsformers</li>
  <li>optimizer:adamW(lr:5e-5)</li>
  <li>github_url: <a href="https://github.com/voidful/BDG">BDG</a></li>
</ul>

<h3 id="3compared-methods">3)compared methods</h3>
<p>比较了不同的distractor generation</p>
<ul>
  <li>CO-Att：出自<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6522">Zhou</a></li>
  <li>DS-Att: 出自<a href="https://ojs.aaai.org//index.php/AAAI/article/view/4606">Gao</a></li>
  <li>GPT:baseline</li>
  <li>BDG: 没有应用P-MLM和Answer negative</li>
  <li>BDG<sub>PM</sub></li>
  <li>BDG<sub>AN+PM</sub></li>
</ul>

<h3 id="4token-score-comparison">4)token score comparison</h3>
<p>BLEU和ROUGE(L)两种判断指标</p>

<p><img src="../assets/img/posts/20211130/13.jpg" /></p>

<p>copying problem的效果</p>

<p><img src="../assets/img/posts/20211130/14.jpg" /></p>

<h3 id="5mcq-model-accuracy-comparison">5)MCQ Model Accuracy Comparison</h3>
<p>与回答系统相结合，将生成好的选项（一个正确答案三个误导选项）放入MCQ answering model，下面是回答正确率的表格</p>

<p><img src="../assets/img/posts/20211130/15.jpg" /></p>

<p>可以看出作者的模型选项的误导性还是很高的。</p>

<h3 id="6parameter-study-on-γ">6）Parameter Study on γ</h3>
<p>之前使用P-MLM并行训练时候有个权重参数γ，下表显示了不同γ值的影响，对于只有PM的模型来说，γ=6，对于既有AN和PM来说，γ=7</p>

<p><img src="../assets/img/posts/20211130/16.jpg" /></p>

<h2 id="conclusion">Conclusion</h2>
<p>现存的DG可以分为cloze-style distractor generation和 reading comprehension distractor generation，前者主要是word filling，后者主要看重语义信息，基于两者的设计出了很多模型，目前来看还是考虑语义信息生成的误导选项更好。</p>

<p><img src="../assets/img/posts/20211130/17.jpg" /></p>

<h2 id="我的看法">我的看法</h2>
<p>文章中的模型提到了三种技术，第一是bert预训练模型使用。第二是P-MLM的并行使用， 它的使用让模型可以考虑段落的语义信息，那么生成的误导选项是sentence-level而不是之前模型所使用的类似word-filling这种word-level。第三是Answer negative loss的使用，它的使用相当于让模型不要考虑与正确答案语义很接近的误导选项，因为目前大多数DG生成多个选项时语义与正确答案都非常接近，这与实际情况不符，同时也起不到误导的作用。  <br />
同时文章提出了生成多个误导选项时使用不同模型生成的误导选项拼在一起作为选项是一种比较好的解决方法，让一次性生成多个误导选型有了一定的可用性。<br />
文章的代码开源，可以去<a href="https://github.com/voidful/BDG">github</a>上看训练细节和网络结构细节。</p>

<h1 id="第二篇">第二篇</h1>
<h2 id="title-1">Title</h2>
<p>Better Distractions: Transformer-based Distractor Generation and Multiple Choice Question Filtering</p>
<h2 id="author-1">Author</h2>
<p>Jeroen Offerijns, Suzan Verberne, Tessa Verhoef</p>
<h2 id="abstract-1">Abstract</h2>
<p>运用GPT2模型生成三个误导选项，同时用BERT模型去回答这个问题，只挑选出回答正确的问题。相当于使用了QA作为一个过滤器(QA filtering)。</p>
<h2 id="method">Method</h2>
<p>作者使用了Question generation model, distractor generation model和question answer filter，作者将从这三方面介绍，下图是大概的流程图。</p>

<p><img src="../assets/img/posts/20211130/18.jpg" /></p>

<h3 id="1question-generation">1)question generation</h3>
<ul>
  <li>预训练模型：GPT-2</li>
  <li>数据集：English SQuAD</li>
  <li>tokenizer：Byte-Pair-Encoding(BPE) tokenizer</li>
  <li>optimizer:Adam</li>
  <li>下图展示了QG的输入，黑框内被tokenizer标记为特殊词元</li>
</ul>

<p><img src="../assets/img/posts/20211130/19.jpg" /></p>

<h3 id="2distractor-generation">2)distractor generation</h3>
<ul>
  <li>预训练模型：GPT-2</li>
  <li>数据集：RACE</li>
  <li>tokenizer:BPE<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup></li>
  <li>使用了repetition penalty技术，保证了尽量不会生成相似的text，并且过滤到那些不好的生成（比如生成了空字符串）</li>
  <li>输入：经典的C(context)，A(answer),Q(question)，下图展示了输入格式</li>
</ul>

<p><img src="../assets/img/posts/20211130/20.jpg" /></p>

<h3 id="3qa-filtering">3)QA filtering</h3>
<ul>
  <li>预训练模型：DistilBERT</li>
  <li>网络结构：CQA<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>输入到distilbert，再连接一个dropout，全连接层和softmax，最后输出一个答案，具体结构如下图</li>
</ul>

<p><img src="../assets/img/posts/20211130/21.jpg" /></p>

<h2 id="results">Results</h2>
<h3 id="1quantitative-evaluation">1)quantitative evaluation</h3>
<p>下表中展示了和上一篇论文类似的指标,与现有的模型进行了比较：SEQ2SEQ,HSA<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup>和CHN<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>。可以看出BLEU明显要比之前模型要好，但是ROUGE没有之前的高。</p>

<p><img src="../assets/img/posts/20211130/22.jpg" /></p>

<h3 id="2question-answering-ability">2)question answering ability</h3>
<p>用GPT-2模型生成误导选项再输入到QAmodel中，具体结果见下图。</p>

<p><img src="../assets/img/posts/20211130/23.jpg" /></p>

<h3 id="3human-evaluation">3)human evaluation</h3>
<p>人工评估，从两方面评估distractor生成的好坏：</p>
<ul>
  <li><strong>Is the question well-formed and can you understand the meaning?</strong></li>
  <li><strong>If the question is at least understandable, does the answer make sense in relation to the question?</strong>
评估过程中，使用了155个没有经过QA筛选和155经过QA筛选的，了解一下QA过滤模型的效果。整体来说QA过滤器还是有一点效果，具体结果如下：</li>
</ul>

<p><img src="../assets/img/posts/20211130/24.jpg" /></p>

<h2 id="conclusion-1">conclusion</h2>
<p>我认为作者使用的DG模型主要有两大特色，一个是使用了GPT2预训练模型，目前使用基于transformer的模型已经成为主流。第二个是使用了QA过滤器来筛选掉回答错误的，有一定提升但不显著。</p>

<h1 id="第三篇">第三篇</h1>
<h2 id="title-2">Title</h2>
<p>Generating Distractors for Reading Comprehension Questions from Real Examinations</p>
<h2 id="author-2">Author</h2>
<p>Yifan Gao, Lidong Bing, Piji Li,
Irwin King, Michael R. Lyu</p>
<h2 id="abstract-2">Abstract</h2>
<p>上面两篇文献都有提到这篇文章。作者使用了<strong>Hierarchical encoder-decoder framework</strong> with <strong>static</strong> and <strong>dynamic</strong> attention mechanisms来生成有语义信息的误导选项。使用了编码器-解码器结构网络和静态和动态注意力机制。</p>
<h2 id="framework-description-网络结构">Framework Description 网络结构</h2>
<h3 id="1task-definition">1)Task Definition</h3>
<p>输入：文章，问题和答案。P代表文章，s<sub>1</sub>,s<sub>2</sub>,s<sub>3</sub>…表示不同的句子，q和a分别表示问题和答案，那么我们的任务是生成误导选项$\overline{d}$。</p>

<p><img src="../assets/img/posts/20211130/25.jpg" /></p>

<h3 id="2framework-overview">2)Framework overview</h3>
<p>网络结构如下图所示，下面将从各个组成部分分别介绍：</p>

<p><img src="../assets/img/posts/20211130/26.jpg" /></p>

<h3 id="3hierarchical-encoder">3)Hierarchical encoder</h3>
<ul>
  <li><strong>word embedding</strong>:词嵌入，将每个句子s<sub>i</sub>中的每个词元变成词向量(w<sub>i,1</sub>,w<sub>i,2</sub>,w<sub>i,3</sub>…)</li>
  <li><strong>word encoder</strong>:将句子s<sub>i</sub>的词向量(w<sub>i,1</sub>,w<sub>i,2</sub>,w<sub>i,3</sub>…)作为输入，用<strong>双向LSTM</strong>作为编码器，获得word-level representation h<sub>i,j</sub><sup>e</sup></li>
</ul>

<p><img src="../assets/img/posts/20211130/27.jpg" /></p>

<ul>
  <li><strong>sentence encoder</strong>:将word encoder中每个句子正向LSTM的最后一个隐藏状态和反向LSTM的最开始的隐藏状态作为输入到另一个双向LSTM中获得<strong>sentence-level representation</strong>(u<sub>1</sub>,u<sub>2</sub>,u<sub>3</sub>…)</li>
</ul>

<h3 id="4static-attention-mechanism">4)static attention mechanism</h3>
<p>目的：生成的误导选项必须和问题Q语义相关，但是和答案A必须语义不相关。我们从(s<sub>1</sub>,s<sub>2</sub>,s<sub>3</sub>…)学习到句子的权重分布(γ<sub>1</sub>,γ<sub>2</sub>,γ<sub>3</sub>…)，然后将问题q和答案a作为query。</p>

<h3 id="5encoding-layer">5)encoding layer</h3>
<p>我们希望把问题q，答案a和句子s都变成一样的长度的向量表示，也就是上图中紫色虚线部分。对于q和a，我们用两个独立的双向LSTM来获得(<strong>a</strong><sub>1</sub>,<strong>a</strong><sub>2</sub>…<strong>a</strong><sub>k</sub>)和(<strong>q</strong><sub>1</sub>,<strong>q</strong><sub>2</sub>…<strong>q</strong><sub>l</sub>)，然后用平均池化层平均一下：</p>

<p><img src="../assets/img/posts/20211130/28.jpg" /></p>

<p>对于句子s，我们不用u而用h：</p>

<p><img src="../assets/img/posts/20211130/29.jpg" /></p>

<h3 id="6matching-layer">6)matching layer</h3>
<p>目的：加重与问题q有关的句子，减轻与答案a有关的句子。o<sub>i</sub>表示不同句子的importance score</p>

<p><img src="../assets/img/posts/20211130/30.jpg" /></p>

<h3 id="7nomalization-layer">7)nomalization layer</h3>
<p>目的：有些问题q和一两个句子有关，而有些问题q和很多句子有关，比如summarizing，下面的τ(temperature)就是这个作用</p>

<p><img src="../assets/img/posts/20211130/31.jpg" /></p>

<p><img src="../assets/img/posts/20211130/32.jpg" /></p>

<p>作者介绍static attention mechanism用了很大篇幅</p>

<h3 id="8distractor-decoder">8)distractor decoder</h3>
<p>解码器使用的也是LSTM，但是并没有使用编码器的最后一个隐藏状态作为初始状态，而是定义了一个
<strong>question-based initializer</strong>来让生成的误导选项语法和问题q一致</p>

<h3 id="9question-based-initializer">9)question-based initializer</h3>
<p>定义了一个question LSTM来编码问题q，使用最后一层的cell state和hidden state作为decoder初始状态，同时输入q<sub>last</sub>，表示问题q的最后一个词元。</p>

<h3 id="10dynamic-hierarchical-attention-mechanism">10)dynamic hierarchical attention mechanism</h3>
<p>常规的注意力机制将一篇文章作为长句子，然后decoder的每一个时间步都与encoder中所有的hidden state进行比较，但是这种方法并不适合目前的模型。原因：首先LSTM不能处理这么长的输入，其次，一些问题只与部分句子有关。<br />
目的：每个decoder时间步只关注<strong>重要句子</strong>，作者将这种注意力机制称为动态注意力机制，因为不同的时间步，word-level和sentence-level 注意力分布都不同。<br />
每一个时间步的输入是词元d<sub>t-1</sub>和上一个隐藏状态h<sub>t-1</sub></p>

<p><img src="../assets/img/posts/20211130/33.jpg" /></p>

<p><img src="../assets/img/posts/20211130/34.jpg" /></p>

<p>α和β分别表示word-level,sentence-level权重，最后使用之前静态注意力机制获得的γ来调节α和β</p>

<p><img src="../assets/img/posts/20211130/35.jpg" /></p>

<p><img src="../assets/img/posts/20211130/36.jpg" /></p>

<p>获得上下文变量<strong>c</strong><sub>t</sub></p>

<p><img src="../assets/img/posts/20211130/37.jpg" /></p>

<p>获得attention vector $\tilde{h}$</p>

<p><img src="../assets/img/posts/20211130/38.jpg" /></p>

<h3 id="11training-and-inference">11)training and inference</h3>
<p>损失函数：</p>

<p><img src="../assets/img/posts/20211130/39.jpg" /></p>

<p>生成多个误导选项的方法是束搜索，但是生成的误导选项很相似，作者做了相应的处理方法，但我觉得效果还是很差</p>

<h2 id="experimental-setting-实验设置">experimental setting 实验设置</h2>
<h3 id="1dataset">1)dataset</h3>
<p>RACE数据集，作者做了相应的处理，去掉了很多不合理的和语义不相关的，作者的处理标准是：对于误导选项中的词元，如果它们在文章中出现的次数小于5次，那么将被保留，同时去掉了那些需要在句子中间和句子开始填空的问题。下表展示了处理后的数据集的一些信息：</p>

<p><img src="../assets/img/posts/20211130/40.jpg" /></p>

<h3 id="2implementation-details-1">2)implementation details</h3>
<p>词表：保留了频率最高的50k个词元，同时使用GloVe作为词嵌入预训练模型。其他的细节都可以在文章中看见，这里不一一列出了，主要是超参数的设置。</p>

<h3 id="3baselines-and-ablations">3)baselines and ablations</h3>
<p>与HRED<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>和seq2seq比较</p>

<h2 id="results-and-analysis-结果与分析">results and analysis 结果与分析</h2>

<p><img src="../assets/img/posts/20211130/41.jpg" /></p>

<p>人工评估：</p>

<p><img src="../assets/img/posts/20211130/42.jpg" /></p>

<p>大致过程是这样：四个误导选项，分别来自seq2seq，HRED，作者的模型和原本的误导选项，让英语能力很好的人来选择最适合的选项，得出的结果可以发现，作者的模型生成的误导选项拥有最好的误导效果。</p>

<p>下图直观展示了static attention distribution：</p>

<p><img src="../assets/img/posts/20211130/43.jpg" /></p>

<h2 id="我的看法-1">我的看法</h2>
<p>这篇文章应该是第一个提出用处理后的RACE数据集来处理MCQ问题，处理后的RACE数据集在后面也有很多文献用到，这篇文章使用了seq2seq网络结构同时使用了静态和动态注意力机制，对于网络结构和注意力机制的解释非常完全和详细，虽然这篇文章的效果放到现在来看可能不是最好了，但是它提出来的评估标准可能会成为一个通用的标准。它的数据集和训练代码在<a href="https://github.com/Yifan-Gao/Distractor-Generation-RACE">github</a>上也完全开源。</p>

<h1 id="第四篇">第四篇</h1>
<h2 id="title-3">Title</h2>
<p>Co-attention hierarchical network: Generating coherent long distractors for reading comprehension</p>
<h2 id="author-3">Author</h2>
<p>Xiaorui Zhou, Senlin Luo, Yunfang Wu</p>
<h2 id="abstract-3">Abstract</h2>
<p>这篇文献是针对上一篇Gao的文章(seq2seq)所作的改进。本篇文章提出了Gao的模型的两个问题：1.没有建立文章和问题的关系，他的解决方法是使用<strong>co-attention enhanced hierarchical architecture</strong>来捕获文章和问题之间的关系，让解码器生成更有关联的误导选项。2.没有加重整篇文章和误导选项的关系。作者的解决思路是添加一个额外的语义相关性损失函数，让生成的误导选项与整篇文章更有关联。</p>
<h2 id="proposed-framework-网络结构">Proposed Framework 网络结构</h2>
<h3 id="1notations-and-task-definition">1)notations and task definition</h3>
<p>article T=(s<sub>1</sub>,s<sub>2</sub>…s<sub>k</sub>)，一篇文章有k个句子s，同时每个句子都有不同的长度l，s<sub>i</sub>=(w<sub>i,1</sub>,w<sub>i,2</sub>…w<sub>i,l</sub>)，每个文章有m个问题和z个误导选项，Q=(q<sub>1</sub>,q<sub>2</sub>…q<sub>m</sub>),D=(d<sub>1</sub>,d<sub>2</sub>…d<sub>z</sub>),我们的任务是根据输入的T和Q生成D</p>

<p><img src="../assets/img/posts/20211130/44.jpg" /></p>

<h3 id="2model-overview">2)model overview</h3>
<p>整体结构如下图所示，下面将从各个部分分别介绍：</p>

<p><img src="../assets/img/posts/20211130/45.jpg" /></p>

<h3 id="3encoding-article-and-question">3)encoding article and question</h3>
<p>文章和问题的编码器结构</p>
<ul>
  <li><strong>hierarchical article encoder</strong>
双向LSTM，和上一篇结构很像，很多部分我就简单列个式子。</li>
</ul>

<p><img src="../assets/img/posts/20211130/46.jpg" /></p>

<p>每一句最后的词元来表示整个句子</p>

<p><img src="../assets/img/posts/20211130/47.jpg" /></p>

<p>sentence-level encoder：</p>

<p><img src="../assets/img/posts/20211130/48.jpg" /></p>

<p>同样，用最后一个句子来表示整篇文章</p>

<p><img src="../assets/img/posts/20211130/49.jpg" /></p>

<p>用<strong>H</strong><sup>*</sup>来作为sentence-level representation of article,我们有<strong>H</strong><sub>:t</sub><sup>*</sup>=h<sub>t</sub><sup>s</sup></p>

<p>这样，通过使用两个双向LSTM获得word-level encoding和sentence-level encoding</p>
<ul>
  <li><strong>question encoder</strong></li>
</ul>

<p><img src="../assets/img/posts/20211130/50.jpg" /></p>

<p>用<strong>U</strong><sup>*</sup>来作为word-level representations of question, 我们有<strong>U</strong><sub>:t</sub><sup>*</sup>=h<sub>t</sub><sup>q</sup></p>

<h3 id="4co-attention-between-article-and-question">4)Co-attention between article and question</h3>
<p>Co-attention mechanism就是使用了两个方向的注意力机制，有从article到question的，也有question到article的。<br />
用一个“相似”矩阵S表示H和U的关系：</p>

<p><img src="../assets/img/posts/20211130/51.jpg" /></p>

<p>S<sub>i,j</sub>就表示第i个句子和第j个问题词元的相似性</p>

<p>我们可以获得两个特殊的矩阵<strong>S</strong><sup><strong>Q</strong></sup>和<strong>S</strong><sup><strong>T</strong></sup></p>

<p><img src="../assets/img/posts/20211130/52.jpg" /></p>

<ul>
  <li>article-to-question attention<br />
$\tilde{U}$<sub>:j</sub> = $\sum$ S<sub>i,j</sub><sup>Q</sup>U<sub>:,i</sub></li>
  <li>question-to-article attention</li>
</ul>

<p><img src="../assets/img/posts/20211130/53.jpg" /></p>

<p>最后，将问题的词级表示H，两个方向的注意力结果$\tilde{U}$和$\tilde{H}$结合一下获得G</p>

<p><img src="../assets/img/posts/20211130/54.jpg" /></p>

<h3 id="5merging-sentence-representation">5)Merging sentence representation</h3>

<p><img src="../assets/img/posts/20211130/55.jpg" /></p>

<p>Z表示final representation of sentence-level hidden states</p>

<h3 id="6question-initialization">6)question initialization</h3>
<p>接下来就进入decoder环节，这里的question initialization和上篇文献处理方法相同</p>

<h3 id="7hierarchical-attention">7)hierarchical attention</h3>
<p>不同时间步有不同的句子相关，和上篇文献的处理方法动态注意力机制相同。</p>

<p><img src="../assets/img/posts/20211130/56.jpg" /></p>

<p><img src="../assets/img/posts/20211130/57.jpg" /></p>

<p><img src="../assets/img/posts/20211130/58.jpg" /></p>

<p><img src="../assets/img/posts/20211130/59.jpg" /></p>

<h3 id="8semantic-similarity-loss">8)semantic similarity loss</h3>
<p>目的：获得文章和误导选项的关系。还记得之前定义的e<sub>T</sub>吗，它表示整篇文章，那么我们通过下面的公式可以获得distractor representation:</p>

<p><img src="../assets/img/posts/20211130/60.jpg" /></p>

<p>其中S<sub>M</sub>是decoder最后一个隐藏状态，那么我们通过cos计算相似关系，那么最终的损失函数</p>

<p><img src="../assets/img/posts/20211130/61.jpg" /></p>

<h2 id="experimental-settings">Experimental Settings</h2>
<h3 id="1dataset-1">1)dataset</h3>
<p>使用了上篇文献处理过的RACE数据集。</p>

<h3 id="2baselines-and-evaluation-metrics">2)baselines and evaluation metrics</h3>
<p>与seq2seq，HRED，HCP<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">11</a></sup>，HSA<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">12</a></sup>比较。</p>

<h3 id="3implementation-details">3)implementation details</h3>
<p>网络超参数设置技巧，不展开了</p>

<h2 id="results-and-analysis-结果与分析-1">Results and Analysis 结果与分析</h2>

<p><img src="../assets/img/posts/20211130/62.jpg" /></p>

<p><img src="../assets/img/posts/20211130/63.jpg" /></p>

<p><img src="../assets/img/posts/20211130/64.jpg" /></p>

<p>介绍一下上面这张表，这张表是人工评估的结果，从三个维度分析，分别是fluency,coherence,distracting ability。可以看出作者的模型并不是在所有维度都是最好的。</p>

<p>下图是案例分析：</p>

<p><img src="../assets/img/posts/20211130/65.jpg" /></p>

<h2 id="我的看法-2">我的看法</h2>
<p>这篇文献是基于上一篇文献的方法进行了两个改进：1.关联了整篇文章和问题，解决方法是使用了Co-attention mechanism。2.让distractor和article语义相关，方法是定义了相关性loss。</p>

<h1 id="补充">补充</h1>
<h2 id="race数据集简介">RACE数据集简介</h2>
<p>RACE数据集是一个来源于中学考试题目的大规模阅读理解数据集，包含了大约 28000 个文章以及近 100000 个问题。它的形式类似于英语考试中的阅读理解（选择题），给定一篇文章，通过阅读并理解文章（Passage），针对提出的问题（Question）从四个选项中选择正确的答案（Answers）。</p>
<h2 id="bleu">BLEU</h2>
<p>BLEU是一个评价指标，最开始用于机器翻译任务，定义如下</p>

<p><img src="../assets/img/posts/20211130/66.jpg" /></p>

<p>它的总体思想就是准确率，假如给定标准译文reference，神经网络生成的句子是candidate，句子长度为n，candidate中有m个单词出现在reference，m/n就是bleu的1-gram的计算公式。BLEU还有许多变种。根据n-gram可以划分成多种评价指标，常见的指标有BLEU-1、BLEU-2、BLEU-3、BLEU-4四种，其中n-gram指的是连续的单词个数为n。</p>

<h2 id="rouge">ROUGE</h2>
<p>Rouge(Recall-Oriented Understudy for Gisting Evaluation)，是评估自动文摘以及机器翻译的一组指标。它通过将自动生成的摘要或翻译与一组参考摘要（通常是人工生成的）进行比较计算，得出相应的分值，以衡量自动生成的摘要或翻译与参考摘要之间的“相似度”。它的定义如下：</p>

<p><img src="../assets/img/posts/20211130/67.jpg" /></p>

<p>文献中使用的ROUGE-L是一种变种，L即是LCS(longest common subsequence，最长公共子序列)的首字母，因为Rouge-L使用了最长公共子序列。Rouge-L计算方式如下图：</p>

<p><img src="../assets/img/posts/20211130/68.jpg" /></p>

<p>其中LCS(X, Y)是X和Y的最长公共子序列的长度,m、n分别表示参考摘要和自动摘要的长度（一般就是所含词的个数），R<sub>lcs</sub>,P<sub>lcs</sub>分别表示召回率和准确率。最后的F<sub>lcs</sub>即是我们所说的Rouge-L。</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>distractor generation 误导选项生成，简称DG <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>当我们test时，只需要Sequential MLM decoder来预测。 <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>multi-choice reading comprehension (MRC) model <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>P-MLM <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Answer negative <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Byte-Pair-Encoding <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>context，question，answer <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>hierarchical encoder-decoder model with static attention <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>hierarchical model enhanced with co-attention <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p>hierarchical encoder-decoder <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p>相当于HRED+copy,是基于HRED的网络结构 <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:12" role="doc-endnote">
      <p>就是上篇文献的网络 <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Quehry</name></author><category term="paper" /><summary type="html"><![CDATA[文献整理]]></summary></entry><entry><title type="html">软件方法</title><link href="http://localhost:4000/%E8%BD%AF%E4%BB%B6%E6%96%B9%E6%B3%95.html" rel="alternate" type="text/html" title="软件方法" /><published>2021-11-30T00:00:00+08:00</published><updated>2021-11-30T00:00:00+08:00</updated><id>http://localhost:4000/%E8%BD%AF%E4%BB%B6%E6%96%B9%E6%B3%95</id><content type="html" xml:base="http://localhost:4000/%E8%BD%AF%E4%BB%B6%E6%96%B9%E6%B3%95.html"><![CDATA[<h1 id="目录">目录</h1>
<!-- TOC -->

<ul>
  <li><a href="#目录">目录</a></li>
  <li><a href="#软件方法">软件方法</a>
    <ul>
      <li><a href="#课程要求">课程要求</a>
        <ul>
          <li><a href="#随记">随记</a></li>
        </ul>
      </li>
      <li><a href="#ppt整理">PPT整理</a>
        <ul>
          <li><a href="#1-对象类">1. 对象，类</a></li>
          <li><a href="#2面向对象">2.面向对象</a></li>
          <li><a href="#3java">3.JAVA</a></li>
          <li><a href="#4数据结构">4.数据结构</a></li>
          <li><a href="#5-常用数据结构方法">5. 常用数据结构方法</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<!-- /TOC -->
<h1 id="软件方法">软件方法</h1>
<h2 id="课程要求">课程要求</h2>
<p>学习面向对象这种软件开发方法（目前概念越来越广），通过java来了解面向对象的编程具体怎么实现。</p>

<h3 id="随记">随记</h3>
<ol>
  <li>类，对象：
    <ul>
      <li>对象是类的一个实例</li>
      <li>c语言可以构建面向对象所有的结构</li>
      <li>类集合了属性和方法</li>
    </ul>
  </li>
  <li>面向对象的三大特征：
    <ul>
      <li>封装（encapsulation）:
        <ul>
          <li>private, protected, public</li>
          <li>可作用于属性和方法，一般构造方法和成员方法都是public, 属性都是private</li>
          <li>一般是隐藏对象的属性和实现细节，但是提供方法的接口</li>
          <li>提供公开的方法</li>
          <li>提高了软件开发的效率</li>
        </ul>

        <center><img src="../assets/img/posts/20211130/1.jpg" /></center>
      </li>
      <li>继承（inheritance）：
        <ul>
          <li>子类与父类</li>
          <li>子类自动具有父类属性和方法，添加自己特有的属性和方法，并且子类使用父类的方法也可以覆盖/重写父类方法</li>
          <li>可以实现代码的复用（当然功能不止于此）</li>
        </ul>
      </li>
      <li>多态（polymorphism）：
        <ul>
          <li>父类有多个子类</li>
          <li>子类覆盖/重写父类方法</li>
          <li>相当于是根据实际创建的对象类型动态决定使用哪个方法</li>
          <li>所有的子类都可以看成父类的类型，运行时，系统会自动调用各种子类的方法</li>
          <li>UML可以画出类之间的关系</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>java程序设计
    <ul>
      <li>百分百面向对象
        <ul>
          <li>不存在类以外代码</li>
          <li>只能采用面向对象方法编程</li>
          <li>java文件命名规范
            <ul>
              <li>必须以.java结尾</li>
              <li>源文件中如果只有一个类，文件类必须与该类名相同</li>
              <li>如果有多个类，且没有public类，文件名可与任一类名相同</li>
              <li>有多个类，且有public类，文件名必须与该类名相同</li>
              <li>一个JAVA源文件只能有一个public类，一个文件中只能有一个main主函数</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>静态方法/static，可以直接用类和函数名直接调用，和普通方法的区别是不用new一个示例
        <ul>
          <li>static 方法可以直接调用，abstract方法存在的类肯定是抽象类</li>
          <li>抽象方法不定义具体内容</li>
        </ul>
      </li>
      <li>多态的实现，先定义抽象的（abstract）父类，然后子类继承父类然后定义父类的抽象方法
        <ul>
          <li>通过抽象方法固定通用接口</li>
          <li>子类通过强制实现抽象方法实现多态</li>
          <li>抽象父类可以定义属性和构造函数</li>
          <li>抽象父类不能实例化，只能通过向上转型的方法定义</li>
          <li>抽象父类可以向下转型成子类</li>
          <li>父类的方法一般是抽象方法，不定义具体内容，留给子类定义，父类出现的抽象方法子类必须全部定义</li>
          <li>多态的主要特点就是父类的方法全部是抽象的</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>多态例子</li>
</ol>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">public</span> <span class="kd">class</span> <span class="nc">Test</span> <span class="o">{</span>
    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="nc">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">show</span><span class="o">(</span><span class="k">new</span> <span class="nc">Cat</span><span class="o">());</span>  <span class="c1">// 以 Cat 对象调用 show 方法</span>
      <span class="n">show</span><span class="o">(</span><span class="k">new</span> <span class="nc">Dog</span><span class="o">());</span>  <span class="c1">// 以 Dog 对象调用 show 方法</span>
                
      <span class="nc">Animal</span> <span class="n">a</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Cat</span><span class="o">();</span>  <span class="c1">// 向上转型  </span>
      <span class="n">a</span><span class="o">.</span><span class="na">eat</span><span class="o">();</span>               <span class="c1">// 调用的是 Cat 的 eat</span>
      <span class="nc">Cat</span> <span class="n">c</span> <span class="o">=</span> <span class="o">(</span><span class="nc">Cat</span><span class="o">)</span><span class="n">a</span><span class="o">;</span>        <span class="c1">// 向下转型  </span>
      <span class="n">c</span><span class="o">.</span><span class="na">work</span><span class="o">();</span>        <span class="c1">// 调用的是 Cat 的 work</span>
  <span class="o">}</span>  
            
    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">show</span><span class="o">(</span><span class="nc">Animal</span> <span class="n">a</span><span class="o">)</span>  <span class="o">{</span>
        <span class="n">a</span><span class="o">.</span><span class="na">eat</span><span class="o">();</span>  
        <span class="c1">// 类型判断</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">a</span> <span class="k">instanceof</span> <span class="nc">Cat</span><span class="o">)</span>  <span class="o">{</span>  <span class="c1">// 猫做的事情 </span>
            <span class="nc">Cat</span> <span class="n">c</span> <span class="o">=</span> <span class="o">(</span><span class="nc">Cat</span><span class="o">)</span><span class="n">a</span><span class="o">;</span> <span class="c1">// 向下转型</span>
            <span class="n">c</span><span class="o">.</span><span class="na">work</span><span class="o">();</span>  
        <span class="o">}</span> 
        <span class="k">else</span> <span class="nf">if</span> <span class="o">(</span><span class="n">a</span> <span class="k">instanceof</span> <span class="nc">Dog</span><span class="o">)</span> <span class="o">{</span> <span class="c1">// 狗做的事情 </span>
            <span class="nc">Dog</span> <span class="n">c</span> <span class="o">=</span> <span class="o">(</span><span class="nc">Dog</span><span class="o">)</span><span class="n">a</span><span class="o">;</span>  
            <span class="n">c</span><span class="o">.</span><span class="na">work</span><span class="o">();</span>  
        <span class="o">}</span>  
    <span class="o">}</span>  
<span class="o">}</span>
 
<span class="kd">abstract</span> <span class="kd">class</span> <span class="nc">Animal</span> <span class="o">{</span>  
    <span class="kd">abstract</span> <span class="kt">void</span> <span class="nf">eat</span><span class="o">();</span>  
<span class="o">}</span>  
  
<span class="kd">class</span> <span class="nc">Cat</span> <span class="kd">extends</span> <span class="nc">Animal</span> <span class="o">{</span>  
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">eat</span><span class="o">()</span> <span class="o">{</span>  
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"吃鱼"</span><span class="o">);</span>  
    <span class="o">}</span>  
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">work</span><span class="o">()</span> <span class="o">{</span>  
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"抓老鼠"</span><span class="o">);</span>  
    <span class="o">}</span>  
<span class="o">}</span>  
  
<span class="kd">class</span> <span class="nc">Dog</span> <span class="kd">extends</span> <span class="nc">Animal</span> <span class="o">{</span>  
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">eat</span><span class="o">()</span> <span class="o">{</span>  
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"吃骨头"</span><span class="o">);</span>  
    <span class="o">}</span>  
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">work</span><span class="o">()</span> <span class="o">{</span>  
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"看家"</span><span class="o">);</span>  
    <span class="o">}</span>  
<span class="o">}</span>
</code></pre></div></div>

<h2 id="ppt整理">PPT整理</h2>

<h3 id="1-对象类">1. 对象，类</h3>
<ul>
  <li>使用对象之前要先声明和创建</li>
  <li>类定义了对象的类型，所有对象都是类的实例，所有的类描述了属性和定义了方法</li>
</ul>

<h3 id="2面向对象">2.面向对象</h3>
<ul>
  <li>面向对象的编程有4个特点</li>
  <li><strong>封装</strong>：保护类的属性和方法, 类里面的属性的数据是private的, public的方法定义了对象的接口。权限修饰符: private, default, protected, public。</li>
  <li><strong>继承</strong>：B继承A，重用，修改，添加，A所有的属性都存在于B中，A的方法可以在B中重新定义，B方法的改动不会影响A</li>
  <li><strong>多态</strong>：一个对象属于多个类，通过使用不同类中的方法属于不同的类，父类是抽象类，各个子类继承父类并定义方法，调用的时候根据不同子类调用方法。判断类型是否相同instanceof，声明的时候可以这么声明: A a = new B(),其中B是A的子类, 这种声明方法叫做向上转型。向下转型: B b = (B) a</li>
  <li><strong>动态链接</strong>: 通过PPT上的例子，我感觉和继承很像，这部分需要更深入了解才能明白。</li>
</ul>

<h3 id="3java">3.JAVA</h3>
<ul>
  <li>main的格式:</li>
</ul>

<center><img src="../assets/img/posts/20211130/71.jpg" /></center>

<ul>
  <li>数据类型: int, float, double, char, string, boolean, byte, long, short, JAVA里面也有这些数据类型的类：</li>
</ul>

<center><img src="../assets/img/posts/20211130/72.jpg" /></center>

<p>其中Characte应该为Character</p>

<ul>
  <li>
    <p>JAVA关键字this, super</p>
  </li>
  <li>
    <p>x = bool ? a : b，表示如果bool为true，执行a，如果为false执行b</p>
  </li>
  <li>
    <p>for(Point p : this.getVect())表示遍历</p>
  </li>
  <li>
    <p>exception 异常:</p>
  </li>
</ul>

<p><img src="../assets/img/posts/20211130/69.jpg" /></p>

<p>还有异常的抛出throws</p>

<p>try-catch-finally</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">try</span> 
<span class="o">{</span>  
	    <span class="c1">// 可能会发生异常的程序代码  </span>
<span class="o">}</span> 
<span class="k">catch</span> <span class="o">(</span><span class="nc">Type1</span> <span class="n">id1</span><span class="o">)</span>
<span class="o">{</span>  
	    <span class="c1">// 捕获并处置try抛出的异常类型Type1  </span>
<span class="o">}</span> 
<span class="k">catch</span> <span class="o">(</span><span class="nc">Type2</span> <span class="n">id2</span><span class="o">)</span>
<span class="o">{</span>  
	    <span class="c1">//捕获并处置try抛出的异常类型Type2  </span>
<span class="o">}</span>
<span class="k">finally</span> 
<span class="o">{</span>  
	    <span class="c1">// 无论是否发生异常，都将执行的语句块  </span>
<span class="o">}</span>
</code></pre></div></div>

<p>自定义异常：</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">class</span> <span class="nc">NombreNegatifException</span> <span class="kd">extends</span> <span class="nc">Exception</span>
<span class="o">{</span>
    <span class="kd">public</span> <span class="nf">NombreNegatifException</span><span class="o">()</span>
    <span class="o">{</span>
    <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"Vous avez un nombre négatif !"</span><span class="o">);</span>
    <span class="o">}</span> 
<span class="o">}</span>
</code></pre></div></div>

<p>在类的方法中抛出新的异常：</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">public</span> <span class="kt">int</span> <span class="nf">Count</span><span class="o">()</span> <span class="kd">throws</span> <span class="nc">Exception</span><span class="o">{</span>
    <span class="k">if</span> <span class="o">(...){</span>
        <span class="k">throw</span> <span class="k">new</span> <span class="nf">Exception</span><span class="o">(</span><span class="s">"..."</span><span class="o">);</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<ul>
  <li>文件读写：</li>
</ul>

<p>类FileReader,FileWriter,使用里面的方法read()和write(x)和close()</p>

<p>比如：</p>

<p><img src="../assets/img/posts/20211130/70.jpg" /></p>

<ul>
  <li>枚举类型enum，举例说明</li>
</ul>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">public</span> <span class="kd">enum</span> <span class="nc">Jour</span> 
<span class="o">{</span>
    <span class="no">LUNDI</span><span class="o">,</span> <span class="no">MARDI</span><span class="o">,</span> <span class="no">MERCREDI</span><span class="o">,</span> <span class="no">JEUDI</span><span class="o">,</span> <span class="no">VENDREDI</span><span class="o">,</span> <span class="no">SAMEDI</span><span class="o">,</span> <span class="no">DIMANCHE</span><span class="o">;</span>
<span class="o">}</span>

<span class="kd">class</span> <span class="nc">EssaiJour</span> 
<span class="o">{</span>
    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="nc">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> 
    <span class="o">{</span>
        <span class="nc">Jour</span> <span class="n">jour</span> <span class="o">=</span> <span class="nc">Jour</span><span class="o">.</span><span class="na">valueOf</span><span class="o">(</span><span class="n">args</span><span class="o">[</span><span class="mi">0</span><span class="o">]);</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">jour</span> <span class="o">==</span> <span class="nc">Jour</span><span class="o">.</span><span class="na">SAMEDI</span><span class="o">)</span> <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="s">"fin de semaine : "</span><span class="o">);</span>
        <span class="k">switch</span><span class="o">(</span><span class="n">jour</span><span class="o">)</span> 
        <span class="o">{</span>
            <span class="k">case</span> <span class="no">SAMEDI</span> <span class="o">:</span>
            <span class="k">case</span> <span class="no">DIMANCHE</span> <span class="o">:</span>
            <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"se reposer"</span><span class="o">);</span>
            <span class="k">break</span><span class="o">;</span>
            <span class="k">default</span> <span class="o">:</span>
            <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"travailler"</span><span class="o">);</span>
            <span class="k">break</span><span class="o">;</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<ul>
  <li>接口interface, <strong>迭代器</strong>iterator</li>
</ul>

<p>举例：</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">class</span> <span class="nc">Main</span> 
<span class="o">{</span>
    <span class="nd">@FunctionalInterface</span>
    <span class="kd">public</span> <span class="kd">interface</span> <span class="nc">maFonction</span> 
    <span class="o">{</span>
        <span class="nc">Integer</span> <span class="nf">appliquer</span><span class="o">(</span><span class="nc">Integer</span> <span class="n">p</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="kd">public</span> <span class="kd">static</span> <span class="nc">Vector</span><span class="o">&lt;</span><span class="nc">Integer</span><span class="o">&gt;</span> <span class="nf">transforme</span><span class="o">(</span><span class="nc">Vector</span><span class="o">&lt;</span><span class="nc">Integer</span><span class="o">&gt;</span> <span class="n">v</span><span class="o">,</span> <span class="n">maFonction</span> <span class="n">function</span><span class="o">)</span> 
    <span class="o">{</span>
        <span class="nc">Vector</span><span class="o">&lt;</span><span class="nc">Integer</span><span class="o">&gt;</span> <span class="n">nouveauVect</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Vector</span><span class="o">&lt;</span><span class="nc">Integer</span><span class="o">&gt;();</span>
        <span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span> 
        <span class="o">{</span>
        <span class="n">nouveauVect</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">function</span><span class="o">.</span><span class="na">appliquer</span><span class="o">(</span><span class="n">v</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">i</span><span class="o">)));</span>
        <span class="o">}</span>
        <span class="n">nouveauVect</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">v</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="mi">3</span><span class="o">));</span>
        <span class="k">return</span> <span class="n">nouveauVect</span><span class="o">;</span>
    <span class="o">}</span>

    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="nc">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> 
    <span class="o">{</span>
        <span class="nc">Vector</span><span class="o">&lt;</span><span class="nc">Integer</span><span class="o">&gt;</span> <span class="n">vi</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Vector</span><span class="o">&lt;</span><span class="nc">Integer</span><span class="o">&gt;(</span><span class="mi">4</span><span class="o">);</span>
        <span class="n">vi</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="mi">1</span><span class="o">);</span> <span class="n">vi</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="mi">4</span><span class="o">);</span> <span class="n">vi</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="mi">83</span><span class="o">);</span> <span class="n">vi</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="mi">18</span><span class="o">);</span>
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"Les valeurs du vecteur initial : "</span><span class="o">);</span>
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">vi</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="mi">0</span><span class="o">)+</span><span class="s">" "</span><span class="o">);</span> <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">vi</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="mi">1</span><span class="o">)+</span><span class="s">" "</span><span class="o">);</span> 
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">vi</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="mi">2</span><span class="o">)+</span><span class="s">" "</span><span class="o">);</span> <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="n">vi</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="mi">3</span><span class="o">));</span>
        <span class="n">vi</span> <span class="o">=</span> <span class="n">transforme</span><span class="o">(</span><span class="n">vi</span><span class="o">,</span><span class="n">s</span> <span class="o">-&gt;</span> <span class="o">(</span><span class="n">s</span> <span class="o">*</span> <span class="mi">2</span><span class="o">));</span>
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"Les valeurs du vecteur modifié : "</span><span class="o">);</span>
        <span class="nc">Iterator</span> <span class="n">iter</span> <span class="o">=</span> <span class="n">vi</span><span class="o">.</span><span class="na">iterator</span><span class="o">();</span>
        <span class="k">while</span> <span class="o">(</span><span class="n">iter</span><span class="o">.</span><span class="na">hasNext</span><span class="o">())</span>
        <span class="o">{</span>
            <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">iter</span><span class="o">.</span><span class="na">next</span><span class="o">()</span> <span class="o">+</span> <span class="s">""</span><span class="o">);</span>
        <span class="o">}</span>
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">();</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<h3 id="4数据结构">4.数据结构</h3>
<ul>
  <li>数据结构一般含有以下功能：创建，插入，寻找，删除，排序</li>
  <li>二维数组,举例说明：</li>
</ul>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">String</span><span class="o">[][]</span> <span class="n">tab</span><span class="o">={</span> <span class="o">{</span><span class="s">"a"</span><span class="o">,</span> <span class="s">"e"</span><span class="o">,</span> <span class="s">"i"</span><span class="o">,</span> <span class="s">"o"</span><span class="o">,</span> <span class="s">"u"</span><span class="o">},</span> <span class="o">{</span><span class="s">"1"</span><span class="o">,</span> <span class="s">"2"</span><span class="o">,</span> <span class="s">"3"</span><span class="o">,</span> <span class="s">"4"</span><span class="o">}</span> <span class="o">};</span>
<span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">,</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span>
<span class="k">for</span><span class="o">(</span><span class="nc">String</span><span class="o">[]</span> <span class="n">sousTab</span> <span class="o">:</span> <span class="n">tab</span><span class="o">)</span> 
<span class="o">{</span>
    <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span>
    <span class="k">for</span><span class="o">(</span><span class="nc">String</span> <span class="n">str</span> <span class="o">:</span> <span class="n">sousTab</span><span class="o">)</span> 
    <span class="o">{</span> 
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"Valeur du tableau à l'indice ["</span><span class="o">+</span><span class="n">i</span><span class="o">+</span><span class="s">"]["</span><span class="o">+</span><span class="n">j</span><span class="o">+</span><span class="s">"]: "</span> <span class="o">+</span> <span class="n">tab</span><span class="o">[</span><span class="n">i</span><span class="o">][</span><span class="n">j</span><span class="o">]);</span>
        <span class="n">j</span><span class="o">++;</span>
    <span class="o">}</span>
    <span class="n">i</span><span class="o">++;</span>
<span class="o">}</span>
</code></pre></div></div>

<p>声明数组：数组类型加变量名</p>

<p>或者</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span><span class="o">[]</span> <span class="n">tabEntiers</span> <span class="o">;</span>
<span class="n">tabEntiers</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">int</span><span class="o">[</span><span class="mi">40</span><span class="o">]</span> <span class="o">;</span> 
<span class="c1">// création effective du tableau précédent</span>
</code></pre></div></div>

<ul>
  <li>列表，包含ArrayList, LinkedList</li>
</ul>

<p>ArrayList是实现了基于动态数组的数据结构，LinkedList基于链表的数据结构。对于随机访问get和set，ArrayList优于LinkedList，因为ArrayList可以随机定位，而LinkedList要移动指针一步一步的移动到节点处。（参考数组与链表来思考）。对于新增和删除操作add和remove，LinedList比较占优势，只需要对指针进行修改即可，而ArrayList要移动数据来填补被删除的对象的空间。</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">public</span> <span class="kd">class</span> <span class="nc">Liste</span><span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> 
<span class="o">{</span>
    <span class="kd">protected</span> <span class="no">T</span> <span class="n">valeur</span><span class="o">;</span> 
    <span class="kd">protected</span> <span class="nc">Liste</span><span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="n">succ</span><span class="o">;</span>
    <span class="kd">protected</span> <span class="nc">Liste</span><span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="n">pred</span><span class="o">;</span>
    
    <span class="kd">public</span> <span class="no">T</span> <span class="nf">valeur</span><span class="o">()</span>
    <span class="o">{</span> 
        <span class="k">return</span> <span class="n">valeur</span><span class="o">;</span>
    <span class="o">}</span>

    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">changerValeur</span><span class="o">(</span><span class="no">T</span> <span class="n">x</span><span class="o">)</span>
    <span class="o">{</span> 
        <span class="n">valeur</span> <span class="o">=</span> <span class="n">x</span><span class="o">;</span> 
    <span class="o">}</span>

    <span class="kd">public</span> <span class="nc">Liste</span><span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="nf">succ</span><span class="o">()</span>
    <span class="o">{</span> 
        <span class="k">return</span> <span class="n">succ</span><span class="o">;</span> 
    <span class="o">}</span>
    
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">changerSucc</span><span class="o">(</span><span class="nc">Liste</span><span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="n">y</span><span class="o">)</span>
    <span class="o">{</span> 
        <span class="n">succ</span> <span class="o">=</span> <span class="n">y</span><span class="o">;</span> 
    <span class="o">}</span>

    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">changerPred</span><span class="o">(</span><span class="nc">Liste</span><span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="n">y</span><span class="o">)</span>
    <span class="o">{</span> 
        <span class="n">pred</span> <span class="o">=</span> <span class="n">y</span><span class="o">;</span> 
    <span class="o">}</span>
<span class="o">}</span> 
</code></pre></div></div>

<p>这是一个链表的简写，每一层包含了上一个元素，这一个元素，下一个元素</p>

<ul>
  <li>哈希表，通过建立KV关系查找，相比于之前的顺序访问或者其他指数访问要快。</li>
</ul>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">java.util.HashMap</span><span class="o">;</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">TestHash</span> 
<span class="o">{</span>
    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="nc">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> 
    <span class="o">{</span>
        <span class="nc">HashMap</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">annuaire</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">HashMap</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span><span class="nc">String</span><span class="o">&gt;();</span>
        <span class="c1">// ajout des valeurs</span>
        <span class="n">annuaire</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">"Alfred"</span><span class="o">,</span><span class="s">"2399020806"</span><span class="o">);</span>
        <span class="n">annuaire</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">"Daniel"</span><span class="o">,</span> <span class="s">"2186000000"</span><span class="o">);</span>
        <span class="c1">// obtention d'un numéro</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">annuaire</span><span class="o">.</span><span class="na">containsKey</span><span class="o">(</span><span class="s">"Danielle"</span><span class="o">))</span> 
        <span class="o">{</span>
            <span class="nc">String</span> <span class="n">num</span> <span class="o">=</span> <span class="n">annuaire</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="s">"Danielle"</span><span class="o">);</span>
            <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="nc">Danielle</span> <span class="o">:</span> <span class="s">"+num"</span><span class="o">);</span>
        <span class="o">}</span>
        <span class="k">else</span> 
        <span class="o">{</span>
            <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"pas trouve"</span><span class="o">);</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<ul>
  <li>树状结构tree</li>
</ul>

<p>一般包含结点，结点的度(该结点下有多少子树的数目)，树的度</p>

<p>不同的遍历方法：</p>
<blockquote>
  <p>前序遍历，首先结点，然后左子树，右子树<br />
中序遍历，左子树，结点，右子树<br />
后序遍历，左子树，右子树，结点<br />
层序遍历，从上到下，从左到右</p>
</blockquote>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">class</span> <span class="nc">Arbre</span> 
<span class="o">{</span>
    <span class="kd">protected</span> <span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="n">valeur</span><span class="o">;</span>
    <span class="kd">protected</span> <span class="nc">Arbre</span> <span class="n">filsGauche</span><span class="o">,</span> <span class="n">filsDroit</span><span class="o">;</span> 
    <span class="kd">public</span> <span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="nf">valeur</span><span class="o">()</span> 
    <span class="o">{</span> 
        <span class="k">return</span> <span class="n">valeur</span><span class="o">;</span> 
    <span class="o">}</span>
    <span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">existeFilsGauche</span><span class="o">()</span> <span class="o">{</span> <span class="k">return</span> <span class="n">filsGauche</span> <span class="o">!=</span> <span class="kc">null</span><span class="o">;</span> <span class="o">}</span> 
    <span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">existeFilsDroit</span><span class="o">()</span> <span class="o">{</span> <span class="k">return</span> <span class="n">filsDroit</span> <span class="o">!=</span> <span class="kc">null</span><span class="o">;</span> <span class="o">}</span>
    <span class="kd">public</span> <span class="nc">Arbre</span> <span class="nf">filsGauche</span><span class="o">()</span> <span class="o">{</span> <span class="k">return</span> <span class="n">filsGauche</span><span class="o">;</span> <span class="o">}</span>
    <span class="kd">public</span> <span class="nc">Arbre</span> <span class="nf">filsDroit</span><span class="o">()</span> <span class="o">{</span> <span class="k">return</span> <span class="n">filsDroit</span><span class="o">;</span> <span class="o">}</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">affecterValeur</span><span class="o">(&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="n">c</span><span class="o">)</span> <span class="o">{</span> <span class="n">valeur</span> <span class="o">=</span> <span class="n">c</span><span class="o">;</span> <span class="o">}</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">affecterFilsGauche</span><span class="o">(</span><span class="nc">Arbre</span> <span class="n">g</span><span class="o">)</span> <span class="o">{</span> <span class="n">filsGauche</span> <span class="o">=</span> <span class="n">g</span><span class="o">;</span> <span class="o">}</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">affecterFilsDroit</span><span class="o">(</span><span class="nc">Arbre</span> <span class="n">d</span><span class="o">)</span> <span class="o">{</span> <span class="n">filsDroit</span> <span class="o">=</span> <span class="n">d</span><span class="o">;}</span>
    <span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">feuille</span><span class="o">()</span> <span class="o">{</span><span class="k">return</span> <span class="o">(</span><span class="n">filsDroit</span><span class="o">==</span><span class="kc">null</span> <span class="o">&amp;&amp;</span> 
    <span class="n">filsGauche</span><span class="o">==</span><span class="kc">null</span><span class="o">);</span> 
<span class="o">}</span>

<span class="kd">public</span> <span class="kt">int</span> <span class="nf">hauteur</span><span class="o">()</span> 
<span class="o">{</span>
    <span class="kt">int</span> <span class="n">g</span> <span class="o">=</span> <span class="n">existeFilsGauche</span><span class="o">()</span> <span class="o">?</span> <span class="n">filsGauche</span><span class="o">.</span><span class="na">hauteur</span><span class="o">()</span> <span class="o">:</span> <span class="mi">0</span><span class="o">;</span>
    <span class="kt">int</span> <span class="n">d</span> <span class="o">=</span> <span class="n">existeFilsDroit</span><span class="o">()</span> <span class="o">?</span> <span class="n">filsDroit</span><span class="o">.</span><span class="na">hauteur</span><span class="o">()</span> <span class="o">:</span> <span class="mi">0</span><span class="o">;</span>
    <span class="k">return</span> <span class="nc">Math</span><span class="o">.</span><span class="na">max</span><span class="o">(</span><span class="n">g</span><span class="o">,</span><span class="n">d</span><span class="o">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">;</span>
<span class="o">}</span>
<span class="c1">// Constructeurs</span>
<span class="kd">public</span> <span class="nf">Arbre</span><span class="o">(</span><span class="no">T</span> <span class="n">val</span><span class="o">)</span> 
<span class="o">{</span>
    <span class="n">valeur</span> <span class="o">=</span> <span class="n">val</span><span class="o">;</span>
    <span class="n">filsGauche</span> <span class="o">=</span> <span class="n">filsDroit</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
<span class="o">}</span>
<span class="kd">public</span> <span class="nf">Arbre</span><span class="o">(</span><span class="no">T</span> <span class="n">val</span><span class="o">,</span> <span class="nc">Arbre</span><span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="n">g</span><span class="o">,</span> <span class="nc">Arbre</span><span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="n">d</span><span class="o">)</span> 
<span class="o">{</span>
    <span class="n">valeur</span> <span class="o">=</span> <span class="n">val</span><span class="o">;</span>
    <span class="n">filsGauche</span> <span class="o">=</span> <span class="n">g</span><span class="o">;</span> <span class="n">filsDroit</span> <span class="o">=</span> <span class="n">d</span><span class="o">;</span>
<span class="o">}</span>

<span class="c1">// Affichage</span>
<span class="kd">public</span> <span class="kt">void</span> <span class="nf">afficherPrefixe</span><span class="o">()</span> 
<span class="o">{</span>
    <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">valeur</span><span class="o">+</span><span class="s">"\t"</span><span class="o">);</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">existeFilsGauche</span><span class="o">())</span> <span class="n">filsGauche</span><span class="o">.</span><span class="na">afficherPrefixe</span><span class="o">();</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">existeFilsDroit</span><span class="o">())</span> <span class="n">filsDroit</span><span class="o">.</span><span class="na">afficherPrefixe</span><span class="o">();</span>
<span class="o">}</span>

<span class="kd">public</span> <span class="kt">void</span> <span class="nf">afficherInfixe</span><span class="o">()</span> 
<span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">existeFilsGauche</span><span class="o">())</span> <span class="n">filsGauche</span><span class="o">.</span><span class="na">afficherInfixe</span><span class="o">();</span>
    <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">valeur</span><span class="o">+</span><span class="s">"\t"</span><span class="o">);</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">existeFilsDroit</span><span class="o">())</span><span class="n">filsDroit</span><span class="o">.</span><span class="na">afficherInfixe</span><span class="o">();</span>
<span class="o">}</span>

<span class="kd">public</span> <span class="kt">void</span> <span class="nf">afficherPostfixe</span><span class="o">()</span> 
<span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">existeFilsGauche</span><span class="o">())</span> <span class="n">filsGauche</span><span class="o">.</span><span class="na">afficherPostfixe</span><span class="o">();</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">existeFilsDroit</span><span class="o">())</span><span class="n">filsDroit</span><span class="o">.</span><span class="na">afficherPostfixe</span><span class="o">();</span>
    <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">valeur</span><span class="o">+</span><span class="s">"\t"</span><span class="o">);</span>
<span class="o">}</span>
</code></pre></div></div>

<p>二叉排序树是指左子树小于结点小于右子树，而且结点值不重复。判断是否为二叉排序树：</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">superieur</span><span class="o">(</span><span class="kt">char</span> <span class="n">x</span><span class="o">)</span> 
<span class="o">{</span>
<span class="c1">// vrai si x est supérieur à tous les éléments de l’arbre</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">feuille</span><span class="o">())</span> <span class="k">return</span> <span class="o">(</span><span class="n">x</span><span class="o">&gt;=</span><span class="n">valeur</span><span class="o">);</span>
    <span class="k">else</span> <span class="nf">return</span> 
    <span class="o">(((</span><span class="k">this</span><span class="o">.</span><span class="na">existeFilsGauche</span><span class="o">())?</span> <span class="o">(</span><span class="k">this</span><span class="o">.</span><span class="na">filsGauche</span><span class="o">).</span><span class="na">superieur</span><span class="o">(</span><span class="n">x</span><span class="o">):</span><span class="kc">true</span><span class="o">)</span> <span class="o">&amp;</span> 
     <span class="o">((</span><span class="k">this</span><span class="o">.</span><span class="na">existeFilsDroit</span><span class="o">())?</span> <span class="o">(</span><span class="k">this</span><span class="o">.</span><span class="na">filsDroit</span><span class="o">).</span><span class="na">superieur</span><span class="o">(</span><span class="n">x</span><span class="o">):</span><span class="kc">true</span><span class="o">));</span>
<span class="o">}</span> 
<span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">inferieur</span><span class="o">(</span><span class="kt">char</span> <span class="n">x</span><span class="o">)</span> <span class="o">{</span><span class="c1">//similaire a superieur ... }</span>
<span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">binrech</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">feuille</span><span class="o">())</span> <span class="k">return</span> <span class="kc">true</span><span class="o">;</span>
    <span class="k">else</span> <span class="nf">return</span>
    <span class="o">((</span><span class="n">existeFilsGauche</span><span class="o">()?(</span><span class="n">filsGauche</span><span class="o">.</span><span class="na">superieur</span><span class="o">(</span><span class="n">valeur</span><span class="o">)</span> <span class="o">&amp;&amp;</span> <span class="n">filsGauche</span><span class="o">.</span><span class="na">binrech</span><span class="o">()):</span><span class="kc">true</span><span class="o">)</span> <span class="o">&amp;</span> 
     <span class="o">(</span><span class="n">existeFilsDroit</span><span class="o">()?(</span><span class="n">filsDroit</span><span class="o">.</span><span class="na">inferieur</span><span class="o">(</span><span class="n">valeur</span><span class="o">)</span> <span class="o">&amp;&amp;</span> <span class="n">filsDroit</span><span class="o">.</span><span class="na">binrech</span><span class="o">()):</span><span class="kc">true</span><span class="o">));</span>
<span class="o">}</span> 
</code></pre></div></div>

<h3 id="5-常用数据结构方法">5. 常用数据结构方法</h3>

<p>二维数组array[][]的定义和访问</p>

<table>
  <tbody>
    <tr>
      <td>数据结构</td>
      <td>vector&lt;String&gt;</td>
      <td>ArrayList&lt;String&gt;</td>
      <td>LinkedList&lt;String&gt;</td>
      <td>HashMap&lt;String,int&gt;</td>
    </tr>
    <tr>
      <td>添加</td>
      <td>add(i, str)</td>
      <td>add(i,str)</td>
      <td>add(i,str)</td>
      <td>put(str,i)</td>
    </tr>
    <tr>
      <td>查找</td>
      <td>get(i)</td>
      <td>get(i)</td>
      <td>get(i)</td>
      <td>get(str)</td>
    </tr>
    <tr>
      <td>索引</td>
      <td>indexOf(str)</td>
      <td>indexOf(str)</td>
      <td>indexOf(str)</td>
      <td> </td>
    </tr>
    <tr>
      <td>删除</td>
      <td>remove(i)/remove(str)</td>
      <td>remove(i/str)</td>
      <td>remove(i/str)</td>
      <td>remove(i)</td>
    </tr>
    <tr>
      <td>清除</td>
      <td>clear()</td>
      <td>clear()</td>
      <td>clear()</td>
      <td> </td>
    </tr>
    <tr>
      <td>查看大小</td>
      <td>size()</td>
      <td>size()</td>
      <td>size()</td>
      <td>size()</td>
    </tr>
    <tr>
      <td>迭代器</td>
      <td>iterator()</td>
      <td>iterator()</td>
      <td>iterator()</td>
      <td> </td>
    </tr>
    <tr>
      <td>变成数组</td>
      <td> </td>
      <td>toArray()</td>
      <td>toArray()</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>hashmap还可以返回键值对entryset()，也可以判断是否含有key和value，containsKey(),containsValue()</p>]]></content><author><name>Quehry</name></author><category term="school" /><summary type="html"><![CDATA[课堂记录]]></summary></entry><entry><title type="html">课程总结</title><link href="http://localhost:4000/%E5%A4%A7%E5%9B%9B%E4%B8%8A%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93.html" rel="alternate" type="text/html" title="课程总结" /><published>2021-11-28T00:00:00+08:00</published><updated>2021-11-28T00:00:00+08:00</updated><id>http://localhost:4000/%E5%A4%A7%E5%9B%9B%E4%B8%8A%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93</id><content type="html" xml:base="http://localhost:4000/%E5%A4%A7%E5%9B%9B%E4%B8%8A%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93.html"><![CDATA[<!-- TOC -->

<ul>
  <li><a href="#概率统计">概率统计</a>
    <ul>
      <li><a href="#简介">简介</a></li>
      <li><a href="#内容总览">内容总览</a></li>
    </ul>
  </li>
  <li><a href="#流体力学">流体力学</a>
    <ul>
      <li><a href="#简介-1">简介</a></li>
      <li><a href="#内容总览-1">内容总览</a></li>
      <li><a href="#a4纸">A4纸</a></li>
      <li><a href="#报告">报告</a></li>
    </ul>
  </li>
  <li><a href="#电磁辐射波">电磁辐射波</a>
    <ul>
      <li><a href="#简介-2">简介</a></li>
      <li><a href="#内容总览-2">内容总览</a></li>
    </ul>
  </li>
  <li><a href="#传感器">传感器</a>
    <ul>
      <li><a href="#简介-3">简介</a></li>
      <li><a href="#内容总览-3">内容总览</a></li>
    </ul>
  </li>
  <li><a href="#结构力学">结构力学</a>
    <ul>
      <li><a href="#简介-4">简介</a></li>
      <li><a href="#内容总览-4">内容总览</a></li>
      <li><a href="#a4纸-1">A4纸</a></li>
    </ul>
  </li>
  <li><a href="#项目管理">项目管理</a>
    <ul>
      <li><a href="#简介-5">简介</a></li>
      <li><a href="#内容总览-5">内容总览</a></li>
    </ul>
  </li>
  <li><a href="#工程热力学">工程热力学</a>
    <ul>
      <li><a href="#简介-6">简介</a></li>
      <li><a href="#报告pre">报告Pre</a></li>
      <li><a href="#内容总览-6">内容总览</a></li>
    </ul>
  </li>
  <li><a href="#presse">Presse</a>
    <ul>
      <li><a href="#简介-7">简介</a></li>
      <li><a href="#内容总览-7">内容总览</a></li>
    </ul>
  </li>
  <li><a href="#audiovisual">Audiovisual</a>
    <ul>
      <li><a href="#简介-8">简介</a></li>
      <li><a href="#内容总览-8">内容总览</a></li>
    </ul>
  </li>
  <li><a href="#软件方法">软件方法</a>
    <ul>
      <li><a href="#简介-9">简介</a></li>
      <li><a href="#内容总览-9">内容总览</a></li>
    </ul>
  </li>
  <li><a href="#机器人">机器人</a>
    <ul>
      <li><a href="#简介-10">简介</a></li>
      <li><a href="#内容总览-10">内容总览</a></li>
      <li><a href="#报告-1">报告</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h1 id="概率统计">概率统计</h1>

<h2 id="简介">简介</h2>
<ul>
  <li>授课老师：牛薇</li>
  <li>授课材料：一份法语讲义，一份习题集（10个EX），上课用的PPT</li>
  <li>B站有录播，up主：却道成归</li>
  <li>笔记记在侧边栏为大四上A的笔记本最前面</li>
</ul>

<h2 id="内容总览">内容总览</h2>

<p>一半时间概率一半时间统计</p>

<ol>
  <li>概率
    <ul>
      <li>先从之前学的概率空间讲起，介绍了概率分布（离散or连续），密度函数，期望方差，收敛性。</li>
      <li>估计，比如说用平均值估计期望，用频率估计概率等等</li>
      <li>估计又分为点估计和区间估计，点估计中介绍了似然函数以及最大似然法来找估计量</li>
    </ul>
  </li>
  <li>统计
    <ul>
      <li>主要介绍了几种检验方法来检验分布、估计量选择的好坏</li>
      <li>包括了参数检验，分布检验，比较检验等等</li>
    </ul>
  </li>
  <li>A4纸</li>
</ol>

<p><img src="../assets/img/posts/20211128/1.jpg" /></p>

<p><img src="../assets/img/posts/20211128/2.jpg" /></p>

<h1 id="流体力学">流体力学</h1>

<h2 id="简介-1">简介</h2>
<ul>
  <li>授课老师：方乐</li>
  <li>授课材料：PPT，TD都是6个，分别对应六大章</li>
  <li>B站有录播</li>
  <li>笔记在侧边栏为大四上A的中后部分和大四上B前面</li>
</ul>

<h2 id="内容总览-1">内容总览</h2>

<p>第一章主要讲了流体的概念和动力学的公式。第二章从能量角度出发，介绍了NS方程（斯托克斯方程），和伯努利原理（压强和流速的关系）。第三章介绍了雷诺数，无量纲分析，雷诺数大的是湍流，雷诺数小的是层流。第四章介绍了边界层，第五章介绍了湍流，系统平均。第六章介绍了涡量。</p>

<h2 id="a4纸">A4纸</h2>

<p><img src="../assets/img/posts/20211128/3.jpg" /></p>

<p><img src="../assets/img/posts/20211128/4.jpg" /></p>

<h2 id="报告">报告</h2>
<p>结课之前需要我们写一个报告，什么形式的都可以，我觉得这种方式挺好的，自由发挥，我做的实验，用牛奶和墨水还原了卡门涡街。</p>

<h1 id="电磁辐射波">电磁辐射波</h1>

<h2 id="简介-2">简介</h2>
<ul>
  <li>授课老师: José Penuelas(负责前几章教学), Bertrand Vilquin(负责后几章教学), 孙鸣捷老师(负责TD)</li>
  <li>授课形式：线上讲解原理，线下TD</li>
  <li>授课材料：PPT，讲义，TD</li>
  <li>B站有录播</li>
  <li>有笔记，侧边栏叫做电磁学(大四上)</li>
  <li>考试闭卷，所以没有A4纸</li>
</ul>

<h2 id="内容总览-2">内容总览</h2>
<p>首先回顾了之前学的波动物理和电磁学，电磁辐射，顾名思义是要将辐射，讲了波导，腔和光电效应，能级跃迁等等</p>

<h1 id="传感器">传感器</h1>

<h2 id="简介-3">简介</h2>
<ul>
  <li>授课老师：徐平</li>
  <li>授课形式：线下授课，做实验</li>
  <li>授课材料：大学生MOOC</li>
  <li>没有考试，没有笔记</li>
</ul>

<h2 id="内容总览-3">内容总览</h2>
<p>讲解了传感器的基本原理，构造和常见传感器，每节课都需要在MOOC上做题，也有安排答辩，我和蔡卓江、宋正浩、刘亚林、马卫一一组讲解了机器狗。做实验是指去214玩小车，上面有不少传感器，也有大疆的线上模拟器，还是挺不错的一次动手实验。</p>

<h1 id="结构力学">结构力学</h1>

<h2 id="简介-4">简介</h2>
<ul>
  <li>授课老师：黄行蓉，Jean-Piere Lainé</li>
  <li>授课形式：J-P录制ppt，黄老师线下授课</li>
  <li>授课材料：讲义，PPT，TD</li>
  <li>B站有录播</li>
  <li>笔记：侧边栏结构力学，还有最后第八章记在大四上C前面</li>
</ul>

<h2 id="内容总览-4">内容总览</h2>
<p>结构力学分为了两大部分，弹性力学和材料力学。在弹性力学部分，首先介绍了应力和应力张量的概念，张量可以写成3*3矩阵形式，其中对角线上的元素被称作正应力。第二章介绍了应变，首先介绍了很多种张量，F、H、C、E，然后介绍了形变张量ε。第三章介绍了本构方程（应力应变关系方程）。第四章介绍了能量，包括最小势能和最大余能等等。</p>

<p>第二部分是材料力学，主题内容和弹性力学类似，但是引进了力螺旋的概念，这个概念在中国授课好像是没有的，它描述了合力和力矩。第一张介绍了内力，在材料力学部分我们主要研究梁这个结构，它包括了中轴线和截面，这部分内容和之前学的理论力学很相似。第二章介绍了应力，可以用内力表示应力，用一些惯性矩、艾力函数连接。第三章介绍了应变和本构方程，第四章介绍了能量部分，主要是三大定理：théorème de ménabréa;théorème de maxwell-betti;théorème de castigliano。</p>

<h2 id="a4纸-1">A4纸</h2>

<p><img src="../assets/img/posts/20211128/5.jpg" /></p>

<p><img src="../assets/img/posts/20211128/6.jpg" /></p>

<p><img src="../assets/img/posts/20211128/7.jpg" /></p>

<p><img src="../assets/img/posts/20211128/8.jpg" /></p>

<p><img src="../assets/img/posts/20211128/9.jpg" /></p>

<p><img src="../assets/img/posts/20211128/10.jpg" /></p>

<h1 id="项目管理">项目管理</h1>

<h2 id="简介-5">简介</h2>
<ul>
  <li>授课老师：张敏</li>
  <li>授课形式：线下授课</li>
  <li>授课材料：书(没买)，PPT</li>
  <li>B站有录播</li>
  <li>开卷考试，没有笔记</li>
</ul>

<h2 id="内容总览-5">内容总览</h2>
<p>是上学期经济管理的一部分展开讲，讲了什么是项目，项目管理系统，基于关键路径的项目管理，项目管理的决策</p>

<h1 id="工程热力学">工程热力学</h1>

<h2 id="简介-6">简介</h2>
<ul>
  <li>授课老师：Guillaume Merle</li>
  <li>授课形式：看课本，付小尧定期答疑</li>
  <li>授课材料：课本、PPT</li>
  <li>闭卷考试，不能带A4纸，有三次小测</li>
  <li>纯英文授课</li>
</ul>

<h2 id="报告pre">报告Pre</h2>
<ul>
  <li>报告题目：SABRE发动机效率分析</li>
</ul>

<h2 id="内容总览-6">内容总览</h2>
<p>一共学习了课本上第1、2、3、4、5、6、7、9、10、11、12章内容，其中第一章介绍热力学基本单位和概念，第二章介绍能量转移的形式是功和热，顺便引出热力学第一定律，第三章介绍纯物质的相和相变，第四章介绍封闭系统的能量变化，即质量不变体积可变的系统，第五章介绍开放系统的质量变化，即体积可变质量不变，第六章介绍热机的效率基本概念，第七章介绍熵的概念以及热力学第二定律，到这为止都是之前热力学学过的东西，第九章介绍了内燃机的工作原理，包括了很多个不同的循环，比如otto、diesel、brayton循环。第十章介绍了热电机的原理和ranking循环。第十一章介绍了冰箱和热泵的基本原理。第十二章是用到的数学表达式的推理。</p>

<h1 id="presse">Presse</h1>
<h2 id="简介-7">简介</h2>
<ul>
  <li>法语课程</li>
  <li>授课老师:Vanessa</li>
  <li>授课形式：线下授课</li>
  <li>授课材料：讲义</li>
  <li>闭卷考试</li>
  <li>期间写过几次PE</li>
</ul>

<h2 id="内容总览-7">内容总览</h2>
<ul>
  <li>Règle de la classe</li>
  <li>Vingt ans qui ont déjà tout changé: 11 septembre(effodrement des Tours Jumelles), l’incendie de la cathédrale Notre-Dame de Paris, les confinements en France et dans le monde, la mort de Michael Jackson, l’accident nucléaire de Fukushima</li>
  <li>Le 11 Septembre apparaît comme la matrice du XXIe siècle</li>
  <li>les personnalités les plus marquantes des 20 dernières années: Barack Obama, Donald Trump, Oussama Ben Laden, Emmanuel Macron, Angela Merkel… François Hollande</li>
  <li>les chanteurs: Johnny Hallydat, Céline Dion, Stromae</li>
  <li>le confinement en France: haltères, machine à pain, graines de tomates…</li>
  <li>des solutions pour prospérer et progresser: Cité flottante, Télétravail, Agriculture spatiale, le train intelligent</li>
  <li>la voiture autonome ne tient pas encore la route: sensible, risque</li>
  <li>le tourisme spatial, bonne ou mauvaise idée: huit, Elon Musk</li>
  <li>l’agence spatiale européenne recrute une nouvelle promotion, Feriez-vous un bon astronaute</li>
  <li>les Français et Internet</li>
  <li>la 5G vous inquiète-t-elle?</li>
  <li>la 5G, amie ou ennemie?</li>
  <li>Sept innovations contre le handicap: le gant qui traduit la langue des signes, les lunettes qui parlent aux malvoyants</li>
  <li>Jeux vidéo</li>
  <li>Bientôt de l’e-sport aux Jeux Olympiques</li>
</ul>

<h1 id="audiovisual">Audiovisual</h1>
<h2 id="简介-8">简介</h2>
<ul>
  <li>法语课程</li>
  <li>授课老师：Fabien</li>
  <li>授课形式：瞩目线上</li>
  <li>授课材料：讲义</li>
  <li>闭卷考试</li>
</ul>

<h2 id="内容总览-8">内容总览</h2>
<ul>
  <li>Séance 1 - les étudiants français sous la Covid-19</li>
  <li>Séance 2 - Afghanistan: partir ou rester</li>
  <li>Séance 3 - Réalité Virtuelle: une méthode d’emphaie</li>
  <li>Séance 4 - Utiliser des produits ménagers bio</li>
  <li>Séance 5 - Pandémies et environnement</li>
  <li>Séance 6 - Les transports du futur</li>
  <li>Séance 7 - Le tourisme spatial</li>
  <li>Séance 8 - Robots de guerre</li>
  <li>Séance 9 - Le bilan de la COP 26</li>
  <li>Séance 10 - Révolution du vin</li>
  <li>Séance 11 – Rouler au whisky</li>
  <li>Séance 12 — Un champion de la pâtisserie</li>
</ul>

<h1 id="软件方法">软件方法</h1>
<h2 id="简介-9">简介</h2>
<ul>
  <li>授课老师：于雷， Olivier Roux</li>
  <li>授课形式：OR线上授课，于雷助教，有报告和TP</li>
  <li>授课材料：PPT，TP</li>
  <li>闭卷考试，手写代码</li>
</ul>

<h2 id="内容总览-9">内容总览</h2>
<p>在另一份博客里有具体整理，主要是介绍面向对象的开发方法，同时用java来展示如何进行面向对象的编程</p>

<h1 id="机器人">机器人</h1>
<h2 id="简介-10">简介</h2>
<ul>
  <li>授课老师：严亮</li>
  <li>授课形式：线下授课</li>
  <li>授课材料：PPT</li>
  <li>无考试，有大作业(报告)</li>
  <li>英文授课</li>
</ul>

<h2 id="内容总览-10">内容总览</h2>
<p>只有四节课，介绍了自由度，动力学，逆动力学，速度和力</p>

<h2 id="报告-1">报告</h2>
<p>用solidworks设计一个三自由度的机械臂，并附上运动学分析</p>]]></content><author><name>Quehry</name></author><category term="school" /><summary type="html"><![CDATA[记录课程和课程笔记]]></summary></entry></feed>