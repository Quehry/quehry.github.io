<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-03-20T17:59:19+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Quehry</title><subtitle>Artificial Intelligence trends and concepts made easy.</subtitle><author><name>Quehry</name></author><entry><title type="html">C语言程序设计</title><link href="http://localhost:4000/C-Language.html" rel="alternate" type="text/html" title="C语言程序设计" /><published>2022-02-25T00:00:00+08:00</published><updated>2022-02-25T00:00:00+08:00</updated><id>http://localhost:4000/C-Language</id><content type="html" xml:base="http://localhost:4000/C-Language.html"><![CDATA[<h1 id="目录">目录</h1>

<!-- TOC -->

<ul>
  <li><a href="#目录">目录</a></li>
  <li><a href="#1-第一章-c语言快速入门">1. 第一章 C语言快速入门</a>
    <ul>
      <li><a href="#11-信息在计算机中的表示">1.1. 信息在计算机中的表示</a>
        <ul>
          <li><a href="#111-用0和1表示各种信息">1.1.1. 用0和1表示各种信息</a></li>
          <li><a href="#112-十进制到二进制的互相转换">1.1.2. 十进制到二进制的互相转换</a></li>
          <li><a href="#113-k进制小数">1.1.3. K进制小数</a></li>
          <li><a href="#114-十六进制数到二进制数的相互转换">1.1.4. 十六进制数到二进制数的相互转换</a></li>
        </ul>
      </li>
      <li><a href="#12-c语言快速入门">1.2. C语言快速入门</a></li>
      <li><a href="#13-变量和数据类型初探">1.3. 变量和数据类型初探</a>
        <ul>
          <li><a href="#131-什么是变量">1.3.1. 什么是变量</a></li>
          <li><a href="#132-变量的命名规则">1.3.2. 变量的命名规则</a></li>
          <li><a href="#133-c的基本数据类型">1.3.3. C++的基本数据类型</a></li>
        </ul>
      </li>
      <li><a href="#14-变量和数据类型进阶">1.4. 变量和数据类型进阶</a>
        <ul>
          <li><a href="#141-数据类型的自动转换">1.4.1. 数据类型的自动转换</a></li>
        </ul>
      </li>
      <li><a href="#15-常量">1.5. 常量</a>
        <ul>
          <li><a href="#151-整型常量">1.5.1. 整型常量</a></li>
          <li><a href="#152-字符型常量">1.5.2. 字符型常量</a></li>
          <li><a href="#153-符号常量">1.5.3. 符号常量</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#2-第二章-输入输出和基本运算">2. 第二章 输入输出和基本运算</a>
    <ul>
      <li><a href="#21-输入输出进阶">2.1. 输入输出进阶</a>
        <ul>
          <li><a href="#211-输入输出控制符">2.1.1. 输入输出控制符</a></li>
          <li><a href="#212-用scanf读入不同类型的变量">2.1.2. 用scanf读入不同类型的变量</a></li>
          <li><a href="#213-控制printf输出整数的宽度">2.1.3. 控制printf输出整数的宽度</a></li>
          <li><a href="#214-用c的cout进行输出">2.1.4. 用C++的cout进行输出</a></li>
          <li><a href="#215-用c的cin进行输入">2.1.5. 用C++的cin进行输入</a></li>
        </ul>
      </li>
      <li><a href="#22-算术运算符和算术表达式">2.2. 算术运算符和算术表达式</a>
        <ul>
          <li><a href="#221-赋值运算符">2.2.1. 赋值运算符</a></li>
          <li><a href="#222-算术运算符">2.2.2. 算术运算符</a></li>
          <li><a href="#223-模运算">2.2.3. 模运算</a></li>
          <li><a href="#224-自增运算符-">2.2.4. 自增运算符 ++</a></li>
        </ul>
      </li>
      <li><a href="#23-关系运算符和逻辑表达式">2.3. 关系运算符和逻辑表达式</a>
        <ul>
          <li><a href="#231-关系运算符">2.3.1. 关系运算符</a></li>
          <li><a href="#232-逻辑运算符和逻辑表达式">2.3.2. 逻辑运算符和逻辑表达式</a></li>
        </ul>
      </li>
      <li><a href="#24-其他运算符及运算符优先级">2.4. 其他运算符及运算符优先级</a>
        <ul>
          <li><a href="#241-强制类型转换运算符">2.4.1. 强制类型转换运算符</a></li>
          <li><a href="#242-部分运算符的优先级">2.4.2. 部分运算符的优先级</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#3-分支语句和循环语句">3. 分支语句和循环语句</a>
    <ul>
      <li><a href="#31-if语句">3.1. if语句</a>
        <ul>
          <li><a href="#311-条件分支结构">3.1.1. 条件分支结构</a></li>
          <li><a href="#312-if语句">3.1.2. if语句</a></li>
        </ul>
      </li>
      <li><a href="#32-switch语句">3.2. switch语句</a></li>
      <li><a href="#33-for循环">3.3. for循环</a>
        <ul>
          <li><a href="#331-for循环语句">3.3.1. for循环语句</a></li>
        </ul>
      </li>
      <li><a href="#34-while循环和do-while循环">3.4. while循环和do while循环</a>
        <ul>
          <li><a href="#341-while循环">3.4.1. while循环</a></li>
          <li><a href="#342-do-while循环">3.4.2. do while循环</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#4-第四章-循环综合应用">4. 第四章 循环综合应用</a>
    <ul>
      <li><a href="#41-break语句和continue语句">4.1. break语句和continue语句</a>
        <ul>
          <li><a href="#411-break语句">4.1.1. break语句</a></li>
          <li><a href="#412-continue语句">4.1.2. continue语句</a></li>
        </ul>
      </li>
      <li><a href="#42-oj输入数据的处理">4.2. OJ输入数据的处理</a>
        <ul>
          <li><a href="#421-scanf表达式的值">4.2.1. scanf表达式的值</a></li>
          <li><a href="#422-用freopen重定向输入">4.2.2. 用freopen重定向输入</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#5-第五章-数组">5. 第五章 数组</a>
    <ul>
      <li><a href="#51-数组">5.1. 数组</a></li>
      <li><a href="#52-筛法求素数">5.2. 筛法求素数</a></li>
      <li><a href="#53-数组初始化">5.3. 数组初始化</a></li>
      <li><a href="#54-数组越界">5.4. 数组越界</a></li>
      <li><a href="#55-二维数组">5.5. 二维数组</a></li>
    </ul>
  </li>
  <li><a href="#6-第六章-函数和位运算">6. 第六章 函数和位运算</a>
    <ul>
      <li><a href="#61-函数">6.1. 函数</a></li>
      <li><a href="#62-递归初步">6.2. 递归初步</a></li>
      <li><a href="#63-库函数和头文件">6.3. 库函数和头文件</a></li>
      <li><a href="#64-位运算">6.4. 位运算</a>
        <ul>
          <li><a href="#641-按位与">6.4.1. 按位与&amp;</a></li>
          <li><a href="#642-按位或\">6.4.2. 按位或|</a></li>
          <li><a href="#643-按位异或^">6.4.3. 按位异或^</a></li>
          <li><a href="#644-按位非">6.4.4. 按位非~</a></li>
          <li><a href="#645-左移运算符">6.4.5. 左移运算符«</a></li>
          <li><a href="#646-右移运算符">6.4.6. 右移运算符»</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#7-第七章-字符串">7. 第七章 字符串</a>
    <ul>
      <li><a href="#71-字符串的形式和存储">7.1. 字符串的形式和存储</a></li>
      <li><a href="#72-输入字符串">7.2. 输入字符串</a></li>
      <li><a href="#73-字符串库函数">7.3. 字符串库函数</a></li>
    </ul>
  </li>
  <li><a href="#8-第八章-指针一">8. 第八章 指针(一)</a>
    <ul>
      <li><a href="#81-指针的基本概念和用法">8.1. 指针的基本概念和用法</a></li>
      <li><a href="#82-指针的意义和相互赋值">8.2. 指针的意义和相互赋值</a></li>
      <li><a href="#83-指针的运算">8.3. 指针的运算</a></li>
      <li><a href="#84-指针作为函数参数">8.4. 指针作为函数参数</a></li>
      <li><a href="#85-指针和数组">8.5. 指针和数组</a></li>
    </ul>
  </li>
  <li><a href="#9-第九章-指针二">9. 第九章 指针(二)</a>
    <ul>
      <li><a href="#91-指针和二维数组指向指针的指针">9.1. 指针和二维数组、指向指针的指针</a></li>
      <li><a href="#92-指针和字符串">9.2. 指针和字符串</a></li>
      <li><a href="#93-字符串库函数">9.3. 字符串库函数</a></li>
      <li><a href="#94-void指针和内存操作函数">9.4. void指针和内存操作函数</a></li>
      <li><a href="#95-函数指针">9.5. 函数指针</a></li>
    </ul>
  </li>
  <li><a href="#10-第十章-程序结构和简单算法">10. 第十章 程序结构和简单算法</a>
    <ul>
      <li><a href="#101-结构">10.1. 结构</a></li>
      <li><a href="#102-全局变量局部变量静态变量">10.2. 全局变量、局部变量、静态变量</a></li>
      <li><a href="#103-变量的作用域和生存周期">10.3. 变量的作用域和生存周期</a></li>
      <li><a href="#104-选择排序和插入排序">10.4. 选择排序和插入排序</a>
        <ul>
          <li><a href="#1041-选择排序">10.4.1. 选择排序</a></li>
          <li><a href="#1042-插入排序">10.4.2. 插入排序</a></li>
        </ul>
      </li>
      <li><a href="#105-冒泡排序">10.5. 冒泡排序</a></li>
      <li><a href="#106-程序或算法的时间复杂度">10.6. 程序或算法的时间复杂度</a></li>
    </ul>
  </li>
  <li><a href="#11-第十一章-文件读写">11. 第十一章 文件读写</a>
    <ul>
      <li><a href="#111-文件读写概述">11.1. 文件读写概述</a>
        <ul>
          <li><a href="#1111-打开文件的函数">11.1.1. 打开文件的函数</a></li>
        </ul>
      </li>
      <li><a href="#112-文本文件读写">11.2. 文本文件读写</a>
        <ul>
          <li><a href="#1121-文本文件读写">11.2.1. 文本文件读写</a></li>
          <li><a href="#1121-文本文件读写另一种函数">11.2.1. 文本文件读写(另一种函数)</a></li>
        </ul>
      </li>
      <li><a href="#113-二进制文件读写概述">11.3. 二进制文件读写概述</a>
        <ul>
          <li><a href="#1131-文件的读写指针">11.3.1. 文件的读写指针</a></li>
          <li><a href="#1132-二进制文件读写">11.3.2. 二进制文件读写</a></li>
        </ul>
      </li>
      <li><a href="#114-创建和读取二进制文件">11.4. 创建和读取二进制文件</a></li>
      <li><a href="#115-修改二进制文件">11.5. 修改二进制文件</a></li>
      <li><a href="#116-文件拷贝程序">11.6. 文件拷贝程序</a></li>
    </ul>
  </li>
  <li><a href="#12-c的stl">12. C++的STL</a>
    <ul>
      <li><a href="#121-stl排序算法sort">12.1. STL排序算法sort</a></li>
      <li><a href="#122-stl二分查找算法">12.2. STL二分查找算法</a>
        <ul>
          <li><a href="#1221-用binary_search进行二分查找">12.2.1. 用binary_search进行二分查找</a></li>
          <li><a href="#1222-用lower_bound二分查找下界">12.2.2. 用lower_bound二分查找下界</a></li>
          <li><a href="#1223-用upper_bound二分查找上界">12.2.3. 用upper_bound二分查找上界</a></li>
        </ul>
      </li>
      <li><a href="#123-multiset">12.3. multiset</a></li>
      <li><a href="#124-set">12.4. set</a></li>
      <li><a href="#125-multimap">12.5. multimap</a></li>
      <li><a href="#126-map">12.6. map</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h1 id="1-第一章-c语言快速入门">1. 第一章 C语言快速入门</h1>

<h2 id="11-信息在计算机中的表示">1.1. 信息在计算机中的表示</h2>

<h3 id="111-用0和1表示各种信息">1.1.1. 用0和1表示各种信息</h3>
<ul>
  <li>计算机中的所有信息都是用0、1表示</li>
  <li>二进制数的一位，称为一个比特(bit), 简写b</li>
  <li>八个二进制位称为一个字节(byte), 简写B</li>
  <li>1KB, 1MB, 1GB, 1TB</li>
  <li>ASCII编码方案：用8个连续的0或1来表示一个字母数字和标点符号，一共有256种不同的组合</li>
</ul>

<h3 id="112-十进制到二进制的互相转换">1.1.2. 十进制到二进制的互相转换</h3>
<ul>
  <li>十进制数是数的十进制表示形式的简称</li>
  <li>短除法，每次除以进制，余数就是这个进制的最小位数</li>
</ul>

<h3 id="113-k进制小数">1.1.3. K进制小数</h3>
<ul>
  <li>K进制小数和整数的定义类似，只不过变成了K的负次方，比如小数后的第一位是$K^{-1}$</li>
</ul>

<h3 id="114-十六进制数到二进制数的相互转换">1.1.4. 十六进制数到二进制数的相互转换</h3>

<center><img src="assets/img/posts/20220225/2.jpg" /></center>

<h2 id="12-c语言快速入门">1.2. C语言快速入门</h2>

<ul>
  <li>空格也是一个字符</li>
  <li>C语言中输入输出: scanf、printf</li>
  <li>程序的注释:
    <ul>
      <li>多行注释: /* …  */</li>
      <li>单行注释: //</li>
    </ul>
  </li>
</ul>

<h2 id="13-变量和数据类型初探">1.3. 变量和数据类型初探</h2>

<h3 id="131-什么是变量">1.3.1. 什么是变量</h3>
<ul>
  <li>变量就是一个代号, 程序运行时系统会自动为变量分配内存空间，于是变量就代表了系统分配的那片内存空间</li>
  <li>变量有名字和类型两种属性，变量的类型决定了一个变量占用多少个字节</li>
  <li>变量的定义要在使用之前</li>
  <li>一个变量不能定义两次</li>
</ul>

<h3 id="132-变量的命名规则">1.3.2. 变量的命名规则</h3>
<ul>
  <li>变量不能以数字开头</li>
  <li>变量只能由大小写字母、数字和下划线组成</li>
  <li>变量名不能和C++系统预留的一些保留字重复</li>
</ul>

<h3 id="133-c的基本数据类型">1.3.3. C++的基本数据类型</h3>

<center><img src="assets/img/posts/20220225/3.jpg" /></center>

<ul>
  <li>float的取值范围是绝对值的范围</li>
  <li>整型、实数型、布尔型、字符型</li>
  <li>用sizeof()可以返回数据类型所占的字节数</li>
  <li>变量在定义的时候可以给它指定一个初始值</li>
</ul>

<h2 id="14-变量和数据类型进阶">1.4. 变量和数据类型进阶</h2>

<ul>
  <li>整型可以分为有符号整型和无符号整型</li>
  <li>有符号整数的表示方式
    <ul>
      <li>将最左边的位看作符号位，符号位为0表示非负数，其绝对值就是除去符号位以外的部分</li>
      <li>符号位为1，则表示是负数，其绝对值是除符号位意外的部分<strong>取反</strong>后加1</li>
      <li>将一个负整数转化为有符号整数是: 符号位取1，其余部分取该负整数的绝对值的二进制表示取反加1</li>
    </ul>
  </li>
</ul>

<h3 id="141-数据类型的自动转换">1.4.1. 数据类型的自动转换</h3>
<ul>
  <li>有些不同的数据类型之间是相容的，可以相互赋值</li>
  <li>int a = 11.34, 其实就是a=11</li>
  <li>整型数据也可以转换为字符型数据, 但只会留下最右边的一个字节</li>
  <li>看一个例子</li>
</ul>

<center><img src="assets/img/posts/20220225/4.jpg" /></center>

<h2 id="15-常量">1.5. 常量</h2>

<h3 id="151-整型常量">1.5.1. 整型常量</h3>
<ul>
  <li>十六进制整型常量以0x开头</li>
  <li>一个十六进制位正好对应四个二进制位</li>
  <li>0开头的是八进制数</li>
</ul>

<h3 id="152-字符型常量">1.5.2. 字符型常量</h3>
<ul>
  <li>字符型常量表示一个字符，用单引号括起来</li>
  <li>字符型常量和变量都占一个字节，内部存放的是ASCII编码</li>
  <li>小写字母的ASCII编码比大写字母大</li>
  <li>字符型常量中有一部分以‘\’开头, 被称为转义字符</li>
</ul>

<center><img src="assets/img/posts/20220225/5.jpg" /></center>

<ul>
  <li>字符串常量用双引号括起来，字符常量用单引号括起来</li>
</ul>

<h3 id="153-符号常量">1.5.3. 符号常量</h3>
<ul>
  <li>为了阅读和修改方便, 常用一个由字母和数字组成的符号来代表某个常量</li>
  <li>#define 常量名 常量值</li>
  <li>尽量多用符号常量，少用数值常量</li>
</ul>

<h1 id="2-第二章-输入输出和基本运算">2. 第二章 输入输出和基本运算</h1>

<h2 id="21-输入输出进阶">2.1. 输入输出进阶</h2>

<h3 id="211-输入输出控制符">2.1.1. 输入输出控制符</h3>
<ul>
  <li>在printf和scanf中可以使用以%开头的控制符，指明要输入和输出的数据类型</li>
  <li>常用的格式控制符</li>
</ul>

<center><img src="assets/img/posts/20220225/6.jpg" /></center>

<h3 id="212-用scanf读入不同类型的变量">2.1.2. 用scanf读入不同类型的变量</h3>
<ul>
  <li>输入字符时，不会跳过空格</li>
  <li>如果在输入中有scanf中出现的非控制字符，则这些字符会被跳过</li>
</ul>

<h3 id="213-控制printf输出整数的宽度">2.1.3. 控制printf输出整数的宽度</h3>
<ul>
  <li>比如用%nd和%0nd控制输出整型的长度</li>
  <li>用%.nf控制输出浮点数的精度</li>
</ul>

<h3 id="214-用c的cout进行输出">2.1.4. 用C++的cout进行输出</h3>
<ul>
  <li>cout « …</li>
  <li>endl可以进行换行</li>
</ul>

<h3 id="215-用c的cin进行输入">2.1.5. 用C++的cin进行输入</h3>
<ul>
  <li>cin » …</li>
  <li>cin、cout的速度比printf、scanf慢，输入输出数据量达到时候用后者</li>
  <li>一个程序不要同时使用cout和printf</li>
</ul>

<h2 id="22-算术运算符和算术表达式">2.2. 算术运算符和算术表达式</h2>

<h3 id="221-赋值运算符">2.2.1. 赋值运算符</h3>
<ul>
  <li>a += b 等同于 a = a + b</li>
</ul>

<h3 id="222-算术运算符">2.2.2. 算术运算符</h3>
<ul>
  <li>加减乘除</li>
  <li>%表示取余数</li>
  <li>两个整数进行加减乘都可能导致计算结果超出了结果类型所能表示的范围，这种情况就是溢出</li>
  <li>如果溢出，则直接丢弃溢出的部分</li>
  <li>有时计算的最终结果似乎不会溢出，但中间结果可能溢出，这也会导致程序出错</li>
  <li>解决溢出的办法是尽量使用高精度的数据类型</li>
  <li>除法的结果，类型和操作数中精度高的类型相同</li>
</ul>

<h3 id="223-模运算">2.2.3. 模运算</h3>
<ul>
  <li>求余数的运算符%也称为模运算符，两个操作数都是整数类型</li>
</ul>

<h3 id="224-自增运算符-">2.2.4. 自增运算符 ++</h3>
<ul>
  <li>自增运算符有前置用法和后置用法</li>
  <li>前置用法: ++ a 表示将a的值加1，表达式返回a+1后的值</li>
  <li>后置用法: a ++ 表示将a的值加1，表达式返回值为a加1前的值</li>
</ul>

<h2 id="23-关系运算符和逻辑表达式">2.3. 关系运算符和逻辑表达式</h2>

<h3 id="231-关系运算符">2.3.1. 关系运算符</h3>
<ul>
  <li>一共有六种关系运算符用于数值的比较</li>
  <li>比较的结果是bool类型</li>
</ul>

<center><img src="assets/img/posts/20220225/7.jpg" /></center>

<h3 id="232-逻辑运算符和逻辑表达式">2.3.2. 逻辑运算符和逻辑表达式</h3>
<ul>
  <li>逻辑运算符用于表达式的逻辑操作，有&amp;&amp;、||、!这三种，操作结果为true或false</li>
  <li>逻辑表达式是短路运算，即对逻辑表达式的计算在整个表达式的值已经能够断定的时候停止</li>
</ul>

<h2 id="24-其他运算符及运算符优先级">2.4. 其他运算符及运算符优先级</h2>

<h3 id="241-强制类型转换运算符">2.4.1. 强制类型转换运算符</h3>
<ul>
  <li>(int)、(char)这样的运算符就是强制将操作数转换为指定类型</li>
</ul>

<h3 id="242-部分运算符的优先级">2.4.2. 部分运算符的优先级</h3>

<center><img src="assets/img/posts/20220225/8.jpg" /></center>

<h1 id="3-分支语句和循环语句">3. 分支语句和循环语句</h1>

<h2 id="31-if语句">3.1. if语句</h2>

<h3 id="311-条件分支结构">3.1.1. 条件分支结构</h3>
<ul>
  <li>有时候我们希望满足一个条件执行一种语句，另一个条件执行另一种语句</li>
</ul>

<h3 id="312-if语句">3.1.2. if语句</h3>
<ul>
  <li>if语句可以没有else if，也可以没有else</li>
  <li>如果语句组只有一条语句，则不需要{}</li>
  <li>if语句可以嵌套</li>
  <li>else总是和离它最近的if配对，加一个花括号可以解决这个问题</li>
</ul>

<h2 id="32-switch语句">3.2. switch语句</h2>

<center><img src="assets/img/posts/20220225/9.jpg" /></center>

<ul>
  <li>可以没有default语句</li>
  <li>注意常量表达式不能带变量</li>
</ul>

<h2 id="33-for循环">3.3. for循环</h2>

<h3 id="331-for循环语句">3.3.1. for循环语句</h3>

<center><img src="assets/img/posts/20220225/10.jpg" /></center>

<ul>
  <li>注意是先执行语句组然后执行表达式3</li>
  <li>表达式1和表达式3都可以是用逗号连接的若干个表达式</li>
  <li>for循环可以嵌套，形成多重for循环</li>
  <li>for语句括号里面的表达式1、表达式2、表达式3可以任何一个都不写，但是分号必须保留</li>
</ul>

<h2 id="34-while循环和do-while循环">3.4. while循环和do while循环</h2>

<h3 id="341-while循环">3.4.1. while循环</h3>

<center><img src="assets/img/posts/20220225/11.jpg" /></center>

<h3 id="342-do-while循环">3.4.2. do while循环</h3>
<ul>
  <li>如果希望循环至少要执行一次，那么可以用do while循环</li>
</ul>

<center><img src="assets/img/posts/20220225/12.jpg" /></center>

<h1 id="4-第四章-循环综合应用">4. 第四章 循环综合应用</h1>

<h2 id="41-break语句和continue语句">4.1. break语句和continue语句</h2>

<h3 id="411-break语句">4.1.1. break语句</h3>
<ul>
  <li>break语句出现在循环体中，其作用是跳出循环</li>
  <li>在多重循环中，break语句只能跳出直接包含它的那一重循环</li>
</ul>

<h3 id="412-continue语句">4.1.2. continue语句</h3>
<ul>
  <li>continue可以出现在循环体中，其作用是立即结束本次循环，并回到循环开头判断是否要进行下一次循环</li>
</ul>

<h2 id="42-oj输入数据的处理">4.2. OJ输入数据的处理</h2>

<h3 id="421-scanf表达式的值">4.2.1. scanf表达式的值</h3>
<ul>
  <li>scanf()表达式其实是有返回值的，返回值为int类型，表示成功读入的变量个数</li>
  <li>scamf()值为EOF则说明输入数据已经结束</li>
  <li>ctrl+z表示输入结束</li>
  <li>这样就可以处理五结束标记的OJ题目输入</li>
</ul>

<h3 id="422-用freopen重定向输入">4.2.2. 用freopen重定向输入</h3>
<ul>
  <li>调试程序时，每次运行程序都要输入测试数据，太麻烦</li>
  <li>可以将测试数据存入文件，然后用freopen将输入由键盘重定向为文件，则运行程序时不再需要输入数据</li>
</ul>

<h1 id="5-第五章-数组">5. 第五章 数组</h1>

<h2 id="51-数组">5.1. 数组</h2>
<ul>
  <li>数组可以用来表示类型相同的元素的集合，集合的名字就是数组名</li>
  <li>数组可以用来表达类型相同的元素的集合，集合的名字就是数组名</li>
  <li>一维数组的定义方法如下：</li>
</ul>

<center><img src="assets/img/posts/20220225/13.jpg" /></center>

<ul>
  <li>元素个数必须是常量或常量表达式</li>
  <li>sizeof()可以访问数组所占字节</li>
  <li>数组名代表数组的地址</li>
  <li>数组一般不要定义在main里面，尤其是大数组</li>
</ul>

<h2 id="52-筛法求素数">5.2. 筛法求素数</h2>
<ul>
  <li>之前我们判断一个数n是不是素数，使用2到根号n之间的所有整数去除n，也就是穷举</li>
  <li>筛法：把2到n中所有的数都列出来，然后从2开始，先划掉n内所有2的倍数，然后每次从下一个剩下的数开始，划掉其n内的所有倍数，最后剩下的数就是素数</li>
  <li>筛法会稍微快一点，用空间换时间</li>
  <li>代码如下：</li>
</ul>

<center><img src="assets/img/posts/20220225/14.jpg" /></center>

<h2 id="53-数组初始化">5.3. 数组初始化</h2>
<ul>
  <li>在定义一个一维数组的同时，可以给数组中的元素赋初值</li>
</ul>

<center><img src="assets/img/posts/20220225/15.jpg" /></center>

<ul>
  <li>如果在定义数组的时候，如给全部元素赋值，则可以不给出数组元素的个数</li>
  <li>可以用数组取代复杂分支结构</li>
  <li>使用string须包含头文件<string></string></li>
</ul>

<h2 id="54-数组越界">5.4. 数组越界</h2>
<ul>
  <li>数组元素的下标，可以是任何整数，可以是负数，也可以大于数组的元素个数，不会导致编译错误</li>
  <li>但如果将越界写入了别的变量的内存空间，就很有可能出错</li>
</ul>

<h2 id="55-二维数组">5.5. 二维数组</h2>
<ul>
  <li>二维数组的定义：</li>
</ul>

<center><img src="assets/img/posts/20220225/16.jpg" /></center>

<ul>
  <li>二维数组的访问可以直接用下标访问</li>
  <li>二维数组的初始化也是用{}</li>
  <li>二维数组初始化时，如果对每行都进行初始化，则不用写行数或列数</li>
</ul>

<h1 id="6-第六章-函数和位运算">6. 第六章 函数和位运算</h1>

<h2 id="61-函数">6.1. 函数</h2>
<ul>
  <li>函数可以实现某一功能，当程序中需要使用该项功能时，只需要写一条语句，调用实现该功能的函数即可</li>
  <li>函数的定义:</li>
</ul>

<center><img src="assets/img/posts/20220225/17.jpg" /></center>

<ul>
  <li>函数的调用: 函数名(参数1, 参数2…)</li>
  <li>函数中至少含有一个return, 如果函数的类型为void, 则用return;</li>
  <li>定义函数的参数叫做形参, 调用函数时的参数叫做实参</li>
  <li>函数的定义一般在调用之前</li>
  <li>但是函数的调用语句前面有函数的声明即可，不一定要有定义</li>
</ul>

<center><img src="assets/img/posts/20220225/18.jpg" /></center>

<ul>
  <li>C/C++程序从main函数开始</li>
  <li>函数的形参是实参的一个拷贝，形参的改变一般不会影响到实参</li>
  <li>一维数组作为形参时不用写出元素的个数，这时候形参的改变会影响实参</li>
  <li>二维数组作为形参时，必须写明数组有多少列，不用写明有多少行</li>
</ul>

<h2 id="62-递归初步">6.2. 递归初步</h2>
<ul>
  <li>一个函数，自己调用自己，就是递归</li>
  <li>递归函数得有终止条件</li>
</ul>

<h2 id="63-库函数和头文件">6.3. 库函数和头文件</h2>
<ul>
  <li>头文件&lt;cmath&gt;中包含很多数学库函数的声明</li>
  <li>库函数的定义一般在.lib文件中</li>
  <li>库函数: C/C++标准规定, 编译器自带的函数</li>
  <li>头文件: C++编译器提供许多头文件, 比如: iostream、cmath、string</li>
  <li>头文件内部包含很多库函数的声明以及其他信息, 比如cin、cout的定义</li>
</ul>

<center><img src="assets/img/posts/20220225/19.jpg" /></center>

<center><img src="assets/img/posts/20220225/20.jpg" /></center>

<h2 id="64-位运算">6.4. 位运算</h2>
<ul>
  <li>位运算: 用于对整数类型变量中的某一位(bit)或者若干位进行操作</li>
  <li>C++提供了六种位运算符来进行位运算操作</li>
</ul>

<center><img src="assets/img/posts/20220225/21.jpg" /></center>

<h3 id="641-按位与">6.4.1. 按位与&amp;</h3>
<ul>
  <li>比如表达式(21 &amp; 18)的结果是16</li>
  <li>通常用来将某变量中的某些位清0且同时保留其他位不变</li>
</ul>

<h3 id="642-按位或">6.4.2. 按位或|</h3>
<ul>
  <li>比如“21|18”的结果是23</li>
  <li>按位或运算通常用来将某些变量中的某些位置1且保留其他位不变</li>
</ul>

<h3 id="643-按位异或">6.4.3. 按位异或^</h3>
<ul>
  <li>异或是逻辑运算, 如果两个值相同返回0, 如果两个值不同返回1</li>
  <li>异或运算通常用来将某变量中的某些位取反</li>
  <li>异或运算的特点:</li>
</ul>

<center><img src="assets/img/posts/20220225/22.jpg" /></center>

<h3 id="644-按位非">6.4.4. 按位非~</h3>
<ul>
  <li>按位非运算符~是单目运算符，其功能是将操作数中的二进制位0变成1，1变成0</li>
</ul>

<h3 id="645-左移运算符">6.4.5. 左移运算符«</h3>
<ul>
  <li>9 « 4 表示将9的二进制表示左移4位</li>
</ul>

<h3 id="646-右移运算符">6.4.6. 右移运算符»</h3>
<ul>
  <li>右移时，移出最右边的位就被丢弃</li>
  <li>对于有符号数，在右移时，符号位将一起移动，并且大多数C++编译器规定，如果圆符号位为1，则右移时高位就补充1，原符号位为0，则右移时高位就补充0</li>
</ul>

<h1 id="7-第七章-字符串">7. 第七章 字符串</h1>

<h2 id="71-字符串的形式和存储">7.1. 字符串的形式和存储</h2>
<ul>
  <li>字符串常量占据内存的字节数等于字符串中字符数目加1，多出来的是结尾字符’\0’</li>
  <li>空串”“也是合法的字符串常量</li>
  <li>包含’\0’字符的一维char数组，就是一个字符串，其中存放的字符串即为’\0’前面的字符组成</li>
  <li>可以给一维数组这么赋值: char title[] = “Prison Break”</li>
  <li>‘\0’可以视为字符数组结束标志</li>
</ul>

<h2 id="72-输入字符串">7.2. 输入字符串</h2>
<ul>
  <li>用scanf也可以将字符串读入字符数组</li>
  <li>scanf会自动添加结尾’\0’</li>
  <li>scanf读入到空格为止</li>
  <li>scanf(“%s”, line) 不用取地址符</li>
  <li>读入一行到字符串组: cin.getline(char buf[], int bufsize), 读入一行，自动添加’\0’, 回车换行符不会写入buf, 但是会从输入流中去掉</li>
  <li>也可以用gets(char buf[])来读入一行到字符数组，回车换行符不会写入buf，但是会从输入流中去掉，可能导致数组越界</li>
</ul>

<h2 id="73-字符串库函数">7.3. 字符串库函数</h2>
<ul>
  <li>使用字符串库函数需要 #include &lt;cstring&gt;</li>
  <li>形参为char []类型，则实参可以是char数组或字符串常量</li>
  <li>字符串拷贝 strcpy(char[] dest, char[] src) 拷贝src到dest</li>
  <li>字符串比较大小 int strcmp(char[] s1, char[] s2) 是根据字符的ASCII码值进行比较，大写字母比小写字母小</li>
  <li>求字符串长度 int strlen(char[] s)</li>
  <li>字符串拼接 strcat(char[] s1, char[] s2) 将s2拼接到s1后面</li>
  <li>字符串转成大写 strupr(char [])</li>
  <li>字符串转成小写 strlwr(char [])</li>
</ul>

<center><img src="assets/img/posts/20220225/23.jpg" /></center>

<h1 id="8-第八章-指针一">8. 第八章 指针(一)</h1>

<h2 id="81-指针的基本概念和用法">8.1. 指针的基本概念和用法</h2>
<ul>
  <li>指针也称作指针变量，大小为4个字节(或8个字节)的变量，其内容代表一个内存地址</li>
  <li>通过指针，能够对该指针指向的内存区域进行读写</li>
  <li>指针的定义: 类型名 * 指针变量名</li>
  <li>比如: int * p = (int *) 40000</li>
  <li>p指向地址40000，地址p就是地址40000</li>
  <li>* p就代表地址40000开始处的若干个字节的内容</li>
  <li>我们可以通过指针访问其指向的内存空间</li>
</ul>

<center><img src="assets/img/posts/20220225/24.jpg" /></center>

<ul>
  <li>指针定义总结</li>
</ul>

<center><img src="assets/img/posts/20220225/25.jpg" /></center>

<ul>
  <li>指针用法，一般是让指针指向一个变量的地址</li>
</ul>

<center><img src="assets/img/posts/20220225/26.jpg" /></center>

<h2 id="82-指针的意义和相互赋值">8.2. 指针的意义和相互赋值</h2>
<ul>
  <li>有了指针，就有了<strong>自由访问内存空间</strong>的手段</li>
  <li>不同类型的指针，如果不经过强制类型转换，不能直接互相赋值</li>
</ul>

<h2 id="83-指针的运算">8.3. 指针的运算</h2>
<ul>
  <li>两个同类型的指针变量，可以比较大小</li>
  <li>两个同类型的指针变量，可以相减</li>
</ul>

<center><img src="assets/img/posts/20220225/27.jpg" /></center>

<ul>
  <li>指针变量加减一个整数的结果是指针</li>
</ul>

<center><img src="assets/img/posts/20220225/28.jpg" /></center>

<ul>
  <li>指针变量可以自增自减</li>
  <li>指针可以用下标运算符[]进行运算</li>
</ul>

<center><img src="assets/img/posts/20220225/29.jpg" /></center>

<h2 id="84-指针作为函数参数">8.4. 指针作为函数参数</h2>
<ul>
  <li>地址0不能访问，指向地址0的指针就是空指针</li>
  <li>可以用NULL关键字对任何类型的指针进行赋值，NULL实际上就是整数0.值为NULL的指针就是空指针</li>
  <li>指针可以作为条件表达式使用，如果指针的值为NULL，则相当于为假，值不为NULL，就相当于为真</li>
</ul>

<h2 id="85-指针和数组">8.5. 指针和数组</h2>
<ul>
  <li>数组的名字是一个指针常量，指向数组的起始地址</li>
</ul>

<center><img src="assets/img/posts/20220225/30.jpg" /></center>

<ul>
  <li>作为函数形参时， T *p与 T p[] 等价</li>
</ul>

<h1 id="9-第九章-指针二">9. 第九章 指针(二)</h1>

<h2 id="91-指针和二维数组指向指针的指针">9.1. 指针和二维数组、指向指针的指针</h2>

<center><img src="assets/img/posts/20220225/31.jpg" /></center>

<ul>
  <li>二维数组的每一行都是一维数组，也就是指针</li>
  <li>指向指针的指针:</li>
</ul>

<center><img src="assets/img/posts/20220225/32.jpg" /></center>

<h2 id="92-指针和字符串">9.2. 指针和字符串</h2>
<ul>
  <li>字符串常量的类型就是char *</li>
  <li>字符数组名的类型也是char *</li>
</ul>

<h2 id="93-字符串库函数">9.3. 字符串库函数</h2>

<ul>
  <li>字符串操作库函数</li>
</ul>

<center><img src="assets/img/posts/20220225/33.jpg" /></center>

<center><img src="assets/img/posts/20220225/34.jpg" /></center>

<ul>
  <li>这些字符串操作库函数都需要include&lt;cstring&gt;</li>
</ul>

<h2 id="94-void指针和内存操作函数">9.4. void指针和内存操作函数</h2>
<ul>
  <li>void指针: void * p</li>
  <li>可以用任何类型的指针对void指针进行赋值或初始化</li>
  <li>对于void指针，*p没有定义，++p、–p，p += n、p+n、p-n均无定义</li>
  <li>内存操作库函数memset</li>
</ul>

<center><img src="assets/img/posts/20220225/35.jpg" /></center>

<ul>
  <li>内存操作库函数memcpy</li>
</ul>

<center><img src="assets/img/posts/20220225/36.jpg" /></center>

<h2 id="95-函数指针">9.5. 函数指针</h2>
<ul>
  <li>程序运行期间，每个函数都会占用一段连续的内存空间。而函数名就是该函数所占内存区域的起始地址(也称入口地址)</li>
  <li>我们可以将函数的入口地址赋给一个指针变量，使该指针变量指向该函数，然后通过指针变量就可以调用这个函数，这种指向函数的指针变量称为函数指针</li>
  <li>定义形式:</li>
</ul>

<center><img src="assets/img/posts/20220225/37.jpg" /></center>

<ul>
  <li>使用方法:</li>
</ul>

<center><img src="assets/img/posts/20220225/38.jpg" /></center>

<ul>
  <li>函数指针和qsort库函数</li>
</ul>

<center><img src="assets/img/posts/20220225/39.jpg" /></center>

<center><img src="assets/img/posts/20220225/40.jpg" /></center>

<ul>
  <li>pfcompare是比较函数</li>
</ul>

<h1 id="10-第十章-程序结构和简单算法">10. 第十章 程序结构和简单算法</h1>

<h2 id="101-结构">10.1. 结构</h2>
<ul>
  <li>在现实问题中，常常需要用一组不同类型的数据来描述一个事物</li>
  <li>C++允许程序员自己定义新的数据类型。因此针对“学生”这种事物，可以定义一种新名为Student的数据类型，一个student类型的变量就能描述一个学生的全部信息，同理，还可以定义数据类型worker来表示工人</li>
  <li>结构(struct): 用struct关键字来定义一个结构，也就定义了一个新的数据类型</li>
</ul>

<center><img src="assets/img/posts/20220225/41.jpg" /></center>

<ul>
  <li>student即成为自定义的类型的名字，可以用来定义变量</li>
  <li>两个同类型的结构变量，可以相互赋值，结构变量之间不能用比较运算符进行计算</li>
  <li>一般来说，一个结构变量所占的内存空间的大小就是结构中所有成员变量大小之和</li>
  <li>一个结构的成员变量可以是任何类型的，包括可以是另一个结构类型</li>
  <li>结构的成员变量可以是指向本结构类型的变量的指针</li>
</ul>

<center><img src="assets/img/posts/20220225/42.jpg" /></center>

<ul>
  <li>访问结构变量的成员变量: 一个结构变量的成员变量完全可以和一个普通变量一样来使用，也可以取得其地址</li>
  <li>结构变量名.成员变量名</li>
  <li>结构变量可以在定义时进行初始化:(使用花括号和逗号)</li>
</ul>

<center><img src="assets/img/posts/20220225/43.jpg" /></center>

<ul>
  <li>结构数组也可以定义，就是把结构体名字看作变量类型使用</li>
  <li>指向结构变量的指针，通过指针访问其指向的结构变量的成员变量</li>
</ul>

<center><img src="assets/img/posts/20220225/44.jpg" /></center>

<h2 id="102-全局变量局部变量静态变量">10.2. 全局变量、局部变量、静态变量</h2>
<ul>
  <li>定义在函数内部的变量叫<strong>局部变量</strong>(函数的形参也是局部变量)</li>
  <li>定义在所有函数的外面的变量叫做<strong>全局变量</strong></li>
  <li>全局变量在所有函数中均可以使用，局部变量只能在定义它的内部函数中使用</li>
  <li><strong>静态变量</strong>: 全局变量都是静态变量，局部变量定义时如果前面加了static关键字，则该变量也成为静态变量</li>
  <li>静态变量在整个程序运行期间都是固定不变的</li>
  <li>局部变量在函数每次调用时地址都可能不同</li>
  <li>如果未明确初始化，则静态变量会被自动初始化为全0，局部非静态变量的值则随机</li>
  <li>静态变量只初始化一次，也就是下次调用函数的时候不进行初始化</li>
</ul>

<h2 id="103-变量的作用域和生存周期">10.3. 变量的作用域和生存周期</h2>
<ul>
  <li>变量名、函数名、类型名统称为标识符</li>
  <li>一个标识符能够起作用的范围，叫做该标识符的作用域</li>
  <li>使用标识符的语句，必须出现在它们的声明或者定义之后</li>
  <li>在单文件的程序中，结构、函数和全局变量的作用域是其定义所在的整个文件</li>
  <li>函数的形参的作用域是整个函数</li>
  <li>局部变量的作用域，是从定义它的语句开始，到包含它的最内层的那一对大括号{}的右大括号为止</li>
  <li>for循环里定义的循环控制变量，其作用域是整个for循环</li>
  <li>同名标识符的作用域，可能一个被另一个包含，则在小的作用域里，作用域大的那个标识符被屏蔽，不起作用</li>
  <li>所谓变量的生存期，值的是在此期间，变量占有内存空间，其占有的内存空间只能归它使用，不会用来存放别的东西</li>
  <li>而变量的生存期终止，就意味着该变量不再占有内存空间，它原来占有的内存空间，随时可能被派作他用</li>
  <li>全局变量的生存期，从程序被装入内存开始，到整个程序结束</li>
  <li>静态局部变量的生存期，从定义它的语句第一次被执行开始，直到程序结束</li>
  <li>函数形参的生存期从函数执行开始，到函数返回时结束，非静态局部变量的生存期，从执行到定义它的语句开始，一旦程序执行了它的作用域之外，其生存期即告终止</li>
</ul>

<h2 id="104-选择排序和插入排序">10.4. 选择排序和插入排序</h2>

<h3 id="1041-选择排序">10.4.1. 选择排序</h3>
<ul>
  <li>排序问题: 编程接收键盘输入的若干个整数，排序后从小到大输出，先输入一个整数n，表明有n个整数需要排序，接下来再输入待排序的n个整数</li>
  <li>选择排序:</li>
</ul>

<center><img src="assets/img/posts/20220225/45.jpg" /></center>

<center><img src="assets/img/posts/20220225/46.jpg" /></center>

<ul>
  <li>选择最小的整数，与第i位的整数更换位置</li>
</ul>

<h3 id="1042-插入排序">10.4.2. 插入排序</h3>

<center><img src="assets/img/posts/20220225/47.jpg" /></center>

<center><img src="assets/img/posts/20220225/48.jpg" /></center>

<ul>
  <li>插入排序就是将数组分为有序和无序，每次让无序最左边的元素与有序分别比较，插入到合适的位置</li>
</ul>

<h2 id="105-冒泡排序">10.5. 冒泡排序</h2>

<center><img src="assets/img/posts/20220225/49.jpg" /></center>

<center><img src="assets/img/posts/20220225/50.jpg" /></center>

<ul>
  <li>冒泡排序同样是将数组分为有序和无序两组，无序在左边，有序在右边，每次将无序部分两两比较，较大的在右边</li>
  <li>上面三种简单排序算法，都要做$n^2$量级次数的比较，其中n是元素个数</li>
  <li>而比较好的排序算法，如快速排序，归并排序等，只需要做$n*log_2n$量级次数的比较</li>
</ul>

<h2 id="106-程序或算法的时间复杂度">10.6. 程序或算法的时间复杂度</h2>
<ul>
  <li>一个程序或算法的时间效率，也称为时间复杂度，有时简称复杂度</li>
  <li>复杂度常用大的字母O和小写字母n来表示，比如O(n), n代表问题的规模</li>
  <li>复杂度也有平均复杂度和最坏复杂度两种，两种可能一致，也可能不一致</li>
  <li>如果复杂度是多个n的函数之和，则只关心随n的增长增长得最快的那个函数</li>
</ul>

<center><img src="assets/img/posts/20220225/51.jpg" /></center>

<ul>
  <li>一些例子</li>
</ul>

<center><img src="assets/img/posts/20220225/52.jpg" /></center>

<h1 id="11-第十一章-文件读写">11. 第十一章 文件读写</h1>

<h2 id="111-文件读写概述">11.1. 文件读写概述</h2>
<ul>
  <li>二进制文件: 本质上所有文件都是0、1串，因此都是二进制文件。但是一般将内容不是文字，记事本打开看是乱码的文件，称为二进制文件</li>
  <li>文本文件: 内容是文字，用记事本打开能看到文字的文件</li>
  <li>文件读写相关函数在头文件cstdio中声明: #include &lt;cstdio&gt;</li>
  <li>fopen函数打开文件，返回FILE * 指针，指向和文件相关的一个FILE变量，FILE是一个struct</li>
  <li>文件读写结束后，一定要fclose关闭文件，否则可能导致数据没被保存，或者无法打开其他文件</li>
  <li>一些读写函数都需要FILE *指针进行</li>
</ul>

<center><img src="assets/img/posts/20220225/53.jpg" /></center>

<h3 id="1111-打开文件的函数">11.1.1. 打开文件的函数</h3>

<center><img src="assets/img/posts/20220225/54.jpg" /></center>

<ul>
  <li>打开文件的模式</li>
</ul>

<center><img src="assets/img/posts/20220225/55.jpg" /></center>

<ul>
  <li>二进制打开和文本打开的区别:</li>
</ul>

<center><img src="assets/img/posts/20220225/56.jpg" /></center>

<ul>
  <li>主要是二进制打开的话会有换行符的区别，最好还是用二进制打开</li>
</ul>

<center><img src="assets/img/posts/20220225/57.jpg" /></center>

<ul>
  <li>文件名的绝对路径和相对路径:</li>
</ul>

<center><img src="assets/img/posts/20220225/58.jpg" /></center>

<h2 id="112-文本文件读写">11.2. 文本文件读写</h2>

<h3 id="1121-文本文件读写">11.2.1. 文本文件读写</h3>

<center><img src="assets/img/posts/20220225/59.jpg" /></center>

<ul>
  <li>我们希望写一个文件读写程序:</li>
</ul>

<center><img src="assets/img/posts/20220225/60.jpg" /></center>

<h3 id="1121-文本文件读写另一种函数">11.2.1. 文本文件读写(另一种函数)</h3>
<ul>
  <li>fgets是读取一行</li>
</ul>

<center><img src="assets/img/posts/20220225/61.jpg" /></center>

<ul>
  <li>读取整个文本文件并输出</li>
</ul>

<center><img src="assets/img/posts/20220225/62.jpg" /></center>

<ul>
  <li>fputs是输出一行</li>
</ul>

<center><img src="assets/img/posts/20220225/63.jpg" /></center>

<h2 id="113-二进制文件读写概述">11.3. 二进制文件读写概述</h2>

<h3 id="1131-文件的读写指针">11.3.1. 文件的读写指针</h3>

<center><img src="assets/img/posts/20220225/64.jpg" /></center>

<ul>
  <li>这都是C语言读写的规则</li>
</ul>

<center><img src="assets/img/posts/20220225/65.jpg" /></center>

<ul>
  <li>fseek的作用是将读写指针定位到距离origin位置offset字节处</li>
</ul>

<h3 id="1132-二进制文件读写">11.3.2. 二进制文件读写</h3>
<ul>
  <li>用fread进行二进制读文件</li>
</ul>

<center><img src="assets/img/posts/20220225/66.jpg" /></center>

<ul>
  <li>用fgetc进行二进制读文件</li>
</ul>

<center><img src="assets/img/posts/20220225/67.jpg" /></center>

<ul>
  <li>
    <p>fgetc是用来读取一个字节</p>
  </li>
  <li>
    <p>用fwrite二进制写文件</p>
  </li>
</ul>

<center><img src="assets/img/posts/20220225/68.jpg" /></center>

<ul>
  <li>用fputc二进制写文件</li>
</ul>

<center><img src="assets/img/posts/20220225/69.jpg" /></center>

<h2 id="114-创建和读取二进制文件">11.4. 创建和读取二进制文件</h2>
<ul>
  <li>用二进制文件存学生信息比用文本方式存的好处: 可能节约空间、便于快速读取、改单个学生信息</li>
</ul>

<h2 id="115-修改二进制文件">11.5. 修改二进制文件</h2>
<ul>
  <li>用r+b打开文件既读又写时，如果做了读操作，则做写操作之前一定要用fssek重新定位文件读写指针</li>
</ul>

<h2 id="116-文件拷贝程序">11.6. 文件拷贝程序</h2>
<ul>
  <li>文件拷贝程序mycopy示例</li>
</ul>

<center><img src="assets/img/posts/20220225/70.jpg" /></center>

<center><img src="assets/img/posts/20220225/71.jpg" /></center>

<center><img src="assets/img/posts/20220225/72.jpg" /></center>

<h1 id="12-c的stl">12. C++的STL</h1>

<h2 id="121-stl排序算法sort">12.1. STL排序算法sort</h2>
<ul>
  <li>STL: standard template library 标准模板库</li>
  <li>包含一些常用的算法如排序查找，还有常用的数据结构如可变长数组、链表、字典等</li>
  <li>要使用其中的算法，需要#include &lt;algorithm&gt;</li>
  <li>用sort进行排序(用法一)</li>
</ul>

<center><img src="assets/img/posts/20220225/73.jpg" /></center>

<ul>
  <li>用sort进行排序(用法二)</li>
</ul>

<center><img src="assets/img/posts/20220225/74.jpg" /></center>

<ul>
  <li>用sort进行排序(用法三)，用自定义的排序规则对任何类型T的数组进行排序</li>
</ul>

<center><img src="assets/img/posts/20220225/75.jpg" /></center>

<ul>
  <li>几个自定义排序规则例子</li>
</ul>

<center><img src="assets/img/posts/20220225/76.jpg" /></center>

<center><img src="assets/img/posts/20220225/77.jpg" /></center>

<h2 id="122-stl二分查找算法">12.2. STL二分查找算法</h2>

<h3 id="1221-用binary_search进行二分查找">12.2.1. 用binary_search进行二分查找</h3>
<ul>
  <li>用法一:</li>
</ul>

<center><img src="assets/img/posts/20220225/78.jpg" /></center>

<ul>
  <li>用法二:</li>
</ul>

<center><img src="assets/img/posts/20220225/79.jpg" /></center>

<h3 id="1222-用lower_bound二分查找下界">12.2.2. 用lower_bound二分查找下界</h3>
<ul>
  <li>用法一:</li>
</ul>

<center><img src="assets/img/posts/20220225/80.jpg" /></center>

<ul>
  <li>用法二:</li>
</ul>

<center><img src="assets/img/posts/20220225/81.jpg" /></center>

<h3 id="1223-用upper_bound二分查找上界">12.2.3. 用upper_bound二分查找上界</h3>
<ul>
  <li>用法一:</li>
</ul>

<center><img src="assets/img/posts/20220225/82.jpg" /></center>

<ul>
  <li>用法二:</li>
</ul>

<center><img src="assets/img/posts/20220225/83.jpg" /></center>

<h2 id="123-multiset">12.3. multiset</h2>
<ul>
  <li>STL中的平衡二叉树数据结构</li>
  <li>有时需要在大量增加、删除数据的同时，还要进行大量数据的查找</li>
  <li>可以使用平衡二叉树数据结构存放数据，体现在STL中，就是以下四种排序容器: multiset、set、multimap、map</li>
  <li>multiset的用法:</li>
</ul>

<center><img src="assets/img/posts/20220225/84.jpg" /></center>

<ul>
  <li>multiset上的迭代器</li>
</ul>

<center><img src="assets/img/posts/20220225/85.jpg" /></center>

<center><img src="assets/img/posts/20220225/86.jpg" /></center>

<ul>
  <li>自定义排序规则的multiset用法:</li>
</ul>

<center><img src="assets/img/posts/20220225/87.jpg" /></center>

<center><img src="assets/img/posts/20220225/88.jpg" /></center>

<h2 id="124-set">12.4. set</h2>
<ul>
  <li>set和multiset的区别在于容器里面不能有重复元素</li>
  <li>set插入元素可能不成功</li>
  <li>pair模板的用法</li>
</ul>

<center><img src="assets/img/posts/20220225/89.jpg" /></center>

<ul>
  <li>set的例子</li>
</ul>

<center><img src="assets/img/posts/20220225/90.jpg" /></center>

<center><img src="assets/img/posts/20220225/91.jpg" /></center>

<h2 id="125-multimap">12.5. multimap</h2>
<ul>
  <li>multimap容器里面的元素，都是pair形式的</li>
</ul>

<center><img src="assets/img/posts/20220225/92.jpg" /></center>

<ul>
  <li>multimap的应用</li>
</ul>

<center><img src="assets/img/posts/20220225/93.jpg" /></center>

<ul>
  <li>代码实现细节</li>
</ul>

<center><img src="assets/img/posts/20220225/94.jpg" /></center>

<center><img src="assets/img/posts/20220225/95.jpg" /></center>

<center><img src="assets/img/posts/20220225/96.jpg" /></center>

<center><img src="assets/img/posts/20220225/97.jpg" /></center>

<h2 id="126-map">12.6. map</h2>
<ul>
  <li>map和multimap的区别: 不能有关键字重复的元素, 可以使用[], 下标为关键字, 返回值为first和关键字相同的元素的second</li>
  <li>插入元素可能失败</li>
</ul>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[网课记录]]></summary></entry><entry><title type="html">机器学习</title><link href="http://localhost:4000/Machine-Learning.html" rel="alternate" type="text/html" title="机器学习" /><published>2021-12-22T00:00:00+08:00</published><updated>2021-12-22T00:00:00+08:00</updated><id>http://localhost:4000/Machine-Learning</id><content type="html" xml:base="http://localhost:4000/Machine-Learning.html"><![CDATA[<h1 id="目录">目录</h1>

<!-- TOC -->

<ul>
  <li><a href="#目录">目录</a></li>
  <li><a href="#1-第1章-绪论">1. 第1章 绪论</a></li>
  <li><a href="#2-第2章-模型评估与选择">2. 第2章 模型评估与选择</a>
    <ul>
      <li><a href="#21-思维导图">2.1. 思维导图</a></li>
      <li><a href="#22-经验误差与过拟合">2.2. 经验误差与过拟合</a></li>
      <li><a href="#23-评估方法">2.3. 评估方法</a>
        <ul>
          <li><a href="#231-留出法">2.3.1. 留出法</a></li>
          <li><a href="#232-交叉验证法">2.3.2. 交叉验证法</a></li>
          <li><a href="#233-自助法">2.3.3. 自助法</a></li>
        </ul>
      </li>
      <li><a href="#24-性能度量">2.4. 性能度量</a>
        <ul>
          <li><a href="#241-错误率与精度">2.4.1. 错误率与精度</a></li>
          <li><a href="#242-查准率查全率与f1">2.4.2. 查准率、查全率与F1</a></li>
          <li><a href="#243-roc与auc">2.4.3. ROC与AUC</a></li>
          <li><a href="#244-代价敏感错误率与代价曲线">2.4.4. 代价敏感错误率与代价曲线</a></li>
        </ul>
      </li>
      <li><a href="#25-比较检验">2.5. 比较检验</a>
        <ul>
          <li><a href="#251-假设检验">2.5.1. 假设检验</a></li>
          <li><a href="#252-交叉验证t检验">2.5.2. 交叉验证t检验</a></li>
          <li><a href="#253-mcnemar检验">2.5.3. McNemar检验</a></li>
          <li><a href="#254-friedman检验与nemenyi后续检验">2.5.4. Friedman检验与Nemenyi后续检验</a></li>
        </ul>
      </li>
      <li><a href="#26-偏差与方差">2.6. 偏差与方差</a></li>
    </ul>
  </li>
  <li><a href="#3-第3章-线性模型">3. 第3章 线性模型</a>
    <ul>
      <li><a href="#31-思维导图">3.1. 思维导图</a></li>
      <li><a href="#32-基本形式">3.2. 基本形式</a></li>
      <li><a href="#33-线性回归">3.3. 线性回归</a></li>
      <li><a href="#34-对数几率回归">3.4. 对数几率回归</a></li>
      <li><a href="#35-线性判别分析">3.5. 线性判别分析</a></li>
      <li><a href="#36-多分类学习">3.6. 多分类学习</a></li>
      <li><a href="#37-类别不平衡问题">3.7. 类别不平衡问题</a></li>
    </ul>
  </li>
  <li><a href="#4-第4章-决策树">4. 第4章 决策树</a>
    <ul>
      <li><a href="#41-思维导图">4.1. 思维导图</a>
        <ul>
          <li><a href="#411-章节导图">4.1.1. 章节导图</a></li>
          <li><a href="#412-如何生成一棵决策树">4.1.2. 如何生成一棵决策树</a></li>
        </ul>
      </li>
      <li><a href="#42-基本流程">4.2. 基本流程</a></li>
      <li><a href="#43-划分选择">4.3. 划分选择</a>
        <ul>
          <li><a href="#431-信息增益">4.3.1. 信息增益</a></li>
          <li><a href="#432-增益率">4.3.2. 增益率</a></li>
          <li><a href="#433-基尼指数">4.3.3. 基尼指数</a></li>
        </ul>
      </li>
      <li><a href="#44-剪枝处理">4.4. 剪枝处理</a>
        <ul>
          <li><a href="#441-预剪枝">4.4.1. 预剪枝</a></li>
          <li><a href="#442-后剪枝">4.4.2. 后剪枝</a></li>
        </ul>
      </li>
      <li><a href="#45-连续与缺失值">4.5. 连续与缺失值</a>
        <ul>
          <li><a href="#451-连续值处理">4.5.1. 连续值处理</a></li>
          <li><a href="#452-缺失值处理">4.5.2. 缺失值处理</a></li>
        </ul>
      </li>
      <li><a href="#46-多变量决策树">4.6. 多变量决策树</a></li>
      <li><a href="#47-阅读材料">4.7. 阅读材料</a></li>
    </ul>
  </li>
  <li><a href="#5-第5章-神经网络">5. 第5章 神经网络</a>
    <ul>
      <li><a href="#50-思维导图">5.0. 思维导图</a></li>
      <li><a href="#51-神经元模型">5.1. 神经元模型</a></li>
      <li><a href="#52-感知机与多层网络">5.2. 感知机与多层网络</a></li>
      <li><a href="#53-误差逆传播算法">5.3. 误差逆传播算法</a></li>
      <li><a href="#54-全局最小与局部极小">5.4. 全局最小与局部极小</a></li>
      <li><a href="#55-其他常见神经网络">5.5. 其他常见神经网络</a>
        <ul>
          <li><a href="#551-rbf网络">5.5.1. RBF网络</a></li>
          <li><a href="#552-art网络">5.5.2. ART网络</a></li>
          <li><a href="#553-som网络">5.5.3. SOM网络</a></li>
          <li><a href="#554-级联相关网络">5.5.4. 级联相关网络</a></li>
          <li><a href="#555-elman网络">5.5.5. Elman网络</a></li>
          <li><a href="#556-boltzmann机">5.5.6. Boltzmann机</a></li>
        </ul>
      </li>
      <li><a href="#56-深度学习">5.6. 深度学习</a></li>
    </ul>
  </li>
  <li><a href="#6-第6章-支持向量机">6. 第6章 支持向量机</a>
    <ul>
      <li><a href="#60-思维导图">6.0. 思维导图</a></li>
      <li><a href="#61-间隔与支持向量">6.1. 间隔与支持向量</a></li>
      <li><a href="#62-对偶问题">6.2. 对偶问题</a></li>
      <li><a href="#63-核函数">6.3. 核函数</a></li>
      <li><a href="#64-软间隔与正则化">6.4. 软间隔与正则化</a></li>
      <li><a href="#65-支持向量回归">6.5. 支持向量回归</a></li>
      <li><a href="#66-核方法">6.6. 核方法</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h1 id="1-第1章-绪论">1. 第1章 绪论</h1>

<h1 id="2-第2章-模型评估与选择">2. 第2章 模型评估与选择</h1>

<h2 id="21-思维导图">2.1. 思维导图</h2>

<center><img src="../assets/img/posts/20211222/16.jpg" /></center>

<h2 id="22-经验误差与过拟合">2.2. 经验误差与过拟合</h2>
<p><strong>定义：</strong></p>
<ul>
  <li><strong>错误率(error rate)</strong>: 如果m个样本中有a个样本分类错误，则错误率E=a/m。</li>
  <li><strong>精度(accuracy)</strong>：精度=1-错误率。</li>
  <li><strong>训练误差</strong>：学习器在训练集上的误差称为训练误差或者经验误差。</li>
  <li><strong>泛化误差(generalization error)</strong>：学习器在新样本上的误差称为泛化误差。</li>
  <li><strong>过拟合(overfitting)</strong>：当学习器把训练样本学得太好了的时候，很可能把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这就会导致泛化性能下降。</li>
  <li><strong>欠拟合(underfitting)</strong>：对训练样本的一般性质尚未学好。</li>
</ul>

<h2 id="23-评估方法">2.3. 评估方法</h2>
<p>通常我们可通过实验测试来对学习器的泛化误差进行评估而做出选择，于是我们需要一个<strong>测试集(testing set)</strong>，然后以测试集上的测试误差作为泛化误差的近似。下面介绍一些常见的处理数据集D的方法(数据集D-&gt;训练集S+测试集T)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<h3 id="231-留出法">2.3.1. 留出法</h3>
<ul>
  <li>留出发(hold-out)直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T。在S上训练出模型后，用T来评估其测试误差。</li>
  <li>以采样的角度来看待数据集划分的过程，则保留类别比例的采样方式通常称为<strong>分层采样</strong>。比如1000个样本，50%的正例，50%负例，以7：3划分数据集，那么训练集包含350个正例和350个负例就是分层采样。</li>
  <li>在使用留出法评估结果时，一般要采用若干次随即划分、重复进行实验评估后取平均值作为留出法的评估结果。</li>
  <li>常用做法时将大约2/3~4/5的样本用于训练。</li>
</ul>

<h3 id="232-交叉验证法">2.3.2. 交叉验证法</h3>
<ul>
  <li><strong>交叉验证法(cross validation)</strong>先将数据集D划分为k个大小相似的互斥子集，每个子集D<sub>i</sub>都尽可能保持数据分布的一致性。然后每次用k-1个自己的并集作为训练集，余下的那个子集作为测试集，这样就可以获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。</li>
  <li>交叉验证也称为k折交叉验证。k的常见取值有10、5、20。</li>
  <li><strong>留一法</strong>就是k=m，其中数据集D有m个样本。</li>
</ul>

<h3 id="233-自助法">2.3.3. 自助法</h3>
<ul>
  <li><strong>自助法(bootstrapping)</strong>：给定包含m个样本的数据集D，每次随机从D中挑选一个样本，将其拷贝放入D’, 然后再将该样本放回最初的数据集D中，这个过程重复执行m次后，我们获得了包含m个样本的数据集D’。我们将D’作为训练集，D\D’作为测试集。</li>
  <li>不难发现大概有36.8%(m趋于无限大时)的样本在m次采样中始终不被采到。</li>
</ul>

<center>$\lim\limits_{m\rightarrow\infty}(1-\frac{1}{m})^m = \frac{1}{e} ≈ 0.368$</center>

<ul>
  <li>缺点：自助法产生的数据集改变了初始数据集的分布，这样会引入估计偏差。因此，在初始数据量足够时，留出法和交叉验证法更常用一些。</li>
</ul>

<h2 id="24-性能度量">2.4. 性能度量</h2>
<ul>
  <li>对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是<strong>性能度量(performance measure)</strong>。</li>
  <li>在预测任务中，给定D = {(x<sub>1</sub>,y<sub>1</sub>), (x<sub>2</sub>,y<sub>2</sub>)…(x<sub>m</sub>,y<sub>m</sub>)}, 其中y<sub>i</sub>是x<sub>i</sub>的真实标记。学习器f。</li>
  <li><strong>均方误差(mean squared error)</strong>：回归任务最常用的性能度量是均方误差：</li>
</ul>

<center>$E(f;D)=\frac{1}{m}\sum_1^m(f(x_i)-y_i)^2$</center>

<p>接下来我将介绍<strong>分类任务</strong>中常用的性能度量</p>

<h3 id="241-错误率与精度">2.4.1. 错误率与精度</h3>
<p>本章开头提到了错误率和精度，这是分类任务中最常用的两种性能度量，既适用于二分类任务，也适用于多分类任务。</p>

<h3 id="242-查准率查全率与f1">2.4.2. 查准率、查全率与F1</h3>

<ul>
  <li>针对二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例(true positive)、假正例(false positive)、真反例(true negative)、假反例(false negative)，分别记为TP、FP、TN、FN。</li>
  <li>混淆矩阵(confusion matrix)</li>
</ul>

<table>
  <tbody>
    <tr>
      <td>真\预</td>
      <td>正例</td>
      <td>反例</td>
    </tr>
    <tr>
      <td>正例</td>
      <td>TP</td>
      <td>FN</td>
    </tr>
    <tr>
      <td>反例</td>
      <td>FP</td>
      <td>TN</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>查准率(precision)，记为P，它表示选择的好瓜中有多少是真正的好瓜</li>
</ul>

<center>$P=\frac{TP}{TP+FP}$</center>

<ul>
  <li>查全率(recall)，记为R，它表示好瓜中有多少被选出来了</li>
</ul>

<center>$R=\frac{TP}{TP+FN}$</center>

<ul>
  <li>
    <p>一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。</p>
  </li>
  <li>
    <p>P-R曲线：在很多情形下，我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器认为最可能是正例的样本，排在最后的则是学习器认为最不可能是正例的样本，按此顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率、查准率，也得到了P-R图。</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/2.jpg" /></center>

<ul>
  <li>
    <p>如果一个学习器的P-R曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者。如果两者曲线相交，则可以通过比较平衡点(break-even-point)来比较，越大越好。</p>
  </li>
  <li>
    <p>F1度量：F1综合考虑了查准率和查全率，是他们的调和平均</p>
  </li>
</ul>

<center>$F1=\frac{2*P*R}{P+R}$</center>

<ul>
  <li>F1的一般形式：有时候我们希望赋予查准率和查重率不同的权重：</li>
</ul>

<center>$F_\beta=\frac{(1+\beta^2)*P*R}{\beta^2*P+R}$</center>

<p>其中β大于1表示查全率有更大影响</p>

<ul>
  <li>
    <p>有时候我们有多个二分类混淆矩阵，我们希望在这n个混淆矩阵上综合考虑查准率和查全率，那么我们有以下的度量</p>
  </li>
  <li>
    <p>宏查准率macro-P，宏查全率macro-R，宏F1：</p>
  </li>
</ul>

<center>$macro-P=\frac{1}{n}\sum_1^nP_i$</center>

<center>$macro-R=\frac{1}{n}\sum_1^nR_i$</center>

<ul>
  <li>微查准率micro-P，微查全率micro-P，微F1：</li>
</ul>

<p>对TP、FP、TN、FN进行平均</p>

<center>$micro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}$</center>

<center>$micro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}$</center>

<h3 id="243-roc与auc">2.4.3. ROC与AUC</h3>
<ul>
  <li>
    <p>很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为反类。</p>
  </li>
  <li>
    <p>ROC曲线是从排序角度来出发研究学习器的泛化性能的工具。ROC的全称是Receiver Operating Characteristic。</p>
  </li>
  <li>
    <p>与P-R曲线类似，我们根据学习器的预测结果进行排序，按此顺序逐个把样本作为正例进行预测，计算出真正例率TPR(true positive rate)与假正例率FPR(false positive rate)并以它们分别为横纵坐标画出ROC曲线</p>
  </li>
</ul>

<center>$TPR=\frac{TP}{TP+FN}$</center>

<center>$FPR=\frac{FP}{FP+TN}$</center>

<center><img src="../assets/img/posts/20211222/3.jpg" /></center>

<ul>
  <li>
    <p>同样的，如果一个学习器的ROC曲线被另一个学习器完全包住，则认为后者的性能优于前者。若两个曲线相交，则比较曲线下的面积AUC(area under curve)</p>
  </li>
  <li>
    <p>形式化的看，AUC考虑的是样本预测的排序质量，那么我们可以定义一个排序损失l<sub>rank</sub></p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/4.jpg" /></center>

<center>AUC=1-l<sub>rank</sub></center>

<h3 id="244-代价敏感错误率与代价曲线">2.4.4. 代价敏感错误率与代价曲线</h3>
<ul>
  <li>有时候将正例误判断为负例与将负例误判断为正例所带来的后果不一样，我们赋予它们非均等代价</li>
  <li>代价矩阵(cost matrix)</li>
</ul>

<table>
  <tbody>
    <tr>
      <td>真\预</td>
      <td>第0类</td>
      <td>第1类</td>
    </tr>
    <tr>
      <td>第0类</td>
      <td>0</td>
      <td>cost<sub>01</sub></td>
    </tr>
    <tr>
      <td>第1类</td>
      <td>cost<sub>10</sub></td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<p>其中cost<sub>ij</sub>表示将第i类样本预测为第j类样本的代价</p>

<ul>
  <li>代价敏感错误率</li>
</ul>

<center><img src="../assets/img/posts/20211222/5.jpg" /></center>

<ul>
  <li>在非均等代价下，ROC曲线不能直接反映出学习器的期望总代价，而代价曲线则可达到此目的，横轴是正例概率代价，纵轴是归一化代价</li>
</ul>

<center><img src="../assets/img/posts/20211222/6.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/7.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/8.jpg" /></center>

<h2 id="25-比较检验">2.5. 比较检验</h2>
<p>我们有了实验评估方法与性能度量，是否就可以对学习器的性能进行评估比较了呢？实际上，机器学习中性能比较这件事比大家想象的要复杂很多，我们需要考虑多种因素的影响，下面介绍一些对学习器性能进行比较的方法，本小节默认以错误率作为性能度量。(但我觉得似乎同一个数据集下就可以比较)</p>

<h3 id="251-假设检验">2.5.1. 假设检验</h3>
<ul>
  <li>假设检验中的假设是对学习器泛化错误率分布的某种猜想或者判断，例如“ε=ε<sub>0</sub>”这样的假设</li>
  <li>现实任务中我们并不知道学习器的泛化错误率，我们只能获知其测试错误率$\hat{\epsilon}$，但直观上，两者接近的可能性比较大，因此可根据测试错误率估推出泛化错误率的分布。</li>
  <li>泛化错误率为$\epsilon$的学习器被测得测试错误率为$\hat{\epsilon}$的概率：</li>
</ul>

<center><img src="../assets/img/posts/20211222/9.jpg" /></center>

<ul>
  <li>我们发现$\epsilon$符合二项分布</li>
</ul>

<center><img src="../assets/img/posts/20211222/10.jpg" /></center>

<ul>
  <li>
    <p>二项检验：我们可以使用<strong>二项检验(binomial test)</strong>来对“$\epsilon$&lt;0.3”这样的假设进行检验，即在$\alpha$显著度下，$1-\alpha$置信度下判断假设是否成立。</p>
  </li>
  <li>
    <p>t检验：我们也可以用t检验(t-test)来检验。</p>
  </li>
  <li>
    <p>上面介绍的都是针对单个学习器泛化性能的假设进行检验</p>
  </li>
</ul>

<h3 id="252-交叉验证t检验">2.5.2. 交叉验证t检验</h3>

<ul>
  <li>
    <p>对于两个学习器A和B，若我们使用k折交叉验证法得到的测试错误率分别是$\epsilon_1^A$, $\epsilon_2^A$…$\epsilon_k^A$和$\epsilon_1^B$, $\epsilon_2^B$…$\epsilon_k^B$。其中$\epsilon_i^A$和$\epsilon_i^B$是在相同第i折训练集上得到的结果。则可以用k折交叉验证“成对t检验”来进行比较检验。</p>
  </li>
  <li>
    <p>我们这里的基本思想是若两个学习器的性能相同，则它们使用相同的训练测试集得到的错误率应该相同，即$\epsilon_i^A=\epsilon_1^B$</p>
  </li>
  <li>
    <p>$\Delta_i$ = $\epsilon_i^A$ - $\epsilon_i^B$，然后对$\Delta$进行分析</p>
  </li>
</ul>

<h3 id="253-mcnemar检验">2.5.3. McNemar检验</h3>
<ul>
  <li>对于二分类问题，使用留出法不仅可以估计出学习器A和B的测试错误率，还可获得两学习器分类结果的差别，即两者都正确、都错误…的样本数</li>
</ul>

<center><img src="../assets/img/posts/20211222/11.jpg" /></center>

<ul>
  <li>若我们假设两学习器性能相同，则应有e<sub>01</sub>=e<sub>10</sub>，那么变量|e<sub>01</sub>-e<sub>10</sub>|应该服从正态分布/卡方分布，然后用McNemar检验</li>
</ul>

<h3 id="254-friedman检验与nemenyi后续检验">2.5.4. Friedman检验与Nemenyi后续检验</h3>

<ul>
  <li>
    <p>交叉验证t检验与McNemar检验都是在一个数据集上比较两个算法的性能，但是很多时候，我们会在一组数据集上比较多个算法。</p>
  </li>
  <li>
    <p>当有多个算法参与比较时，一种做法是在每个数据集上本别列出两两比较的结果。另一种方法则是基于算法排列的<strong>Friedman检验</strong></p>
  </li>
  <li>
    <p>假定我们用D<sub>1</sub>, D<sub>2</sub>, D<sub>3</sub>, D<sub>4</sub>四个数据集对算法A、B、C进行比较。首先，使用留出法或交叉验证法得到每个算法在每个数据集上的测试结果。然后在每个数据集上根据测试性能由好到坏排序，并赋予序值1,2,…若算法的测试性能相同，则平分序值。</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/12.jpg" /></center>

<ul>
  <li>
    <p>然后使用Fredman检验来判断这些算法是否性能都相同，若相同，那么它们的平均序值应当相同。r<sub>i</sub>表示第i个算法的平均序值，那么它的均值和方差应该满足…</p>
  </li>
  <li>
    <p>若“所有算法的性能相同”这个假设被拒绝，则说明算法的性能显著不同，这时需要后续检验(post-hoc test)来进一步区分各算法，常用的有Nemenyi后续检验</p>
  </li>
  <li>
    <p>Nemenyi检验计算出平均序值差别的临界值域</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/13.jpg" /></center>

<ul>
  <li>在表中找到k=3时q<sub>0.05</sub>=2.344，根据公式计算出临界值域CD=1.657，由表中的平均序值可知A与B算法的差距，以及算法B与C的差距均未超过临界值域，而算法A与C的差距超过临界值域，因此检验结果认为算法A与C的性能显著不同，而算法A与B以及算法B与C的性能没有显著差别。</li>
</ul>

<center><img src="../assets/img/posts/20211222/14.jpg" /></center>

<h2 id="26-偏差与方差">2.6. 偏差与方差</h2>

<ul>
  <li>
    <p>对学习算法除了通过实验估计其泛化性能，人们往往还希望了解它“为什么”具有这样的性能。偏差-方差分解是解释学习算法泛化性能的一种重要工具</p>
  </li>
  <li><strong>偏差</strong>度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力</li>
  <li><strong>方差</strong>度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响</li>
  <li>
    <p><strong>噪声</strong>则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度</p>
  </li>
  <li>一般来说，偏差与方差是有冲突的，也就是偏差大的方差小，偏差小的方差大</li>
</ul>

<center><img src="../assets/img/posts/20211222/15.jpg" /></center>

<h1 id="3-第3章-线性模型">3. 第3章 线性模型</h1>
<h2 id="31-思维导图">3.1. 思维导图</h2>

<center><img src="../assets/img/posts/20211222/42.jpg" /></center>

<h2 id="32-基本形式">3.2. 基本形式</h2>

<ul>
  <li>
    <p>给定由d个属性描述的示例$x=(x_1;x_2;…x_d)$，这是一个列向量，其中$x_i$是$x$在第i个属性上的取值。</p>
  </li>
  <li>
    <p><strong>线性模型(linear model)</strong>试图学得一个通过属性线性组合来进行预测的函数：</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/17.jpg" /></center>

<ul>
  <li>向量形式：</li>
</ul>

<center>$f(x)=\omega^Tx+b$</center>

<p>其中$\omega=(\omega_1;\omega_2…\omega_d)$</p>

<ul>
  <li>
    <p>当$\omega$和b学得后，模型就得以确定</p>
  </li>
  <li>
    <p>线性模型的优点：形式简单，易于建模，良好的可解释性(comprehensibility)</p>
  </li>
</ul>

<h2 id="33-线性回归">3.3. 线性回归</h2>
<ul>
  <li>
    <p>对离散属性的处理，若属性值存在“序”的关系，可通过连续化将其转化为连续值，比如高、矮变成1、0；2.若不存在序的关系，可转化为k维向量。</p>
  </li>
  <li>
    <p>均方误差是回归任务中最常用的性能度量，试图让均方误差最小化：</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/18.jpg" /></center>

<ul>
  <li>
    <p>均方误差有非常好的几何意义，它对应了欧氏距离。基于均方误差最小化来进行模型求解的方法称为<strong>最小二乘法(least square method)</strong>。在线性回归中，最小二乘法就是试图找到一条直线，使得样本到直线上的欧氏距离之和最小</p>
  </li>
  <li>
    <p>首先观察一个属性值的情况。求解$\omega$和$b$使得均方误差最小化的过程，称为线性回归的最小二乘“参数估计”，我们令均方误差分别对$\omega$和$b$求导令其为零，可以得到最优解的<strong>闭式解(closed-form)</strong>,即解析解</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/19.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/20.jpg" /></center>

<ul>
  <li>更一般的情况是d个属性，称其为“多元线性回归”，同样的步骤，只是$\omega$变成向量形式，自变量写成m*(d+1)的矩阵形式，m对应了m个样本，d+1对应了d个属性和偏置。同样的求导为0然后得出$\hat{\omega}$最优解的闭式解，其中$\hat{\omega}=(\omega;b)$。当$X^TX$为满秩矩阵<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>或正定矩阵时，有唯一的解：</li>
</ul>

<center><img src="../assets/img/posts/20211222/21.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/22.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/23.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/24.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/25.jpg" /></center>

<ul>
  <li>
    <p>然而现实任务中往往不是满秩矩阵，例如在许多任务中我们会遇到大量的变量其数目甚至超过样例数。那么我们会求出$\hat{\omega}=(\omega;b)$的多个解，它们都能使均方误差最小化。选择哪一个解作为输出与学习算法的归纳偏好决定，常见的做法是<strong>引入正则化(regularization)</strong></p>
  </li>
  <li>
    <p><strong>广义线性模型(generalized linear model)</strong>:</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/26.jpg" /></center>

<center>$g(y)=\omega^Tx+b$</center>

<p>其中g()单调可微，被称为联系函数。比如当g()=ln()时称为对数线性回归</p>

<h2 id="34-对数几率回归">3.4. 对数几率回归</h2>
<ul>
  <li>
    <p>上一节讨论的是线性回归进行回归学习。那么如果我们要做的是分类任务应该怎么改变？我们只需要找到一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。</p>
  </li>
  <li>
    <p>考虑二分类问题，那么我们可以用单位阶跃函数/Heaviside函数联系y与z，其中y是真是标记，z是预测值，$z=\omega^Tx+b$</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/27.jpg" /></center>

<ul>
  <li>
    <p>但是它有个问题就是不连续，所以我们希望找到能在一定程度上近似它的替代函数。</p>
  </li>
  <li>
    <p><strong>对数几率函数(logistic function)</strong>就是一个替代函数：</p>
  </li>
</ul>

<center>$y=\frac{1}{1+e^{-z}}$</center>

<center><img src="../assets/img/posts/20211222/28.jpg" /></center>

<ul>
  <li>那么这个式子可以转化为：可以把y视为样本x作为正例的可能性，则1-y是其反例的可能性，两者的比值称为几率(odds)：</li>
</ul>

<center><img src="../assets/img/posts/20211222/29.jpg" /></center>

<ul>
  <li>若将y视为类后验概率估计。则式子可以重写为：</li>
</ul>

<center><img src="../assets/img/posts/20211222/30.jpg" /></center>

<ul>
  <li>接下来我们可以通过<strong>极大似然法(maximum likelihood method)</strong>来估计$\omega$和$b$。给定数据集，对数似然函数<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>为：</li>
</ul>

<center><img src="../assets/img/posts/20211222/31.jpg" /></center>

<p>即每个样本属于其真实标记的概率越大越好。</p>

<ul>
  <li>推导过程：</li>
</ul>

<center><img src="../assets/img/posts/20211222/32.jpg" /></center>

<p>上面有个式子应该有问题，(3.26)应该是</p>

<center>$p(y_i|x_i;\omega,b) = p_1(\hat{x_i};\beta)^{y_i}p_0(\hat{x_i};\beta)^{1-y_i}$</center>

<p>因为$\beta$是高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法如<strong>梯度下降法(gradient descent method)</strong>和牛顿法都可以求得最优解</p>

<h2 id="35-线性判别分析">3.5. 线性判别分析</h2>

<ul>
  <li>
    <p>线性判别分析(Linear Discriminant Analysis，简称LDA)是一种经典的线性学习方法。</p>
  </li>
  <li>
    <p>LDA的思想非常朴素: 给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离;在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/33.jpg" /></center>

<ul>
  <li>令$X_i$、$\mu_i$、$\Sigma_i$分别表示第i类示例的集合、均值向量<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>、协方差矩阵<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>。</li>
</ul>

<ul>
  <li>欲使同类样例的投影点尽可能接近，可以让同类样例投影点的协方差尽可能小，即$\omega^T\Sigma_0\omega+\omega^T\Sigma_1\omega$尽可能小;而欲使异类样例的投影点尽可能远离，可以让类中心之间的距离尽可能大，即$||\omega^T\mu_0-\omega^T\mu_1||$尽可能大:</li>
</ul>

<center><img src="../assets/img/posts/20211222/34.jpg" /></center>

<ul>
  <li>剩余推导过程：</li>
</ul>

<center><img src="../assets/img/posts/20211222/35.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/36.jpg" /></center>

<ul>
  <li>值得一提的是，LDA可从贝时斯决策理论的角度来阐释，并可证明，当两类数据同先验、满足高斯分布且协方差相等时，LDA可达到最优分类。</li>
</ul>

<h2 id="36-多分类学习">3.6. 多分类学习</h2>
<ul>
  <li>
    <p>现实中常遇到多分类学习任务，有些二分类学习方法可直接推广到多分类，但在更多情形下，我们是基于一些基本策略，利用二分类学习器来解决多分类问题。</p>
  </li>
  <li>
    <p>不失一般性，考虑N个类别$C_1$、$C_2$…$C_N$，多分类学习的基本思路是”<strong>拆解法</strong>”，即将多分类任务拆为若干个二分类任务求解。具体来说，先对问题进行拆分，然后为拆出的每个二分类任务训练一个分类器;在测试时，对这些分类器的预测结果进行集成以获得最终的多分类结果。</p>
  </li>
  <li>
    <p>这里我们着重介绍如何拆分，最经典的拆分策略有三种：一对一(One vs One)、一对其余(One vs Rest)、多对多(Many vs Many)</p>
  </li>
  <li>
    <p>一对一：将这N个类别两两配对，从而产生 N(N-1)/2个二分类任务。在测试阶段，新样本将同时提交给所有分类器，于是我们将得到N(N-1)/2个分类结果，最终结果可通过投票产生:即把被预测得最
多的类别作为最终分类结果。</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/37.jpg" /></center>

<ul>
  <li>一对其余：OvR则是每次将一个类的样例作为正例，所有其他类的样例作为反例来训练N个分类器。在测试时若仅有一个分类器预测为正类，则对应的类别标记作为最终分类结果。若有多个分类器预测为正类，则通常考虑各分类器的预测置信度，选择置信度最大的类别标记作为分类结果。</li>
</ul>

<center><img src="../assets/img/posts/20211222/38.jpg" /></center>

<ul>
  <li>
    <p>多对多MvM是每次将若干个类作为正类，若干个其他类作为反类。这里我们介绍一种最常用的MvM技术：<strong>纠错输出码(ECOC)</strong></p>
  </li>
  <li>
    <p>ECOC是将编码的思想引入类别拆分，主要分为两步：</p>
  </li>
</ul>

<p>  1.编码： 对N个类别做M次划分，每次划分将一部分类别划为正类，一部分划为反类，从而形成一个二分类训练集;这样一共产生M个训练集，可训练出M个分类器</p>

<p>  2.解码：:M个分类器分别对测试样本进行预测，这些预测标记组成一个编码.将这个预测编码与每个类别各自的编码进行比较，返回其中距离最小的类别作为最终预测结果。</p>

<ul>
  <li>类别划分通过<strong>编码矩阵</strong>指定。编码矩阵有多种形式，常见的有二元码和三元码，前者将每个类别分别指定为正类和反类，后者在正、反类之外，还可指定“停用类”</li>
</ul>

<center><img src="../assets/img/posts/20211222/39.jpg" /></center>

<h2 id="37-类别不平衡问题">3.7. 类别不平衡问题</h2>

<ul>
  <li>
    <p>前面介绍的分类学习方法都有一个共同的基本假设：即不同类别的训练样例数目相当。如果不同类别的样例数差别很大，会对学习过程造成困扰。</p>
  </li>
  <li>
    <p><strong>类别不平衡(class-imbalance)</strong>就是指分类任务中不同类别的训练样例数目差别很大的情况。</p>
  </li>
  <li>
    <p><strong>再缩放(rescaling)</strong>是类别不平衡中的一个基本策略：比如在最简单的二分类问题中，我们假设y大于0.5为正例，y小于0.5为负例，但是在类别不平衡时，我们可以改变阈值来达到再平衡：</p>
  </li>
</ul>

<p>  将</p>

<center><img src="../assets/img/posts/20211222/40.jpg" /></center>

<p>  变成</p>

<center><img src="../assets/img/posts/20211222/41.jpg" /></center>

<ul>
  <li>现有的解决类别不平衡的技术大体上有三类做法(这里我们均假设正例样本少):</li>
</ul>

<p>  1.第一类是直接对训练集里的反类样例进行”欠采样” (undersampling)，即去除一些反例使得正、反例数日接近，然后再进行学习;</p>

<p>  2.第二类是对训练集里的正类样例进行”过采样” (oversampling)，即增加一些正例使得正、反例数目接近，然后再进行学习;</p>

<p>  3.第三类则是直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将上面的公式(改变阈值)嵌入到其决策过程中，称为”阔值移动” (threshold-moving)</p>

<ul>
  <li>需注意的是，过采样法不能简单地对初始正例样本进行重复来样，否则会招致严重的过拟合；过采样法的代表性算法SMOTE是通过对训练集里的正例进行插值来产生额外的正例.另一方面，欠采样法若随机丢弃反例可能丢失一些重要信息;欠采样法的代表性算法EasyEnsemble则是利用集成学习机制，将反例划分为若干个集合供不同学习器使用，这样对每个学习器来看都进行了欠采样，但在全局来看却不会丢失重要信息。</li>
</ul>

<h1 id="4-第4章-决策树">4. 第4章 决策树</h1>
<h2 id="41-思维导图">4.1. 思维导图</h2>
<h3 id="411-章节导图">4.1.1. 章节导图</h3>

<center><img src="../assets/img/posts/20211222/61.jpg" /></center>

<h3 id="412-如何生成一棵决策树">4.1.2. 如何生成一棵决策树</h3>

<center><img src="../assets/img/posts/20211222/62.jpg" /></center>

<h2 id="42-基本流程">4.2. 基本流程</h2>
<ul>
  <li>决策树是基于树结构来进行决策，其中包含一个根结点，多个内部结点和多个叶结点</li>
  <li>叶结点对应决策结果，其他每个结点都对应一个属性测试</li>
  <li>每个结点包含的样本集合根据属性测试被划分到子结点中，那么根结点包含样本全集</li>
  <li>决策树学习的目的是为了产生一棵泛化能力强的决策树</li>
  <li>决策树的生成是一个递归过程，下面这张图展示了递归的过程：</li>
</ul>

<center><img src="../assets/img/posts/20211222/43.jpg" /></center>

<p>对于每个结点，首先判断该结点的样本集是否属于同一个类别C，如果是，则将该结点标记为C类叶结点。再判断该结点的样本集的属性值是否完全相同(或者是否为空集)，如果是，则将该结点标记为D类叶结点，其中D类是这些样本中最多的类别。如果该结点即不是<strong>同属于一个类别</strong>也不是<strong>属性值取值相同</strong>，那么则需要继续划分，选择一个最优的划分属性$a_*$,创建新的分支，对于每个分支结点首先判断是否为空，如果为空则判定为E类叶结点，其中E类是父结点中类别最多的类。如果子结点不为空则递归。</p>

<h2 id="43-划分选择">4.3. 划分选择</h2>
<p>可以发现生成决策树最关键的步骤就是选择最优划分属性，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的<strong>纯度</strong>越来越高。我们有很多指标来确定选择哪一个属性作为最优划分选择，下面将分别介绍：</p>

<h3 id="431-信息增益">4.3.1. 信息增益</h3>
<ul>
  <li><strong>信息熵(information entropy)</strong>是度量样本集合纯度最常用的一种指标。下面是信息熵的定义公式：</li>
</ul>

<center><img src="../assets/img/posts/20211222/44.jpg" /></center>

<p>  其中$p_k$表示第k类样本在样本集D中所占比例，信息熵越小表示D的纯度越高。</p>

<ul>
  <li>假设离散属性a有V个可能的取值${a^1,a^2…a^V}$,那么我们可以计算出在使用a作为划分属性前后的信息熵差别，也就是<strong>信息增益(information gain)</strong>：</li>
</ul>

<center><img src="../assets/img/posts/20211222/45.jpg" /></center>

<p>  对每一个子结点$D^v$都赋予权重同时相加。</p>

<ul>
  <li>
    <p>著名的ID3决策树学习算法就是以信息增益作为准则来选择划分属性，我们希望找到信息增益最大的属性。</p>
  </li>
  <li>
    <p>书上使用信息增益划分的例子：</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/46.jpg" /></center>

<h3 id="432-增益率">4.3.2. 增益率</h3>
<ul>
  <li>
    <p>信息增益对可取值数目较多的属性有所偏好，比如我们使用编号这一属性来划分，每一个编号都只有一个样本，那么信息增益肯定增大了，但是决策树的泛化能力显然下降了。</p>
  </li>
  <li>
    <p><strong>增益率(gain ratio)</strong>，我们通过对信息增益除以IV来平衡属性数目带来的影响，增益率的定义如下：</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/47.jpg" /></center>

<p>  IV(intrinsic value)的定义如下：</p>

<center><img src="../assets/img/posts/20211222/48.jpg" /></center>

<p>  IV是属性a的固有值，属性a可取的数值数目越多，那么IV就越大</p>

<ul>
  <li>但是增益率也有问题，那就是对于可取值数目较少的属性有偏好，所以著名的C4.5决策树算法并不是直接使用增益率，而是先从候选划分属性中找出信息增益高于平均水平的属性，然后再从中选择增益率最高的属性</li>
</ul>

<h3 id="433-基尼指数">4.3.3. 基尼指数</h3>
<ul>
  <li>CART决策树使用基尼指数来选择划分属性</li>
  <li>数据集D的纯度定义如下</li>
</ul>

<center><img src="../assets/img/posts/20211222/49.jpg" /></center>

<p>  直观来说，Gini反映了从数据集随便抽取两个样本，它们类别不一致概率</p>

<ul>
  <li><strong>基尼指数</strong>定义如下：</li>
</ul>

<center><img src="../assets/img/posts/20211222/50.jpg" /></center>

<p>  很明显，我们希望基尼指数越小越好，所以我们选择基尼指数最小的属性最为最优划分属性。</p>

<h2 id="44-剪枝处理">4.4. 剪枝处理</h2>
<ul>
  <li>不难发现，上面对于属性的划分很容易过拟合，所以针对过拟合现象，决策树选择<strong>剪枝(pruning)</strong>来对付过拟合</li>
  <li>剪枝就是去掉一些分支来降低过拟合的风险，剪枝可以分为预剪枝和后剪枝</li>
  <li><strong>预剪枝(prepruning)</strong>:在决策树生成的过程中，对每个结点在划分前进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分</li>
  <li><strong>后剪枝(postpruning)</strong>:从训练集生成了一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能的提升，则将该子树替换为叶结点。</li>
</ul>

<h3 id="441-预剪枝">4.4.1. 预剪枝</h3>
<ul>
  <li>如何判断决策树泛化性能？可以使用留出法预留一部分数据用作验证集进行性能评估，性能度量可以用之前介绍的那些，本小节使用精度作为性能度量</li>
  <li>预剪枝生成的决策树：</li>
</ul>

<center><img src="../assets/img/posts/20211222/51.jpg" /></center>

<ul>
  <li>可以发现预剪枝显著减少了分支的数量，这样可以减少决策树的训练时间开销。但是这样也有一个问题，就是有些分支的当前划分虽然不能提升泛化性能，但是后续划分却有可能导致性能显著提高，这样就带来了欠拟合的风险</li>
</ul>

<h3 id="442-后剪枝">4.4.2. 后剪枝</h3>
<ul>
  <li>首先生成决策树，然后对每个结点进行评估是否需要剪枝</li>
</ul>

<center><img src="../assets/img/posts/20211222/52.jpg" /></center>

<ul>
  <li>虽然后剪枝决策树的欠拟合风险小，泛化性能也往往优于预剪枝决策树，但是后剪枝的时间开销大</li>
</ul>

<h2 id="45-连续与缺失值">4.5. 连续与缺失值</h2>
<h3 id="451-连续值处理">4.5.1. 连续值处理</h3>
<ul>
  <li>到目前为止仅讨论了基于离散属性来生成决策树，但是现实学习任务中通常会遇到连续属性</li>
  <li>很明显连续属性不能根据连续属性的可取值来对结点进行划分，我们需要用到<strong>连续属性离散化</strong>的技术，最简单的策略是<strong>二分法</strong></li>
  <li>给定样本集D和和连续属性a，假定a在D上出现了n个不同的取值，将这些值从小到大排序，然后每次选择每两个数的中位数t作为划分点，那么连续值就可以当作离散值来处理了，分出的子样本集分别记作$D_t^+$和$D_t^-$</li>
</ul>

<center><img src="../assets/img/posts/20211222/53.jpg" /></center>

<ul>
  <li>划分结果：</li>
</ul>

<center><img src="../assets/img/posts/20211222/54.jpg" /></center>

<ul>
  <li>需要注意的是：<strong>连续属性在划分后并不会被丢失，后续划分仍然可以使用</strong></li>
</ul>

<h3 id="452-缺失值处理">4.5.2. 缺失值处理</h3>
<ul>
  <li>在实际数据中一般都有很多缺失值，所以我们需要考虑如何对含有缺失值的数据进行学习</li>
  <li>给几个定义：$\tilde{D}$表示D中属性a上没有缺失值的样本子集，假设属性值a可取值{$a^1$,$a^2$…$a^V$}, $\tilde{D}^v$表示$\tilde{D}$中属性值a取值为$a^v$的子集，$\tilde{D}_k$表示样本子集，我们为每个样本赋予权重$\omega_x$(决策树开始阶段，根结点中权重初始化为1)并定义：</li>
</ul>

<center><img src="../assets/img/posts/20211222/55.jpg" /></center>

<ul>
  <li>
    <p>直观地看，对属性a，$\rho$表示无缺失值样本所占比例，$\tilde{p}_k$表示无缺失样本中第k类样本所占的比例，$\tilde{r}_v$表示无缺失值样本在属性上取值为$a^v$所占的比例</p>
  </li>
  <li>
    <p>那么我们可以将信息增益的公式推广为</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/56.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/57.jpg" /></center>

<ul>
  <li>
    <p>那么对于那些在该属性上缺失的值如何处理呢？分两种情况：若样本$x$在划分属性$a$上的取值己知, 则将$x$划入与其取值对应的子结点，且样本权值在于结点中保持为$\omega_x$, 若样本$x$在划分属性$a$上的取值未知，则将$x$同时划入所有子结点, 且样本权值在与属性值$a^v$对应的子结点中调整为$\tilde{r}_v*\omega_x$，直观地看，这就是让同一个样本以不同的概率划入到不同的子结点中去。</p>
  </li>
  <li>
    <p>C4.5就是使用了上述的解决方法</p>
  </li>
</ul>

<h2 id="46-多变量决策树">4.6. 多变量决策树</h2>
<ul>
  <li>若我们把每个属性视为坐标空间中的一个坐标轴，则d个属性描述的样本就对应了d维空间中的一个数据点，对样本分类则意味着在这个坐标空间中寻找不同类样本之间的分类边界，决策树生成的分类边界有个明显的特点：<strong>轴平行</strong>，即它的分类边界由若干个与坐标轴平行的分段组成</li>
</ul>

<center><img src="../assets/img/posts/20211222/58.jpg" /></center>

<ul>
  <li>这样的决策树由于要进行大量的属性测试，预测时间开销会很大，所以我们希望使用如下图红线所示的斜划分。<strong>多变量决策树</strong>就是能实现这样斜划分甚至更复杂划分的决策树</li>
</ul>

<center><img src="../assets/img/posts/20211222/59.jpg" /></center>

<ul>
  <li>以实现斜划分的决策树为例，非叶结点不再是仅对某一个属性，而是对属性的线性组合进行测试</li>
</ul>

<center><img src="../assets/img/posts/20211222/60.jpg" /></center>

<h2 id="47-阅读材料">4.7. 阅读材料</h2>
<ul>
  <li>
    <p>多变量决策树算法主要有OC1，还有一些算法试图在决策树的叶结点上嵌入神经网络，比如感知机树在每个叶结点上训练一个感知机</p>
  </li>
  <li>
    <p>有些决策树学习算法可进行<strong>“增量学习”(incrementallearning)</strong>，即在接收到新样本后可对己学得的模型进行调整，而不用完全重新学习。主要机制是通过调整分支路径上的划分属性次序来对树进行部分重构，代表性算法ID4、ID5R、ITI等。增量学习可有效地降低每次接收到新样本后的训练时间开销，但多步增量学习后的模型会与基于全部数据训练而得的模型有较大差别。</p>
  </li>
</ul>

<h1 id="5-第5章-神经网络">5. 第5章 神经网络</h1>

<h2 id="50-思维导图">5.0. 思维导图</h2>

<center><img src="../assets/img/posts/20211222/73.jpg" /></center>

<h2 id="51-神经元模型">5.1. 神经元模型</h2>
<ul>
  <li>神经网络定义: 神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应</li>
  <li>神经网络中最基本的成分是神经元模型(neuron)</li>
  <li>M-P神经元模型:</li>
</ul>

<center><img src="../assets/img/posts/20211222/63.jpg" /></center>

<ul>
  <li>在这个模型中，神经元接收来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过激活函数处理以产生神经元的输出</li>
  <li>激活函数: 有<strong>阶越函数</strong>和<strong>Sigmoid函数</strong>等等</li>
</ul>

<center><img src="../assets/img/posts/20211222/64.jpg" /></center>

<h2 id="52-感知机与多层网络">5.2. 感知机与多层网络</h2>
<ul>
  <li>感知机(perceptron)由两层神经元组成，如下图所示</li>
</ul>

<center><img src="../assets/img/posts/20211222/65.jpg" /></center>

<ul>
  <li>
    <p>权重$\omega_i$以及阈值$\theta$可通过学习得到</p>
  </li>
  <li>
    <p>感知机的学习规则非常简单，对训练样例(x, y)，若当前感知机的输出为$\hat{y}$, 则感知机权重将这样调整</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/66.jpg" /></center>

<ul>
  <li>其中$\eta$称为学习率(learning rate)</li>
  <li>感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元，其学习能力非常有限</li>
  <li>要解决非线性可分问题，需考虑使用多层功能神经元，比如下图，输入层和输出层之间的一层神经元被称为隐含层(hidden layer)，隐含层和输出层神经元都是拥有激活函数的功能神经元</li>
</ul>

<center><img src="../assets/img/posts/20211222/67.jpg" /></center>

<ul>
  <li>多层前馈神经网络(multi-layer feedforward neural networks): 每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接</li>
</ul>

<h2 id="53-误差逆传播算法">5.3. 误差逆传播算法</h2>
<ul>
  <li>误差逆传播算法(error BackPropagation)简称BP算法，可用于很多类型的神经网络</li>
</ul>

<center><img src="../assets/img/posts/20211222/68.jpg" /></center>

<ul>
  <li>误差采用均方误差:</li>
</ul>

<center><img src="../assets/img/posts/20211222/69.jpg" /></center>

<ul>
  <li>BP算法基于梯度下降(gradient descent)策略，以目标的负梯度方向对参数进行调整</li>
</ul>

<center><img src="../assets/img/posts/20211222/70.jpg" /></center>

<ul>
  <li>
    <p>学习率控制着算法每一轮迭代中的更新步长，若太大则容易振荡，太小则收敛速度又会过慢</p>
  </li>
  <li>上面介绍的标准BP算法每次仅针对<strong>一个训练样例</strong>更新连接权和阈值，我们也可以简单推出基于累积误差最小化的更新规则，就得到了累积BP算法</li>
  <li>正是由于其强大的表达能力，BP神经网络经常遭遇过拟合，其训练误差持续降低，但测试误差却可能上升，由两种策略常用来缓解BP网络的过拟合。
    <ul>
      <li>第一种策略是<strong>早停(early stopping)</strong>: 若训练集误差降低但验证集误差升高，则停止训练</li>
      <li>第二种策略是正则化，其基本思路是在误差目标函数中增加一个用于描述网络复杂度的部分，例如连接权和阈值的平方和，那么误差目标函数变成:</li>
    </ul>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/71.jpg" /></center>

<h2 id="54-全局最小与局部极小">5.4. 全局最小与局部极小</h2>
<ul>
  <li>我们常常会谈到两种最优: <strong>局部极小(local minimum)</strong>和<strong>全局最小(global minimum)</strong></li>
  <li>直观地看，局部极小解是参数空间中的某个点，其领域点的误差函数值均不小于该点的函数值</li>
  <li>全局最小解是指参数空间中所有点的误差函数值均不小于该点的误差函数值</li>
</ul>

<center><img src="../assets/img/posts/20211222/72.jpg" /></center>

<ul>
  <li>在现实任务中，人们常采用以下策略来试图跳出局部极小，从而进一步实现全局最小
    <ul>
      <li>以多组不同参数值初始化多个神经网络</li>
      <li>使用“模拟退火”(simulated annealing)技术，每一步都以一定概率接受比当前解更差的结果，从而有助于跳出局部极小</li>
      <li>随机梯度下降，即每次使用随机的样本进行误差计算</li>
    </ul>
  </li>
</ul>

<h2 id="55-其他常见神经网络">5.5. 其他常见神经网络</h2>

<h3 id="551-rbf网络">5.5.1. RBF网络</h3>
<ul>
  <li>RBF(Radial Basis Function, 径向基函数)网络是一种单隐层前馈神经网络，它使用径向基函数作为隐层神经元激活函数，而输出层则是对隐层神经元输出的线性组合</li>
  <li>具有足够多隐层神经元的RBF网络能以任意精度逼近任意连续函数</li>
</ul>

<h3 id="552-art网络">5.5.2. ART网络</h3>
<ul>
  <li>竞争型学习(competitive learning)是神经网络中一种常用的无监督学习策略，在使用该策略时，网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元的状态被抑制。这种机制亦称胜者通吃(winner-take-all)原则</li>
  <li>ART(Adaptive Resonance Theory，自适应谐振理论)网络是竞争型学习的重要代表。该网络由比较层、识别层、识别阔值和重置模块构成。其中，比较层负责接受输入样本，并将其传递给识别层神经元。识别层每个神经元对应一个模式类，神经元数目可在训练过程中动态增长以增加新的模式类</li>
</ul>

<h3 id="553-som网络">5.5.3. SOM网络</h3>
<ul>
  <li>SOM(Self-Organizing Map，自组织映射)网络是一种竞争学习型的无监督神经网络，它能将高维输入数据映射到低维空间(通常为二维) ，同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元</li>
</ul>

<center><img src="../assets/img/posts/20211222/74.jpg" /></center>

<h3 id="554-级联相关网络">5.5.4. 级联相关网络</h3>
<ul>
  <li>一般的神经网络模型通常假定网络结构是事先固定的，训练的目的是利用训练样本来确定合适的连接权、阈值等参数。与此不同，结构自适网络则将网络结构也当作学习的目标之一，并希望能在训练过程中找到最符合数据特点的网络结构-级联相关(Cascade-Correlation)网络是结构自适应网络的重要代表</li>
</ul>

<center><img src="../assets/img/posts/20211222/75.jpg" /></center>

<h3 id="555-elman网络">5.5.5. Elman网络</h3>
<ul>
  <li>与前馈神经网络不同<strong>递归神经网络(recurrent neural networks)</strong>允许网络中出现环形结构，从而可让一些神经元的输出反馈回来作为输入信号。这样的结构与信息反馈过程，使得网络在t时刻的输出状态不仅与t时刻的输入有关，还与t-1时刻的网络状态有关，从而能处理与时间有关的动态变化</li>
  <li>Elman网络是最常用的递归神经网络之一，其结构如图所示，它的结构与多层前馈网络很相似，但隐层神经元的输出被反馈回来，与下一时刻输入层神经元提供的信号一起，作为隐层神经元在下一时刻的输入。隐层神经元通常采用Sigmoid激活函数，而网络的训练则常通过推广的BP算法进行</li>
</ul>

<center><img src="../assets/img/posts/20211222/76.jpg" /></center>

<h3 id="556-boltzmann机">5.5.6. Boltzmann机</h3>
<ul>
  <li>神经网络中有一类模型是为网络状态定义一个能量(energy)，能量最小化时网络达到理想状态，而网络的训练就是在最小化这个能量函数。Boltzmann机就是一种基于能量的模型(energy-based model)，常见结构如图所示，其神经元分为两层:显层与隐层。显层用于表示数据的输入与输出，隐层则被理解为数据的内在表达</li>
</ul>

<center><img src="../assets/img/posts/20211222/77.jpg" /></center>

<h2 id="56-深度学习">5.6. 深度学习</h2>
<ul>
  <li>理论上来说，参数越多的模型复杂度越高、 “容量” (capacity) 越大，这意味着它能完成更复杂的学习任务。但一般情形下，复杂模型的训练效率低，易陷入过拟合，因此难以受到人们青睐。而随着云计算、大数据时代的到来，计算能力的大幅提高可缓解训练低效性，训练数据的大幅增加则可降低过拟合风险，因此，以”深度学习”(deep learning)为代表的复杂模型开始受到人们的关注。</li>
  <li>无监督逐层训练(unsupervised layer-wise training)是多隐层网络训练的有效手段，其基本思想是每次训练一层隐结点，训练时将上一层隐结点的输出作为输入，向本层隐结点的输出作为下一层隐结点的输入，这称为<strong>预训练(pre-training)</strong>;在预训练全部完成后，再对整个网络进行<strong>微调(fine-tuning)</strong>训练</li>
  <li>另一种节省训练开销的策略是权共享，即让一组神经元使用相同的连接权，比如CNN</li>
  <li>以往在机器学习用于现实任务时，描述样本的特征通常需由人类专家来设计，这称为<strong>特征工程(feature engineering)</strong>，深度学习则通过机器学习技术自身产生好特征</li>
  <li>神经网络是一种难以解释的黑箱模型</li>
</ul>

<h1 id="6-第6章-支持向量机">6. 第6章 支持向量机</h1>

<h2 id="60-思维导图">6.0. 思维导图</h2>

<center><img src="../assets/img/posts/20211222/134.jpg" /></center>

<h2 id="61-间隔与支持向量">6.1. 间隔与支持向量</h2>
<ul>
  <li>给定训练样本集$D={(x_1, y_1),(x_2, y_2),…,(x_m, y_m)}$, 其中$y_i\in${$-1, +1$}, 也就是一个传统的分类问题。分类学习最基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开，但能将训练样本分开的划分超平面可能有很多，如下图</li>
</ul>

<center><img src="../assets/img/posts/20211222/78.jpg" /></center>

<ul>
  <li>直观上看，应该去找位于两类训练样本正中间的划分超平面，因为该划分超平面对训练样本局部扰动的容忍性最好</li>
  <li>在样本空间中，划分超平面可通过如下线性方程来描述: $\omega^Tx+b=0$, 其中$\omega$为法向量, 决定了超平面的方向, b为位移项, 决定了超平面与原点的距离, 显然划分超平面可被法向量$\omega$和b确定</li>
  <li>样本空间中任意点x到超平面的距离可写为</li>
</ul>

<center><img src="../assets/img/posts/20211222/79.jpg" /></center>

<ul>
  <li>假设超平面能将训练样本正确分类, 即对($x_i, y_i$)$\in$D, 我们有以下结果:</li>
</ul>

<center><img src="../assets/img/posts/20211222/80.jpg" /></center>

<ul>
  <li><strong>支持向量</strong>: 距离超平面最近的几个训练样本点使得上述条件等号成立，它们就被称为支持向量(support vector), 两个异类支持向量到超平面的距离之和为<strong>间隔</strong>$\gamma$</li>
</ul>

<center><img src="../assets/img/posts/20211222/81.jpg" /></center>

<ul>
  <li>图示:</li>
</ul>

<center><img src="../assets/img/posts/20211222/82.jpg" /></center>

<ul>
  <li>欲找到具有最大间隔的划分超平面，也就是找到能满足约束的最大的$\gamma$</li>
</ul>

<center><img src="../assets/img/posts/20211222/83.jpg" /></center>

<ul>
  <li>为了最大化间隔，仅需最大化||$\omega$||$^{-1}$, 这等价于最小化||$\omega$||$^{2}$, 于是上述式子可重写为:</li>
</ul>

<center><img src="../assets/img/posts/20211222/84.jpg" /></center>

<ul>
  <li>这就是支持向量机(Support Vector Machine, SVM)的基本型</li>
</ul>

<h2 id="62-对偶问题">6.2. 对偶问题</h2>
<ul>
  <li>我们注意到SVM的基本型本身是一个凸二次规划问题<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>, 能直接用现成的优化计算包求解，但我们可以有更高效的方法</li>
</ul>

<ul>
  <li>对上述式子使用拉格朗日乘子法<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>可得到其对偶问题，则该问题的拉格朗日函数可写为</li>
</ul>

<center><img src="../assets/img/posts/20211222/91.jpg" /></center>

<ul>
  <li>其中$\alpha$=($\alpha_1$;$\alpha_2$…;$\alpha_m$), 令L($\omega$, b, $\alpha$)对$\omega$和b的偏导为零可得</li>
</ul>

<center><img src="../assets/img/posts/20211222/92.jpg" /></center>

<ul>
  <li>将这两个条件带入之前的拉格朗日函数中，即可将$\omega$和b消去，得到以下的对偶问题:</li>
</ul>

<center><img src="../assets/img/posts/20211222/93.jpg" /></center>

<center><img src="../assets/img/posts/20211222/94.jpg" /></center>

<ul>
  <li>从对偶问题中解出的$\alpha_i$是拉格朗日乘子，它恰对应着训练样本($x_i$, $y_i$), 在主问题中有不等式约束，因此上述过程需满足<strong>KTT条件</strong>, 即要求:</li>
</ul>

<center><img src="../assets/img/posts/20211222/96.jpg" /></center>

<ul>
  <li>解出$\alpha$后，求出$\omega$与b即可得到模型:</li>
</ul>

<center><img src="../assets/img/posts/20211222/95.jpg" /></center>

<ul>
  <li>于是, 对任意训练样本($x_i$, $y_i$), 总有$\alpha_i=0$或者$y_if(x_i)=1$。若$\alpha_i=0$，则该样本将不会在最终的模型的求和中出现，也就不会对f(x)有任何影响;若$\alpha_i&gt;0$, 则必有$y_if(x_i)=1$，所对应的样本点位于最大间隔边界上，是一个支持向量。这显示出支持向量机的一个重要性质:训练完成后, 大部分的训练样本都不需保留，最终模型仅与支持向量有关</li>
  <li>那么如何求解对偶问题解出$\alpha_i$呢？这是一个二次规划问题, 可使用通用的二次规划算法求解，然而，该问题的规模正比于训练样本数，这会在实际任务中造成很大的开销，为了避开这个障碍，人们通过利用问题本身的特性，提出了很多高效算法, SMO是其中一个著名的代表</li>
  <li>SMO的基本思路是先固定$\alpha_i$之外的所有参数，然后求$\alpha_i$上的极值，由于存在约束$\sum_1^m\alpha_iy_i=0$,若固定$\alpha_i$之外的其他变量，则$\alpha_i$可由其他变量导出，于是SMO每次选择两个变量$\alpha_i$和$\alpha_j$，并固定其他参数。这样，在参数初始化后，SMO不断执行如下两个步骤直至收敛
    <ul>
      <li>选取一对需更新的变量$\alpha_i$和$\alpha_j$</li>
      <li>固定$\alpha_i$和$\alpha_j$以外的参数，求解获得更新后的$\alpha_i$和$\alpha_j$</li>
    </ul>
  </li>
  <li>如何确定偏移项b呢？注意到对任意支持向量($x_s$, $y_s$)都有$y_sf(x_s)=1$:</li>
</ul>

<center><img src="../assets/img/posts/20211222/97.jpg" /></center>

<ul>
  <li>其中S为所有支持向量的下标集，理论上可以选取任意支持向量来求出b，但现实任务中常采用一种更鲁棒的做法: 使用所有支持向量求解的平均值</li>
</ul>

<center><img src="../assets/img/posts/20211222/98.jpg" /></center>

<h2 id="63-核函数">6.3. 核函数</h2>
<ul>
  <li>在本章前面的讨论中，我们假设训练样本是线性可分的，然而在现实任务中，原始样本空间内也许并不存在一个能正确划分两类样本的超平面，例如异或问题就不是线性可分的</li>
  <li>对这样的问题，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分，例如将异或问题的原始二维空间映射到一个合适的三维空间，就能找到一个合适的划分超平面。如果原始空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本可分</li>
</ul>

<center><img src="../assets/img/posts/20211222/99.jpg" /></center>

<ul>
  <li>令$\phi(x)$表示将x映射后的特征向量，于是在特征空间中划分超平面所对应的模型可表示为</li>
</ul>

<center><img src="../assets/img/posts/20211222/100.jpg" /></center>

<ul>
  <li>其中$\omega$和b是模型参数，我们有以下二次规划:</li>
</ul>

<center><img src="../assets/img/posts/20211222/101.jpg" /></center>

<ul>
  <li>其对偶问题是:</li>
</ul>

<center><img src="../assets/img/posts/20211222/102.jpg" /></center>

<center><img src="../assets/img/posts/20211222/103.jpg" /></center>

<ul>
  <li>我们观察到在对偶问题中涉及到计算$\phi(x_i)^T\phi(x_j)$, 这是样本$x_i$与$x_j$映射到特征空间之后的内积。由于特征空间维数可能很高，甚至是无穷维，因此直接计算$\phi(x_i)^T\phi(x_j)$通常是困难的，为了避开这个障碍，可以设想这样一个函数:</li>
</ul>

<center><img src="../assets/img/posts/20211222/104.jpg" /></center>

<ul>
  <li>即$x_i$与$x_j$在特征空间的内积等于它们在原始样本空间中通过函数$\kappa(·,·)$计算的结果, 有了这样的函数，我们就不必直接去计算高维甚至无穷维特征空间中的内积，于是式子可重写为</li>
</ul>

<center><img src="../assets/img/posts/20211222/105.jpg" /></center>

<ul>
  <li>求解后可得到:</li>
</ul>

<center><img src="../assets/img/posts/20211222/106.jpg" /></center>

<ul>
  <li>上面式子显示出模型最优解可通过训练样本的核函数展开，这一展式亦称<strong>支持向量展式</strong></li>
  <li>显然，若已知合适映射$\phi(·)$的具体形式，则可写出核函数$\kappa(·,·)$，但在现实任务中我们通常不知道$\phi(·)$是什么形式，那么合适的核函数是否一定存在呢？什么样的函数能做核函数呢？我们有以下定理:</li>
</ul>

<center><img src="../assets/img/posts/20211222/107.jpg" /></center>

<ul>
  <li>核函数选择成为支持向量机的最大变数，若核函数选择不合适，则意味着将样本映射到一个不合适的特征空间，很可能导致性能不佳</li>
  <li>常用核函数:</li>
</ul>

<center><img src="../assets/img/posts/20211222/108.jpg" /></center>

<ul>
  <li>此外，还可通过函数组合得到:
    <ul>
      <li>两个核函数的线性组合</li>
      <li>两个核函数的直积</li>
    </ul>

    <center><img src="../assets/img/posts/20211222/109.jpg" /></center>

    <ul>
      <li>若$\kappa_1$为核函数，则对任意函数g(x):</li>
    </ul>

    <center><img src="../assets/img/posts/20211222/110.jpg" /></center>

    <p>也是核函数</p>
  </li>
</ul>

<h2 id="64-软间隔与正则化">6.4. 软间隔与正则化</h2>
<ul>
  <li>在前面的讨论中，我们一直假定训练样本在样本空间或特征空间中是线性可分的，即存在一个超平面能将不同类的样本完全划分开。然而，在现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分</li>
  <li>缓解该问题的一个方法是允许支持向量机在一些样本上出错，为此，要引入软间隔(soft margin)的概念</li>
  <li>具体来说，前面介绍的支持向量机形式都是要求所有样本均满足约束，即所有样本必须划分正确，这称为硬间隔，而软间隔则允许某些样本不满足约束</li>
  <li>优化目标可写为:</li>
</ul>

<center><img src="../assets/img/posts/20211222/111.jpg" /></center>

<ul>
  <li>其中C&gt;0是一个常数，$l_{0/1}$是0/1损失函数</li>
</ul>

<center><img src="../assets/img/posts/20211222/112.jpg" /></center>

<ul>
  <li>显然，当C无穷大时，上面式子迫使所有样本均满足约束，当C取有限值时，式子允许一些样本不满足约束</li>
  <li>然而0/1损失函数非凸、非连续，数学性质不太好，于是人们通常用其他一些函数来替代它，下面给出三个常用的替代损失函数：</li>
</ul>

<center><img src="../assets/img/posts/20211222/113.jpg" /></center>

<ul>
  <li>若采用hinge损失，则式子变成:</li>
</ul>

<center><img src="../assets/img/posts/20211222/114.jpg" /></center>

<ul>
  <li>引入松弛变量$\xi_i$≥0，可将式子重写为</li>
</ul>

<center><img src="../assets/img/posts/20211222/115.jpg" /></center>

<center><img src="../assets/img/posts/20211222/116.jpg" /></center>

<ul>
  <li>这就是常用的软间隔支持向量机，可以看出每个样本都有一个对应的松弛变量，用以表征样本不满足约束的程度</li>
  <li>这仍是一个二次规划的问题，于是，通过拉格朗日乘子法可得到拉格朗日函数:</li>
</ul>

<center><img src="../assets/img/posts/20211222/117.jpg" /></center>

<ul>
  <li>其中$\alpha_i≥0$, $\mu_i≥0$是拉格朗日乘子</li>
  <li>偏导为0后可得到对偶问题:</li>
</ul>

<center><img src="../assets/img/posts/20211222/118.jpg" /></center>

<ul>
  <li>KTT条件:</li>
</ul>

<center><img src="../assets/img/posts/20211222/119.jpg" /></center>

<ul>
  <li>可以发现软间隔支持向量机的最终模型仅与支持向量有关</li>
  <li>我们还可以把0/1损失函数替换成别的替代损失函数以得到其他学习模型，这些模型的性质与所用的替代函数直接相关，但它们具有一个共性: 优化目标中的第一项用来描述划分超平面的间隔大小，另一项用来描述训练集上的误差</li>
</ul>

<center><img src="../assets/img/posts/20211222/120.jpg" /></center>

<ul>
  <li>其中$\Omega(f)$称为“结构风险”(structural risk)，用于描述模型f的某些性质;第二项$\sum_i^ml(f(x_i),y_i)$称为“经验风险”(empirical risk)，用于描述模型与训练数据的契合程度; C用于对二者进行折中。从经验风险最小化的角度来看，$\Omega(f)$表述了我们希望获得具有何种性质的模型(例如希望获得复杂度较小的模型)，这为引入领域知识和用户意图提供了途径; 另一方面，该信息有助于削减假设空间，从而降低了最小化训练误差的过拟合风险。从这个角度来说，上面的公式为“正则化”(regularization)问题，$\Omega(f)$称为正则化项，C则称为正则化常数。$L_p$范数(norm)是常用的正则化项，其中$L_2$范数$||\omega||_2$倾向于$\omega$的分量取值尽量均衡，即非零分量个数尽量稠密，而$L_0$范数$||\omega||_0$和$L_1$范数$||\omega||_1$则倾向于$\omega$的分量尽量稀疏，即非零分量个数尽量少</li>
</ul>

<h2 id="65-支持向量回归">6.5. 支持向量回归</h2>
<ul>
  <li>现在我们来考虑回归问题，给定训练样本$D={(x_1, y_1), (x_2, y_2), …, (x_m, y_m))}，希望学得一个形如$f(x)=\omega^Tx+b$的回归模型，使得f(x)与y尽可能接近。</li>
  <li>传统回归模型通常直接基于输出f(x)与真实输出y之间的差别来计算损失，当且仅当f(x)与y完全相同时，损失才为0。与此不同，支持向量回归(Support Vector Regression, 简称SVR)假设我们能容忍f(x)与y之间最多有$\epsilon$的偏差，如下图所示，若样本落入此间隔带，则被认为预测正确</li>
</ul>

<center><img src="../assets/img/posts/20211222/121.jpg" /></center>

<ul>
  <li>于是SVR问题可形式化为:</li>
</ul>

<center><img src="../assets/img/posts/20211222/122.jpg" /></center>

<ul>
  <li>其中C为正则化常数，$l_{\epsilon}$是$\epsilon$-不敏感损失函数:</li>
</ul>

<center><img src="../assets/img/posts/20211222/123.jpg" /></center>

<ul>
  <li>引入松弛变量$\xi_i$和$\hat{\xi_i}$，可将上述式子重写为:</li>
</ul>

<center><img src="../assets/img/posts/20211222/124.jpg" /></center>

<center><img src="../assets/img/posts/20211222/125.jpg" /></center>

<ul>
  <li>类似地，通过引入拉格朗日乘子可得到拉格朗日函数:</li>
</ul>

<center><img src="../assets/img/posts/20211222/126.jpg" /></center>

<ul>
  <li>偏导为零得到对偶问题:</li>
</ul>

<center><img src="../assets/img/posts/20211222/127.jpg" /></center>

<ul>
  <li>KTT条件:</li>
</ul>

<center><img src="../assets/img/posts/20211222/128.jpg" /></center>

<ul>
  <li>将上述条件代入，SVR的解形如:</li>
</ul>

<center><img src="../assets/img/posts/20211222/129.jpg" /></center>

<ul>
  <li>能使式子中$(\hat{\alpha}_i-\alpha_i)≠0$的样本即为SVR的支持向量，它们必落在$\epsilon$-间隔带之外</li>
  <li>由KTT条件可看出，对每个样本($x_i, y_i$)都有$(C-\alpha_i)\xi_i=0$且$\alpha_i(f(x_i)-y_i-\epsilon-\xi_i)=0$，于是在得到$\alpha_i$后，若0&lt;$\alpha_i$&lt;C, 则必有$\xi_i=0$,进而有:</li>
</ul>

<center><img src="../assets/img/posts/20211222/130.jpg" /></center>

<ul>
  <li>理论上说，可任意选取满足的$\alpha_i$来得到b，但是实践中常采用一种更鲁棒的方法: 选取多个满足0&lt;$\alpha_i$&lt;C的样本通过上述公式求解b后取平均值</li>
  <li>若考虑特征映射，则:</li>
</ul>

<center><img src="../assets/img/posts/20211222/131.jpg" /></center>

<center><img src="../assets/img/posts/20211222/132.jpg" /></center>

<h2 id="66-核方法">6.6. 核方法</h2>
<ul>
  <li>若不考虑偏移项b，则无论SVM还是SVR，学得的模型总能表示成核函数的线性组合。不仅如此，事实上我们有下面这个称为“表示定理”(representer theorem)的更一般的结论</li>
</ul>

<center><img src="../assets/img/posts/20211222/133.jpg" /></center>

<ul>
  <li>人们发展出一系列基于核函数的学习方法，统称为核方法，最常见的是通过引入核函数来将线性学习器拓展为非线性学习器</li>
  <li>支持向量机是针对二分类任务设计的，对多分类任务要进行专门的推广</li>
</ul>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>注意这里的测试集其实是验证集，我们通常把实际情况中遇到的数据集称为测试集。 <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>满秩矩阵指方阵的秩等于矩阵的行数/列数，满秩矩阵有逆矩阵且对于y=Xb有唯一的解 <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>统计学中，似然函数是一种关于统计模型参数的函数。给定输出x时，关于参数θ的似然函数L(θ|x)（在数值上）等于给定参数θ后变量X的概率：L(θ|x)=P(X=x|θ)。 <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>随机变量的期望组成的向量称为期望向量或者均值向量 <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>协方差矩阵的每个元素是各个向量元素之间的协方差。协方差就是Covariance <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>二次规划(Quadratic Programming)是一种典型的优化问题，在此类问题中，目标函数是变量的二次函数，而约束条件是变量的线性不等式，假定变量个数为d，约束条件的个数为m，则标准的二次规划问题形如: <center><img src="../assets/img/posts/20211222/85.jpg" /></center> 其中x为d维向量，Q为实对称矩阵，A为实矩阵，若Q为半正定矩阵，则目标函数为凸函数，相应的二次规划是凸二次优化问题。半正定矩阵即满足: A是n阶方阵，如果对任何非零向量X，都有X’AX≥0，其中X’表示X的转置，就称A为半正定矩阵。正定矩阵定义类似，只是把大于等于变成大于。此时若约束条件Ax≤b定义的可行域不为空，且目标函数在此可行域有下界，则该问题将有全局最小值。若Q为正定矩阵，则该问题有唯一全局最小值 <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>拉格朗日乘子法是一种寻找多元函数在一组约束下的极值的方法，通过引入拉格朗日乘子, 可将d个变量与k个约束条件的最优化问题转化为具有d+k个变量的无约束优化问题求解。首先考虑等式约束的优化问题，假定x为d维向量，欲寻找x的某个取值$x^*$, 使目标函数f(x)最小且同时满足g(x)=0的约束，从几何角度看，该问题的目标是在由方程g(x)=0确定的d-1维曲面上寻找能使目标函数f(x)最小化的点，此时不难得到如下结论: 1.对于约束曲面上的任意点x, 该点的梯度▽g(x)正交于约束曲面; 2.在最优点$x^*$，目标函数在该点的梯度▽f($x^*$)正交于约束曲面<center><img src="../assets/img/posts/20211222/86.jpg" /></center> 由此可知，在最优点$x^*$，存在$\lambda$≠0使得<center><img src="../assets/img/posts/20211222/87.jpg" /></center>$\lambda$称为拉格朗日算子，定义拉格朗日函数<center><img src="../assets/img/posts/20211222/88.jpg" /></center>现在考虑不等式约束g(x)≤0,如图所示<center><img src="../assets/img/posts/20211222/89.jpg" /></center> 此时最优点$x^*$或在g(x)&lt;0的区域中，或在边界g(x)=0上。对于g(x)&lt;0的情形，约束g(x)≤0不起作用，可直接通过条件▽f(x)=0来获得最优点，这相当于将$\lambda$置零，g(x)=0的情形类似于上面等式约束的分析。因此，在约束g(x)≤0下最小化f(x)，可转化为在如下约束下最小化拉格朗日函数: <center><img src="../assets/img/posts/20211222/90.jpg" /></center>上述约束条件称为KTT条件。一个优化问题可以从两个角度来考察，即主问题和对偶问题，主问题就是没有引入拉格朗日乘子的原始目标函数，对偶问题就是加上了拉格朗日乘子的目标函数，这个目标函数是原始目标函数的最优值的一个下界，所以我们希望找到最好的下界，也就是最大的下界，所以对偶函数是寻找最大值 <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[《机器学习》周志华读书笔记]]></summary></entry><entry><title type="html">组会记录</title><link href="http://localhost:4000/1221%E7%BB%84%E4%BC%9A.html" rel="alternate" type="text/html" title="组会记录" /><published>2021-12-21T00:00:00+08:00</published><updated>2021-12-21T00:00:00+08:00</updated><id>http://localhost:4000/1221%E7%BB%84%E4%BC%9A</id><content type="html" xml:base="http://localhost:4000/1221%E7%BB%84%E4%BC%9A.html"><![CDATA[<h1 id="vae">VAE</h1>
<h2 id="ae">AE</h2>
<p>Auto-Encoder自动编码器，比如Seq2seq模型。</p>

<h2 id="vaevariational-auto-encoder">VAE(Variational Auto-Encoder)</h2>
<p>在实际情况中，我们需要在模型的准确率上与隐含向量服从标准正态分布之间做一个权衡，所谓模型的准确率就是指解码器生成的图片与原图片的相似程度。我们可以让网络自己来做这个决定，非常简单，我们只需要将这两者都做一个loss，然后在将他们求和作为总的loss，这样网络就能够自己选择如何才能够使得这个总的loss下降。另外我们要衡量两种分布的相似程度，如何看过之前一片GAN的数学推导，你就知道会有一个东西叫KL-divergence来衡量两种分布的相似程度，这里我们就是用KL-divergence来表示隐含向量与标准正态分布之间差异的loss，另外一个loss仍然使用生成图片与原图片的均方误差来表示。</p>

<h2 id="kl消失">KL消失</h2>
<p>KL消失后，VAE就变成了AE
原因：</p>
<ul>
  <li>KL项本身太容易被优化</li>
  <li>一旦崩塌，Decoder会忽视Z<sub>x</sub></li>
  <li>Z<sub>x</sub>的表示学习依赖于Decoder</li>
</ul>

<h2 id="解决kl消失的思路">解决KL消失的思路</h2>
<p>…</p>

<h1 id="analyze-pretraining-language-model">Analyze Pretraining Language Model</h1>
<h2 id="perspective-of-knowledge">Perspective of knowledge</h2>
<ul>
  <li>Syntacitic/Semantic/lexical 句法，语义，词汇</li>
  <li>重构语法树</li>
  <li>Attention中很多头可能没有用，学到了很多冗余的信息</li>
  <li>Analyze Feed Forward Neural Network</li>
  <li>浅层词汇信息，深层语义信息</li>
  <li>Prompt</li>
</ul>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[组会笔记]]></summary></entry><entry><title type="html">制作类RACE数据集</title><link href="http://localhost:4000/RACElike-datasets.html" rel="alternate" type="text/html" title="制作类RACE数据集" /><published>2021-12-21T00:00:00+08:00</published><updated>2021-12-21T00:00:00+08:00</updated><id>http://localhost:4000/RACElike-datasets</id><content type="html" xml:base="http://localhost:4000/RACElike-datasets.html"><![CDATA[<h1 id="目录">目录</h1>
<!-- TOC -->

<ul>
  <li><a href="#目录">目录</a></li>
  <li><a href="#race">RACE</a>
    <ul>
      <li><a href="#简介">简介</a></li>
      <li><a href="#race数据集格式">RACE数据集格式</a></li>
      <li><a href="#race数据集分布">RACE数据集分布</a></li>
      <li><a href="#race数据集中的长度">RACE数据集中的长度</a></li>
      <li><a href="#race数据集中的问题的统计信息">RACE数据集中的问题的统计信息</a></li>
    </ul>
  </li>
  <li><a href="#gaorace">GaoRACE</a>
    <ul>
      <li><a href="#gao他们对于race数据集的处理">Gao他们对于RACE数据集的处理</a></li>
      <li><a href="#gao处理后的race数据集统计信息">Gao处理后的RACE数据集统计信息</a></li>
      <li><a href="#gao处理后的数据集格式">Gao处理后的数据集格式</a>
        <ul>
          <li><a href="#预处理">预处理</a></li>
          <li><a href="#updated">updated</a></li>
          <li><a href="#预处理代码">预处理代码</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#mrc-阅读理解数据集">MRC 阅读理解数据集</a>
    <ul>
      <li><a href="#简介-1">简介</a></li>
      <li><a href="#title">Title</a></li>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#table-一张十分完整的表格">Table 一张十分完整的表格</a></li>
      <li><a href="#值得关注的地方">值得关注的地方</a></li>
    </ul>
  </li>
  <li><a href="#自制数据集">自制数据集</a>
    <ul>
      <li><a href="#大型题库">大型题库</a></li>
      <li><a href="#方法">方法</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->
<h1 id="race">RACE</h1>
<h2 id="简介">简介</h2>
<p>RACE数据集包含了中国初高中阅读理解题目，最初发布在2017年，一共含有28k短文和100k个问题，最开始发布的目的是为了<strong>阅读理解</strong>任务。它的特点是包含了很多需要推理的问题。</p>

<ul>
  <li>原RACE数据集<a href="http://www.cs.cmu.edu/~glai1/data/race/">地址</a></li>
  <li>下载地址<a href="http://www.cs.cmu.edu/~glai1/data/race/RACE.tar.gz">url</a></li>
  <li>论文地址：<a href="https://arxiv.org/abs/1704.04683">RACE: Large-scale ReAding Comprehension Dataset From Examinations</a></li>
</ul>

<h2 id="race数据集格式">RACE数据集格式</h2>
<p>Each passage is a JSON file. The JSON file contains following fields:</p>

<ol>
  <li>article: A string, which is the passage. 文章</li>
  <li>questions: A string list. Each string is a query. We have two types of questions. First one is an interrogative sentence. Another one has a placeholder, which is represented by _. 四个问题题干</li>
  <li>options: A list of the options list. Each options list contains 4 strings, which are the candidate option. 四个题目的四个选项</li>
  <li>answers: A list contains the golden label of each query.四个题目的正确答案</li>
  <li>id: Each passage has a unique id in this dataset.</li>
</ol>

<h2 id="race数据集分布">RACE数据集分布</h2>

<p><img src="../assets/img/posts/20211221/3.jpg" /></p>

<p>RACE-M表示初中题目，RACE-H表示高中题目</p>

<h2 id="race数据集中的长度">RACE数据集中的长度</h2>

<p><img src="../assets/img/posts/20211221/4.jpg" /></p>

<h2 id="race数据集中的问题的统计信息">RACE数据集中的问题的统计信息</h2>

<p><img src="../assets/img/posts/20211221/5.jpg" /></p>

<h1 id="gaorace">GaoRACE</h1>
<h2 id="gao他们对于race数据集的处理">Gao他们对于RACE数据集的处理</h2>
<ul>
  <li>去掉了那些误导选项和文章语义不相关的数据</li>
  <li>去掉了那些需要<code class="language-plaintext highlighter-rouge">world knowledge</code>生成的选项</li>
  <li>github<a href="https://github.com/Yifan-Gao/Distractor-Generation-RACE">url</a>,上面有预处理RACE数据集的代码</li>
</ul>

<h2 id="gao处理后的race数据集统计信息">Gao处理后的RACE数据集统计信息</h2>

<p><img src="../assets/img/posts/20211221/7.jpg" /></p>

<h2 id="gao处理后的数据集格式">Gao处理后的数据集格式</h2>

<h3 id="预处理">预处理</h3>

<p>首先把数据集规整到一个json文件里，分为dev,test,train三个json文件。</p>

<p>每一行包含以下信息：</p>

<p>article, sent(sentence), question(问题有两种，一种是疑问句，一种是填空), answer_text, answer, id, word_overlap_score, word_overlap_count, article_id, question_id, distractor_id.</p>

<p>那么一个问题会有2-3个误导选项，一篇文章又会有3-4个问题。相比于原本的数据集多了word-overlap指标，word-overlap就是词重叠率，交集比上并集。</p>

<h3 id="updated">updated</h3>
<p>updated数据集和original数据集格式类似，少了overlap，内容上去掉了一些语义不相关的题目。</p>

<h3 id="预处理代码">预处理代码</h3>
<p>利用torchtext框架预处理文本，流程大概如下：</p>
<ul>
  <li>定义Field：声明如何处理数据 定义</li>
  <li>Dataset：得到数据集，此时数据集里每一个样本是一个 经过 Field声明的预处理 预处理后的 wordlist</li>
  <li>建立vocab：在这一步建立词汇表，词向量(word embeddings)</li>
  <li>构造迭代器：构造迭代器，用来分批次训练模型</li>
</ul>

<p>Gao说有去掉一些语义不相关的误导选项，但是在代码中并没有看见这步操作？？</p>

<p><img src="../assets/img/posts/20211221/8.jpg" /></p>

<h1 id="mrc-阅读理解数据集">MRC 阅读理解数据集</h1>

<h2 id="简介-1">简介</h2>
<p>发现了一篇很好的综述，里面涵盖了2021年之前用到的所有MRC数据集。现在对这篇综述简单介绍一下</p>

<h2 id="title">Title</h2>
<p>English Machine Reading Comprehension Datasets: A Survey</p>

<h2 id="abstract">Abstract</h2>
<p>文献收集了60个英语阅读理解数据集，分别从不同维度进行比较，包括size, vocabulary, data source, method of creation, human performance level, first question word。调研发现维基百科是最多的数据来源，同时也发现了缺少很多why,when,where问题。</p>

<h2 id="table-一张十分完整的表格">Table 一张十分完整的表格</h2>

<p><img src="../assets/img/posts/20211221/44.jpg" /></p>

<p>首先我简单解释以下这个表格，这个表格一个收录了18个Multiple Choice Datasets,也就是说这18个数据集都着眼于多选题。</p>
<ul>
  <li>第一列是数据集的名称。</li>
  <li>第二列表示数据集中问题的个数(size)。</li>
  <li>第三列表示数据集中文章的来源，其中ER表示education resource, AG表示automatically generated即自动生成,CRW表示crowdsourcing。</li>
  <li>第四列表示答案的来源(answer)，其中UG表示user generated。</li>
  <li>第五列LB表示leader board available，即是否有排行榜，带*表示排行榜在<a href="https://paperswithcode.com/">网站</a>上发布。</li>
  <li>第六列表示人在该数据集上的表现。</li>
  <li>第七列表示该数据集是否有被解决，也就是说是否有比较好的模型能在该数据集上表现良好。</li>
  <li>第八列表示问题第一个单词出现最频繁的是哪个？比如what,how,which这样的单词。</li>
  <li>第九列PAD表示是否开源。</li>
</ul>

<h2 id="值得关注的地方">值得关注的地方</h2>
<p>这么多数据集中，来源于考试题目的有RACE,RACE-C,DREAM,ReClor,这些数据集的收集方法可以借鉴。</p>

<h1 id="自制数据集">自制数据集</h1>
<h2 id="大型题库">大型题库</h2>
<p>泸江，星火英语…</p>
<h2 id="方法">方法</h2>
<p>Python爬取网页</p>]]></content><author><name>Quehry</name></author><category term="work" /><summary type="html"><![CDATA[帮助学长制作RACE数据集]]></summary></entry><entry><title type="html">计算机图形学</title><link href="http://localhost:4000/Computer_Graphics.html" rel="alternate" type="text/html" title="计算机图形学" /><published>2021-12-21T00:00:00+08:00</published><updated>2021-12-21T00:00:00+08:00</updated><id>http://localhost:4000/Computer_Graphics</id><content type="html" xml:base="http://localhost:4000/Computer_Graphics.html"><![CDATA[<!-- TOC -->

<ul>
  <li><a href="#1-lecture-01-overview-of-computer-graphics">1. Lecture 01 Overview of Computer Graphics</a>
    <ul>
      <li><a href="#11-课程情况">1.1. 课程情况</a></li>
      <li><a href="#12-什么是好的画面">1.2. 什么是好的画面</a></li>
      <li><a href="#13-应用场景">1.3. 应用场景</a></li>
      <li><a href="#14-rasterization-光栅化">1.4. Rasterization 光栅化</a></li>
      <li><a href="#15-计算机视觉">1.5. 计算机视觉</a></li>
      <li><a href="#16-推荐书籍">1.6. 推荐书籍</a></li>
    </ul>
  </li>
  <li><a href="#2-lecture-02-review-of-linear-algebra">2. Lecture 02 Review of Linear Algebra</a>
    <ul>
      <li><a href="#21-图形学依赖学科">2.1. 图形学依赖学科</a></li>
      <li><a href="#22-向量">2.2. 向量</a></li>
      <li><a href="#23-矩阵">2.3. 矩阵</a></li>
    </ul>
  </li>
  <li><a href="#3-lecture-03-transformation">3. Lecture 03 Transformation</a>
    <ul>
      <li><a href="#31-why-transformation-为什么要变换">3.1. why transformation 为什么要变换</a></li>
      <li><a href="#32-d变换">3.2. D变换</a></li>
      <li><a href="#33-齐次坐标-homogeneous-coordinate">3.3. 齐次坐标 homogeneous coordinate</a></li>
    </ul>
  </li>
  <li><a href="#4-lecture-04-transformation-cont">4. Lecture 04 Transformation Cont.</a>
    <ul>
      <li><a href="#41-d-transformations">4.1. D Transformations</a></li>
      <li><a href="#42-view-transformation-视图变换">4.2. view transformation 视图变换</a></li>
      <li><a href="#43-projection-transformation-投影变换">4.3. projection transformation 投影变换</a></li>
    </ul>
  </li>
  <li><a href="#5-lecture05-rasterization-1triangles">5. Lecture05 Rasterization 1(Triangles)</a>
    <ul>
      <li><a href="#51-perspective-projection-透视投影">5.1. Perspective Projection 透视投影</a></li>
      <li><a href="#52-canonical-cube-to-screen-光栅化">5.2. Canonical Cube to Screen 光栅化</a></li>
      <li><a href="#53-different-raster-displays-不同的成像设备">5.3. Different Raster Displays 不同的成像设备</a></li>
      <li><a href="#54-三角形光栅化">5.4. 三角形光栅化</a></li>
    </ul>
  </li>
  <li><a href="#6-lecture-06-rasterization-2antialiasing-and-z-buffering">6. Lecture 06 Rasterization 2(Antialiasing and Z-Buffering)</a>
    <ul>
      <li><a href="#61-sampling-采样原理">6.1. sampling 采样原理</a></li>
      <li><a href="#62-frequency-domaine-信号处理频率">6.2. Frequency domaine 信号处理频率</a></li>
      <li><a href="#63-antialiasing-反走样抗锯齿">6.3. antialiasing 反走样/抗锯齿</a></li>
      <li><a href="#64-antialiasing-today-目前反走样的方法">6.4. antialiasing today 目前反走样的方法</a></li>
    </ul>
  </li>
  <li><a href="#7-lecture-07-shadingillumination-shading-and-graphics-pipeline">7. Lecture 07 Shading(Illumination, Shading, and Graphics Pipeline)</a>
    <ul>
      <li><a href="#71-painters-algorithm-画家算法">7.1. Painter’s Algorithm 画家算法</a></li>
      <li><a href="#72-z-buffer-深度缓存">7.2. Z-buffer 深度缓存</a></li>
      <li><a href="#73-目前为止学到了什么">7.3. 目前为止学到了什么</a></li>
      <li><a href="#74-shading-着色">7.4. shading 着色</a></li>
    </ul>
  </li>
  <li><a href="#8-shading-2shading-pipeline-texture-mapping">8. Shading 2(Shading, Pipeline, Texture Mapping)</a>
    <ul>
      <li><a href="#81-specular-term-高光项">8.1. Specular Term 高光项</a></li>
      <li><a href="#82-ambient-term-环境项">8.2. Ambient Term 环境项</a></li>
      <li><a href="#83-shading-frequencies-着色频率">8.3. Shading Frequencies 着色频率</a></li>
      <li><a href="#84-graphics-pipeline-图像管线实时渲染管线">8.4. Graphics Pipeline 图像管线/实时渲染管线</a></li>
      <li><a href="#85-texture-mapping-纹理映射">8.5. Texture Mapping 纹理映射</a></li>
    </ul>
  </li>
  <li><a href="#9-lecture-09-shading-3-texture-mapping">9. Lecture 09 Shading 3 (Texture Mapping)</a>
    <ul>
      <li><a href="#91-barycentric-coordinates重心坐标系">9.1. Barycentric Coordinates重心坐标系</a></li>
      <li><a href="#92-interpolate-插值">9.2. Interpolate 插值</a></li>
      <li><a href="#93-simple-texture-mapping-简单的纹理映射模型">9.3. Simple Texture Mapping 简单的纹理映射模型</a></li>
      <li><a href="#94-texture-magnification-纹理放大">9.4. Texture Magnification 纹理放大</a></li>
      <li><a href="#95-point-sampling-textures">9.5. Point Sampling Textures</a></li>
      <li><a href="#96-mipmap-范围查询">9.6. Mipmap 范围查询</a></li>
    </ul>
  </li>
  <li><a href="#10-lecture-10-geomrtry-1introduction">10. Lecture 10 Geomrtry 1(introduction)</a>
    <ul>
      <li><a href="#101-纹理的应用">10.1. 纹理的应用</a>
        <ul>
          <li><a href="#1011-environment-map-环境光映射">10.1.1. Environment Map 环境光映射</a></li>
          <li><a href="#1012-spherical-environment-map-球形环境光映射">10.1.2. Spherical Environment Map 球形环境光映射</a></li>
          <li><a href="#1013-纹理凹凸贴图bump-mapping">10.1.3. 纹理凹凸贴图bump mapping</a></li>
          <li><a href="#1014-位移贴图-displacement-mapping">10.1.4. 位移贴图 displacement mapping</a></li>
          <li><a href="#1015-三维纹理">10.1.5. 三维纹理</a></li>
        </ul>
      </li>
      <li><a href="#102-几何">10.2. 几何</a>
        <ul>
          <li><a href="#1021-分类">10.2.1. 分类</a></li>
          <li><a href="#1022-隐式几何">10.2.2. 隐式几何</a></li>
          <li><a href="#1023-显式几何">10.2.3. 显式几何</a></li>
          <li><a href="#1024-隐式的表达方式">10.2.4. 隐式的表达方式</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#11-lecture-11-geometry-2curves-and-surfaces">11. Lecture 11 Geometry 2(Curves and Surfaces)</a>
    <ul>
      <li><a href="#111-显式几何的表示方法">11.1. 显式几何的表示方法</a>
        <ul>
          <li><a href="#1111-point-cloud-点云">11.1.1. Point Cloud 点云</a></li>
          <li><a href="#1112-polygone-mesh">11.1.2. Polygone Mesh</a></li>
          <li><a href="#1113-一个例子">11.1.3. 一个例子</a></li>
        </ul>
      </li>
      <li><a href="#112-curves-曲线">11.2. Curves 曲线</a>
        <ul>
          <li><a href="#1121-贝塞尔曲线">11.2.1. 贝塞尔曲线</a></li>
          <li><a href="#1122-如何画一条贝塞尔曲线">11.2.2. 如何画一条贝塞尔曲线</a></li>
          <li><a href="#1123-piecewise-bézier-curves-逐段的贝塞尔曲线">11.2.3. Piecewise Bézier Curves 逐段的贝塞尔曲线</a></li>
          <li><a href="#1124-spline-样条">11.2.4. Spline 样条</a></li>
        </ul>
      </li>
      <li><a href="#113-曲面">11.3. 曲面</a>
        <ul>
          <li><a href="#1131-贝塞尔曲面">11.3.1. 贝塞尔曲面</a></li>
          <li><a href="#1132-曲面细分">11.3.2. 曲面细分</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#12-lecture-12-geometry-3">12. Lecture 12 Geometry 3</a>
    <ul>
      <li><a href="#121-mesh-subdivisionupsampling-网格细分">12.1. Mesh Subdivision(upsampling) 网格细分</a></li>
      <li><a href="#122-mesh-simplification-网格简化">12.2. Mesh Simplification 网格简化</a></li>
      <li><a href="#123-阴影-shadow-mapping">12.3. 阴影 Shadow mapping</a></li>
    </ul>
  </li>
  <li><a href="#13-lecture-13-ray-tracing-1">13. Lecture 13 Ray Tracing 1</a>
    <ul>
      <li><a href="#131-why-ray-tracing">13.1. Why ray tracing</a></li>
      <li><a href="#132-light-rays">13.2. Light Rays</a></li>
      <li><a href="#133-ray-casting-光线投射">13.3. Ray Casting 光线投射</a></li>
      <li><a href="#134-recursive-ray-tracing-递归光线追踪">13.4. Recursive Ray Tracing 递归光线追踪</a></li>
      <li><a href="#135-ray-surface-interaction-光线和表面相交">13.5. Ray-Surface interaction 光线和表面相交</a>
        <ul>
          <li><a href="#1351-ray-equation">13.5.1. Ray Equation</a></li>
          <li><a href="#1352-与圆相交的交点">13.5.2. 与圆相交的交点</a></li>
          <li><a href="#1353-intersection-with-implicit-surface">13.5.3. intersection with implicit surface</a></li>
          <li><a href="#1354-intersection-with-triangle-mesh">13.5.4. intersection with triangle mesh</a></li>
          <li><a href="#1355-accelerating-ray-surface-intersection">13.5.5. accelerating ray-surface intersection</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#14-lecture-14-ray-tracing-2">14. Lecture 14 Ray Tracing 2</a>
    <ul>
      <li><a href="#141-uniform-spatial-partitions-grids">14.1. Uniform Spatial Partitions (Grids)</a></li>
      <li><a href="#142-spatial-partitions-空间划分">14.2. Spatial Partitions 空间划分</a>
        <ul>
          <li><a href="#1421-一些划分示例">14.2.1. 一些划分示例</a></li>
          <li><a href="#1422-kd-tree">14.2.2. KD-Tree</a></li>
        </ul>
      </li>
      <li><a href="#143-object-partitions-物体划分">14.3. Object Partitions 物体划分</a>
        <ul>
          <li><a href="#1431-bounding-volume-hierarchybvh">14.3.1. Bounding Volume Hierarchy(BVH)</a></li>
          <li><a href="#1432-building-bvh">14.3.2. Building BVH</a></li>
          <li><a href="#1433-与空间划分的对比">14.3.3. 与空间划分的对比</a></li>
        </ul>
      </li>
      <li><a href="#144-whitted-style">14.4. Whitted style</a></li>
      <li><a href="#145-radiometry-辐射度量学">14.5. Radiometry 辐射度量学</a>
        <ul>
          <li><a href="#1451-一些物理量">14.5.1. 一些物理量</a></li>
          <li><a href="#1452-radiant-energy-and-flux">14.5.2. Radiant Energy and Flux</a></li>
          <li><a href="#1453-radiant-intensity">14.5.3. Radiant Intensity</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#15-lecture-15-ray-tracing">15. Lecture 15 Ray Tracing</a>
    <ul>
      <li><a href="#151-radiometry-cont-辐射度量学">15.1. Radiometry cont. 辐射度量学</a>
        <ul>
          <li><a href="#1511-继续上节课的内容">15.1.1. 继续上节课的内容</a></li>
          <li><a href="#1512-irradiance">15.1.2. Irradiance</a></li>
          <li><a href="#1513-radiance">15.1.3. Radiance</a></li>
        </ul>
      </li>
      <li><a href="#152-bidirectional-reflectance-distribution-function-brdf">15.2. Bidirectional Reflectance Distribution Function (BRDF)</a></li>
      <li><a href="#153-rendering-equation-渲染方程">15.3. Rendering Equation 渲染方程</a>
        <ul>
          <li><a href="#1531-如何理解渲染方程">15.3.1. 如何理解渲染方程</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#16-lecture-16-ray-tracing-4">16. Lecture 16 Ray Tracing 4</a>
    <ul>
      <li><a href="#161-monte-carlo-integration-蒙特卡洛积分">16.1. Monte Carlo Integration 蒙特卡洛积分</a></li>
      <li><a href="#162-path-tracing-路径追踪">16.2. Path Tracing 路径追踪</a>
        <ul>
          <li><a href="#1621-解渲染方程">16.2.1. 解渲染方程</a></li>
          <li><a href="#1622-最终的代码">16.2.2. 最终的代码</a></li>
        </ul>
      </li>
      <li><a href="#163-路径追踪">16.3. 路径追踪</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h1 id="1-lecture-01-overview-of-computer-graphics">1. Lecture 01 Overview of Computer Graphics</h1>
<h2 id="11-课程情况">1.1. 课程情况</h2>
<ul>
  <li>授课老师：闫令琪</li>
  <li>授课形式：网课（B站）</li>
</ul>

<h2 id="12-什么是好的画面">1.2. 什么是好的画面</h2>
<p>画面<strong>亮</strong></p>
<h2 id="13-应用场景">1.3. 应用场景</h2>
<p>电影，游戏，动画，设计，可视化，虚拟现实，增强现实，模拟，GUI图形用户接口。</p>

<p>电影中里程碑：阿凡达，大量应用面部捕捉技术。</p>
<h2 id="14-rasterization-光栅化">1.4. Rasterization 光栅化</h2>
<p>实时，FPS&gt;30</p>

<p>离线, FPS&lt;30</p>
<h2 id="15-计算机视觉">1.5. 计算机视觉</h2>
<p>计算机图形学离不开计算机视觉，但是视觉一般是对图像的处理。</p>

<h2 id="16-推荐书籍">1.6. 推荐书籍</h2>
<p>Tiger虎书</p>

<h1 id="2-lecture-02-review-of-linear-algebra">2. Lecture 02 Review of Linear Algebra</h1>
<h2 id="21-图形学依赖学科">2.1. 图形学依赖学科</h2>
<p>Optics, Mechanics, Linear algebra, statics, Singal processing, numerical analysis数值分析</p>

<h2 id="22-向量">2.2. 向量</h2>

<p>向量的定义</p>

<p><img src="../assets/img/posts/20211221/9.jpg" /></p>

<p>单位向量</p>

<p><img src="../assets/img/posts/20211221/10.jpg" /></p>

<p>向量计算，向量加法</p>

<p><img src="../assets/img/posts/20211221/11.jpg" /></p>

<p>用笛卡尔坐标系表示向量</p>

<p><img src="../assets/img/posts/20211221/12.jpg" /></p>

<p>向量乘法，点乘和叉乘，点乘在笛卡尔坐标系中就是对应元素相乘。</p>

<p>在图形学中，点乘是为了寻找两个向量的夹角(夹角可以判断两个向量方向的接近程度)，或者获得一个向量在另一个向量的投影，还可以获得向量的分解。</p>

<p><img src="../assets/img/posts/20211221/13.jpg" /></p>

<p>叉乘，叉积结果垂直于这两个向量所在的平面，满足右手定则。向量的叉乘可以写成矩阵形式。</p>

<p>在图形学中的应用：判断左右关系，比如a^b&gt;0，说明b在a的左边。还可以判断内外，比如判断一个点是否在一个三角形内。</p>

<p><img src="../assets/img/posts/20211221/14.jpg" /></p>

<p>坐标系的定义，右手坐标系</p>

<p><img src="../assets/img/posts/20211221/15.jpg" /></p>

<h2 id="23-矩阵">2.3. 矩阵</h2>

<p>矩阵定义</p>

<p><img src="../assets/img/posts/20211221/16.jpg" /></p>

<p>矩阵乘法</p>

<p><img src="../assets/img/posts/20211221/17.jpg" /></p>

<p>矩阵乘法没有交换律，但是有结合律</p>

<p>矩阵转置，矩阵的逆</p>

<p>向量的点乘和叉乘都可以写成矩阵乘法形式</p>

<p><img src="../assets/img/posts/20211221/18.jpg" /></p>

<h1 id="3-lecture-03-transformation">3. Lecture 03 Transformation</h1>

<h2 id="31-why-transformation-为什么要变换">3.1. why transformation 为什么要变换</h2>
<p>viewing: 3D to 2D projection</p>

<h2 id="32-d变换">3.2. D变换</h2>
<ul>
  <li>缩放 scale transform</li>
</ul>

<p><img src="../assets/img/posts/20211221/19.jpg" /></p>

<ul>
  <li>非均匀缩放 scale(non-uniform)</li>
</ul>

<p><img src="../assets/img/posts/20211221/20.jpg" /></p>

<ul>
  <li>翻转 reflection matrix</li>
</ul>

<p><img src="../assets/img/posts/20211221/21.jpg" /></p>

<ul>
  <li>切变 shear matrix</li>
</ul>

<p>竖直方向上没有变化，水平方向上发生了变化</p>

<p><img src="../assets/img/posts/20211221/22.jpg" /></p>

<ul>
  <li>旋转 Rotate</li>
</ul>

<p>旋转默认绕零点逆时针旋转</p>

<p><img src="../assets/img/posts/20211221/23.jpg" /></p>

<p>二维旋转矩阵R</p>

<p>上述所有的变化都可以写成x$\prime$=Mx，也就是线性变换</p>

<h2 id="33-齐次坐标-homogeneous-coordinate">3.3. 齐次坐标 homogeneous coordinate</h2>

<ul>
  <li>
    <p>为什么要引入齐次坐标，因为对于简单的平移操作并不能写成线性变换的形式，但是人们也不想认为平移是一种特殊的变换，所以引入齐次坐标</p>
  </li>
  <li>
    <p>齐次坐标</p>
  </li>
</ul>

<p>注意点和向量的表示方法不同</p>

<p><img src="../assets/img/posts/20211221/24.jpg" /></p>

<ul>
  <li>仿射变换 affine transformations</li>
</ul>

<p><img src="../assets/img/posts/20211221/25.jpg" /></p>

<ul>
  <li>2D Transformations</li>
</ul>

<p><img src="../assets/img/posts/20211221/26.jpg" /></p>

<ul>
  <li>
    <p>逆变换就是乘以逆矩阵</p>
  </li>
  <li>
    <p>复杂的变换都是简单的变换的组合，变换的组合顺序很重要</p>
  </li>
  <li>
    <p>绕着某一个点（非原点）旋转的分解</p>
  </li>
</ul>

<p><img src="../assets/img/posts/20211221/27.jpg" /></p>

<h1 id="4-lecture-04-transformation-cont">4. Lecture 04 Transformation Cont.</h1>

<h2 id="41-d-transformations">4.1. D Transformations</h2>

<ul>
  <li>齐次坐标</li>
</ul>

<p>对于w不等于1，每一个坐标除以w</p>

<p><img src="../assets/img/posts/20211221/28.jpg" /></p>

<ul>
  <li>正交矩阵</li>
</ul>

<p>一个矩阵的逆等于矩阵的转置，旋转矩阵就是一个正交矩阵</p>

<ul>
  <li>仿射变换（旋转+平移）</li>
</ul>

<p>仿射变换是先进行旋转再进行平移</p>

<p><img src="../assets/img/posts/20211221/29.jpg" /></p>

<ul>
  <li>矩阵表示（缩放，平移）</li>
</ul>

<p><img src="../assets/img/posts/20211221/30.jpg" /></p>

<ul>
  <li>旋转</li>
</ul>

<p>绕着某一个轴旋转</p>

<p><img src="../assets/img/posts/20211221/31.jpg" /></p>

<p>一般的旋转（分解成三个坐标轴的旋转）</p>

<p><img src="../assets/img/posts/20211221/32.jpg" /></p>

<p>Rodrigues’ Rotation Formula, 用向量n表示旋转轴，最终推出这个公式</p>

<p><img src="../assets/img/posts/20211221/33.jpg" /></p>

<h2 id="42-view-transformation-视图变换">4.2. view transformation 视图变换</h2>

<ul>
  <li>
    <p>观测变换viewing，包括了视图变化和投影变化</p>
  </li>
  <li>
    <p>MVP变换(model-&gt;view-&gt;projection)</p>
  </li>
</ul>

<p><img src="../assets/img/posts/20211221/34.jpg" /></p>

<ul>
  <li>view transformation(不等于viewing) 视图变换</li>
</ul>

<p>视图变换是把相机放到标准位置上，located at origin, look at -Z</p>

<p><img src="../assets/img/posts/20211221/35.jpg" /></p>

<p>利用逆变换，先平移再旋转</p>

<p><img src="../assets/img/posts/20211221/36.jpg" /></p>

<p>一般把model和view变换统称为view transformation</p>

<h2 id="43-projection-transformation-投影变换">4.3. projection transformation 投影变换</h2>
<ul>
  <li>orthographic vs perspectiive projection</li>
</ul>

<p><img src="../assets/img/posts/20211221/37.jpg" /></p>

<ul>
  <li>orthographic projection 正交投影</li>
</ul>

<p><img src="../assets/img/posts/20211221/38.jpg" /></p>

<p>平移，缩放（不考虑旋转）</p>

<p><img src="../assets/img/posts/20211221/39.jpg" /></p>

<ul>
  <li>perspective projection 透视投影</li>
</ul>

<p>满足近大远小</p>

<p>透视投影就是先把物体挤压成立方体，然后对立方体进行正交投影</p>

<p><img src="../assets/img/posts/20211221/41.jpg" /></p>

<p><img src="../assets/img/posts/20211221/40.jpg" /></p>

<p><img src="../assets/img/posts/20211221/42.jpg" /></p>

<p><img src="../assets/img/posts/20211221/43.jpg" /></p>

<h1 id="5-lecture05-rasterization-1triangles">5. Lecture05 Rasterization 1(Triangles)</h1>

<h2 id="51-perspective-projection-透视投影">5.1. Perspective Projection 透视投影</h2>
<ul>
  <li>首先是对上节课的透视投影的一些补充, 其中l=left, r=right, b=bottom, t=top, n=near, f=far，这些量可以描述视锥Frustum</li>
</ul>

<center><img src="../assets/img/posts/20211221/45.jpg" /></center>

<ul>
  <li>视锥Frustum的描述还可以用fovY(field of view)垂直视角和aspect ratio宽高比</li>
</ul>

<center><img src="../assets/img/posts/20211221/46.jpg" /></center>

<h2 id="52-canonical-cube-to-screen-光栅化">5.2. Canonical Cube to Screen 光栅化</h2>
<ul>
  <li>
    <p>把物体的数学描述以及与物体相关的颜色信息转换为屏幕上用于对应位置的像素及用于填充像素的颜色，这个过程称为光栅化。</p>
  </li>
  <li>
    <p>屏幕是最常见的光栅设备，每一个像素都是一个小方块，像素是最小的单位，一个像素的颜色可以用rgb三种颜色表示</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/47.jpg" /></center>

<ul>
  <li>屏幕空间screen space</li>
</ul>

<center><img src="../assets/img/posts/20211221/48.jpg" /></center>

<ul>
  <li>把之前投影后的小方块变成屏幕空间</li>
</ul>

<center><img src="../assets/img/posts/20211221/49.jpg" /></center>

<center><img src="../assets/img/posts/20211221/50.jpg" /></center>

<h2 id="53-different-raster-displays-不同的成像设备">5.3. Different Raster Displays 不同的成像设备</h2>
<ul>
  <li>
    <p>Oscilloscope 示波器</p>
  </li>
  <li>
    <p>Cathode Ray Tube 阴极射线管成像原理。早期电视屏幕就是这样实现成像，扫描成像。</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/51.jpg" /></center>

<ul>
  <li>Frame Buffer: Memory for a Raster Display 内存中的一块区域存储图像信息。</li>
</ul>

<center><img src="../assets/img/posts/20211221/52.jpg" /></center>

<ul>
  <li>LCD(liquid crystal display)液晶显示器，光的波动性原理。</li>
</ul>

<center><img src="../assets/img/posts/20211221/53.jpg" /></center>

<ul>
  <li>LED发光二极管</li>
</ul>

<center><img src="../assets/img/posts/20211221/54.jpg" /></center>

<h2 id="54-三角形光栅化">5.4. 三角形光栅化</h2>
<ul>
  <li>三角形是最基本的多边形，有很多好的性质。</li>
</ul>

<center><img src="../assets/img/posts/20211221/55.jpg" /></center>

<ul>
  <li>sampling 采样。三角形离散化。</li>
</ul>

<center><img src="../assets/img/posts/20211221/56.jpg" /></center>

<center><img src="../assets/img/posts/20211221/57.jpg" /></center>

<p>在不同的像素中心，确定是0还是1,表示在三角形里还是外</p>

<center><img src="../assets/img/posts/20211221/58.jpg" /></center>

<ul>
  <li>如何判断点和三角形关系，利用叉积，边界上的点自己定义。</li>
</ul>

<center><img src="../assets/img/posts/20211221/59.jpg" /></center>

<center><img src="../assets/img/posts/20211221/60.jpg" /></center>

<ul>
  <li>jaggies锯齿，走样aliasing</li>
</ul>

<center><img src="../assets/img/posts/20211221/61.jpg" /></center>

<center><img src="../assets/img/posts/20211221/62.jpg" /></center>

<h1 id="6-lecture-06-rasterization-2antialiasing-and-z-buffering">6. Lecture 06 Rasterization 2(Antialiasing and Z-Buffering)</h1>

<h2 id="61-sampling-采样原理">6.1. sampling 采样原理</h2>
<ul>
  <li>视频就是对时间进行采样</li>
  <li>采样的artifact(瑕疵)：锯齿，摩尔纹，轮胎效应(在时间上采样)</li>
</ul>

<center><img src="../assets/img/posts/20211221/63.jpg" /></center>

<ul>
  <li>反走样采样：可以对原始的图像进行滤波(模糊处理)然后再采样。</li>
</ul>

<center><img src="../assets/img/posts/20211221/64.jpg" /></center>

<ul>
  <li>采样速度跟不上信号变化的速度就会走样(aliasing)</li>
</ul>

<h2 id="62-frequency-domaine-信号处理频率">6.2. Frequency domaine 信号处理频率</h2>
<ul>
  <li>傅里叶变换：所有的周期函数都可以写成不同平吕的正弦函数的组合。傅里叶变换就是频域和时域/空间域的变换</li>
</ul>

<center><img src="../assets/img/posts/20211221/66.jpg" /></center>

<ul>
  <li>走样的原因(时域)：高频信号欠采样，高频信号和低频信号在某一采样速度下没有差别，就会产生走样</li>
</ul>

<center><img src="../assets/img/posts/20211221/65.jpg" /></center>

<center><img src="../assets/img/posts/20211221/67.jpg" /></center>

<ul>
  <li>
    <p>滤波：抹掉特定的频率。比如高通滤波(过滤到低频信号)</p>
  </li>
  <li>
    <p>卷积：图形学上的简化定义，见下图</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/68.jpg" /></center>

<ul>
  <li>卷积定律：时域上的卷积等于频域上的乘积</li>
</ul>

<center><img src="../assets/img/posts/20211221/69.jpg" /></center>

<ul>
  <li>采样：重复频域上的内容</li>
</ul>

<center><img src="../assets/img/posts/20211221/70.jpg" /></center>

<ul>
  <li>走样在频率上的解释：采样频率小会让频域上发生重叠</li>
</ul>

<center><img src="../assets/img/posts/20211221/71.jpg" /></center>

<h2 id="63-antialiasing-反走样抗锯齿">6.3. antialiasing 反走样/抗锯齿</h2>

<ul>
  <li>
    <p>第一种解决方法：增加采样率，相当于增加了频域上的两个信号的距离</p>
  </li>
  <li>
    <p>第二种解决方法：反走样。即先对信号进行滤波再采样</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/72.jpg" /></center>

<ul>
  <li>比如对于之前三角形的问题</li>
</ul>

<center><img src="../assets/img/posts/20211221/73.jpg" /></center>

<ul>
  <li>但是这种反走样的方法比较复杂，有一种更简单的近似方法(对滤波这一步的近似)：supersampling，就是在对每个像素点变成更多的小点</li>
</ul>

<center><img src="../assets/img/posts/20211221/74.jpg" /></center>

<h2 id="64-antialiasing-today-目前反走样的方法">6.4. antialiasing today 目前反走样的方法</h2>
<p>介绍了两种新的抗锯齿的操作：FXAA和TAA。FXAA的做法是把边界找到然后对边界进行处理。</p>

<center><img src="../assets/img/posts/20211221/75.jpg" /></center>

<h1 id="7-lecture-07-shadingillumination-shading-and-graphics-pipeline">7. Lecture 07 Shading(Illumination, Shading, and Graphics Pipeline)</h1>

<h2 id="71-painters-algorithm-画家算法">7.1. Painter’s Algorithm 画家算法</h2>
<ul>
  <li>首先画出远处的物体，然后再画近处的物体。画近处的物体再覆盖远处的物体。</li>
  <li>需要定义深度信息，根据深度信息排序</li>
</ul>

<h2 id="72-z-buffer-深度缓存">7.2. Z-buffer 深度缓存</h2>
<ul>
  <li>对每个像素都有最小的z值，除了一个frame buffer储存颜色信息外，还需要z-buffer储存深度信息。</li>
</ul>

<center><img src="../assets/img/posts/20211221/76.jpg" /></center>

<center><img src="../assets/img/posts/20211221/77.jpg" /></center>

<ul>
  <li>
    <p>假设每个像素最开始的时候深度为无限远</p>
  </li>
  <li>
    <p>特点是在像素维度进行操作</p>
  </li>
</ul>

<h2 id="73-目前为止学到了什么">7.3. 目前为止学到了什么</h2>

<center><img src="../assets/img/posts/20211221/78.jpg" /></center>

<h2 id="74-shading-着色">7.4. shading 着色</h2>
<ul>
  <li>
    <p>着色：对不同物体应用不同的材质</p>
  </li>
  <li>
    <p>一个简单的着色模型(Blinn-Phong Reflection model)</p>
  </li>
  <li>
    <p>局部着色，不考虑阴影</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/79.jpg" /></center>

<ul>
  <li>diffuse reflection 漫反射，一个物体有多亮与接收到多少光的能量有关。点光源的能量随距离缩减。在点光源的光线到达物体表面时被物体接受多少能量又与光线和法线的夹角的cos值有关，也就是说直射时接受的能量最大(相同距离)。漫反射表示不论观测角度在哪，你观测到的亮度应该是一样的。</li>
</ul>

<center><img src="../assets/img/posts/20211221/80.jpg" /></center>

<h1 id="8-shading-2shading-pipeline-texture-mapping">8. Shading 2(Shading, Pipeline, Texture Mapping)</h1>
<h2 id="81-specular-term-高光项">8.1. Specular Term 高光项</h2>
<ul>
  <li>着色包括三部分：漫反射，高光，环境光</li>
  <li>高光就是观测方向和镜面反射方向相同，即半程向量是否和法向量接近</li>
</ul>

<center><img src="../assets/img/posts/20211221/81.jpg" /></center>

<ul>
  <li>通常高光都是白色的</li>
</ul>

<h2 id="82-ambient-term-环境项">8.2. Ambient Term 环境项</h2>
<ul>
  <li>
    <p>环境光就是一些其他物体反射的光照亮背光物体</p>
  </li>
  <li>
    <p>这里介绍非常简化的模型</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/82.jpg" /></center>

<ul>
  <li>最终结果</li>
</ul>

<center><img src="../assets/img/posts/20211221/83.jpg" /></center>

<h2 id="83-shading-frequencies-着色频率">8.3. Shading Frequencies 着色频率</h2>
<ul>
  <li>
    <p>之前介绍的着色是应用在着色点，对应在屏幕空间是如何的呢？</p>
  </li>
  <li>
    <p>第一种：Shading ecah triangle 对每个三角形着色</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/84.jpg" /></center>

<ul>
  <li>第二种：shading each vertex 对顶点着色，然后插值</li>
</ul>

<center><img src="../assets/img/posts/20211221/85.jpg" /></center>

<ul>
  <li>第三种：shading each pixel 对每个像素点着色</li>
</ul>

<center><img src="../assets/img/posts/20211221/86.jpg" /></center>

<ul>
  <li>如何定义顶点的法向量呢？对周围的面的法向量求平均</li>
</ul>

<center><img src="../assets/img/posts/20211221/87.jpg" /></center>

<ul>
  <li>如何定义像素的法向量？</li>
</ul>

<center><img src="../assets/img/posts/20211221/88.jpg" /></center>

<h2 id="84-graphics-pipeline-图像管线实时渲染管线">8.4. Graphics Pipeline 图像管线/实时渲染管线</h2>
<ul>
  <li>一个实时渲染的流程/流水线</li>
</ul>

<center><img src="../assets/img/posts/20211221/89.jpg" /></center>

<ul>
  <li>现代的GPU允许写入顶点着色部分与片段着色部分的代码</li>
</ul>

<h2 id="85-texture-mapping-纹理映射">8.5. Texture Mapping 纹理映射</h2>
<ul>
  <li>
    <p>希望在物体的不同位置定义不同的属性，比如漫反射系数等等</p>
  </li>
  <li>
    <p>3维物体的表现都是一个平面</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/90.jpg" /></center>

<ul>
  <li>纹理映射就是对于一个平面定义不同的属性，有一个映射关系</li>
</ul>

<center><img src="../assets/img/posts/20211221/91.jpg" /></center>

<ul>
  <li>纹理也有坐标系</li>
</ul>

<center><img src="../assets/img/posts/20211221/92.jpg" /></center>

<h1 id="9-lecture-09-shading-3-texture-mapping">9. Lecture 09 Shading 3 (Texture Mapping)</h1>

<h2 id="91-barycentric-coordinates重心坐标系">9.1. Barycentric Coordinates重心坐标系</h2>

<center><img src="../assets/img/posts/20211221/93.jpg" /></center>

<h2 id="92-interpolate-插值">9.2. Interpolate 插值</h2>
<ul>
  <li>重心坐标系插值</li>
</ul>

<center><img src="../assets/img/posts/20211221/94.jpg" /></center>

<h2 id="93-simple-texture-mapping-简单的纹理映射模型">9.3. Simple Texture Mapping 简单的纹理映射模型</h2>

<center><img src="../assets/img/posts/20211221/95.jpg" /></center>

<h2 id="94-texture-magnification-纹理放大">9.4. Texture Magnification 纹理放大</h2>

<center><img src="../assets/img/posts/20211221/96.jpg" /></center>

<h2 id="95-point-sampling-textures">9.5. Point Sampling Textures</h2>
<ul>
  <li>就是走样问题</li>
</ul>

<center><img src="../assets/img/posts/20211221/97.jpg" /></center>

<h2 id="96-mipmap-范围查询">9.6. Mipmap 范围查询</h2>
<ul>
  <li>生成不同分辨率的图片</li>
</ul>

<center><img src="../assets/img/posts/20211221/98.jpg" /></center>

<ul>
  <li>
    <p>任何一个像素可以映射到纹理区域的一个点，mipmap可以让像素点快速查阅，因为他又很多层，不同的纹理区域的面积对应不同的层</p>
  </li>
  <li>
    <p>mipmap也不是最好的方法，只是一种折中的办法</p>
  </li>
  <li>
    <p>anisotropic filtering 各向异性过滤</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/99.jpg" /></center>

<h1 id="10-lecture-10-geomrtry-1introduction">10. Lecture 10 Geomrtry 1(introduction)</h1>
<h2 id="101-纹理的应用">10.1. 纹理的应用</h2>
<h3 id="1011-environment-map-环境光映射">10.1.1. Environment Map 环境光映射</h3>
<ul>
  <li>纹理可以用来映射环境光</li>
</ul>

<center><img src="../assets/img/posts/20211221/100.jpg" /></center>

<ul>
  <li>假设环境光来自无限远</li>
</ul>

<h3 id="1012-spherical-environment-map-球形环境光映射">10.1.2. Spherical Environment Map 球形环境光映射</h3>
<ul>
  <li>将环境光信息存在球上</li>
</ul>

<center><img src="../assets/img/posts/20211221/101.jpg" /></center>

<ul>
  <li>但是在边缘部分会有扭曲，解决方法有环境光存在正方体上</li>
</ul>

<h3 id="1013-纹理凹凸贴图bump-mapping">10.1.3. 纹理凹凸贴图bump mapping</h3>
<ul>
  <li>
    <p>纹理不仅可以表示颜色，还可以应用一个复杂的纹理来定义高度，也就改变了法线的方向</p>
  </li>
  <li>
    <p>凹凸贴图只增加表面细节，不添加新的三角形</p>
  </li>
</ul>

<h3 id="1014-位移贴图-displacement-mapping">10.1.4. 位移贴图 displacement mapping</h3>
<ul>
  <li>和凹凸贴图很像，但是移动了顶点</li>
</ul>

<center><img src="../assets/img/posts/20211221/102.jpg" /></center>

<h3 id="1015-三维纹理">10.1.5. 三维纹理</h3>
<ul>
  <li>
    <p>定义了空间中任意一个点的纹理坐标</p>
  </li>
  <li>
    <p>广泛应用于体积渲染</p>
  </li>
</ul>

<h2 id="102-几何">10.2. 几何</h2>
<h3 id="1021-分类">10.2.1. 分类</h3>
<ul>
  <li>隐式几何</li>
  <li>显式几何</li>
</ul>

<h3 id="1022-隐式几何">10.2.2. 隐式几何</h3>
<ul>
  <li>不给出点的具体坐标，而是给出点的坐标关系，比如$x^2+y^2+z^2=1$</li>
  <li>推广到一般形式, $f(x,y,z)=0$</li>
  <li>缺点：不直观，不好采样</li>
  <li>优点：可以很容易的判断点在不在几何体内</li>
</ul>

<h3 id="1023-显式几何">10.2.3. 显式几何</h3>
<ul>
  <li>直接给出或者参数映射的方式给出</li>
</ul>

<center><img src="../assets/img/posts/20211221/103.jpg" /></center>

<ul>
  <li>优点：采样方便，直观</li>
  <li>缺点：不好判断点是否在几何体内还是外</li>
</ul>

<h3 id="1024-隐式的表达方式">10.2.4. 隐式的表达方式</h3>
<ul>
  <li>公式定义</li>
</ul>

<center><img src="../assets/img/posts/20211221/104.jpg" /></center>

<ul>
  <li>通过几何体的布尔组合，目前有很多建模软件就是这么表示的</li>
</ul>

<center><img src="../assets/img/posts/20211221/105.jpg" /></center>

<ul>
  <li>距离函数定义，SDF有向距离场</li>
</ul>

<center><img src="../assets/img/posts/20211221/106.jpg" /></center>

<h1 id="11-lecture-11-geometry-2curves-and-surfaces">11. Lecture 11 Geometry 2(Curves and Surfaces)</h1>
<h2 id="111-显式几何的表示方法">11.1. 显式几何的表示方法</h2>

<h3 id="1111-point-cloud-点云">11.1.1. Point Cloud 点云</h3>
<ul>
  <li>点的集合</li>
  <li>优点：可以表示任何几何体</li>
</ul>

<h3 id="1112-polygone-mesh">11.1.2. Polygone Mesh</h3>
<ul>
  <li>使用顶点和图形表示(三角形，正方形)</li>
</ul>

<h3 id="1113-一个例子">11.1.3. 一个例子</h3>

<center><img src="../assets/img/posts/20211221/107.jpg" /></center>

<p>里面定义了顶点坐标，法线，纹理坐标和哪几个点组成一个三角形</p>

<h2 id="112-curves-曲线">11.2. Curves 曲线</h2>
<h3 id="1121-贝塞尔曲线">11.2.1. 贝塞尔曲线</h3>
<ul>
  <li>用一系列控制点定义曲线</li>
</ul>

<center><img src="../assets/img/posts/20211221/108.jpg" /></center>

<ul>
  <li>曲线不一定要经过控制点</li>
</ul>

<h3 id="1122-如何画一条贝塞尔曲线">11.2.2. 如何画一条贝塞尔曲线</h3>
<ul>
  <li>Casteljau Algorithm：这个算法的核心是画出每个时间t的点的位置(递归)</li>
</ul>

<center><img src="../assets/img/posts/20211221/109.jpg" /></center>

<p>其中$b_0^2$就是时间t的点的位置</p>

<ul>
  <li>大致流程</li>
</ul>

<center><img src="../assets/img/posts/20211221/110.jpg" /></center>

<ul>
  <li>代数形式</li>
</ul>

<center><img src="../assets/img/posts/20211221/111.jpg" /></center>

<ul>
  <li>生成的曲线只能在控制点的凸包内</li>
</ul>

<h3 id="1123-piecewise-bézier-curves-逐段的贝塞尔曲线">11.2.3. Piecewise Bézier Curves 逐段的贝塞尔曲线</h3>

<ul>
  <li>每四个控制点定义一条贝塞尔曲线</li>
</ul>

<center><img src="../assets/img/posts/20211221/112.jpg" /></center>

<ul>
  <li>C0连续(点连续)，C1连续(切线连续)</li>
</ul>

<h3 id="1124-spline-样条">11.2.4. Spline 样条</h3>
<ul>
  <li>样条是用一系列的点画出线条</li>
</ul>

<center><img src="../assets/img/posts/20211221/113.jpg" /></center>

<h2 id="113-曲面">11.3. 曲面</h2>
<h3 id="1131-贝塞尔曲面">11.3.1. 贝塞尔曲面</h3>
<ul>
  <li>使用贝塞尔曲线生成贝塞尔曲面</li>
</ul>

<center><img src="../assets/img/posts/20211221/114.jpg" /></center>

<ul>
  <li>竖直方向生成四条曲线，然后对于t来说四个点再作为控制前生成曲线</li>
</ul>

<center><img src="../assets/img/posts/20211221/115.jpg" /></center>

<h3 id="1132-曲面细分">11.3.2. 曲面细分</h3>
<ul>
  <li>使用很多三角形网格来表示曲面</li>
</ul>

<center><img src="../assets/img/posts/20211221/116.jpg" /></center>

<h1 id="12-lecture-12-geometry-3">12. Lecture 12 Geometry 3</h1>
<h2 id="121-mesh-subdivisionupsampling-网格细分">12.1. Mesh Subdivision(upsampling) 网格细分</h2>
<ul>
  <li>引入更多三角形，微调它们的位置</li>
  <li>Loop Subdivision：第一步增加三角形的数量，第二部调整三角形的位置</li>
</ul>

<center><img src="../assets/img/posts/20211221/117.jpg" /></center>

<ul>
  <li>Loop细分规则：</li>
</ul>

<center><img src="../assets/img/posts/20211221/118.jpg" /></center>

<ul>
  <li>另一种细分规则：Catmull-Clark Subdivision</li>
</ul>

<p>奇异点是这个点的度不是4的点(就是连接的边数不等于4)</p>

<center><img src="../assets/img/posts/20211221/119.jpg" /></center>

<center><img src="../assets/img/posts/20211221/120.jpg" /></center>

<ul>
  <li>这种细分方法可以用于任何面</li>
</ul>

<h2 id="122-mesh-simplification-网格简化">12.2. Mesh Simplification 网格简化</h2>
<ul>
  <li>
    <p>基本思路是为了减少网格数目但是保持它的基本形状</p>
  </li>
  <li>
    <p>一种方法：Collapsing an edge 边坍缩。删除一些点</p>
  </li>
  <li>
    <p>判断标准：quadric error metrics 二次误差度量</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/121.jpg" /></center>

<ul>
  <li>实际效果</li>
</ul>

<center><img src="../assets/img/posts/20211221/122.jpg" /></center>

<h2 id="123-阴影-shadow-mapping">12.3. 阴影 Shadow mapping</h2>
<ul>
  <li>
    <p>光栅化着色的时候是局部的，但是有时候会有问题，比如有东西挡在shading point和光源之间时，所以需要在这种情况下生成阴影</p>
  </li>
  <li>
    <p>光栅化生成阴影的方法叫做shadow mapping</p>
  </li>
  <li>
    <p>shadow mapping 的两步</p>
  </li>
  <li>
    <p>第一步：从光源出发，看向shading point，记录能看见的点的深度</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/123.jpg" /></center>

<ul>
  <li>第二步：从摄像机出发，看向shading point，如果看见的点的深度和光源所看见的深度相同，那么这个点不在阴影内，否则，它在阴影内。</li>
</ul>

<center><img src="../assets/img/posts/20211221/124.jpg" /></center>

<ul>
  <li>具体的例子：</li>
</ul>

<center><img src="../assets/img/posts/20211221/125.jpg" /></center>

<ul>
  <li>问题：走样，阴影分辨率，只能做硬阴影(hard shadow)…</li>
</ul>

<center><img src="../assets/img/posts/20211221/126.jpg" /></center>

<h1 id="13-lecture-13-ray-tracing-1">13. Lecture 13 Ray Tracing 1</h1>
<h2 id="131-why-ray-tracing">13.1. Why ray tracing</h2>
<ul>
  <li>光栅化的缺点：无法表示全局的光照、毛玻璃效果无法很好表示、阴影处理不算好</li>
  <li>光纤追踪很精准但是比较慢，经常做离线(电影制作)</li>
</ul>

<h2 id="132-light-rays">13.2. Light Rays</h2>
<ul>
  <li>光线沿直线传播</li>
  <li>光线不会交叉</li>
  <li>光线是不断折回然后打到人眼</li>
  <li>光路可逆性</li>
</ul>

<h2 id="133-ray-casting-光线投射">13.3. Ray Casting 光线投射</h2>
<ul>
  <li>从眼睛到像素点出发，到虚拟世界，再到光源(Local)</li>
</ul>

<center><img src="../assets/img/posts/20211221/127.jpg" /></center>

<ul>
  <li>从眼睛到像素点到虚拟世界的线叫做eye ray</li>
</ul>

<h2 id="134-recursive-ray-tracing-递归光线追踪">13.4. Recursive Ray Tracing 递归光线追踪</h2>
<ul>
  <li>如果在shading point 处可以折射，能量损失，则继续折射然后对每个点都算着色值</li>
</ul>

<center><img src="../assets/img/posts/20211221/128.jpg" /></center>

<ul>
  <li>对每个点都要计算是否处在阴影中</li>
</ul>

<h2 id="135-ray-surface-interaction-光线和表面相交">13.5. Ray-Surface interaction 光线和表面相交</h2>
<h3 id="1351-ray-equation">13.5.1. Ray Equation</h3>

<center><img src="../assets/img/posts/20211221/129.jpg" /></center>

<h3 id="1352-与圆相交的交点">13.5.2. 与圆相交的交点</h3>

<center><img src="../assets/img/posts/20211221/130.jpg" /></center>

<ul>
  <li>一个交点就是相切，两个交点就是相交</li>
</ul>

<h3 id="1353-intersection-with-implicit-surface">13.5.3. intersection with implicit surface</h3>
<ul>
  <li>与隐式表面相交</li>
</ul>

<center><img src="../assets/img/posts/20211221/131.jpg" /></center>

<h3 id="1354-intersection-with-triangle-mesh">13.5.4. intersection with triangle mesh</h3>
<ul>
  <li>
    <p>也就是与显式表面(三角形网格)相交</p>
  </li>
  <li>
    <p>第一种想法就是光线与每个三角形进行计算，但这样计算量太大</p>
  </li>
  <li>
    <p>第二种想法是光线与三角形所在的平面相交，然后判断交点是不是在三角形内</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/132.jpg" /></center>

<ul>
  <li>如何定义平面？一个点+法线</li>
</ul>

<center><img src="../assets/img/posts/20211221/133.jpg" /></center>

<ul>
  <li>
    <p>然后将光线方程带入平面方程中，就可以得出光线与平面的交点</p>
  </li>
  <li>
    <p>如何简化判断交点与三角形的位置关系？MT算法：</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/134.jpg" /></center>

<p>这个算法的核心就是利用重心坐标系：解出重心坐标后，如果它们都为正，那么点在三角形内</p>

<h3 id="1355-accelerating-ray-surface-intersection">13.5.5. accelerating ray-surface intersection</h3>
<ul>
  <li>
    <p>加速交点(一般指与三角形网格的交点)计算过程</p>
  </li>
  <li>
    <p>bounding volume 包围盒</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/135.jpg" /></center>

<p>引入包围盒的思路是：如果光线与包围盒都不相交，那么肯定不会与里面的几何体有交点</p>

<ul>
  <li>包围盒由三个对面的交集</li>
</ul>

<center><img src="../assets/img/posts/20211221/136.jpg" /></center>

<p>轴对齐包围盒(就是对面与坐标轴平行)axis-aligned bounding box</p>

<ul>
  <li>先考虑二维的情况Ray intersection with aabb</li>
</ul>

<center><img src="../assets/img/posts/20211221/137.jpg" /></center>

<p>找到最大的时间和最小的时间</p>

<ul>
  <li>三维：对于三组对面，计算$t_{min}$和$t_{max}$，然后找到$t_{enter}$和$t_{exit}$。那么我们就知道了进入的时间和出去的时间，如果进去的时间小于出去的时间，那么光线进入了aabb，表示光线在盒子里呆过一段时间</li>
</ul>

<center><img src="../assets/img/posts/20211221/138.jpg" /></center>

<ul>
  <li>还要要保证进入的时间和出去的时间都要大于0</li>
</ul>

<h1 id="14-lecture-14-ray-tracing-2">14. Lecture 14 Ray Tracing 2</h1>
<h2 id="141-uniform-spatial-partitions-grids">14.1. Uniform Spatial Partitions (Grids)</h2>
<ul>
  <li>继续上节课的加速计算话题</li>
  <li>一种加速方法：生成grid</li>
</ul>

<center><img src="../assets/img/posts/20211221/139.jpg" /></center>

<p>找到aabb后，创建网格，存储aabb内几何体</p>

<ul>
  <li>然后光线沿着这些小格子相交</li>
</ul>

<center><img src="../assets/img/posts/20211221/140.jpg" /></center>

<h2 id="142-spatial-partitions-空间划分">14.2. Spatial Partitions 空间划分</h2>
<h3 id="1421-一些划分示例">14.2.1. 一些划分示例</h3>

<center><img src="../assets/img/posts/20211221/141.jpg" /></center>

<p>八叉树Oct-Tree，KD-Tree，BSP-Tree</p>

<h3 id="1422-kd-tree">14.2.2. KD-Tree</h3>

<center><img src="../assets/img/posts/20211221/142.jpg" /></center>

<ul>
  <li>
    <p>每次划分都沿着坐标轴移动，对于中间的结点都有子节点，只存储叶子结点的数据</p>
  </li>
  <li>
    <p>缺点：一个物体可能存在在多个叶子节点里</p>
  </li>
</ul>

<h2 id="143-object-partitions-物体划分">14.3. Object Partitions 物体划分</h2>
<h3 id="1431-bounding-volume-hierarchybvh">14.3.1. Bounding Volume Hierarchy(BVH)</h3>
<ul>
  <li>这种方法是目前图形学中使用较多的方法</li>
</ul>

<center><img src="../assets/img/posts/20211221/143.jpg" /></center>

<ul>
  <li>
    <p>沿着物体不断细分出bbox</p>
  </li>
  <li>
    <p>bvh的缺点：两部分bbox可能相交</p>
  </li>
</ul>

<h3 id="1432-building-bvh">14.3.2. Building BVH</h3>
<ul>
  <li>如何划分结点？选择一个维度进行划分，每次找最长的结点进行细分，细分的结点在中位数，当结点处图形较少，则停止</li>
</ul>

<center><img src="../assets/img/posts/20211221/144.jpg" /></center>

<h3 id="1433-与空间划分的对比">14.3.3. 与空间划分的对比</h3>

<center><img src="../assets/img/posts/20211221/145.jpg" /></center>

<h2 id="144-whitted-style">14.4. Whitted style</h2>
<ul>
  <li>到目前为止，已经讲了国内光线追踪会讲的内容。也就是讲完了Whitted style光线追踪</li>
</ul>

<h2 id="145-radiometry-辐射度量学">14.5. Radiometry 辐射度量学</h2>
<h3 id="1451-一些物理量">14.5.1. 一些物理量</h3>
<ul>
  <li>new terms: radiant flux, intensity, irradiance, radiance</li>
</ul>

<h3 id="1452-radiant-energy-and-flux">14.5.2. Radiant Energy and Flux</h3>
<ul>
  <li>randiant flux就是单位时间能量/功率</li>
</ul>

<center><img src="../assets/img/posts/20211221/146.jpg" /></center>

<h3 id="1453-radiant-intensity">14.5.3. Radiant Intensity</h3>
<ul>
  <li>辐射强度就是单位立体角(solid angle)的功率</li>
</ul>

<center><img src="../assets/img/posts/20211221/147.jpg" /></center>

<ul>
  <li>那么立体角是什么呢？立体角就是二维空间的角在三维空间的沿伸，就是球面面积除以半径的平方</li>
</ul>

<center><img src="../assets/img/posts/20211221/148.jpg" /></center>

<h1 id="15-lecture-15-ray-tracing">15. Lecture 15 Ray Tracing</h1>
<h2 id="151-radiometry-cont-辐射度量学">15.1. Radiometry cont. 辐射度量学</h2>
<h3 id="1511-继续上节课的内容">15.1.1. 继续上节课的内容</h3>

<ul>
  <li>微分立体角，就是球坐标系上对$\theta$和$\phi$的微分</li>
</ul>

<center><img src="../assets/img/posts/20211221/149.jpg" /></center>

<h3 id="1512-irradiance">15.1.2. Irradiance</h3>
<ul>
  <li>单位面积的功率</li>
</ul>

<center><img src="../assets/img/posts/20211221/150.jpg" /></center>

<ul>
  <li>面积是投影的面积</li>
</ul>

<h3 id="1513-radiance">15.1.3. Radiance</h3>
<ul>
  <li>randiance就是单位投影面积单位立体角的功率</li>
</ul>

<center><img src="../assets/img/posts/20211221/151.jpg" /></center>

<ul>
  <li>irradiance和radiance的区别：irradiance是某一个面积上接受的能量，而radiance是某一个面积某一个角度上接受的能量</li>
</ul>

<center><img src="../assets/img/posts/20211221/152.jpg" /></center>

<h2 id="152-bidirectional-reflectance-distribution-function-brdf">15.2. Bidirectional Reflectance Distribution Function (BRDF)</h2>
<ul>
  <li>
    <p>双向反射分布方程BRDF是描述光线传播的方程</p>
  </li>
  <li>
    <p>某一个方向$\omega_i$的光线打到某一个表面然后被吸收同时从另一个方向$\omega_r$反射出去</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/153.jpg" /></center>

<ul>
  <li>反射方程</li>
</ul>

<center><img src="../assets/img/posts/20211221/154.jpg" /></center>

<ul>
  <li>
    <p>观察某一个物体的反射光线不止从光源有光线，还有其他物体反射的光</p>
  </li>
  <li>
    <p>渲染方程Rendering Equation</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/155.jpg" /></center>

<ul>
  <li>渲染方程两部分组成，一部分是自身发光，另一部分是接受的光线的反射光线(半球上每个方向)</li>
</ul>

<h2 id="153-rendering-equation-渲染方程">15.3. Rendering Equation 渲染方程</h2>

<h3 id="1531-如何理解渲染方程">15.3.1. 如何理解渲染方程</h3>
<ul>
  <li>
    <p>反射的光线由两个个部分组成：自身的emission和从各个方向的反射光</p>
  </li>
  <li>
    <p>如何考虑物体反射的光？把物体看作一个光源，也就是看作一个递归的过程</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/156.jpg" /></center>

<ul>
  <li>通过数学式子简化渲染方程：</li>
</ul>

<center><img src="../assets/img/posts/20211221/157.jpg" /></center>

<ul>
  <li>然后通过逆矩阵可以解出L</li>
</ul>

<center><img src="../assets/img/posts/20211221/158.jpg" /></center>

<ul>
  <li>
    <p>光线弹射一次叫做直接光照、弹射两次及以上叫做间接光照</p>
  </li>
  <li>
    <p>那么就可以发现与光栅化的区别</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/159.jpg" /></center>

<ul>
  <li>在多次弹射后场景会趋于一个固定的亮度</li>
</ul>

<h1 id="16-lecture-16-ray-tracing-4">16. Lecture 16 Ray Tracing 4</h1>
<h2 id="161-monte-carlo-integration-蒙特卡洛积分">16.1. Monte Carlo Integration 蒙特卡洛积分</h2>
<ul>
  <li>有些函数不太好用解析式写出来</li>
  <li>蒙特卡洛积分就是数值积分的方法</li>
</ul>

<center><img src="../assets/img/posts/20211221/160.jpg" /></center>

<ul>
  <li>就是采样值除以采样密度</li>
</ul>

<center><img src="../assets/img/posts/20211221/161.jpg" /></center>

<h2 id="162-path-tracing-路径追踪">16.2. Path Tracing 路径追踪</h2>
<ul>
  <li>与whitted sytle的区别：whitted sytle没有考虑全局光照</li>
</ul>

<h3 id="1621-解渲染方程">16.2.1. 解渲染方程</h3>
<ul>
  <li>考虑一个简单的模型，只有直接光照</li>
</ul>

<center><img src="../assets/img/posts/20211221/162.jpg" /></center>

<ul>
  <li>每一个$\omega_i$都看作采样，那么可以应用蒙特卡洛积分</li>
</ul>

<center><img src="../assets/img/posts/20211221/163.jpg" /></center>

<ul>
  <li>应用全局光照，将物体反射面也看做光源，做一个递归</li>
</ul>

<center><img src="../assets/img/posts/20211221/164.jpg" /></center>

<ul>
  <li>
    <p>但是这样会出现一个问题，那就是爆炸，如果我取多个X，那么弹射很多次后就会爆炸</p>
  </li>
  <li>
    <p>解决方法，对每个点只取一个方向，也就是N=1，所以它叫做路径追踪</p>
  </li>
  <li>
    <p>这样噪声会比较大，但是从每个像素点有多个路径，所以还是可以接受</p>
  </li>
  <li>
    <p>第二个问题是递归不会停止？解决方法：俄罗斯轮盘赌，即在某一个程度停止递归</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/165.jpg" /></center>

<ul>
  <li>那么我们可以设定一个概率P来决定每个点是否打出一条光线，同时保证期望不变</li>
</ul>

<center><img src="../assets/img/posts/20211221/166.jpg" /></center>

<ul>
  <li>
    <p>到目前为止已经是一个正确的path tracing的渲染方法，但是这样效率比较低</p>
  </li>
  <li>
    <p>效率低的原因：每个点打到或者打不到光源是随机的，也就是说浪费了很多光线</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/167.jpg" /></center>

<ul>
  <li>可以在光源上采样，这样没有光线会浪费，渲染方程就需要写成在光源上采样</li>
</ul>

<center><img src="../assets/img/posts/20211221/168.jpg" /></center>

<ul>
  <li>那么我们就可以将渲染方程分为两部分，一部分是光源直接光照，方法使用上面提到的在光源上采样，另一部分是间接光照，保持不变</li>
</ul>

<h3 id="1622-最终的代码">16.2.2. 最终的代码</h3>

<center><img src="../assets/img/posts/20211221/169.jpg" /></center>

<ul>
  <li>但还有一个小问题，就是中间有物体遮挡，需要添加一个判断</li>
</ul>

<center><img src="../assets/img/posts/20211221/170.jpg" /></center>

<h2 id="163-路径追踪">16.3. 路径追踪</h2>
<ul>
  <li>在之前，ray tracing主要指whitted-style ray tracing</li>
  <li>但现在，只要设计了光线传播方法，就是ray tracing，路径追踪只是其中的一个方法</li>
</ul>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[Games 101 introduction to computer graphics 课程笔记]]></summary></entry><entry><title type="html">推荐系统</title><link href="http://localhost:4000/Recommender_system.html" rel="alternate" type="text/html" title="推荐系统" /><published>2021-12-16T00:00:00+08:00</published><updated>2021-12-16T00:00:00+08:00</updated><id>http://localhost:4000/Recommender_system</id><content type="html" xml:base="http://localhost:4000/Recommender_system.html"><![CDATA[<!-- TOC -->

<ul>
  <li><a href="#1-推荐系统总览">1. 推荐系统总览</a>
    <ul>
      <li><a href="#11-协同过滤-collaborative-filtering">1.1. 协同过滤 Collaborative Filtering</a></li>
      <li><a href="#12-显式反馈和隐式反馈">1.2. 显式反馈和隐式反馈</a></li>
      <li><a href="#13-推荐任务">1.3. 推荐任务</a></li>
    </ul>
  </li>
  <li><a href="#2-矩阵分解-matrix-factorization">2. 矩阵分解 Matrix Factorization</a></li>
  <li><a href="#3-autorec">3. AutoRec</a>
    <ul>
      <li><a href="#31-overview">3.1. overview</a></li>
      <li><a href="#32-formula">3.2. formula</a></li>
    </ul>
  </li>
  <li><a href="#4-personalized-ranking-for-recommender-system">4. Personalized Ranking for Recommender System</a>
    <ul>
      <li><a href="#41-overview">4.1. overview</a></li>
      <li><a href="#42-bayesian-personalized-ranking-loss-贝叶斯损失">4.2. Bayesian Personalized Ranking loss 贝叶斯损失</a></li>
      <li><a href="#43-hinge-loss">4.3. Hinge Loss</a></li>
    </ul>
  </li>
  <li><a href="#5-neural-collaborative-filtering-for-personalized-ranking-使用协同过滤网络个性化排序">5. Neural Collaborative Filtering for Personalized Ranking 使用协同过滤网络个性化排序</a>
    <ul>
      <li><a href="#51-the-neumf-model">5.1. The NeuMF model</a></li>
      <li><a href="#52-evaluator">5.2. Evaluator</a></li>
      <li><a href="#53-代码">5.3. 代码</a></li>
    </ul>
  </li>
  <li><a href="#6-sequence-aware-recommender-systems">6. Sequence-Aware Recommender Systems</a>
    <ul>
      <li><a href="#61-model-architectures">6.1. Model Architectures</a></li>
      <li><a href="#62-negative-sampling-负采样">6.2. Negative Sampling 负采样</a></li>
    </ul>
  </li>
  <li><a href="#7-feature-rich-recommender-systems">7. Feature-Rich Recommender Systems</a></li>
  <li><a href="#8-factorization-machines-因子分解机">8. Factorization Machines 因子分解机</a>
    <ul>
      <li><a href="#81-2-way-factorization-machines">8.1. 2-Way Factorization Machines</a></li>
      <li><a href="#82-an-efficient-optimization-citerion">8.2. An Efficient Optimization Citerion</a></li>
    </ul>
  </li>
  <li><a href="#9-deep-factorization-machines-深度因子分解机deeofm">9. Deep Factorization Machines 深度因子分解机DeeoFM</a>
    <ul>
      <li><a href="#91-model-architectures-模型架构">9.1. Model Architectures 模型架构</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h2 id="1-推荐系统总览">1. 推荐系统总览</h2>
<h3 id="11-协同过滤-collaborative-filtering">1.1. 协同过滤 Collaborative Filtering</h3>
<p>协同过滤最早出现在1992年Tapestry system，“人们相互协作，相互帮助，执行过滤程序，以处理大量的电子邮件和张贴到新闻组的信息。”现在协同过滤的概念更加广泛，从广义上讲，它是利用涉及多个用户、代理和数据源之间协作的技术来过滤<strong>信息或模式</strong>的过程。</p>

<p>协同过滤模型可以分为:1.memory-based CF; 2.model-based CF. 其中Memory-based CF又可以分为item-based和user-based CF。model-based CF有矩阵分解模型。</p>

<p>总的来说，协同过滤就是利用用户-物品的数据来预测和推荐。</p>

<h3 id="12-显式反馈和隐式反馈">1.2. 显式反馈和隐式反馈</h3>
<p>为了学习用户的偏好，系统需要收集用户的反馈feedback。反馈可以分为显式和隐式。</p>

<p>显式反馈就是需要用户主动提供兴趣偏好。比如点赞、点踩。</p>

<p>隐式反馈则是间接反映用户的喜好，比如购物历史记录，浏览记录，观看记录甚至是鼠标移动。</p>

<h3 id="13-推荐任务">1.3. 推荐任务</h3>
<p>电影推荐、新闻推荐、评分预测rating prediction task、top-n reommendation。如果使用了时间戳信息，那么我们构建了sequence-aware recommendation。针对新用户推荐新物品称为cold-start recommendation冷启动推荐。</p>

<h2 id="2-矩阵分解-matrix-factorization">2. 矩阵分解 Matrix Factorization</h2>
<p>The Matrix Factorization Model矩阵分解模型</p>

<p>R是user-item矩阵，行数是用户数量，列数是物品数量,那么R∈R<sup>mxn</sup>。P是user latent matrix，P∈R<sup>mxk</sup>，Q是item latent matrix，Q∈R<sup>nxk</sup></p>

<p>矩阵分解就是把R分解成P和Q，那么预测的评分就是：</p>

<p><img src="../assets/img/posts/20211216/2.jpg" /></p>

<p>但是上面这个式子没有考虑偏置，我们会有下面这个完整的式子：</p>

<p><img src="../assets/img/posts/20211216/3.jpg" /></p>

<p>那么<strong>目标函数</strong>可以定义为：</p>

<p><img src="../assets/img/posts/20211216/4.jpg" /></p>

<p>右边那一串是正则项，为了避免过拟合</p>

<p>下面这张图值观的展示了矩阵分解过程：</p>

<p><img src="../assets/img/posts/20211216/5.jpg" /></p>

<h2 id="3-autorec">3. AutoRec</h2>
<h3 id="31-overview">3.1. overview</h3>
<p>使用autoencoder预测评分，上小节介绍的矩阵分解模型是线性模型，它不能捕捉复杂的非线性关系，比如用户的偏好。这一小节介绍一个非线性协同过滤神经网络模型AutoRec。</p>

<p>AutoRec是基于自编码器的结构，自编码器是一种特殊的神经网络架构，他的输入和输出的架构是相同的，自编码器通过无监督学习来训练获取输入数据在较低维度的表达，在神经网络的后段，这些低纬度的信息再次被重构回高维的数据表达。</p>

<p>所以AutoRec的架构也是输入层，隐藏层和重构输出层。它的目的是输入一个只有部分兴趣矩阵，输出一个完整的兴趣矩阵。</p>

<p>AutoRec可以分为user-based 和 item-based</p>

<h3 id="32-formula">3.2. formula</h3>
<p>针对item-based：</p>

<p>$R_{*i}$表示兴趣矩阵的第i列，不知道的项填为0。那么神经网络的构架可以定义为：</p>

<center><img src="../assets/img/posts/20211216/6.jpg" /></center>

<p>h()表示最终的输出，输出一个完整的兴趣矩阵，那么误差定义为：</p>

<center><img src="../assets/img/posts/20211216/7.jpg" /></center>

<h2 id="4-personalized-ranking-for-recommender-system">4. Personalized Ranking for Recommender System</h2>
<h3 id="41-overview">4.1. overview</h3>
<p>在上一节中，我们用到了显式反馈，同时模型只在能观察到的评分上训练。那么这种模型有两个缺点：第一个是很多的反馈并不是显式的。第二个是没有观察到的评分被完全忽略了。</p>

<p>个性化推荐可以分为:1.pointwise;2.pairwise;3.listwise。Pointwise表示每次预测单个偏好，pairwise则是预测出一系列的偏好然后进行排序，listwise则是预测所有的item并进行排序。</p>

<h3 id="42-bayesian-personalized-ranking-loss-贝叶斯损失">4.2. Bayesian Personalized Ranking loss 贝叶斯损失</h3>
<ul>
  <li>
    <p>贝叶斯损失是一种pairwise个性化推荐损失。它被广泛应用于多种推荐系统中。它假设用户相对于无观察项，更加喜欢positive item</p>
  </li>
  <li>
    <p>训练集格式是(u, i, j)表示用户u喜欢i超过j，BPR希望最大化下面这个后验概率：</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211216/8.jpg" /></center>

<p>其中$\Theta$表示推荐系统的参数，$&gt;_u$表示用户u对所有item的排序。</p>

<center><img src="../assets/img/posts/20211216/9.jpg" /></center>

<h3 id="43-hinge-loss">4.3. Hinge Loss</h3>
<ul>
  <li>数学表达式</li>
</ul>

<center><img src="../assets/img/posts/20211216/10.jpg" /></center>

<p>其中m表示安全系数，它的目的是让不喜欢的项离喜欢的项更远。它和贝叶斯都是为了优化positive sample和negative sample之间的距离。</p>

<h2 id="5-neural-collaborative-filtering-for-personalized-ranking-使用协同过滤网络个性化排序">5. Neural Collaborative Filtering for Personalized Ranking 使用协同过滤网络个性化排序</h2>
<p>本小节重新将目光聚集到隐式反馈中，介绍协同过滤推荐系统NeuMF。NeuMF利用隐式反馈，它由两个子结构组成，分别是generalized matrix factorization(GMF)和MLP。不同于评分的预测如AutoRec，它将生成一系列的推荐，它根据用户是否看过这场电影来区分为正例和反例</p>

<h3 id="51-the-neumf-model">5.1. The NeuMF model</h3>
<p>NeuMF的网络结构由两部分组成。</p>

<ul>
  <li>一部分是GMF，也就是matrix factorization的类似形式，输入用户向量$p_u$和物品向量$q_i$，返回x</li>
</ul>

<center><img src="../assets/img/posts/20211216/12.jpg" /></center>

<ul>
  <li>另一部分是MLP，输入和GMF一样，但是用不同的字母表示，具体公式如下：</li>
</ul>

<center><img src="../assets/img/posts/20211216/13.jpg" /></center>

<ul>
  <li>最后对这两个子结构concatenate一下，就是最终的输出</li>
</ul>

<center><img src="../assets/img/posts/20211216/14.jpg" /></center>

<ul>
  <li>大体的网络结构如下</li>
</ul>

<center><img src="../assets/img/posts/20211216/11.jpg" /></center>

<h3 id="52-evaluator">5.2. Evaluator</h3>
<p>有两个性能度量指标</p>

<ul>
  <li>hit rate at given cutting off l，记作Hit@l</li>
</ul>

<center><img src="../assets/img/posts/20211216/15.jpg" /></center>

<p>这个式子的主题思路是判断推荐的物品是否在top l中，m表示用户的数量，$rank_{u,g_u}$表示对于用户u和物品$g_u$的排名，1表示指标函数</p>

<ul>
  <li>AUC，即ROC曲线下的面积，也是模型泛化能力的一个指标</li>
</ul>

<center><img src="../assets/img/posts/20211216/16.jpg" /></center>

<p>其中$S_u$表示模型对于u的推荐物品集，I表示item set，AUC越大越好</p>

<h3 id="53-代码">5.3. 代码</h3>
<p>网络结构就是上面介绍的那样，net的输出是用户和物品匹配出的一个推荐值(我的想法)。在进行训练的时候，会给出正例物品(即用户有过评分的物品)和反例物品(用户没有评分，也就是没有看过)分别与用户得到一个推荐值，然后利用上一小节介绍的贝叶斯损失来优化(让评分过的物品有更高的推荐值)，然后最终我们希望返回一系列的推荐物品，这些推荐物品都是没有负例物品，然后根据推荐值进行排序。性能指标是hit或者auc。hit的思想是让真实评分的物品在推荐列表中。</p>

<h2 id="6-sequence-aware-recommender-systems">6. Sequence-Aware Recommender Systems</h2>
<p>之前的模型都没有考虑时序信息，这小节的Caser模型将会考虑用户的时序信息。</p>
<h3 id="61-model-architectures">6.1. Model Architectures</h3>
<p>模型的输入$E^{(u,t)}$表示用户u的近期L个评价的物品，Caser模型有横向和纵向的卷积层，输入矩阵分别与卷积层作用后，结果concatenate变成$z$，$z$再和用户的一般信息结合，也就是$z$和$p_u$concatenate最终输出$\hat{y}_{uit}$，其中$p_u$表示用户u的item信息</p>

<center><img src="../assets/img/posts/20211216/17.jpg" /></center>

<h3 id="62-negative-sampling-负采样">6.2. Negative Sampling 负采样</h3>
<p>我们需要对数据集进行重新处理，比如一个人喜欢9部电影，同时我们的L=5，那么我们将最近的一部电影留出来作为test，其余的都作为训练集，可以划分出3个训练集。同时我们也需要进行负采样(采样没有评分的item)</p>

<h2 id="7-feature-rich-recommender-systems">7. Feature-Rich Recommender Systems</h2>
<p>之前的模型大都用到了用户物品的交互矩阵，但是很少有用到一些额外的信息，比如物品的特征，用户的简介，发生交互的背景等等…利用这些信息可以获得用户的兴趣特征。本节提出了一个新的任务CTR(click-through rate)，也就是点击率任务，对象可以是广告、电影等等。</p>

<h2 id="8-factorization-machines-因子分解机">8. Factorization Machines 因子分解机</h2>
<p>Factorization machines(FM)是一个监督算法，可用于分类，回归和排名任务。它有两个优点：1.它能处理稀疏的数据；2.它能减少时间复杂度和线性复杂度</p>

<h3 id="81-2-way-factorization-machines">8.1. 2-Way Factorization Machines</h3>
<p>$x$表示样本的特征值，而$y$表示它的标签值，即click/non-click。第二项表示线性项，第三项表示矩阵分解项</p>

<center><img src="../assets/img/posts/20211216/18.jpg" /></center>

<h3 id="82-an-efficient-optimization-citerion">8.2. An Efficient Optimization Citerion</h3>
<p>上面式子的第三项时间复杂度太高，我们可以简化一下</p>

<center><img src="../assets/img/posts/20211216/19.jpg" /></center>

<h2 id="9-deep-factorization-machines-深度因子分解机deeofm">9. Deep Factorization Machines 深度因子分解机DeeoFM</h2>
<p>上小节提到的因子分解机用到的都是线性模型(单线性和双线性)，这种模型在真实数据表现并不好。这里我们就可以结合因子分解机和深度神经网络，比如我们这小节即将介绍的DeepFM。</p>

<h3 id="91-model-architectures-模型架构">9.1. Model Architectures 模型架构</h3>
<p>DeepFM由两部分组成，FM component和deep component，FM部分和上小节提到的2-way FM做法一样，主要是处理低纬度特征，而deep部分用到的MLP来处理高维度和非线性。这两部分使用相同的输入/嵌入层然后它们的结果整合成最终的预测。模型结构如下图：</p>

<center><img src="../assets/img/posts/20211216/20.jpg" /></center>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[d2l推荐系统笔记]]></summary></entry><entry><title type="html">Robotics</title><link href="http://localhost:4000/Robotics.html" rel="alternate" type="text/html" title="Robotics" /><published>2021-12-13T00:00:00+08:00</published><updated>2021-12-13T00:00:00+08:00</updated><id>http://localhost:4000/Robotics</id><content type="html" xml:base="http://localhost:4000/Robotics.html"><![CDATA[<h1 id="报告">报告</h1>
<h2 id="报告内容">报告内容</h2>
<p>用solidworks设计一个至少三个自由度的机械臂，并且描述它的动能，移动能力等等。提交的是截图，源文件等等。</p>
<h2 id="报告格式">报告格式</h2>
<ol>
  <li>标题，下面有姓名学号电话等等</li>
  <li>摘要</li>
  <li>正文</li>
</ol>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[《机器人》课程随堂笔记]]></summary></entry><entry><title type="html">数据挖掘</title><link href="http://localhost:4000/datamining.html" rel="alternate" type="text/html" title="数据挖掘" /><published>2021-12-10T00:00:00+08:00</published><updated>2021-12-10T00:00:00+08:00</updated><id>http://localhost:4000/datamining</id><content type="html" xml:base="http://localhost:4000/datamining.html"><![CDATA[<!-- TOC -->

<ul>
  <li><a href="#总体情况">总体情况</a>
    <ul>
      <li><a href="#第一章-开始数据挖掘之旅">第一章 开始数据挖掘之旅</a>
        <ul>
          <li><a href="#11-亲和性分析">1.1 亲和性分析</a></li>
          <li><a href="#12-分类">1.2 分类</a></li>
        </ul>
      </li>
      <li><a href="#第二章-用scikit-learn估计器分类">第二章 用scikit-learn估计器分类</a>
        <ul>
          <li><a href="#21-scikit-learn">2.1 scikit-learn</a></li>
          <li><a href="#22-邻近算法knn">2.2 邻近算法KNN</a></li>
        </ul>
      </li>
      <li><a href="#第三章-用决策树预测获胜球队">第三章 用决策树预测获胜球队</a>
        <ul>
          <li><a href="#31-决策树">3.1 决策树</a></li>
          <li><a href="#32-随机森林">3.2 随机森林</a></li>
        </ul>
      </li>
      <li><a href="#第四章-用亲和性分析方法推荐电影">第四章 用亲和性分析方法推荐电影</a>
        <ul>
          <li><a href="#41-亲和性分析">4.1 亲和性分析</a></li>
          <li><a href="#42-apriori算法">4.2 Apriori算法</a></li>
        </ul>
      </li>
      <li><a href="#第五章-用转换器抽取特征">第五章 用转换器抽取特征</a>
        <ul>
          <li><a href="#51-抽取特征">5.1 抽取特征</a></li>
          <li><a href="#52-特征选择">5.2 特征选择</a></li>
          <li><a href="#53-创建特征">5.3 创建特征</a></li>
        </ul>
      </li>
      <li><a href="#第六章-使用朴素贝叶斯进行社会媒体挖掘">第六章 使用朴素贝叶斯进行社会媒体挖掘</a>
        <ul>
          <li><a href="#61-消歧">6.1 消歧</a></li>
          <li><a href="#62-文本转换器">6.2 文本转换器</a></li>
          <li><a href="#63-朴素贝叶斯">6.3 朴素贝叶斯</a></li>
          <li><a href="#64-f1值">6.4 F1值</a></li>
        </ul>
      </li>
      <li><a href="#第九章-作者归属问题">第九章 作者归属问题</a>
        <ul>
          <li><a href="#91-作者归属">9.1 作者归属</a></li>
          <li><a href="#92-支持向量机">9.2 支持向量机</a></li>
          <li><a href="#93-基础svm的局限性">9.3 基础SVM的局限性</a></li>
        </ul>
      </li>
      <li><a href="#第十章-新闻语料分类">第十章 新闻语料分类</a>
        <ul>
          <li><a href="#101-新闻语料聚类">10.1 新闻语料聚类</a></li>
          <li><a href="#102-k-means算法">10.2 K-means算法</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h1 id="总体情况">总体情况</h1>
<ul>
  <li>书籍:Python数据挖掘入门与实践</li>
  <li>github_url:https://github.com/LinXueyuanStdio/PythonDataMining</li>
  <li>配套代码和笔记，很适合迅速上手</li>
  <li>这篇博客主要记录一些比较重要的算法</li>
</ul>

<h2 id="第一章-开始数据挖掘之旅">第一章 开始数据挖掘之旅</h2>
<h3 id="11-亲和性分析">1.1 亲和性分析</h3>
<ul>
  <li>亲和性分析根据样本个体（物体）之间的<strong>相似度</strong>，确定它们关系的亲疏。</li>
  <li>例子：商品推荐。</li>
  <li>我们要找出“如果顾客购买了商品X，那么他们可能愿意购买商品Y”这样的规则。简单粗暴的做法是，找出数据集中所有同时购买的两件商品。找出规则后，还需要判断其优劣，我们挑好的规则用。</li>
  <li>规则的优劣有多种判断标准，常用的有支持度(support)和置信度(confidence)</li>
  <li>支持度：数据集中规则应验的次数，统计起来很简单。有时候，还需要对支持度进行规范化，即再除以规则有效前提下的总数量。</li>
  <li>置信度是衡量规则的准确性如何。</li>
</ul>

<h3 id="12-分类">1.2 分类</h3>
<ul>
  <li>根据特征分出类别</li>
  <li>例子：Iris植物分类数据集，通过四个特征分出三个类别</li>
  <li>特征连续值变成离散值</li>
  <li>OneR算法：它根据已有数据中，具有相同特征值的个体最可能属于哪个类别进行分类。比如对于某一个特征值来说，属于A的类别有80个，属于B的类别有20个，那么对于这个特征值来说，取值为1代表为A类别，错误率有20％。给出所有特征值，找出错误率最小的特征值作为判断标准。</li>
</ul>

<h2 id="第二章-用scikit-learn估计器分类">第二章 用scikit-learn估计器分类</h2>
<h3 id="21-scikit-learn">2.1 scikit-learn</h3>
<p>scikit-learn里面已经封装好很多数据挖掘的算法</p>

<p>现介绍数据挖掘框架的搭建方法：</p>

<ul>
  <li>转换器（Transformer）用于数据预处理，数据转换</li>
  <li>流水线（Pipeline）组合数据挖掘流程，方便再次使用（封装）</li>
  <li>估计器（Estimator）用于分类，聚类，回归分析（各种算法对象）
    <ul>
      <li>所有的估计器都有下面2个函数
        <ul>
          <li>fit() 训练
            <ul>
              <li>用法：estimator.fit(X_train, y_train)，</li>
              <li>estimator = KNeighborsClassifier() 是scikit-learn算法对象</li>
              <li>X_train = dataset.data 是numpy数组</li>
              <li>y_train = dataset.target 是numpy数组</li>
            </ul>
          </li>
          <li>predict() 预测
            <ul>
              <li>用法：estimator.predict(X_test)</li>
              <li>estimator = KNeighborsClassifier() 是scikit-learn算法对象</li>
              <li>X_test = dataset.data 是numpy数组</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="22-邻近算法knn">2.2 邻近算法KNN</h3>
<p>邻近算法，或者说K最邻近（KNN，K-NearestNeighbor）分类算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是K个最近的邻居的意思，说的是每个样本都可以用它最接近的K个邻近值来代表。近邻算法就是将数据集合中每一个记录进行分类的方法。</p>

<p>例子：分类，Ionosphere数据集</p>

<h2 id="第三章-用决策树预测获胜球队">第三章 用决策树预测获胜球队</h2>

<h3 id="31-决策树">3.1 决策树</h3>
<p>例子：预测NBA球队获胜情况</p>

<p>决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。</p>

<p>分类树（决策树）是一种十分常用的分类方法。它是一种监督学习，所谓监督学习就是给定一堆样本，每个样本都有一组属性和一个类别，这些类别是事先确定的，那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类。这样的机器学习就被称之为监督学习。</p>

<p>scikit-learn库实现了分类回归树（Classification and Regression Trees，CART）算法并将其作为生成决策树的默认算法，它支持连续型特征和类别型特征。</p>

<h3 id="32-随机森林">3.2 随机森林</h3>
<p>随机森林指的是利用多棵树对样本进行训练并预测的一种分类器。</p>

<p>在机器学习中，随机森林是一个包含多个决策树的分类器， 并且其输出的类别是由个别树输出的类别的众数而定。</p>

<h2 id="第四章-用亲和性分析方法推荐电影">第四章 用亲和性分析方法推荐电影</h2>
<h3 id="41-亲和性分析">4.1 亲和性分析</h3>
<p>亲和性分析就是分析两个样本之间的疏密关系，常用的算法有Apriori，Apriori算法的一大特点是根据最小支持度生成<strong>频繁项集</strong>（frequent itemest），它只从数据集中频繁出现的商品中选取共同出现的商品组成频繁项集。其他亲和性分析算法有Eclat和频繁项集挖掘算法（FP-growth）。</p>

<h3 id="42-apriori算法">4.2 Apriori算法</h3>
<p>Apriori算法主要有两个阶段，第一个阶段是根据最小支持度生成频繁项集，第二个阶段是根据最小置信度选择规则，返回规则。</p>

<p>本章的例子是电影推荐。</p>

<p>第一个阶段，算法会先生成长度较小的项集，再将这个项集作为超集寻找长度较大的项集。</p>

<p>第二个阶段是从频繁项集中抽取关联规则。把其中几部电影作为前提，另一部电影作为结论。组成如下形式的规则：如果用户喜欢前提中的所有电影，那么他们也会喜欢结论中的电影。</p>

<h2 id="第五章-用转换器抽取特征">第五章 用转换器抽取特征</h2>
<h3 id="51-抽取特征">5.1 抽取特征</h3>
<p>抽取数据集的特征是重要的一步，在之前的学习中我们都获得了数据集的特征，但很多没有处理的文本特征并不是很明显，比如一段文本等等。特征值可以分为连续特征，序数特征，类别型特征。</p>

<h3 id="52-特征选择">5.2 特征选择</h3>
<p>通常特征有很多，但我们只想选择其中一部分。<strong>选用干净的数据，选取更具描述性的特征。</strong>判断特征相关性：书中列举的例子是判断一个人的收入能不能超过五万，利用单变量卡方检验(或者皮尔逊相关系数)判断各个特征的相关性，然后给出了三个最好的特征，分别是年龄，资本收入和资本损失。</p>

<h3 id="53-创建特征">5.3 创建特征</h3>
<p>主成分分析算法（Principal Component Analysis，PCA）的目的是找到能用较少信息描述数据集的特征组合。</p>

<h2 id="第六章-使用朴素贝叶斯进行社会媒体挖掘">第六章 使用朴素贝叶斯进行社会媒体挖掘</h2>
<h3 id="61-消歧">6.1 消歧</h3>
<p>本章我们将处理文本，文本通常被称为无结构格式。文本挖掘的一个难点来自于歧义，比如bank一词多义。本章将探讨区别Twitter消息中Python的意思。</p>

<h3 id="62-文本转换器">6.2 文本转换器</h3>
<p>Python中处理文本的库NLTK(Natural Language Toolkit)。据作者说很好用，可以作自然语言处理。N元语法是指由连续的词组成的子序列。</p>

<h3 id="63-朴素贝叶斯">6.3 朴素贝叶斯</h3>
<p>朴素贝叶斯概率模型是以对贝叶斯统计方法的朴素解释为基础。</p>

<p>贝叶斯定理公式如下：</p>

<p>$ P(A|B) = \frac {P(B|A)P(A)}{P(B)} $</p>

<p>贝叶斯公式可以用它来计算个体属于给定类别的概率。朴素贝叶斯算法假定了各个特征之间相互独立，那么我们计算文档D属于类别C的概率为P(D|C)=P(D1|C)*P(D2|C)…P(Dn|C)。贝叶斯分类器是输入数据来更新贝叶斯的先验概率和后验概率，输入贝叶斯模型后，返回不同类别中概率的最大值。</p>

<p>示例：</p>

<blockquote>
  <p>举例说明下计算过程，假如数据集中有以下一条用二值特征表示的数据：[1, 0, 0, 1]。<br />
训练集中有75%的数据属于类别0，25%属于类别1，且每个特征属于每个类别的似然度如下。<br />
类别0：[0.3, 0.4, 0.4, 0.7] <br />
类别1：[0.7, 0.3, 0.4, 0.9] <br />
拿类别0中特征1的似然度举例子，上面这两行数据可以这样理解：类别0中有30%的数据，特征1的值为1。<br />
我们来计算一下这条数据属于类别0的概率。类别为0时，P(C=0) = 0.75。<br />
朴素贝叶斯算法用不到P(D)，因此我们不用计算它。我们来看下计算过程。<br />
P(D|C=0) = P(D1|C=0) x P(D2|C=0) x P(D3|C=0) x P(D4|C=0)<br />
= 0.3 x 0.6 x 0.6 x 0.7 <br />
= 0.0756 <br />
现在，我们就可以计算该条数据从属于每个类别的概率。需要提醒的是，我们没有计算P(D)，因此，计算结果不是实际的概率。由于两次都不计算P(D)，结果具有可比较性，能够区分出大小就足够了。来看下计算结果。<br />
P(C=0|D) = P(C=0) P(D|C=0) <br />
= 0.75 * 0.0756 <br />
= 0.0567</p>
</blockquote>

<h3 id="64-f1值">6.4 F1值</h3>
<p>F1值是一种评价指标。F1值是以每个类别为基础进行定义的，包括两大概念：准确率（precision）和召回率（recall）。准确率是指预测结果属于某一类的个体，实际属于该类的比例。召回率是指被正确预测为某个类别的个体数量与数据集中该类别个体总量的比例。F1值是准确率和召回率的调和平均数。</p>

<h2 id="第九章-作者归属问题">第九章 作者归属问题</h2>
<h3 id="91-作者归属">9.1 作者归属</h3>
<p>作者归属（authorship attribution）是作者分析的一个细分领域，研究目标是从一组可能的作者中找到文档真正的主人。利用功能词进行分类，功能词是指本身含义很少，但是是组成句子必不可少的部分。</p>

<h3 id="92-支持向量机">9.2 支持向量机</h3>
<p>支持向量机（SVM）分类算法背后的思想很简单，它是一种二类分类器（扩展后可用来对多个类别进行分类）。假如我们有两个类别的数据，而这两个类别恰好能被一条线分开，线上所有点为一类，线下所有点属于另一类。SVM要做的就是找到这条线，用它来做预测，跟线性回归原理很像。</p>

<p>下图中有三条线，那么哪一条线的分类效果最好呢？直觉告诉我们从左下到右上的这一条线效果最好，因为每一个点到这条线的距离最远，那么寻找这条线就变成了最优化问题。</p>

<p><img src="../assets/img/posts/20211210/2.jpg" /></p>

<p>对于多种类别的分类问题，我们创建多个SVM分类器，其中每个SVM分类器还是二分类。连接多个分类器的方法有很多，比如说我们可以将每个类别创建一对多分类器。把训练数据分为两个类别——属于特定类别的数据和其他所有类别数据。对新数据进行分类时，从这些类别中找出最匹配的。</p>

<h3 id="93-基础svm的局限性">9.3 基础SVM的局限性</h3>
<p>最基础的SVM只能区分线性可分的两种类别，如果数据线性不可分，就需要将其置入更高维的空间中，加入更多伪特征直到数据线性可分。寻找最佳分隔线时往往需要计算个体之间的内积。我们把内核函数定义为数据集中两个个体函数的点积。</p>

<p>常用的内核函数有几种。线性内核最简单，它无外乎两个个体的特征向量的点积、带权重的特征和偏置项。多项式内核提高点积的阶数（比如2）。此外，还有高斯内核（rbf）、Sigmoind内核。</p>

<h2 id="第十章-新闻语料分类">第十章 新闻语料分类</h2>
<h3 id="101-新闻语料聚类">10.1 新闻语料聚类</h3>
<p>之前我们研究的都是监督学习，在已经知道类别的情况下进行分类。本章着眼于无监督学习，聚类。</p>

<h3 id="102-k-means算法">10.2 K-means算法</h3>
<p>k-means聚类算法迭代寻找最能够代表数据的聚类质心点。算法开始时使用从训练数据中随机选取的几个数据点作为质心点。k-means中的k表示寻找多少个质心点，同时也是算法将会找到的簇的数量。例如，把k设置为3，数据集所有数据将会被分成3个簇。</p>

<p>k-means算法分为两个步骤：为每一个数据点分配簇标签，更新各簇的质心点。k-means算法会重复上述两个步骤；每次更新质心点时，所有质心点将会小范围移动。这会
轻微改变每个数据点在簇内的位置，从而引发下一次迭代时质心点的变动。这个过程会重复执行直到条件不再满足时为止。通常是在迭代一定次数后，或者当质心点的整体移动量很小时，就可以终止算法的运行。有时可以等算法自行终止运行，这表明簇已经相当稳定——数据点所属的簇不再变动，质心点也不再改变时。</p>

<p><img src="../assets/img/posts/20211210/3.jpg" /></p>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[《Python数据挖掘入门与实践》笔记]]></summary></entry><entry><title type="html">RACE数据集相关文献</title><link href="http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html" rel="alternate" type="text/html" title="RACE数据集相关文献" /><published>2021-11-30T00:00:00+08:00</published><updated>2021-11-30T00:00:00+08:00</updated><id>http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE</id><content type="html" xml:base="http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html"><![CDATA[<h1 id="目录"><strong>目录</strong></h1>

<ul>
  <li><a href="#目录"><strong>目录</strong></a></li>
  <li><a href="#文献整理">文献整理</a>
    <ul>
      <li><a href="#要求">要求</a></li>
      <li><a href="#搜集到相关文献标题和地址">搜集到相关文献标题和地址</a></li>
    </ul>
  </li>
  <li><a href="#第一篇">第一篇</a>
    <ul>
      <li><a href="#title">Title</a></li>
      <li><a href="#author">Author</a></li>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#introduction">Introduction</a></li>
      <li><a href="#bert-distractor-generation">BERT distractor generation</a>
        <ul>
          <li><a href="#1bert-based-distractor-generationbdg">1)BERT-based distractor generation(BDG)</a></li>
          <li><a href="#2multi-task-with-parallel-mlm">2)Multi-task with Parallel MLM</a></li>
          <li><a href="#3answer-negative-regularization">3)Answer Negative Regularization</a></li>
        </ul>
      </li>
      <li><a href="#multiple-distractor-generation">Multiple Distractor Generation</a>
        <ul>
          <li><a href="#1selecting-distractors-by-entropy-maximization">1)Selecting Distractors by Entropy Maximization</a></li>
          <li><a href="#2bdg-em">2)BDG-EM</a></li>
        </ul>
      </li>
      <li><a href="#performance-evaluation">Performance Evaluation</a>
        <ul>
          <li><a href="#1datasets">1)datasets</a></li>
          <li><a href="#2implementation-details">2)implementation details</a></li>
          <li><a href="#3compared-methods">3)compared methods</a></li>
          <li><a href="#4token-score-comparison">4)token score comparison</a></li>
          <li><a href="#5mcq-model-accuracy-comparison">5)MCQ Model Accuracy Comparison</a></li>
          <li><a href="#6parameter-study-on-γ">6）Parameter Study on γ</a></li>
        </ul>
      </li>
      <li><a href="#conclusion">Conclusion</a></li>
      <li><a href="#我的看法">我的看法</a></li>
    </ul>
  </li>
  <li><a href="#第二篇">第二篇</a>
    <ul>
      <li><a href="#title-1">Title</a></li>
      <li><a href="#author-1">Author</a></li>
      <li><a href="#abstract-1">Abstract</a></li>
      <li><a href="#method">Method</a>
        <ul>
          <li><a href="#1question-generation">1)question generation</a></li>
          <li><a href="#2distractor-generation">2)distractor generation</a></li>
          <li><a href="#3qa-filtering">3)QA filtering</a></li>
        </ul>
      </li>
      <li><a href="#results">Results</a>
        <ul>
          <li><a href="#1quantitative-evaluation">1)quantitative evaluation</a></li>
          <li><a href="#2question-answering-ability">2)question answering ability</a></li>
          <li><a href="#3human-evaluation">3)human evaluation</a></li>
        </ul>
      </li>
      <li><a href="#conclusion">conclusion</a></li>
    </ul>
  </li>
  <li><a href="#第三篇">第三篇</a>
    <ul>
      <li><a href="#title-2">Title</a></li>
      <li><a href="#author-2">Author</a></li>
      <li><a href="#abstract-2">Abstract</a></li>
      <li><a href="#framework-description-网络结构">Framework Description 网络结构</a>
        <ul>
          <li><a href="#1task-definition">1)Task Definition</a></li>
          <li><a href="#2framework-overview">2)Framework overview</a></li>
          <li><a href="#3hierarchical-encoder">3)Hierarchical encoder</a></li>
          <li><a href="#4static-attention-mechanism">4)static attention mechanism</a></li>
          <li><a href="#5encoding-layer">5)encoding layer</a></li>
          <li><a href="#6matching-layer">6)matching layer</a></li>
          <li><a href="#7nomalization-layer">7)nomalization layer</a></li>
          <li><a href="#8distractor-decoder">8)distractor decoder</a></li>
          <li><a href="#9question-based-initializer">9)question-based initializer</a></li>
          <li><a href="#10dynamic-hierarchical-attention-mechanism">10)dynamic hierarchical attention mechanism</a></li>
          <li><a href="#11training-and-inference">11)training and inference</a></li>
        </ul>
      </li>
      <li><a href="#experimental-setting-实验设置">experimental setting 实验设置</a>
        <ul>
          <li><a href="#1dataset">1)dataset</a></li>
          <li><a href="#2implementation-details-1">2)implementation details</a></li>
          <li><a href="#3baselines-and-ablations">3)baselines and ablations</a></li>
        </ul>
      </li>
      <li><a href="#results-and-analysis-结果与分析">results and analysis 结果与分析</a></li>
      <li><a href="#我的看法-1">我的看法</a></li>
    </ul>
  </li>
  <li><a href="#第四篇">第四篇</a>
    <ul>
      <li><a href="#title-3">Title</a></li>
      <li><a href="#author-3">Author</a></li>
      <li><a href="#abstract-3">Abstract</a></li>
      <li><a href="#proposed-framework-网络结构">Proposed Framework 网络结构</a>
        <ul>
          <li><a href="#1notations-and-task-definition">1)notations and task definition</a></li>
          <li><a href="#2model-overview">2)model overview</a></li>
          <li><a href="#3encoding-article-and-question">3)encoding article and question</a></li>
          <li><a href="#4co-attention-between-article-and-question">4)Co-attention between article and question</a></li>
          <li><a href="#5merging-sentence-representation">5)Merging sentence representation</a></li>
          <li><a href="#6question-initialization">6)question initialization</a></li>
          <li><a href="#7hierarchical-attention">7)hierarchical attention</a></li>
          <li><a href="#8semantic-similarity-loss">8)semantic similarity loss</a></li>
        </ul>
      </li>
      <li><a href="#experimental-settings">Experimental Settings</a>
        <ul>
          <li><a href="#1dataset-1">1)dataset</a></li>
          <li><a href="#2baselines-and-evaluation-metrics">2)baselines and evaluation metrics</a></li>
          <li><a href="#3implementation-details">3)implementation details</a></li>
        </ul>
      </li>
      <li><a href="#results-and-analysis-结果与分析">Results and Analysis 结果与分析</a></li>
      <li><a href="#我的看法-2">我的看法</a></li>
    </ul>
  </li>
  <li><a href="#补充">补充</a>
    <ul>
      <li><a href="#race数据集简介">RACE数据集简介</a></li>
      <li><a href="#bleu">BLEU</a></li>
      <li><a href="#rouge">ROUGE</a></li>
    </ul>
  </li>
</ul>

<h1 id="文献整理">文献整理</h1>

<h2 id="要求">要求</h2>

<p><img src="../assets/img/posts/20211130/requirements.jpg" /></p>

<h2 id="搜集到相关文献标题和地址">搜集到相关文献标题和地址</h2>
<ul>
  <li><a href="https://arxiv.org/pdf/2010.05384.pdf">A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies</a></li>
  <li><a href="https://arxiv.org/pdf/2010.09598.pdf">Better Distractions: Transformer-based Distractor Generation and Multiple Choice Question Filtering</a></li>
  <li><a href="https://ojs.aaai.org//index.php/AAAI/article/view/4606">Generating Distractors for Reading Comprehension Questions from Real Examinations</a></li>
  <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6522">Co-attention hierarchical network: Generating coherent long distractors for reading comprehension</a></li>
  <li><a href="https://aclanthology.org/2020.coling-main.189.pdf">Automatic Distractor Generation for Multiple Choice Questions in Standard Tests</a></li>
  <li><a href="https://aclanthology.org/W18-0533.pdf">Distractor Generation for Multiple Choice Questions Using Learning to Rank</a></li>
  <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/16559">Knowledge-Driven Distractor Generation for Cloze-style Multiple Choice Questions</a></li>
</ul>

<h1 id="第一篇">第一篇</h1>
<h2 id="title">Title</h2>
<p>A BERT-based Distractor Generation Scheme with Multi-tasking and
Negative Answer Training Strategies</p>
<h2 id="author">Author</h2>
<p>Ho-Lam Chung, Ying-Hong Chan, Yao-Chung Fan</p>
<h2 id="abstract">Abstract</h2>
<p>现有的DG<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>局限在只能生成一个误导选项，我们需要生成多个误导选项，文章中提到他们团队用multi-tasking和negative answer training技巧来生成多个误导选项，模型结果达到了学界顶尖。</p>

<h2 id="introduction">Introduction</h2>
<p>DG效果不好，文章提出了两个提升的空间：</p>
<ol>
  <li>DG质量提升：<br />
 BERT模型来提升误导选项质量</li>
  <li>多个误导选项生成：
 运用了覆盖的方法来选择distractor，而不是选择概率最高但是语义很相近的distractor</li>
</ol>

<h2 id="bert-distractor-generation">BERT distractor generation</h2>
<h3 id="1bert-based-distractor-generationbdg">1)BERT-based distractor generation(BDG)</h3>
<p>输入：段落P，答案A，问题Q，用C表示这三者concatenate后的结果。<br />
BDG模型是一个自回归模型，在预测阶段，每次输入C和上一次预测的词元，BDG迭代预测词元，直到预测出特殊词元[S]停止。下面这张图简单介绍了这个过程。</p>

<p><img src="../assets/img/posts/20211130/2.jpg" /></p>

<p>网络结构简单介绍：h<sub>[M]</sub>表示bert输出的隐藏状态，隐藏状态再输入到一个全连接层中用来预测词元。</p>

<p><img src="../assets/img/posts/20211130/3.jpg" /></p>

<h3 id="2multi-task-with-parallel-mlm">2)Multi-task with Parallel MLM</h3>
<p>MLM全称masked language model，遮蔽语言模型,通过并行BDG和P-MLM来训练模型让模型有更好的效果。</p>

<p><img src="../assets/img/posts/20211130/4.jpg" /></p>

<p>上图中左边的sequential MLM就是之前提到的BDG，BDG模型是一个词接一个词的预测，P-MLM是对所有的masked token进行预测，最后的损失函数是这两者相加<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>，公式如下：</p>

<p><img src="../assets/img/posts/20211130/5.jpg" /></p>

<p><img src="../assets/img/posts/20211130/6.jpg" /></p>

<p><img src="../assets/img/posts/20211130/7.jpg" /></p>

<p>作者如此设计的思路是：BDG可能会忽略整体语义语义信息，但是会过拟合单个词预测。那么并行一个P-MLM可以防止过拟合。</p>

<h3 id="3answer-negative-regularization">3)Answer Negative Regularization</h3>
<p>目前机器预测的distractor和answer有很高的相似度，下面一张表可以展示相似度。其中PM表示机器，Gold表示人工，作者将这类问题称为answer copying problem。</p>

<p><img src="../assets/img/posts/20211130/8.jpg" /></p>

<p>为了解决这个问题，作者提出了answer negative loss来让机器更多的选择与answer不同的词来表示新的distractor，公式如下：</p>

<p><img src="../assets/img/posts/20211130/9.jpg" /></p>

<p>可以看出BDG的loss替换成了AN的loss，每一项都减去了Answer negative loss。</p>

<h2 id="multiple-distractor-generation">Multiple Distractor Generation</h2>
<h3 id="1selecting-distractors-by-entropy-maximization">1)Selecting Distractors by Entropy Maximization</h3>
<p>选择语义不同的distractor set。文章借鉴了MRC<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>的方法，让BDGmodel生成很多distractor组成 $\hat{D}$ = {$\hat{d}$<sub>1</sub>, $\hat{d}$<sub>2</sub>, $\hat{d}$<sub>3</sub>…}，然后找出最好的一组选项，一般情况下由三个误导选项和一个答案组成。选择的一句是最大化下面这个公式：</p>

<p><img src="../assets/img/posts/20211130/10.jpg" /></p>

<h3 id="2bdg-em">2)BDG-EM</h3>
<p>我们可以通过不同的BDG模型来生成不同的误导选项最后组合，不同的模型区别是有没有answer negative/multi-task training，比如我们有这几个模型:$\hat{D}$,$\hat{D}$<sub>PM</sub>,$\hat{D}$<sub>PM+AN</sub>，它们分别代表含PM<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>和含AN<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup></p>

<p><img src="../assets/img/posts/20211130/11.jpg" /></p>

<h2 id="performance-evaluation">Performance Evaluation</h2>
<h3 id="1datasets">1)datasets</h3>
<p>RACE,沿用了<a href="https://ojs.aaai.org//index.php/AAAI/article/view/4606">Gao</a>那篇论文的处理,后面也会梳理那篇论文</p>

<p><img src="../assets/img/posts/20211130/12.jpg" /></p>

<h3 id="2implementation-details">2)implementation details</h3>
<ul>
  <li>tokenizer: wordpiece tokenizer</li>
  <li>framewordk:huggingface trainsformers</li>
  <li>optimizer:adamW(lr:5e-5)</li>
  <li>github_url: <a href="https://github.com/voidful/BDG">BDG</a></li>
</ul>

<h3 id="3compared-methods">3)compared methods</h3>
<p>比较了不同的distractor generation</p>
<ul>
  <li>CO-Att：出自<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6522">Zhou</a></li>
  <li>DS-Att: 出自<a href="https://ojs.aaai.org//index.php/AAAI/article/view/4606">Gao</a></li>
  <li>GPT:baseline</li>
  <li>BDG: 没有应用P-MLM和Answer negative</li>
  <li>BDG<sub>PM</sub></li>
  <li>BDG<sub>AN+PM</sub></li>
</ul>

<h3 id="4token-score-comparison">4)token score comparison</h3>
<p>BLEU和ROUGE(L)两种判断指标</p>

<p><img src="../assets/img/posts/20211130/13.jpg" /></p>

<p>copying problem的效果</p>

<p><img src="../assets/img/posts/20211130/14.jpg" /></p>

<h3 id="5mcq-model-accuracy-comparison">5)MCQ Model Accuracy Comparison</h3>
<p>与回答系统相结合，将生成好的选项（一个正确答案三个误导选项）放入MCQ answering model，下面是回答正确率的表格</p>

<p><img src="../assets/img/posts/20211130/15.jpg" /></p>

<p>可以看出作者的模型选项的误导性还是很高的。</p>

<h3 id="6parameter-study-on-γ">6）Parameter Study on γ</h3>
<p>之前使用P-MLM并行训练时候有个权重参数γ，下表显示了不同γ值的影响，对于只有PM的模型来说，γ=6，对于既有AN和PM来说，γ=7</p>

<p><img src="../assets/img/posts/20211130/16.jpg" /></p>

<h2 id="conclusion">Conclusion</h2>
<p>现存的DG可以分为cloze-style distractor generation和 reading comprehension distractor generation，前者主要是word filling，后者主要看重语义信息，基于两者的设计出了很多模型，目前来看还是考虑语义信息生成的误导选项更好。</p>

<p><img src="../assets/img/posts/20211130/17.jpg" /></p>

<h2 id="我的看法">我的看法</h2>
<p>文章中的模型提到了三种技术，第一是bert预训练模型使用。第二是P-MLM的并行使用， 它的使用让模型可以考虑段落的语义信息，那么生成的误导选项是sentence-level而不是之前模型所使用的类似word-filling这种word-level。第三是Answer negative loss的使用，它的使用相当于让模型不要考虑与正确答案语义很接近的误导选项，因为目前大多数DG生成多个选项时语义与正确答案都非常接近，这与实际情况不符，同时也起不到误导的作用。  <br />
同时文章提出了生成多个误导选项时使用不同模型生成的误导选项拼在一起作为选项是一种比较好的解决方法，让一次性生成多个误导选型有了一定的可用性。<br />
文章的代码开源，可以去<a href="https://github.com/voidful/BDG">github</a>上看训练细节和网络结构细节。</p>

<h1 id="第二篇">第二篇</h1>
<h2 id="title-1">Title</h2>
<p>Better Distractions: Transformer-based Distractor Generation and Multiple Choice Question Filtering</p>
<h2 id="author-1">Author</h2>
<p>Jeroen Offerijns, Suzan Verberne, Tessa Verhoef</p>
<h2 id="abstract-1">Abstract</h2>
<p>运用GPT2模型生成三个误导选项，同时用BERT模型去回答这个问题，只挑选出回答正确的问题。相当于使用了QA作为一个过滤器(QA filtering)。</p>
<h2 id="method">Method</h2>
<p>作者使用了Question generation model, distractor generation model和question answer filter，作者将从这三方面介绍，下图是大概的流程图。</p>

<p><img src="../assets/img/posts/20211130/18.jpg" /></p>

<h3 id="1question-generation">1)question generation</h3>
<ul>
  <li>预训练模型：GPT-2</li>
  <li>数据集：English SQuAD</li>
  <li>tokenizer：Byte-Pair-Encoding(BPE) tokenizer</li>
  <li>optimizer:Adam</li>
  <li>下图展示了QG的输入，黑框内被tokenizer标记为特殊词元</li>
</ul>

<p><img src="../assets/img/posts/20211130/19.jpg" /></p>

<h3 id="2distractor-generation">2)distractor generation</h3>
<ul>
  <li>预训练模型：GPT-2</li>
  <li>数据集：RACE</li>
  <li>tokenizer:BPE<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup></li>
  <li>使用了repetition penalty技术，保证了尽量不会生成相似的text，并且过滤到那些不好的生成（比如生成了空字符串）</li>
  <li>输入：经典的C(context)，A(answer),Q(question)，下图展示了输入格式</li>
</ul>

<p><img src="../assets/img/posts/20211130/20.jpg" /></p>

<h3 id="3qa-filtering">3)QA filtering</h3>
<ul>
  <li>预训练模型：DistilBERT</li>
  <li>网络结构：CQA<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>输入到distilbert，再连接一个dropout，全连接层和softmax，最后输出一个答案，具体结构如下图</li>
</ul>

<p><img src="../assets/img/posts/20211130/21.jpg" /></p>

<h2 id="results">Results</h2>
<h3 id="1quantitative-evaluation">1)quantitative evaluation</h3>
<p>下表中展示了和上一篇论文类似的指标,与现有的模型进行了比较：SEQ2SEQ,HSA<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup>和CHN<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>。可以看出BLEU明显要比之前模型要好，但是ROUGE没有之前的高。</p>

<p><img src="../assets/img/posts/20211130/22.jpg" /></p>

<h3 id="2question-answering-ability">2)question answering ability</h3>
<p>用GPT-2模型生成误导选项再输入到QAmodel中，具体结果见下图。</p>

<p><img src="../assets/img/posts/20211130/23.jpg" /></p>

<h3 id="3human-evaluation">3)human evaluation</h3>
<p>人工评估，从两方面评估distractor生成的好坏：</p>
<ul>
  <li><strong>Is the question well-formed and can you understand the meaning?</strong></li>
  <li><strong>If the question is at least understandable, does the answer make sense in relation to the question?</strong>
评估过程中，使用了155个没有经过QA筛选和155经过QA筛选的，了解一下QA过滤模型的效果。整体来说QA过滤器还是有一点效果，具体结果如下：</li>
</ul>

<p><img src="../assets/img/posts/20211130/24.jpg" /></p>

<h2 id="conclusion-1">conclusion</h2>
<p>我认为作者使用的DG模型主要有两大特色，一个是使用了GPT2预训练模型，目前使用基于transformer的模型已经成为主流。第二个是使用了QA过滤器来筛选掉回答错误的，有一定提升但不显著。</p>

<h1 id="第三篇">第三篇</h1>
<h2 id="title-2">Title</h2>
<p>Generating Distractors for Reading Comprehension Questions from Real Examinations</p>
<h2 id="author-2">Author</h2>
<p>Yifan Gao, Lidong Bing, Piji Li,
Irwin King, Michael R. Lyu</p>
<h2 id="abstract-2">Abstract</h2>
<p>上面两篇文献都有提到这篇文章。作者使用了<strong>Hierarchical encoder-decoder framework</strong> with <strong>static</strong> and <strong>dynamic</strong> attention mechanisms来生成有语义信息的误导选项。使用了编码器-解码器结构网络和静态和动态注意力机制。</p>
<h2 id="framework-description-网络结构">Framework Description 网络结构</h2>
<h3 id="1task-definition">1)Task Definition</h3>
<p>输入：文章，问题和答案。P代表文章，s<sub>1</sub>,s<sub>2</sub>,s<sub>3</sub>…表示不同的句子，q和a分别表示问题和答案，那么我们的任务是生成误导选项$\overline{d}$。</p>

<p><img src="../assets/img/posts/20211130/25.jpg" /></p>

<h3 id="2framework-overview">2)Framework overview</h3>
<p>网络结构如下图所示，下面将从各个组成部分分别介绍：</p>

<p><img src="../assets/img/posts/20211130/26.jpg" /></p>

<h3 id="3hierarchical-encoder">3)Hierarchical encoder</h3>
<ul>
  <li><strong>word embedding</strong>:词嵌入，将每个句子s<sub>i</sub>中的每个词元变成词向量(w<sub>i,1</sub>,w<sub>i,2</sub>,w<sub>i,3</sub>…)</li>
  <li><strong>word encoder</strong>:将句子s<sub>i</sub>的词向量(w<sub>i,1</sub>,w<sub>i,2</sub>,w<sub>i,3</sub>…)作为输入，用<strong>双向LSTM</strong>作为编码器，获得word-level representation h<sub>i,j</sub><sup>e</sup></li>
</ul>

<p><img src="../assets/img/posts/20211130/27.jpg" /></p>

<ul>
  <li><strong>sentence encoder</strong>:将word encoder中每个句子正向LSTM的最后一个隐藏状态和反向LSTM的最开始的隐藏状态作为输入到另一个双向LSTM中获得<strong>sentence-level representation</strong>(u<sub>1</sub>,u<sub>2</sub>,u<sub>3</sub>…)</li>
</ul>

<h3 id="4static-attention-mechanism">4)static attention mechanism</h3>
<p>目的：生成的误导选项必须和问题Q语义相关，但是和答案A必须语义不相关。我们从(s<sub>1</sub>,s<sub>2</sub>,s<sub>3</sub>…)学习到句子的权重分布(γ<sub>1</sub>,γ<sub>2</sub>,γ<sub>3</sub>…)，然后将问题q和答案a作为query。</p>

<h3 id="5encoding-layer">5)encoding layer</h3>
<p>我们希望把问题q，答案a和句子s都变成一样的长度的向量表示，也就是上图中紫色虚线部分。对于q和a，我们用两个独立的双向LSTM来获得(<strong>a</strong><sub>1</sub>,<strong>a</strong><sub>2</sub>…<strong>a</strong><sub>k</sub>)和(<strong>q</strong><sub>1</sub>,<strong>q</strong><sub>2</sub>…<strong>q</strong><sub>l</sub>)，然后用平均池化层平均一下：</p>

<p><img src="../assets/img/posts/20211130/28.jpg" /></p>

<p>对于句子s，我们不用u而用h：</p>

<p><img src="../assets/img/posts/20211130/29.jpg" /></p>

<h3 id="6matching-layer">6)matching layer</h3>
<p>目的：加重与问题q有关的句子，减轻与答案a有关的句子。o<sub>i</sub>表示不同句子的importance score</p>

<p><img src="../assets/img/posts/20211130/30.jpg" /></p>

<h3 id="7nomalization-layer">7)nomalization layer</h3>
<p>目的：有些问题q和一两个句子有关，而有些问题q和很多句子有关，比如summarizing，下面的τ(temperature)就是这个作用</p>

<p><img src="../assets/img/posts/20211130/31.jpg" /></p>

<p><img src="../assets/img/posts/20211130/32.jpg" /></p>

<p>作者介绍static attention mechanism用了很大篇幅</p>

<h3 id="8distractor-decoder">8)distractor decoder</h3>
<p>解码器使用的也是LSTM，但是并没有使用编码器的最后一个隐藏状态作为初始状态，而是定义了一个
<strong>question-based initializer</strong>来让生成的误导选项语法和问题q一致</p>

<h3 id="9question-based-initializer">9)question-based initializer</h3>
<p>定义了一个question LSTM来编码问题q，使用最后一层的cell state和hidden state作为decoder初始状态，同时输入q<sub>last</sub>，表示问题q的最后一个词元。</p>

<h3 id="10dynamic-hierarchical-attention-mechanism">10)dynamic hierarchical attention mechanism</h3>
<p>常规的注意力机制将一篇文章作为长句子，然后decoder的每一个时间步都与encoder中所有的hidden state进行比较，但是这种方法并不适合目前的模型。原因：首先LSTM不能处理这么长的输入，其次，一些问题只与部分句子有关。<br />
目的：每个decoder时间步只关注<strong>重要句子</strong>，作者将这种注意力机制称为动态注意力机制，因为不同的时间步，word-level和sentence-level 注意力分布都不同。<br />
每一个时间步的输入是词元d<sub>t-1</sub>和上一个隐藏状态h<sub>t-1</sub></p>

<p><img src="../assets/img/posts/20211130/33.jpg" /></p>

<p><img src="../assets/img/posts/20211130/34.jpg" /></p>

<p>α和β分别表示word-level,sentence-level权重，最后使用之前静态注意力机制获得的γ来调节α和β</p>

<p><img src="../assets/img/posts/20211130/35.jpg" /></p>

<p><img src="../assets/img/posts/20211130/36.jpg" /></p>

<p>获得上下文变量<strong>c</strong><sub>t</sub></p>

<p><img src="../assets/img/posts/20211130/37.jpg" /></p>

<p>获得attention vector $\tilde{h}$</p>

<p><img src="../assets/img/posts/20211130/38.jpg" /></p>

<h3 id="11training-and-inference">11)training and inference</h3>
<p>损失函数：</p>

<p><img src="../assets/img/posts/20211130/39.jpg" /></p>

<p>生成多个误导选项的方法是束搜索，但是生成的误导选项很相似，作者做了相应的处理方法，但我觉得效果还是很差</p>

<h2 id="experimental-setting-实验设置">experimental setting 实验设置</h2>
<h3 id="1dataset">1)dataset</h3>
<p>RACE数据集，作者做了相应的处理，去掉了很多不合理的和语义不相关的，作者的处理标准是：对于误导选项中的词元，如果它们在文章中出现的次数小于5次，那么将被保留，同时去掉了那些需要在句子中间和句子开始填空的问题。下表展示了处理后的数据集的一些信息：</p>

<p><img src="../assets/img/posts/20211130/40.jpg" /></p>

<h3 id="2implementation-details-1">2)implementation details</h3>
<p>词表：保留了频率最高的50k个词元，同时使用GloVe作为词嵌入预训练模型。其他的细节都可以在文章中看见，这里不一一列出了，主要是超参数的设置。</p>

<h3 id="3baselines-and-ablations">3)baselines and ablations</h3>
<p>与HRED<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>和seq2seq比较</p>

<h2 id="results-and-analysis-结果与分析">results and analysis 结果与分析</h2>

<p><img src="../assets/img/posts/20211130/41.jpg" /></p>

<p>人工评估：</p>

<p><img src="../assets/img/posts/20211130/42.jpg" /></p>

<p>大致过程是这样：四个误导选项，分别来自seq2seq，HRED，作者的模型和原本的误导选项，让英语能力很好的人来选择最适合的选项，得出的结果可以发现，作者的模型生成的误导选项拥有最好的误导效果。</p>

<p>下图直观展示了static attention distribution：</p>

<p><img src="../assets/img/posts/20211130/43.jpg" /></p>

<h2 id="我的看法-1">我的看法</h2>
<p>这篇文章应该是第一个提出用处理后的RACE数据集来处理MCQ问题，处理后的RACE数据集在后面也有很多文献用到，这篇文章使用了seq2seq网络结构同时使用了静态和动态注意力机制，对于网络结构和注意力机制的解释非常完全和详细，虽然这篇文章的效果放到现在来看可能不是最好了，但是它提出来的评估标准可能会成为一个通用的标准。它的数据集和训练代码在<a href="https://github.com/Yifan-Gao/Distractor-Generation-RACE">github</a>上也完全开源。</p>

<h1 id="第四篇">第四篇</h1>
<h2 id="title-3">Title</h2>
<p>Co-attention hierarchical network: Generating coherent long distractors for reading comprehension</p>
<h2 id="author-3">Author</h2>
<p>Xiaorui Zhou, Senlin Luo, Yunfang Wu</p>
<h2 id="abstract-3">Abstract</h2>
<p>这篇文献是针对上一篇Gao的文章(seq2seq)所作的改进。本篇文章提出了Gao的模型的两个问题：1.没有建立文章和问题的关系，他的解决方法是使用<strong>co-attention enhanced hierarchical architecture</strong>来捕获文章和问题之间的关系，让解码器生成更有关联的误导选项。2.没有加重整篇文章和误导选项的关系。作者的解决思路是添加一个额外的语义相关性损失函数，让生成的误导选项与整篇文章更有关联。</p>
<h2 id="proposed-framework-网络结构">Proposed Framework 网络结构</h2>
<h3 id="1notations-and-task-definition">1)notations and task definition</h3>
<p>article T=(s<sub>1</sub>,s<sub>2</sub>…s<sub>k</sub>)，一篇文章有k个句子s，同时每个句子都有不同的长度l，s<sub>i</sub>=(w<sub>i,1</sub>,w<sub>i,2</sub>…w<sub>i,l</sub>)，每个文章有m个问题和z个误导选项，Q=(q<sub>1</sub>,q<sub>2</sub>…q<sub>m</sub>),D=(d<sub>1</sub>,d<sub>2</sub>…d<sub>z</sub>),我们的任务是根据输入的T和Q生成D</p>

<p><img src="../assets/img/posts/20211130/44.jpg" /></p>

<h3 id="2model-overview">2)model overview</h3>
<p>整体结构如下图所示，下面将从各个部分分别介绍：</p>

<p><img src="../assets/img/posts/20211130/45.jpg" /></p>

<h3 id="3encoding-article-and-question">3)encoding article and question</h3>
<p>文章和问题的编码器结构</p>
<ul>
  <li><strong>hierarchical article encoder</strong>
双向LSTM，和上一篇结构很像，很多部分我就简单列个式子。</li>
</ul>

<p><img src="../assets/img/posts/20211130/46.jpg" /></p>

<p>每一句最后的词元来表示整个句子</p>

<p><img src="../assets/img/posts/20211130/47.jpg" /></p>

<p>sentence-level encoder：</p>

<p><img src="../assets/img/posts/20211130/48.jpg" /></p>

<p>同样，用最后一个句子来表示整篇文章</p>

<p><img src="../assets/img/posts/20211130/49.jpg" /></p>

<p>用<strong>H</strong><sup>*</sup>来作为sentence-level representation of article,我们有<strong>H</strong><sub>:t</sub><sup>*</sup>=h<sub>t</sub><sup>s</sup></p>

<p>这样，通过使用两个双向LSTM获得word-level encoding和sentence-level encoding</p>
<ul>
  <li><strong>question encoder</strong></li>
</ul>

<p><img src="../assets/img/posts/20211130/50.jpg" /></p>

<p>用<strong>U</strong><sup>*</sup>来作为word-level representations of question, 我们有<strong>U</strong><sub>:t</sub><sup>*</sup>=h<sub>t</sub><sup>q</sup></p>

<h3 id="4co-attention-between-article-and-question">4)Co-attention between article and question</h3>
<p>Co-attention mechanism就是使用了两个方向的注意力机制，有从article到question的，也有question到article的。<br />
用一个“相似”矩阵S表示H和U的关系：</p>

<p><img src="../assets/img/posts/20211130/51.jpg" /></p>

<p>S<sub>i,j</sub>就表示第i个句子和第j个问题词元的相似性</p>

<p>我们可以获得两个特殊的矩阵<strong>S</strong><sup><strong>Q</strong></sup>和<strong>S</strong><sup><strong>T</strong></sup></p>

<p><img src="../assets/img/posts/20211130/52.jpg" /></p>

<ul>
  <li>article-to-question attention<br />
$\tilde{U}$<sub>:j</sub> = $\sum$ S<sub>i,j</sub><sup>Q</sup>U<sub>:,i</sub></li>
  <li>question-to-article attention</li>
</ul>

<p><img src="../assets/img/posts/20211130/53.jpg" /></p>

<p>最后，将问题的词级表示H，两个方向的注意力结果$\tilde{U}$和$\tilde{H}$结合一下获得G</p>

<p><img src="../assets/img/posts/20211130/54.jpg" /></p>

<h3 id="5merging-sentence-representation">5)Merging sentence representation</h3>

<p><img src="../assets/img/posts/20211130/55.jpg" /></p>

<p>Z表示final representation of sentence-level hidden states</p>

<h3 id="6question-initialization">6)question initialization</h3>
<p>接下来就进入decoder环节，这里的question initialization和上篇文献处理方法相同</p>

<h3 id="7hierarchical-attention">7)hierarchical attention</h3>
<p>不同时间步有不同的句子相关，和上篇文献的处理方法动态注意力机制相同。</p>

<p><img src="../assets/img/posts/20211130/56.jpg" /></p>

<p><img src="../assets/img/posts/20211130/57.jpg" /></p>

<p><img src="../assets/img/posts/20211130/58.jpg" /></p>

<p><img src="../assets/img/posts/20211130/59.jpg" /></p>

<h3 id="8semantic-similarity-loss">8)semantic similarity loss</h3>
<p>目的：获得文章和误导选项的关系。还记得之前定义的e<sub>T</sub>吗，它表示整篇文章，那么我们通过下面的公式可以获得distractor representation:</p>

<p><img src="../assets/img/posts/20211130/60.jpg" /></p>

<p>其中S<sub>M</sub>是decoder最后一个隐藏状态，那么我们通过cos计算相似关系，那么最终的损失函数</p>

<p><img src="../assets/img/posts/20211130/61.jpg" /></p>

<h2 id="experimental-settings">Experimental Settings</h2>
<h3 id="1dataset-1">1)dataset</h3>
<p>使用了上篇文献处理过的RACE数据集。</p>

<h3 id="2baselines-and-evaluation-metrics">2)baselines and evaluation metrics</h3>
<p>与seq2seq，HRED，HCP<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">11</a></sup>，HSA<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">12</a></sup>比较。</p>

<h3 id="3implementation-details">3)implementation details</h3>
<p>网络超参数设置技巧，不展开了</p>

<h2 id="results-and-analysis-结果与分析-1">Results and Analysis 结果与分析</h2>

<p><img src="../assets/img/posts/20211130/62.jpg" /></p>

<p><img src="../assets/img/posts/20211130/63.jpg" /></p>

<p><img src="../assets/img/posts/20211130/64.jpg" /></p>

<p>介绍一下上面这张表，这张表是人工评估的结果，从三个维度分析，分别是fluency,coherence,distracting ability。可以看出作者的模型并不是在所有维度都是最好的。</p>

<p>下图是案例分析：</p>

<p><img src="../assets/img/posts/20211130/65.jpg" /></p>

<h2 id="我的看法-2">我的看法</h2>
<p>这篇文献是基于上一篇文献的方法进行了两个改进：1.关联了整篇文章和问题，解决方法是使用了Co-attention mechanism。2.让distractor和article语义相关，方法是定义了相关性loss。</p>

<h1 id="补充">补充</h1>
<h2 id="race数据集简介">RACE数据集简介</h2>
<p>RACE数据集是一个来源于中学考试题目的大规模阅读理解数据集，包含了大约 28000 个文章以及近 100000 个问题。它的形式类似于英语考试中的阅读理解（选择题），给定一篇文章，通过阅读并理解文章（Passage），针对提出的问题（Question）从四个选项中选择正确的答案（Answers）。</p>
<h2 id="bleu">BLEU</h2>
<p>BLEU是一个评价指标，最开始用于机器翻译任务，定义如下</p>

<p><img src="../assets/img/posts/20211130/66.jpg" /></p>

<p>它的总体思想就是准确率，假如给定标准译文reference，神经网络生成的句子是candidate，句子长度为n，candidate中有m个单词出现在reference，m/n就是bleu的1-gram的计算公式。BLEU还有许多变种。根据n-gram可以划分成多种评价指标，常见的指标有BLEU-1、BLEU-2、BLEU-3、BLEU-4四种，其中n-gram指的是连续的单词个数为n。</p>

<h2 id="rouge">ROUGE</h2>
<p>Rouge(Recall-Oriented Understudy for Gisting Evaluation)，是评估自动文摘以及机器翻译的一组指标。它通过将自动生成的摘要或翻译与一组参考摘要（通常是人工生成的）进行比较计算，得出相应的分值，以衡量自动生成的摘要或翻译与参考摘要之间的“相似度”。它的定义如下：</p>

<p><img src="../assets/img/posts/20211130/67.jpg" /></p>

<p>文献中使用的ROUGE-L是一种变种，L即是LCS(longest common subsequence，最长公共子序列)的首字母，因为Rouge-L使用了最长公共子序列。Rouge-L计算方式如下图：</p>

<p><img src="../assets/img/posts/20211130/68.jpg" /></p>

<p>其中LCS(X, Y)是X和Y的最长公共子序列的长度,m、n分别表示参考摘要和自动摘要的长度（一般就是所含词的个数），R<sub>lcs</sub>,P<sub>lcs</sub>分别表示召回率和准确率。最后的F<sub>lcs</sub>即是我们所说的Rouge-L。</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>distractor generation 误导选项生成，简称DG <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>当我们test时，只需要Sequential MLM decoder来预测。 <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>multi-choice reading comprehension (MRC) model <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>P-MLM <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Answer negative <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Byte-Pair-Encoding <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>context，question，answer <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>hierarchical encoder-decoder model with static attention <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>hierarchical model enhanced with co-attention <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p>hierarchical encoder-decoder <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p>相当于HRED+copy,是基于HRED的网络结构 <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:12" role="doc-endnote">
      <p>就是上篇文献的网络 <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Quehry</name></author><category term="paper" /><summary type="html"><![CDATA[文献整理]]></summary></entry><entry><title type="html">软件方法</title><link href="http://localhost:4000/%E8%BD%AF%E4%BB%B6%E6%96%B9%E6%B3%95.html" rel="alternate" type="text/html" title="软件方法" /><published>2021-11-30T00:00:00+08:00</published><updated>2021-11-30T00:00:00+08:00</updated><id>http://localhost:4000/%E8%BD%AF%E4%BB%B6%E6%96%B9%E6%B3%95</id><content type="html" xml:base="http://localhost:4000/%E8%BD%AF%E4%BB%B6%E6%96%B9%E6%B3%95.html"><![CDATA[<h1 id="目录">目录</h1>
<!-- TOC -->

<ul>
  <li><a href="#目录">目录</a></li>
  <li><a href="#软件方法">软件方法</a>
    <ul>
      <li><a href="#课程要求">课程要求</a>
        <ul>
          <li><a href="#随记">随记</a></li>
        </ul>
      </li>
      <li><a href="#ppt整理">PPT整理</a>
        <ul>
          <li><a href="#1-对象类">1. 对象，类</a></li>
          <li><a href="#2面向对象">2.面向对象</a></li>
          <li><a href="#3java">3.JAVA</a></li>
          <li><a href="#4数据结构">4.数据结构</a></li>
          <li><a href="#5-常用数据结构方法">5. 常用数据结构方法</a></li>
        </ul>
      </li>
      <li><a href="#tp整理">TP整理</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->
<h1 id="软件方法">软件方法</h1>
<h2 id="课程要求">课程要求</h2>
<p>学习面向对象这种软件开发方法（目前概念越来越广），通过java来了解面向对象的编程具体怎么实现。</p>

<h3 id="随记">随记</h3>
<ol>
  <li>类，对象：
    <ul>
      <li>对象是类的一个实例</li>
      <li>c语言可以构建面向对象所有的结构</li>
      <li>类集合了属性和方法</li>
    </ul>
  </li>
  <li>面向对象的三大特征：
    <ul>
      <li>封装（encapsulation）:
        <ul>
          <li>private, protected, public</li>
          <li>可作用于属性和方法，一般构造方法和成员方法都是public, 属性都是private</li>
          <li>一般是隐藏对象的属性和实现细节，但是提供方法的接口</li>
          <li>提供公开的方法</li>
          <li>提高了软件开发的效率</li>
        </ul>

        <center><img src="../assets/img/posts/20211130/1.jpg" /></center>
      </li>
      <li>继承（inheritance）：
        <ul>
          <li>子类与父类</li>
          <li>子类自动具有父类属性和方法，添加自己特有的属性和方法，并且子类使用父类的方法也可以覆盖/重写父类方法</li>
          <li>可以实现代码的复用（当然功能不止于此）</li>
        </ul>
      </li>
      <li>多态（polymorphism）：
        <ul>
          <li>父类有多个子类</li>
          <li>子类覆盖/重写父类方法</li>
          <li>相当于是根据实际创建的对象类型动态决定使用哪个方法</li>
          <li>所有的子类都可以看成父类的类型，运行时，系统会自动调用各种子类的方法</li>
          <li>UML可以画出类之间的关系</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>java程序设计
    <ul>
      <li>百分百面向对象
        <ul>
          <li>不存在类以外代码</li>
          <li>只能采用面向对象方法编程</li>
          <li>java文件命名规范
            <ul>
              <li>必须以.java结尾</li>
              <li>源文件中如果只有一个类，文件类必须与该类名相同</li>
              <li>如果有多个类，且没有public类，文件名可与任一类名相同</li>
              <li>有多个类，且有public类，文件名必须与该类名相同</li>
              <li>一个JAVA源文件只能有一个public类，一个文件中只能有一个main主函数</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>静态方法/static，可以直接用类和函数名直接调用，和普通方法的区别是不用new一个示例
        <ul>
          <li>static 方法可以直接调用，abstract方法存在的类肯定是抽象类</li>
          <li>抽象方法不定义具体内容</li>
        </ul>
      </li>
      <li>多态的实现，先定义抽象的（abstract）父类，然后子类继承父类然后定义父类的抽象方法
        <ul>
          <li>通过抽象方法固定通用接口</li>
          <li>子类通过强制实现抽象方法实现多态</li>
          <li>抽象父类可以定义属性和构造函数</li>
          <li>抽象父类不能实例化，只能通过向上转型的方法定义</li>
          <li>抽象父类可以向下转型成子类</li>
          <li>父类的方法一般是抽象方法，不定义具体内容，留给子类定义，父类出现的抽象方法子类必须全部定义</li>
          <li>多态的主要特点就是父类的方法全部是抽象的</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>多态例子</li>
</ol>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">public</span> <span class="kd">class</span> <span class="nc">Test</span> <span class="o">{</span>
    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="nc">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">show</span><span class="o">(</span><span class="k">new</span> <span class="nc">Cat</span><span class="o">());</span>  <span class="c1">// 以 Cat 对象调用 show 方法</span>
      <span class="n">show</span><span class="o">(</span><span class="k">new</span> <span class="nc">Dog</span><span class="o">());</span>  <span class="c1">// 以 Dog 对象调用 show 方法</span>
                
      <span class="nc">Animal</span> <span class="n">a</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Cat</span><span class="o">();</span>  <span class="c1">// 向上转型  </span>
      <span class="n">a</span><span class="o">.</span><span class="na">eat</span><span class="o">();</span>               <span class="c1">// 调用的是 Cat 的 eat</span>
      <span class="nc">Cat</span> <span class="n">c</span> <span class="o">=</span> <span class="o">(</span><span class="nc">Cat</span><span class="o">)</span><span class="n">a</span><span class="o">;</span>        <span class="c1">// 向下转型  </span>
      <span class="n">c</span><span class="o">.</span><span class="na">work</span><span class="o">();</span>        <span class="c1">// 调用的是 Cat 的 work</span>
  <span class="o">}</span>  
            
    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">show</span><span class="o">(</span><span class="nc">Animal</span> <span class="n">a</span><span class="o">)</span>  <span class="o">{</span>
        <span class="n">a</span><span class="o">.</span><span class="na">eat</span><span class="o">();</span>  
        <span class="c1">// 类型判断</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">a</span> <span class="k">instanceof</span> <span class="nc">Cat</span><span class="o">)</span>  <span class="o">{</span>  <span class="c1">// 猫做的事情 </span>
            <span class="nc">Cat</span> <span class="n">c</span> <span class="o">=</span> <span class="o">(</span><span class="nc">Cat</span><span class="o">)</span><span class="n">a</span><span class="o">;</span> <span class="c1">// 向下转型</span>
            <span class="n">c</span><span class="o">.</span><span class="na">work</span><span class="o">();</span>  
        <span class="o">}</span> 
        <span class="k">else</span> <span class="nf">if</span> <span class="o">(</span><span class="n">a</span> <span class="k">instanceof</span> <span class="nc">Dog</span><span class="o">)</span> <span class="o">{</span> <span class="c1">// 狗做的事情 </span>
            <span class="nc">Dog</span> <span class="n">c</span> <span class="o">=</span> <span class="o">(</span><span class="nc">Dog</span><span class="o">)</span><span class="n">a</span><span class="o">;</span>  
            <span class="n">c</span><span class="o">.</span><span class="na">work</span><span class="o">();</span>  
        <span class="o">}</span>  
    <span class="o">}</span>  
<span class="o">}</span>
 
<span class="kd">abstract</span> <span class="kd">class</span> <span class="nc">Animal</span> <span class="o">{</span>  
    <span class="kd">abstract</span> <span class="kt">void</span> <span class="nf">eat</span><span class="o">();</span>  
<span class="o">}</span>  
  
<span class="kd">class</span> <span class="nc">Cat</span> <span class="kd">extends</span> <span class="nc">Animal</span> <span class="o">{</span>  
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">eat</span><span class="o">()</span> <span class="o">{</span>  
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"吃鱼"</span><span class="o">);</span>  
    <span class="o">}</span>  
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">work</span><span class="o">()</span> <span class="o">{</span>  
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"抓老鼠"</span><span class="o">);</span>  
    <span class="o">}</span>  
<span class="o">}</span>  
  
<span class="kd">class</span> <span class="nc">Dog</span> <span class="kd">extends</span> <span class="nc">Animal</span> <span class="o">{</span>  
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">eat</span><span class="o">()</span> <span class="o">{</span>  
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"吃骨头"</span><span class="o">);</span>  
    <span class="o">}</span>  
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">work</span><span class="o">()</span> <span class="o">{</span>  
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"看家"</span><span class="o">);</span>  
    <span class="o">}</span>  
<span class="o">}</span>
</code></pre></div></div>

<h2 id="ppt整理">PPT整理</h2>

<h3 id="1-对象类">1. 对象，类</h3>
<ul>
  <li>使用对象之前要先声明和创建</li>
  <li>类定义了对象的类型，所有对象都是类的实例，所有的类描述了属性和定义了方法</li>
</ul>

<h3 id="2面向对象">2.面向对象</h3>
<ul>
  <li>面向对象的编程有4个特点</li>
  <li><strong>封装</strong>：保护类的属性和方法, 类里面的属性的数据是private的, public的方法定义了对象的接口。权限修饰符: private, default, protected, public。</li>
  <li><strong>继承</strong>：B继承A，重用，修改，添加，A所有的属性都存在于B中，A的方法可以在B中重新定义，B方法的改动不会影响A</li>
  <li><strong>多态</strong>：一个对象属于多个类，通过使用不同类中的方法属于不同的类，父类是抽象类，各个子类继承父类并定义方法，调用的时候根据不同子类调用方法。判断类型是否相同instanceof，声明的时候可以这么声明: A a = new B(),其中B是A的子类, 这种声明方法叫做向上转型。向下转型: B b = (B) a</li>
  <li><strong>动态链接</strong>: 通过PPT上的例子，我感觉和继承很像，这部分需要更深入了解才能明白。</li>
</ul>

<h3 id="3java">3.JAVA</h3>
<ul>
  <li>main的格式:</li>
</ul>

<center><img src="../assets/img/posts/20211130/71.jpg" /></center>

<ul>
  <li>数据类型: int, float, double, char, string, boolean, byte, long, short, JAVA里面也有这些数据类型的类：</li>
</ul>

<center><img src="../assets/img/posts/20211130/72.jpg" /></center>

<p>其中Characte应该为Character</p>

<ul>
  <li>
    <p>JAVA关键字this, super</p>
  </li>
  <li>
    <p>x = bool ? a : b，表示如果bool为true，执行a，如果为false执行b</p>
  </li>
  <li>
    <p>for(Point p : this.getVect())表示遍历</p>
  </li>
  <li>
    <p>exception 异常:</p>
  </li>
</ul>

<p><img src="../assets/img/posts/20211130/69.jpg" /></p>

<p>还有异常的抛出throws</p>

<p>try-catch-finally</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">try</span> 
<span class="o">{</span>  
	    <span class="c1">// 可能会发生异常的程序代码  </span>
<span class="o">}</span> 
<span class="k">catch</span> <span class="o">(</span><span class="nc">Type1</span> <span class="n">id1</span><span class="o">)</span>
<span class="o">{</span>  
	    <span class="c1">// 捕获并处置try抛出的异常类型Type1  </span>
<span class="o">}</span> 
<span class="k">catch</span> <span class="o">(</span><span class="nc">Type2</span> <span class="n">id2</span><span class="o">)</span>
<span class="o">{</span>  
	    <span class="c1">//捕获并处置try抛出的异常类型Type2  </span>
<span class="o">}</span>
<span class="k">finally</span> 
<span class="o">{</span>  
	    <span class="c1">// 无论是否发生异常，都将执行的语句块  </span>
<span class="o">}</span>
</code></pre></div></div>

<p>自定义异常：</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">class</span> <span class="nc">NombreNegatifException</span> <span class="kd">extends</span> <span class="nc">Exception</span>
<span class="o">{</span>
    <span class="kd">public</span> <span class="nf">NombreNegatifException</span><span class="o">()</span>
    <span class="o">{</span>
    <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"Vous avez un nombre négatif !"</span><span class="o">);</span>
    <span class="o">}</span> 
<span class="o">}</span>
</code></pre></div></div>

<p>在类的方法中抛出新的异常：</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">public</span> <span class="kt">int</span> <span class="nf">Count</span><span class="o">()</span> <span class="kd">throws</span> <span class="nc">Exception</span><span class="o">{</span>
    <span class="k">if</span> <span class="o">(...){</span>
        <span class="k">throw</span> <span class="k">new</span> <span class="nf">Exception</span><span class="o">(</span><span class="s">"..."</span><span class="o">);</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<ul>
  <li>文件读写：</li>
</ul>

<p>类FileReader,FileWriter,使用里面的方法read()和write(x)和close()</p>

<p>比如：</p>

<p><img src="../assets/img/posts/20211130/70.jpg" /></p>

<ul>
  <li>枚举类型enum，举例说明</li>
</ul>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">public</span> <span class="kd">enum</span> <span class="nc">Jour</span> 
<span class="o">{</span>
    <span class="no">LUNDI</span><span class="o">,</span> <span class="no">MARDI</span><span class="o">,</span> <span class="no">MERCREDI</span><span class="o">,</span> <span class="no">JEUDI</span><span class="o">,</span> <span class="no">VENDREDI</span><span class="o">,</span> <span class="no">SAMEDI</span><span class="o">,</span> <span class="no">DIMANCHE</span><span class="o">;</span>
<span class="o">}</span>

<span class="kd">class</span> <span class="nc">EssaiJour</span> 
<span class="o">{</span>
    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="nc">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> 
    <span class="o">{</span>
        <span class="nc">Jour</span> <span class="n">jour</span> <span class="o">=</span> <span class="nc">Jour</span><span class="o">.</span><span class="na">valueOf</span><span class="o">(</span><span class="n">args</span><span class="o">[</span><span class="mi">0</span><span class="o">]);</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">jour</span> <span class="o">==</span> <span class="nc">Jour</span><span class="o">.</span><span class="na">SAMEDI</span><span class="o">)</span> <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="s">"fin de semaine : "</span><span class="o">);</span>
        <span class="k">switch</span><span class="o">(</span><span class="n">jour</span><span class="o">)</span> 
        <span class="o">{</span>
            <span class="k">case</span> <span class="no">SAMEDI</span> <span class="o">:</span>
            <span class="k">case</span> <span class="no">DIMANCHE</span> <span class="o">:</span>
            <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"se reposer"</span><span class="o">);</span>
            <span class="k">break</span><span class="o">;</span>
            <span class="k">default</span> <span class="o">:</span>
            <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"travailler"</span><span class="o">);</span>
            <span class="k">break</span><span class="o">;</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<ul>
  <li>接口interface, <strong>迭代器</strong>iterator</li>
</ul>

<p>举例：</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">class</span> <span class="nc">Main</span> 
<span class="o">{</span>
    <span class="nd">@FunctionalInterface</span>
    <span class="kd">public</span> <span class="kd">interface</span> <span class="nc">maFonction</span> 
    <span class="o">{</span>
        <span class="nc">Integer</span> <span class="nf">appliquer</span><span class="o">(</span><span class="nc">Integer</span> <span class="n">p</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="kd">public</span> <span class="kd">static</span> <span class="nc">Vector</span><span class="o">&lt;</span><span class="nc">Integer</span><span class="o">&gt;</span> <span class="nf">transforme</span><span class="o">(</span><span class="nc">Vector</span><span class="o">&lt;</span><span class="nc">Integer</span><span class="o">&gt;</span> <span class="n">v</span><span class="o">,</span> <span class="n">maFonction</span> <span class="n">function</span><span class="o">)</span> 
    <span class="o">{</span>
        <span class="nc">Vector</span><span class="o">&lt;</span><span class="nc">Integer</span><span class="o">&gt;</span> <span class="n">nouveauVect</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Vector</span><span class="o">&lt;</span><span class="nc">Integer</span><span class="o">&gt;();</span>
        <span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span> 
        <span class="o">{</span>
        <span class="n">nouveauVect</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">function</span><span class="o">.</span><span class="na">appliquer</span><span class="o">(</span><span class="n">v</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">i</span><span class="o">)));</span>
        <span class="o">}</span>
        <span class="n">nouveauVect</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">v</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="mi">3</span><span class="o">));</span>
        <span class="k">return</span> <span class="n">nouveauVect</span><span class="o">;</span>
    <span class="o">}</span>

    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="nc">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> 
    <span class="o">{</span>
        <span class="nc">Vector</span><span class="o">&lt;</span><span class="nc">Integer</span><span class="o">&gt;</span> <span class="n">vi</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Vector</span><span class="o">&lt;</span><span class="nc">Integer</span><span class="o">&gt;(</span><span class="mi">4</span><span class="o">);</span>
        <span class="n">vi</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="mi">1</span><span class="o">);</span> <span class="n">vi</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="mi">4</span><span class="o">);</span> <span class="n">vi</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="mi">83</span><span class="o">);</span> <span class="n">vi</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="mi">18</span><span class="o">);</span>
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"Les valeurs du vecteur initial : "</span><span class="o">);</span>
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">vi</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="mi">0</span><span class="o">)+</span><span class="s">" "</span><span class="o">);</span> <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">vi</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="mi">1</span><span class="o">)+</span><span class="s">" "</span><span class="o">);</span> 
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">vi</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="mi">2</span><span class="o">)+</span><span class="s">" "</span><span class="o">);</span> <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="n">vi</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="mi">3</span><span class="o">));</span>
        <span class="n">vi</span> <span class="o">=</span> <span class="n">transforme</span><span class="o">(</span><span class="n">vi</span><span class="o">,</span><span class="n">s</span> <span class="o">-&gt;</span> <span class="o">(</span><span class="n">s</span> <span class="o">*</span> <span class="mi">2</span><span class="o">));</span>
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"Les valeurs du vecteur modifié : "</span><span class="o">);</span>
        <span class="nc">Iterator</span> <span class="n">iter</span> <span class="o">=</span> <span class="n">vi</span><span class="o">.</span><span class="na">iterator</span><span class="o">();</span>
        <span class="k">while</span> <span class="o">(</span><span class="n">iter</span><span class="o">.</span><span class="na">hasNext</span><span class="o">())</span>
        <span class="o">{</span>
            <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">iter</span><span class="o">.</span><span class="na">next</span><span class="o">()</span> <span class="o">+</span> <span class="s">""</span><span class="o">);</span>
        <span class="o">}</span>
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">();</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<h3 id="4数据结构">4.数据结构</h3>
<ul>
  <li>数据结构一般含有以下功能：创建，插入，寻找，删除，排序</li>
  <li>二维数组,举例说明：</li>
</ul>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">String</span> <span class="n">tab</span><span class="o">[][]={</span> <span class="o">{</span><span class="s">"a"</span><span class="o">,</span> <span class="s">"e"</span><span class="o">,</span> <span class="s">"i"</span><span class="o">,</span> <span class="s">"o"</span><span class="o">,</span> <span class="s">"u"</span><span class="o">},</span> <span class="o">{</span><span class="s">"1"</span><span class="o">,</span> <span class="s">"2"</span><span class="o">,</span> <span class="s">"3"</span><span class="o">,</span> <span class="s">"4"</span><span class="o">}</span> <span class="o">};</span>
<span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">,</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span>
<span class="k">for</span><span class="o">(</span><span class="nc">String</span> <span class="n">sousTab</span><span class="o">[]</span> <span class="o">:</span> <span class="n">tab</span><span class="o">)</span> 
<span class="o">{</span>
    <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span>
    <span class="k">for</span><span class="o">(</span><span class="nc">String</span> <span class="n">str</span> <span class="o">:</span> <span class="n">sousTab</span><span class="o">)</span> 
    <span class="o">{</span> 
        <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"Valeur du tableau à l'indice ["</span><span class="o">+</span><span class="n">i</span><span class="o">+</span><span class="s">"]["</span><span class="o">+</span><span class="n">j</span><span class="o">+</span><span class="s">"]: "</span> <span class="o">+</span> <span class="n">tab</span><span class="o">[</span><span class="n">i</span><span class="o">][</span><span class="n">j</span><span class="o">]);</span>
        <span class="n">j</span><span class="o">++;</span>
    <span class="o">}</span>
    <span class="n">i</span><span class="o">++;</span>
<span class="o">}</span>
</code></pre></div></div>

<p>声明数组：数组类型加变量名</p>

<p>或者</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">tabEntiers</span><span class="o">[];</span>
<span class="n">tabEntiers</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">int</span><span class="o">[</span><span class="mi">40</span><span class="o">];</span> 
<span class="c1">// création effective du tableau précédent</span>
</code></pre></div></div>

<ul>
  <li>列表，包含ArrayList, LinkedList</li>
</ul>

<p>ArrayList是实现了基于动态数组的数据结构，LinkedList基于链表的数据结构。对于随机访问get和set，ArrayList优于LinkedList，因为ArrayList可以随机定位，而LinkedList要移动指针一步一步的移动到节点处。（参考数组与链表来思考）。对于新增和删除操作add和remove，LinedList比较占优势，只需要对指针进行修改即可，而ArrayList要移动数据来填补被删除的对象的空间。</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">public</span> <span class="kd">class</span> <span class="nc">Liste</span><span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> 
<span class="o">{</span>
    <span class="kd">protected</span> <span class="no">T</span> <span class="n">valeur</span><span class="o">;</span> 
    <span class="kd">protected</span> <span class="nc">Liste</span><span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="n">succ</span><span class="o">;</span>
    <span class="kd">protected</span> <span class="nc">Liste</span><span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="n">pred</span><span class="o">;</span>
    
    <span class="kd">public</span> <span class="no">T</span> <span class="nf">valeur</span><span class="o">()</span>
    <span class="o">{</span> 
        <span class="k">return</span> <span class="n">valeur</span><span class="o">;</span>
    <span class="o">}</span>

    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">changerValeur</span><span class="o">(</span><span class="no">T</span> <span class="n">x</span><span class="o">)</span>
    <span class="o">{</span> 
        <span class="n">valeur</span> <span class="o">=</span> <span class="n">x</span><span class="o">;</span> 
    <span class="o">}</span>

    <span class="kd">public</span> <span class="nc">Liste</span><span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="nf">succ</span><span class="o">()</span>
    <span class="o">{</span> 
        <span class="k">return</span> <span class="n">succ</span><span class="o">;</span> 
    <span class="o">}</span>
    
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">changerSucc</span><span class="o">(</span><span class="nc">Liste</span><span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="n">y</span><span class="o">)</span>
    <span class="o">{</span> 
        <span class="n">succ</span> <span class="o">=</span> <span class="n">y</span><span class="o">;</span> 
    <span class="o">}</span>

    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">changerPred</span><span class="o">(</span><span class="nc">Liste</span><span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="n">y</span><span class="o">)</span>
    <span class="o">{</span> 
        <span class="n">pred</span> <span class="o">=</span> <span class="n">y</span><span class="o">;</span> 
    <span class="o">}</span>
<span class="o">}</span> 
</code></pre></div></div>

<p>这是一个链表的简写，每一层包含了上一个元素，这一个元素，下一个元素</p>

<ul>
  <li>哈希表，通过建立KV关系查找，相比于之前的顺序访问或者其他指数访问要快。</li>
</ul>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">java.util.HashMap</span><span class="o">;</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">TestHash</span> 
<span class="o">{</span>
    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="nc">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> 
    <span class="o">{</span>
        <span class="nc">HashMap</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">annuaire</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">HashMap</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span><span class="nc">String</span><span class="o">&gt;();</span>
        <span class="c1">// ajout des valeurs</span>
        <span class="n">annuaire</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">"Alfred"</span><span class="o">,</span><span class="s">"2399020806"</span><span class="o">);</span>
        <span class="n">annuaire</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">"Daniel"</span><span class="o">,</span> <span class="s">"2186000000"</span><span class="o">);</span>
        <span class="c1">// obtention d'un numéro</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">annuaire</span><span class="o">.</span><span class="na">containsKey</span><span class="o">(</span><span class="s">"Danielle"</span><span class="o">))</span> 
        <span class="o">{</span>
            <span class="nc">String</span> <span class="n">num</span> <span class="o">=</span> <span class="n">annuaire</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="s">"Danielle"</span><span class="o">);</span>
            <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="nc">Danielle</span> <span class="o">:</span> <span class="s">"+num"</span><span class="o">);</span>
        <span class="o">}</span>
        <span class="k">else</span> 
        <span class="o">{</span>
            <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"pas trouve"</span><span class="o">);</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<ul>
  <li>树状结构tree</li>
</ul>

<p>一般包含结点，结点的度(该结点下有多少子树的数目)，树的度</p>

<p>不同的遍历方法：</p>
<blockquote>
  <p>前序遍历，首先结点，然后左子树，右子树<br />
中序遍历，左子树，结点，右子树<br />
后序遍历，左子树，右子树，结点<br />
层序遍历，从上到下，从左到右</p>
</blockquote>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">class</span> <span class="nc">Arbre</span> 
<span class="o">{</span>
    <span class="kd">protected</span> <span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="n">valeur</span><span class="o">;</span>
    <span class="kd">protected</span> <span class="nc">Arbre</span> <span class="n">filsGauche</span><span class="o">,</span> <span class="n">filsDroit</span><span class="o">;</span> 
    <span class="kd">public</span> <span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="nf">valeur</span><span class="o">()</span> 
    <span class="o">{</span> 
        <span class="k">return</span> <span class="n">valeur</span><span class="o">;</span> 
    <span class="o">}</span>
    <span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">existeFilsGauche</span><span class="o">()</span> <span class="o">{</span> <span class="k">return</span> <span class="n">filsGauche</span> <span class="o">!=</span> <span class="kc">null</span><span class="o">;</span> <span class="o">}</span> 
    <span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">existeFilsDroit</span><span class="o">()</span> <span class="o">{</span> <span class="k">return</span> <span class="n">filsDroit</span> <span class="o">!=</span> <span class="kc">null</span><span class="o">;</span> <span class="o">}</span>
    <span class="kd">public</span> <span class="nc">Arbre</span> <span class="nf">filsGauche</span><span class="o">()</span> <span class="o">{</span> <span class="k">return</span> <span class="n">filsGauche</span><span class="o">;</span> <span class="o">}</span>
    <span class="kd">public</span> <span class="nc">Arbre</span> <span class="nf">filsDroit</span><span class="o">()</span> <span class="o">{</span> <span class="k">return</span> <span class="n">filsDroit</span><span class="o">;</span> <span class="o">}</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">affecterValeur</span><span class="o">(&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="n">c</span><span class="o">)</span> <span class="o">{</span> <span class="n">valeur</span> <span class="o">=</span> <span class="n">c</span><span class="o">;</span> <span class="o">}</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">affecterFilsGauche</span><span class="o">(</span><span class="nc">Arbre</span> <span class="n">g</span><span class="o">)</span> <span class="o">{</span> <span class="n">filsGauche</span> <span class="o">=</span> <span class="n">g</span><span class="o">;</span> <span class="o">}</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">affecterFilsDroit</span><span class="o">(</span><span class="nc">Arbre</span> <span class="n">d</span><span class="o">)</span> <span class="o">{</span> <span class="n">filsDroit</span> <span class="o">=</span> <span class="n">d</span><span class="o">;}</span>
    <span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">feuille</span><span class="o">()</span> <span class="o">{</span><span class="k">return</span> <span class="o">(</span><span class="n">filsDroit</span><span class="o">==</span><span class="kc">null</span> <span class="o">&amp;&amp;</span> 
    <span class="n">filsGauche</span><span class="o">==</span><span class="kc">null</span><span class="o">);</span> 
<span class="o">}</span>

<span class="kd">public</span> <span class="kt">int</span> <span class="nf">hauteur</span><span class="o">()</span> 
<span class="o">{</span>
    <span class="kt">int</span> <span class="n">g</span> <span class="o">=</span> <span class="n">existeFilsGauche</span><span class="o">()</span> <span class="o">?</span> <span class="n">filsGauche</span><span class="o">.</span><span class="na">hauteur</span><span class="o">()</span> <span class="o">:</span> <span class="mi">0</span><span class="o">;</span>
    <span class="kt">int</span> <span class="n">d</span> <span class="o">=</span> <span class="n">existeFilsDroit</span><span class="o">()</span> <span class="o">?</span> <span class="n">filsDroit</span><span class="o">.</span><span class="na">hauteur</span><span class="o">()</span> <span class="o">:</span> <span class="mi">0</span><span class="o">;</span>
    <span class="k">return</span> <span class="nc">Math</span><span class="o">.</span><span class="na">max</span><span class="o">(</span><span class="n">g</span><span class="o">,</span><span class="n">d</span><span class="o">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">;</span>
<span class="o">}</span>
<span class="c1">// Constructeurs</span>
<span class="kd">public</span> <span class="nf">Arbre</span><span class="o">(</span><span class="no">T</span> <span class="n">val</span><span class="o">)</span> 
<span class="o">{</span>
    <span class="n">valeur</span> <span class="o">=</span> <span class="n">val</span><span class="o">;</span>
    <span class="n">filsGauche</span> <span class="o">=</span> <span class="n">filsDroit</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
<span class="o">}</span>
<span class="kd">public</span> <span class="nf">Arbre</span><span class="o">(</span><span class="no">T</span> <span class="n">val</span><span class="o">,</span> <span class="nc">Arbre</span><span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="n">g</span><span class="o">,</span> <span class="nc">Arbre</span><span class="o">&lt;</span><span class="no">T</span><span class="o">&gt;</span> <span class="n">d</span><span class="o">)</span> 
<span class="o">{</span>
    <span class="n">valeur</span> <span class="o">=</span> <span class="n">val</span><span class="o">;</span>
    <span class="n">filsGauche</span> <span class="o">=</span> <span class="n">g</span><span class="o">;</span> <span class="n">filsDroit</span> <span class="o">=</span> <span class="n">d</span><span class="o">;</span>
<span class="o">}</span>

<span class="c1">// Affichage</span>
<span class="kd">public</span> <span class="kt">void</span> <span class="nf">afficherPrefixe</span><span class="o">()</span> 
<span class="o">{</span>
    <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">valeur</span><span class="o">+</span><span class="s">"\t"</span><span class="o">);</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">existeFilsGauche</span><span class="o">())</span> <span class="n">filsGauche</span><span class="o">.</span><span class="na">afficherPrefixe</span><span class="o">();</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">existeFilsDroit</span><span class="o">())</span> <span class="n">filsDroit</span><span class="o">.</span><span class="na">afficherPrefixe</span><span class="o">();</span>
<span class="o">}</span>

<span class="kd">public</span> <span class="kt">void</span> <span class="nf">afficherInfixe</span><span class="o">()</span> 
<span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">existeFilsGauche</span><span class="o">())</span> <span class="n">filsGauche</span><span class="o">.</span><span class="na">afficherInfixe</span><span class="o">();</span>
    <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">valeur</span><span class="o">+</span><span class="s">"\t"</span><span class="o">);</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">existeFilsDroit</span><span class="o">())</span><span class="n">filsDroit</span><span class="o">.</span><span class="na">afficherInfixe</span><span class="o">();</span>
<span class="o">}</span>

<span class="kd">public</span> <span class="kt">void</span> <span class="nf">afficherPostfixe</span><span class="o">()</span> 
<span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">existeFilsGauche</span><span class="o">())</span> <span class="n">filsGauche</span><span class="o">.</span><span class="na">afficherPostfixe</span><span class="o">();</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">existeFilsDroit</span><span class="o">())</span><span class="n">filsDroit</span><span class="o">.</span><span class="na">afficherPostfixe</span><span class="o">();</span>
    <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">print</span><span class="o">(</span><span class="n">valeur</span><span class="o">+</span><span class="s">"\t"</span><span class="o">);</span>
<span class="o">}</span>
</code></pre></div></div>

<p>二叉排序树是指左子树小于结点小于右子树，而且结点值不重复。判断是否为二叉排序树：</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">superieur</span><span class="o">(</span><span class="kt">char</span> <span class="n">x</span><span class="o">)</span> 
<span class="o">{</span>
<span class="c1">// vrai si x est supérieur à tous les éléments de l’arbre</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">feuille</span><span class="o">())</span> <span class="k">return</span> <span class="o">(</span><span class="n">x</span><span class="o">&gt;=</span><span class="n">valeur</span><span class="o">);</span>
    <span class="k">else</span> <span class="nf">return</span> 
    <span class="o">(((</span><span class="k">this</span><span class="o">.</span><span class="na">existeFilsGauche</span><span class="o">())?</span> <span class="o">(</span><span class="k">this</span><span class="o">.</span><span class="na">filsGauche</span><span class="o">).</span><span class="na">superieur</span><span class="o">(</span><span class="n">x</span><span class="o">):</span><span class="kc">true</span><span class="o">)</span> <span class="o">&amp;</span> 
     <span class="o">((</span><span class="k">this</span><span class="o">.</span><span class="na">existeFilsDroit</span><span class="o">())?</span> <span class="o">(</span><span class="k">this</span><span class="o">.</span><span class="na">filsDroit</span><span class="o">).</span><span class="na">superieur</span><span class="o">(</span><span class="n">x</span><span class="o">):</span><span class="kc">true</span><span class="o">));</span>
<span class="o">}</span> 
<span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">inferieur</span><span class="o">(</span><span class="kt">char</span> <span class="n">x</span><span class="o">)</span> <span class="o">{</span><span class="c1">//similaire a superieur ... }</span>
<span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">binrech</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">feuille</span><span class="o">())</span> <span class="k">return</span> <span class="kc">true</span><span class="o">;</span>
    <span class="k">else</span> <span class="nf">return</span>
    <span class="o">((</span><span class="n">existeFilsGauche</span><span class="o">()?(</span><span class="n">filsGauche</span><span class="o">.</span><span class="na">superieur</span><span class="o">(</span><span class="n">valeur</span><span class="o">)</span> <span class="o">&amp;&amp;</span> <span class="n">filsGauche</span><span class="o">.</span><span class="na">binrech</span><span class="o">()):</span><span class="kc">true</span><span class="o">)</span> <span class="o">&amp;</span> 
     <span class="o">(</span><span class="n">existeFilsDroit</span><span class="o">()?(</span><span class="n">filsDroit</span><span class="o">.</span><span class="na">inferieur</span><span class="o">(</span><span class="n">valeur</span><span class="o">)</span> <span class="o">&amp;&amp;</span> <span class="n">filsDroit</span><span class="o">.</span><span class="na">binrech</span><span class="o">()):</span><span class="kc">true</span><span class="o">));</span>
<span class="o">}</span> 
</code></pre></div></div>

<h3 id="5-常用数据结构方法">5. 常用数据结构方法</h3>

<p>二维数组array[][]的定义和访问</p>

<table>
  <tbody>
    <tr>
      <td>数据结构</td>
      <td>vector&lt;String&gt;</td>
      <td>ArrayList&lt;String&gt;</td>
      <td>LinkedList&lt;String&gt;</td>
      <td>HashMap&lt;String,int&gt;</td>
    </tr>
    <tr>
      <td>添加</td>
      <td>add(i, str)</td>
      <td>add(i,str)</td>
      <td>add(i,str)</td>
      <td>put(str,i)</td>
    </tr>
    <tr>
      <td>查找</td>
      <td>get(i)</td>
      <td>get(i)</td>
      <td>get(i)</td>
      <td>get(str)</td>
    </tr>
    <tr>
      <td>索引</td>
      <td>indexOf(str)</td>
      <td>indexOf(str)</td>
      <td>indexOf(str)</td>
      <td> </td>
    </tr>
    <tr>
      <td>删除</td>
      <td>remove(i)/remove(str)</td>
      <td>remove(i/str)</td>
      <td>remove(i/str)</td>
      <td>remove(i)</td>
    </tr>
    <tr>
      <td>清除</td>
      <td>clear()</td>
      <td>clear()</td>
      <td>clear()</td>
      <td> </td>
    </tr>
    <tr>
      <td>查看大小</td>
      <td>size()</td>
      <td>size()</td>
      <td>size()</td>
      <td>size()</td>
    </tr>
    <tr>
      <td>迭代器</td>
      <td>iterator()</td>
      <td>iterator()</td>
      <td>iterator()</td>
      <td> </td>
    </tr>
    <tr>
      <td>变成数组</td>
      <td> </td>
      <td>toArray()</td>
      <td>toArray()</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>hashmap还可以返回键值对entryset()，也可以判断是否含有key和value，containsKey(),containsValue()</p>

<p>Hashmap创建的时候需要继承</p>

<h2 id="tp整理">TP整理</h2>
<ol>
  <li>定义构造函数时也需要声明权限修饰符，构造函数不需要返回值，调用的时候直接在声明一个对象的时候调用</li>
  <li>访问成员变量的时候用.</li>
  <li>方法中除了构造函数都需要权限修饰符和返回类型名</li>
  <li>在类中调用成员变量可以直接调用</li>
  <li>字符串之间用+连接</li>
  <li>当一个类中有main函数时，一般这么定义: public static void main(String[] a){}</li>
  <li>在定义类的时候不要有()</li>
  <li>向量的创建和声明: Vector&lt;T&gt; mesObjectsG = new Vector&lt;T&gt;();</li>
  <li>打印的使用: System.out.println()</li>
  <li>成员函数在类的开头只是声明，并没有创建，所以在方法中创建</li>
  <li>for循环的使用: for(Point p : seg.getVecP())</li>
  <li>Vector.add(), 向量的添加</li>
  <li>Vector.get(i), i从0开始, 向量的访问</li>
  <li>Vector.size()返回int值，是vector的大小</li>
  <li>TP1中为了让图形可视化，每个类都需要extends ObjectGraphique</li>
  <li>什么时候需要在类定义方法时new一个对象: 当这个对象来自其他类同时改变这个对象会对新创建的类产生影响，所以一般还是new一个对象比较好</li>
  <li>基本的数据类型: int, string, float, boolean不需要new, 声明的时候也不需要构造函数</li>
  <li>继承的时候可以用super()来调用继承类的构造函数</li>
  <li>异常的抛出: 在定义类中的方法的时候，需要抛出的方法需要在创建时添加一句throws Exception, 调用的时候使用throw new Exception(“…”)</li>
  <li>也可以throws IOException这样已经定义好的抛出类型</li>
  <li>文件读入: Reader reader = new FileReader(textname.txt)</li>
  <li>逐行读入: BufferedReader in = new BufferedReader(reader); String line = in.readline()</li>
  <li>分词器: StringTokenizer st = new StringTokenlizer(line, DeleteChar); 然后就可以像迭代器一样调用, if(st.hasMoreTokens()){st.nextToken()}</li>
  <li>文件写入: Writer writer = new FileWriter(newtextname); writer.write(“…”)</li>
  <li>将链表中所有的单词写入一个文件: for (object k : this){writer.write(k+”\n”);writer.flush()} 写入完毕后, writer.close()</li>
  <li>哈希表判断key是否在哈希表中: HashMap.containsKey(…), 返回值为布尔类型</li>
  <li>哈希表中添加元素: HashMap.put(key, value)</li>
  <li>哈希表可以返回键值对，键对和值对，调用HashMap.entrySet()、HashMap.keySet()、HashMap.valueSet()</li>
  <li>通过键来访问哈希表中的值: HashMap.get(key)</li>
  <li>EntrySet同样可以通过getKey()和getValue()来访问键值</li>
  <li>在创建一个新类时，对于类的名字可以这么取: Arbre&lt;T&gt;, 这样就表示Arbre类中的数据类型是T</li>
  <li>java中的空是null，小写</li>
  <li>记住二叉树高度是怎么写的</li>
  <li>记住二叉树前序中序后序遍历是如何写的</li>
  <li>在进行没有变量x时怎么计算数的结果: 先用迭代访问出左值和右值，然后根据根结点的符号决定进行什么运算</li>
  <li>Integer.parseInt(String a)的作用是将整数的String表示转化为Integer</li>
  <li>对于含变量x的二叉树如何在输入x的值的情况下算出结果: 首先遍历左子树，如果左子树的左边有x，那么将输入的值赋给这个结点的x，然后迭代回来计算左结点的值，具体的可以看网站截图</li>
  <li>java中final关键字是保证了变量无法修改，只能访问</li>
  <li>字符串的比较用.equals()</li>
  <li>Vector.contains(…)可以判断一个值是否在这个向量里</li>
  <li>String.charAt(i), 可以将字符串第i个字符赋给char</li>
  <li>instanceOf用来判断一个对象是不是一个类的实例: if (a instanceOf A)</li>
  <li>多态的一种应用是: 抽象父类，子类继承抽象父类</li>
</ol>

<center><img src="../assets/img/posts/20211130/73.jpg" /></center>]]></content><author><name>Quehry</name></author><category term="school" /><summary type="html"><![CDATA[课堂记录]]></summary></entry></feed>