<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-11-26T15:37:17+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Quehry</title><subtitle>Artificial Intelligence trends and concepts made easy.</subtitle><author><name>Quehry</name></author><entry><title type="html">C++面向对象程序设计</title><link href="http://localhost:4000/Object-Oriented-Programming.html" rel="alternate" type="text/html" title="C++面向对象程序设计" /><published>2022-11-20T00:00:00+08:00</published><updated>2022-11-20T00:00:00+08:00</updated><id>http://localhost:4000/Object-Oriented-Programming</id><content type="html" xml:base="http://localhost:4000/Object-Oriented-Programming.html"><![CDATA[<h1 id="1-简介">1. 简介</h1>
<p>网课程序设计与算法(三):C++面向对象程序设计的随堂笔记，授课平台是中国大学MOOC，授课老师是北大的郭炜老师，开课次数是第十五次开课。程序设计与算法是一个系列的课程，C++面向对象程序设计是该系列的最后一门课程。课程文件有每节课的讲义及习题(openjudge上)，相关链接:</p>
<ul>
  <li><a href="https://www.icourse163.org/learn/PKU-1002029030?tid=1469134446#/learn/content" target="_blank">课程链接，开课阶段过了可能会失效</a></li>
  <li><a href="http://cxsjsxmooc.openjudge.cn/2022t3fall/" target="_blank">openjudge习题</a></li>
</ul>

<h1 id="2-随堂笔记">2. 随堂笔记</h1>
<h2 id="21-第一章-从c到c">2.1. 第一章 从C到C++</h2>
<h3 id="211-引用">2.1.1. 引用</h3>
<ul>
  <li>引用的概念: <code class="language-plaintext highlighter-rouge">int &amp; a = b</code></li>
</ul>

<center><img src="../assets/img/posts/20221120/2.jpg" /></center>

<ul>
  <li>引用的一些注意事项</li>
</ul>

<center><img src="../assets/img/posts/20221120/3.jpg" /></center>

<ul>
  <li>C语言中，形参的值改变不会影响实参的值</li>
  <li>C语言中交换两个数的值的函数swap的写法:</li>
</ul>

<center><img src="../assets/img/posts/20221120/4.png" /></center>

<ul>
  <li>C++中有了引入的概念后，swap函数便可以这么写:</li>
</ul>

<center><img src="../assets/img/posts/20221120/5.jpg" /></center>

<ul>
  <li>除此之外，引用还可以作为函数的返回值，目前还不知道具体作用，后续会了解</li>
</ul>

<center><img src="../assets/img/posts/20221120/6.jpg" /></center>

<ul>
  <li>常引用:</li>
</ul>

<center><img src="../assets/img/posts/20221120/7.jpg" /></center>

<ul>
  <li>不能通过常引用来修改其引用内容:</li>
</ul>

<center><img src="../assets/img/posts/20221120/8.jpg" /></center>

<ul>
  <li><code class="language-plaintext highlighter-rouge">const T &amp;</code> 和 <code class="language-plaintext highlighter-rouge">T &amp;</code>的相互转换:</li>
</ul>

<center><img src="../assets/img/posts/20221120/9.jpg" /></center>

<h3 id="212-const关键字">2.1.2. const关键字</h3>
<ul>
  <li>const的第一个作用就是定义常量</li>
  <li>const的第二个作用是定义常量指针:</li>
</ul>

<center><img src="../assets/img/posts/20221120/10.jpg" /></center>
<center><img src="../assets/img/posts/20221120/11.jpg" /></center>

<ul>
  <li>常量指针常作为函数的参数，可避免函数内部不小心改变参数指针所指地方的内容</li>
</ul>

<center><img src="../assets/img/posts/20221120/12.jpg" /></center>

<h3 id="213-动态内存分配">2.1.3. 动态内存分配</h3>
<ul>
  <li>用new运算符实现动态内存分配<code class="language-plaintext highlighter-rouge">P = new T</code>:</li>
</ul>

<center><img src="../assets/img/posts/20221120/13.jpg" /></center>

<ul>
  <li>用new运算符分配一个数组:</li>
</ul>

<center><img src="../assets/img/posts/20221120/14.jpg" /></center>

<ul>
  <li>用new动态分配的内存空间，一定要用delete运算符进行释放，<code class="language-plaintext highlighter-rouge">delete 指针</code>:</li>
</ul>

<center><img src="../assets/img/posts/20221120/15.jpg" /></center>

<ul>
  <li>如果定义了一个数组的动态空间，delete的时候一定要加上中括号:</li>
</ul>

<center><img src="../assets/img/posts/20221120/16.jpg" /></center>

<h3 id="214-内联函数">2.1.4. 内联函数</h3>
<ul>
  <li>内联函数的主要作用是加速简单的函数的调用，所以一般是用于相对简单的函数，在前面加上<code class="language-plaintext highlighter-rouge">inline</code>关键字:</li>
</ul>

<center><img src="../assets/img/posts/20221120/17.jpg" /></center>

<center><img src="../assets/img/posts/20221120/18.jpg" /></center>

<ul>
  <li>函数重载: 名字相同但参数个数或参数类型不同</li>
</ul>

<center><img src="../assets/img/posts/20221120/19.png" /></center>

<ul>
  <li>C++中，定义函数的时候可以让<strong>最右边</strong>的连续若干个参数有缺省值，那么调用函数的时候，相应位置不写参数，参数就是缺省值:</li>
</ul>

<center><img src="../assets/img/posts/20221120/20.jpg" /></center>

<ul>
  <li>函数的缺省参数的作用:</li>
</ul>

<center><img src="../assets/img/posts/20221120/21.jpg" /></center>

<h2 id="22-第二章-类和对象基础">2.2. 第二章 类和对象基础</h2>
<h3 id="221-类和对象的基本概念">2.2.1. 类和对象的基本概念</h3>
<ul>
  <li>在面向对象编程之前，设计一个程序的方法是结构化程序设计(structured programming)，具体来说就是数据结构+算法</li>
  <li>结构化程序设计有很多不足，结构化的程序，在规模庞大时，会变得难以理解，难以扩充，难以查错，难以重用</li>
  <li>软件业的目标时更快、更正确、更经济地建立软件</li>
  <li>面向对象程序设计可以很好的解决上述问题</li>
  <li>面向对象的程序=类+类+..+类</li>
  <li>面向对象的程序设计具有抽象、封装、继承、多态四个基本特点</li>
  <li>类有成员变量和成员函数，成员变量和成员函数统称为类的成员，类看上去就像带函数的结构</li>
  <li>一个类的例子:</li>
</ul>

<center><img src="../assets/img/posts/20221120/22.jpg" /></center>

<ul>
  <li>类在实例化之后就是一个对象，通过类，可以定义变量，类定义出来的变量，也称为类的实例，就是我们所说的对象</li>
  <li>对象所占用的内存空间大小等于所有成员变量的大小之和</li>
  <li>和结构体类似，既可以通过<code class="language-plaintext highlighter-rouge">对象名.成员名</code>的方法来访问类的成员变量和成员函数，也可以通过<code class="language-plaintext highlighter-rouge">指针-&gt;成员名</code>来访问指针指向的类的成员变量:</li>
</ul>

<center><img src="../assets/img/posts/20221120/23.jpg" /></center>

<ul>
  <li>C++中，也可以通过引用的方法来访问类的成员，因为引用之后，两者就等价了</li>
  <li>类的成员函数和类的定义可以分开写，可以在类的内部声明类的成员函数，然后在类的外部通过<code class="language-plaintext highlighter-rouge">类名::类的成员函数</code>来定义:</li>
</ul>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">CRectangle</span><span class="o">::</span><span class="n">Area</span><span class="p">(){</span>
    <span class="k">return</span> <span class="n">w</span> <span class="o">*</span> <span class="n">h</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>
<ul>
  <li>在类的定义中，用下列访问关键字来说明类成员可被访问的范围(private,public,protected):</li>
</ul>

<center><img src="../assets/img/posts/20221120/24.jpg" /></center>

<ul>
  <li>如果类的某个成员前面没有上述关键字，则缺省地被认为是私有成员:</li>
</ul>

<center><img src="../assets/img/posts/20221120/25.jpg" /></center>

<ul>
  <li>类成员的可访问范围:</li>
</ul>

<center><img src="../assets/img/posts/20221120/26.jpg" /></center>

<ul>
  <li>设置私有成员的机制，叫做<strong>隐藏</strong>，隐藏的目的是强制对成员变量的访问一定要通过成员函数进行，那么以后成员变量的类型等属性修改后，只需要更改成员函数即可</li>
  <li><code class="language-plaintext highlighter-rouge">strcpy(char * a, char* b)</code>可以将字符串b的值赋给字符串a</li>
  <li>析构函数(destructor)与构造函数相反，当对象结束其生命周期，如对象所在的函数已调用完毕时，系统自动执行析构函数。比如在建立对象时使用<code class="language-plaintext highlighter-rouge">new</code>开辟了一片内存空间，<code class="language-plaintext highlighter-rouge">delete</code>会自动调用析构函数后释放内存</li>
  <li>成员函数的重载及参数缺省: 成员函数可以重载，也可以带缺省的参数</li>
</ul>

<center><img src="../assets/img/posts/20221120/27.jpg" /></center>

<ul>
  <li>使用重载的时候一定要注意避免函数重载时的二义性</li>
</ul>

<h3 id="222-构造函数">2.2.2. 构造函数</h3>
<ul>
  <li>构造函数是成员函数的一种，名字与类名相同，不能有返回值，可以有参数，作用是对对象实例化时进行初始化，如果定义类的时候没写构造函数，则编译器默认生成一个无参数的构造函数，默认的构造函数不做任何操作</li>
</ul>

<center><img src="../assets/img/posts/20221120/28.jpg" /></center>

<ul>
  <li>构造函数的作用是执行必要的初始化工作，有个构造函数，就不必再写初始化函数，也不用担心忘记调用初始化函数</li>
  <li>对象实例化的方法:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">Complex c1</code></li>
      <li><code class="language-plaintext highlighter-rouge">Complex c1(3,4)</code></li>
      <li><code class="language-plaintext highlighter-rouge">Complex * pc = new Complex</code></li>
      <li><code class="language-plaintext highlighter-rouge">Complex * pc = new Complex(3,4)</code></li>
    </ul>
  </li>
  <li>构造函数也可以重载</li>
  <li>利用类来构造数组时，如果声明数组的时候没有传参，那么默认无参数，如果传递了参数，那么就认为是有参数的构造函数</li>
</ul>

<center><img src="../assets/img/posts/20221120/29.png" /></center>
<center><img src="../assets/img/posts/20221120/30.jpg" /></center>

<ul>
  <li>构造函数最好是public的</li>
  <li>new完之后一定记得delete</li>
  <li>用指针来实例化对象的时候一定要开辟内存空间，不然不知道这个指针指向哪里</li>
</ul>

<h3 id="223-复制构造函数">2.2.3. 复制构造函数</h3>
<ul>
  <li>复制构造函数的概念:</li>
</ul>

<center><img src="../assets/img/posts/20221120/31.jpg" /></center>

<ul>
  <li>复制构造函数的一个例子:</li>
</ul>

<center><img src="../assets/img/posts/20221120/32.jpg" /></center>

<ul>
  <li>如果定义自己的复制构造函数，则默认的复制构造函数不会存在</li>
  <li>复制构造函数起作用的三种情况:
    <ul>
      <li>用一个对象去初始化同类的另一个对象时</li>
      <li>如果某函数有一个参数是类A的对象(不是类A的引用)，那么该函数被调用时，类A的复制构造函数将被调用</li>
    </ul>

    <center><img src="../assets/img/posts/20221120/33.jpg" /></center>

    <ul>
      <li>如果函数的返回值是类A的对象时，则函数返回时，A的复制构造函数将被调用</li>
    </ul>

    <center><img src="../assets/img/posts/20221120/34.png" /></center>
  </li>
  <li>对象间赋值并不导致复制构造函数被调用，会直接把a的成员变量的值赋给b</li>
</ul>

<center><img src="../assets/img/posts/20221120/35.png" /></center>

<ul>
  <li>如果某个函数的参数包含类A的对象时，调用函数时会调用类A的复制构造函数，调用开销比较大，这时可以将函数的参数变成常量引用<code class="language-plaintext highlighter-rouge">const Complex &amp;</code>，这样形参的改变不会影响实参</li>
</ul>

<center><img src="../assets/img/posts/20221120/36.png" /></center>

<h3 id="224-类型转换构造函数">2.2.4. 类型转换构造函数</h3>
<ul>
  <li>类型转换构造函数的概念:</li>
</ul>

<center><img src="../assets/img/posts/20221120/37.png" /></center>

<ul>
  <li>例子:</li>
</ul>

<center><img src="../assets/img/posts/20221120/38.jpg" /></center>

<ul>
  <li><code class="language-plaintext highlighter-rouge">float a = int b</code>其实就是定义对象a的时候调用了float类的类型转换构造函数(显式)</li>
  <li>关键字<code class="language-plaintext highlighter-rouge">explicit</code>用于只有一个参数的构造函数，表明构造函数是显式的:</li>
</ul>

<center><img src="../assets/img/posts/20221120/39.jpg" /></center>

<ul>
  <li>析构函数的概念:</li>
</ul>

<center><img src="../assets/img/posts/20221120/40.jpg" /></center>

<ul>
  <li>例子:</li>
</ul>

<center><img src="../assets/img/posts/20221120/41.jpg" /></center>

<ul>
  <li>类的数组在生命周期结束时，会调用每个元素的析构函数</li>
  <li>析构函数在对象作为函数的返回值返回后被调用(下面这个例子中析构函数被调用了三次):</li>
</ul>

<center><img src="../assets/img/posts/20221120/42.jpg" /></center>

<h3 id="225-构造函数和析构函数调用时机">2.2.5. 构造函数和析构函数调用时机</h3>
<ul>
  <li>构造函数和析构函数什么时候会被调用? 看一个例子:</li>
</ul>

<center><img src="../assets/img/posts/20221120/43.jpg" /></center>

<ul>
  <li>静态局部变量在函数消亡时不会消失</li>
</ul>

<center><img src="../assets/img/posts/20221120/44.jpg" /></center>

<ul>
  <li>复制构造函数在不同编译器里不同的表现:</li>
</ul>

<center><img src="../assets/img/posts/20221120/45.jpg" /></center>
<center><img src="../assets/img/posts/20221120/46.jpg" /></center>
<center><img src="../assets/img/posts/20221120/47.png" /></center>

<h2 id="23-类和对象提高">2.3. 类和对象提高</h2>
<h3 id="231-this指针">2.3.1. this指针</h3>
<ul>
  <li>new返回的是指针</li>
  <li>this指针的作用就是指向成员函数所作用的对象</li>
  <li>非静态成员函数中可以直接使用this来代表指向该函数作用的对象的指针:</li>
</ul>

<center><img src="../assets/img/posts/20221120/48.jpg" /></center>

<ul>
  <li>静态成员函数中不能使用this指针，因为静态成员函数并不具体作用于某个对象，因此，静态成员函数的真实的参数的个数，就是程序中写出的参数个数</li>
</ul>

<h3 id="232-静态成员变量">2.3.2. 静态成员变量</h3>
<ul>
  <li>静态成员: 在声明前加了<code class="language-plaintext highlighter-rouge">static</code>关键字的成员，包含了静态成员变量和静态成员函数:</li>
</ul>

<center><img src="../assets/img/posts/20221120/49.jpg" /></center>

<ul>
  <li>普通的成员变量每个对象各有一份，而静态成员变量一共就一份，为所有对象所共享，<code class="language-plaintext highlighter-rouge">sizeof</code>运算符不会计算静态成员变量</li>
</ul>

<center><img src="../assets/img/posts/20221120/50.jpg" /></center>

<ul>
  <li>访问静态成员的方法(静态成员并不需要通过对象就可以访问):</li>
</ul>

<center><img src="../assets/img/posts/20221120/51.jpg" /></center>

<ul>
  <li>静态成员变量本质上就是针对于某个类的<strong>全局变量</strong>，哪怕一个对象都不存在，类的静态成员变量也存在，静态成员函数本质上就是<strong>全局函数</strong></li>
</ul>

<center><img src="../assets/img/posts/20221120/52.jpg" /></center>

<ul>
  <li>例子:</li>
</ul>

<center><img src="../assets/img/posts/20221120/53.jpg" /></center>

<center><img src="../assets/img/posts/20221120/54.jpg" /></center>

<center><img src="../assets/img/posts/20221120/55.jpg" /></center>

<ul>
  <li>静态成员变量必须在定义类的文件中对静态变量进行一次说明或初始化:</li>
</ul>

<center><img src="../assets/img/posts/20221120/56.jpg" /></center>

<ul>
  <li>注意: 在静态成员函数中，不能访问非静态成员变量，也不能调用非静态成员函数</li>
</ul>

<center><img src="../assets/img/posts/20221120/57.jpg" /></center>

<ul>
  <li>上述的写法其实有缺陷，因为在定义对象的时候可能会调用复制构造函数，这些临时对象在生成时不会增加totalnumber，但是在消亡时会减少totalnumber，会出错:</li>
</ul>

<center><img src="../assets/img/posts/20221120/58.jpg" /></center>

<ul>
  <li>解决方法就是定义一个复制构造函数:</li>
</ul>

<center><img src="../assets/img/posts/20221120/59.jpg" /></center>

<h3 id="233-成员对象和封闭类">2.3.3. 成员对象和封闭类</h3>
<ul>
  <li>有成员对象的类叫封闭类</li>
  <li>定义构造函数时可以利用初始化列表来初始化成员变量: <code class="language-plaintext highlighter-rouge">CTyre(int w, int r):radius(r),width(w){};</code></li>
  <li>封闭类的例子:</li>
</ul>

<center><img src="../assets/img/posts/20221120/60.png" /></center>

<ul>
  <li>如果CCar类不定义构造函数，则编译器不明白CCar类的成员对象如何初始化</li>
  <li>任何生成封闭类对象的语句，都要让编译器明白，对象中的成员对象是如何初始化的</li>
</ul>

<center><img src="../assets/img/posts/20221120/61.png" /></center>

<ul>
  <li>封闭类构造函数和析构函数的执行顺序:</li>
</ul>

<center><img src="../assets/img/posts/20221120/62.jpg" /></center>

<ul>
  <li>如果封闭类的对象是用默认的复制构造函数初始化的，那么封闭类的成员对象也会用复制构造函数初始化:</li>
</ul>

<center><img src="../assets/img/posts/20221120/63.jpg" /></center>

<h3 id="234-常量对象和常量成员函数">2.3.4. 常量对象和常量成员函数</h3>
<ul>
  <li>如果不希望某个对象的值发生改变，则定义该对象的时候可以在前面加上<code class="language-plaintext highlighter-rouge">const</code>关键字</li>
  <li>在类的成员函数声明<strong>后</strong>加上<code class="language-plaintext highlighter-rouge">const</code>关键字，则该成员函数为常量成员函数，常量成员函数在执行期间不应该修改其所作用的对象</li>
</ul>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Sample</span><span class="p">{</span>
    <span class="nl">public:</span>
        <span class="kt">int</span> <span class="n">value</span><span class="p">;</span>
        <span class="kt">void</span> <span class="n">GetValue</span><span class="p">()</span> <span class="k">const</span><span class="p">;</span>
        <span class="kt">void</span> <span class="n">func</span><span class="p">(){};</span>
<span class="p">}</span>
<span class="kt">void</span> <span class="n">Sample</span><span class="o">::</span><span class="n">GetValue</span><span class="p">()</span> <span class="k">const</span><span class="p">{</span>
    <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c1">// wrong</span>
    <span class="n">func</span><span class="p">();</span> <span class="c1">//wrong</span>
<span class="p">}</span>
</code></pre></div></div>
<ul>
  <li>常量成员函数和不加<code class="language-plaintext highlighter-rouge">const</code>的成员函数算重载关系</li>
  <li>对象作为函数的参数时，生成该参数需要调用复制构造函数，效率比较低，用指针作为参数，代码又不好看，那么解决方法就是用对象的引用作为参数</li>
</ul>

<h3 id="235-友元">2.3.5. 友元</h3>
<ul>
  <li>友元(friends)分为友元函数和友元类两种</li>
  <li>友元函数: 加上<code class="language-plaintext highlighter-rouge">friend</code>关键字就成为了友元函数，一个类的友元函数可以访问该类的私有成员</li>
</ul>

<center><img src="../assets/img/posts/20221120/64.jpg" /></center>
<center><img src="../assets/img/posts/20221120/65.jpg" /></center>

<ul>
  <li>可以将一个类的成员函数说明为另一个类的友元</li>
  <li>友元类: 如果A是B的友元类，那么A的成员函数可以访问B的私有成员</li>
</ul>

<center><img src="../assets/img/posts/20221120/66.jpg" /></center>

<ul>
  <li>友元类之间的关系不能传递，不能继承</li>
</ul>

<h2 id="24-运算符重载">2.4. 运算符重载</h2>
<h3 id="241-运算符重载的基本概念">2.4.1. 运算符重载的基本概念</h3>
<ul>
  <li>基本运算符只能支持c++基本的数据类型的运算</li>
</ul>
<center><img src="../assets/img/posts/20221120/67.jpg" /></center>

<ul>
  <li>运算符重载的意义:</li>
</ul>
<center><img src="../assets/img/posts/20221120/68.jpg" /></center>

<ul>
  <li>运算符重载的形式:</li>
</ul>
<center><img src="../assets/img/posts/20221120/69.jpg" /></center>

<ul>
  <li>具体形式:</li>
</ul>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">返回值类型</span> <span class="k">operator</span> <span class="err">运算符</span><span class="p">(</span><span class="err">形参表</span><span class="p">){</span>

<span class="p">}</span>
</code></pre></div></div>

<ul>
  <li>运算符重载实例:</li>
</ul>
<center><img src="../assets/img/posts/20221120/70.png" /></center>

<ul>
  <li>运算符重载作为不同函数时参数的个数不一样，作为成员函数时，不用包含自身</li>
</ul>

<h3 id="242-赋值运算符的重载">2.4.2. 赋值运算符的重载</h3>
<ul>
  <li>赋值运算符=的重载，相当于一种方便的初始化手段，只能作为成员函数重载</li>
</ul>
<center><img src="../assets/img/posts/20221120/71.jpg" /></center>

<ul>
  <li>一个例子:</li>
</ul>
<center><img src="../assets/img/posts/20221120/72.png" /></center>

<ul>
  <li>深拷贝和浅拷贝</li>
</ul>
<center><img src="../assets/img/posts/20221120/73.png" /></center>
<center><img src="../assets/img/posts/20221120/74.jpg" /></center>

<ul>
  <li>解决方法:</li>
</ul>
<center><img src="../assets/img/posts/20221120/75.jpg" /></center>

<ul>
  <li>但是还会存在问题:</li>
</ul>
<center><img src="../assets/img/posts/20221120/76.jpg" /></center>

<ul>
  <li>为什么赋值运算符的返回值类型是<code class="language-plaintext highlighter-rouge">String &amp;</code>？</li>
</ul>
<center><img src="../assets/img/posts/20221120/77.jpg" /></center>

<ul>
  <li>复制构造函数:</li>
</ul>
<center><img src="../assets/img/posts/20221120/78.jpg" /></center>

<h3 id="243-运算符重载为友元">2.4.3. 运算符重载为友元</h3>
<ul>
  <li>有时候会出现比较特殊的情况，需要将运算符重载函数写在类外，同时还需要访问类的私有成员，这时候就可以让友元函数发挥作用</li>
</ul>
<center><img src="../assets/img/posts/20221120/79.jpg" /></center>
<center><img src="../assets/img/posts/20221120/80.jpg" /></center>
<center><img src="../assets/img/posts/20221120/81.jpg" /></center>

<h3 id="244-可变长数组的实现">2.4.4. 可变长数组的实现</h3>
<ul>
  <li>希望这个可变长数组实现的功能:</li>
</ul>
<center><img src="../assets/img/posts/20221120/82.png" /></center>

<ul>
  <li>因为我们希望实现动态分配内存来存储数组元素，那么需要一个指针成员变量</li>
  <li>取下标的操作<code class="language-plaintext highlighter-rouge">[]</code>也可以重载:</li>
</ul>
<center><img src="../assets/img/posts/20221120/83.jpg" /></center>

<ul>
  <li>构造函数的实现:</li>
</ul>
<center><img src="../assets/img/posts/20221120/84.png" /></center>
<ul>
  <li>析构函数和赋值运算符重载:</li>
</ul>
<center><img src="../assets/img/posts/20221120/85.png" /></center>
<center><img src="../assets/img/posts/20221120/86.jpg" /></center>

<ul>
  <li><code class="language-plaintext highlighter-rouge">push_back()</code>比较麻烦的实现: 每次重新分配内存空间</li>
</ul>
<center><img src="../assets/img/posts/20221120/87.jpg" /></center>

<h3 id="245-流插入运算符的重载">2.4.5. 流插入运算符的重载</h3>
<ul>
  <li><code class="language-plaintext highlighter-rouge">cout</code>是ostream类的对象</li>
</ul>
<center><img src="../assets/img/posts/20221120/88.jpg" /></center>

<ul>
  <li>为了实现<code class="language-plaintext highlighter-rouge">cout&lt;&lt;5&lt;&lt;'this'</code>这样连续输出的功能，重载<code class="language-plaintext highlighter-rouge">&lt;&lt;</code>时返回值为<code class="language-plaintext highlighter-rouge">ostream &amp;</code>:</li>
</ul>
<center><img src="../assets/img/posts/20221120/89.png" /></center>

<ul>
  <li>一个例子:</li>
</ul>
<center><img src="../assets/img/posts/20221120/90.jpg" /></center>
<center><img src="../assets/img/posts/20221120/91.jpg" /></center>

<ul>
  <li>重载<code class="language-plaintext highlighter-rouge">&gt;&gt;</code>运算符:</li>
</ul>
<center><img src="../assets/img/posts/20221120/92.jpg" /></center>
<center><img src="../assets/img/posts/20221120/93.jpg" /></center>
<center><img src="../assets/img/posts/20221120/94.jpg" /></center>]]></content><author><name>Quehry</name></author><category term="course" /><category term="notes" /><summary type="html"><![CDATA[郭炜网课，C++的面向对象的程序设计]]></summary></entry><entry><title type="html">Bert</title><link href="http://localhost:4000/Bert.html" rel="alternate" type="text/html" title="Bert" /><published>2022-11-17T00:00:00+08:00</published><updated>2022-11-17T00:00:00+08:00</updated><id>http://localhost:4000/Bert</id><content type="html" xml:base="http://localhost:4000/Bert.html"><![CDATA[<h1 id="1-bert简介">1. Bert简介</h1>
<p>BERT是GOOGLE团队于2018年提出的模型，在NLP领域里有着举足轻重的地位，BERT开创了NLP领域的预训练时代。BERT其实就是Transformer的编码器。BERT的双向性体现在MLM任务上，该任务可以让模型在得到全局的信息之后再去做预测</p>
<ul>
  <li><a href="https://arxiv.org/abs/1810.04805" target="_blank">论文地址</a></li>
  <li><a href="https://www.bilibili.com/video/BV1PL411M7eQ/?spm_id_from=333.999.0.0&amp;vd_source=64c99329fc39a0e3f42825a4c837e2a5" target="_blank">论文精度</a></li>
  <li><a href="https://github.com/google-research/bert" target="_blank">开源代码</a></li>
</ul>

<h1 id="2-相关工作">2. 相关工作</h1>
<p>在BERT之前，将预训练的自然语言表示应用到下游任务有两种方法，一种是feature-based，代表模型就是ELMo，另一种是finetune，代表模型是GPT。这两种模型都是基于语言模型实现损失函数，也就是单向的，Bert提出了用掩码语言模型来做损失函数，这样既可以学到从左往右的信息，也可以学到从右往左的信息</p>

<h1 id="3-模型架构">3. 模型架构</h1>
<p>模型主体架构就是Transformer的编码器</p>

<h2 id="31-输入表示">3.1. 输入表示</h2>
<p>由于BERT预训练的数据集很大，所以BERT的采用WordPiece的方法来获得词元，WordPiece常见的方法有BPE，这样处理后有词表大小为30000。除此之外，BERT还有特殊的词元&lt;cls&gt;和&lt;sep&gt;，分别表示序列的整体信息和序列对的信息，嵌入层:</p>

<center><img src="../assets/img/posts/20221117/1.jpg" /></center>

<p>Segment Embedding就是句段的信息，如果有两个句子，那么输入为0或者1</p>

<h2 id="32-预训练的task">3.2. 预训练的Task</h2>
<p>预训练任务有两个，掩码语言模型(MLM)和下一句预测(NSP)，MLM主要为了学习词元级别的信息，NSP为了学习序列对的信息，主要为了服务QA等任务。掩码语言模型就是随机替换或者掩蔽某一词元，具体来说，选择15%的词元来做掩蔽，10%的几率不替换，10%的几率替换成随机词元，80%的几率替换成[Mask]词元。</p>

<h1 id="4-预训练数据集和实验">4. 预训练数据集和实验</h1>
<p>预训练数据集为: BooksCorpus和English Wikipedia</p>

<p>做了很多很多实验，刷了很多榜，GLUE，SQuAD，SWAG，针对不同的问题，需要给模型加上输出层做微调，其中很多任务都使用特殊字符&lt;cls&gt;来做预测</p>

<p>Bert-Base有110M参数，Bert-Large有340M个参数</p>]]></content><author><name>Quehry</name></author><category term="Bert" /><category term="Paper Reading" /><category term="NLP" /><summary type="html"><![CDATA[notes for Bert]]></summary></entry><entry><title type="html">Transformer</title><link href="http://localhost:4000/Transformer.html" rel="alternate" type="text/html" title="Transformer" /><published>2022-11-15T00:00:00+08:00</published><updated>2022-11-15T00:00:00+08:00</updated><id>http://localhost:4000/Transformer</id><content type="html" xml:base="http://localhost:4000/Transformer.html"><![CDATA[<h1 id="1-transformer简介">1. Transformer简介</h1>
<p>2017年Google团队提出的完全基于attention机制的神经网络结构，是一种全新的网络架构，影响力十分大，后续的BERT, ViT等多个熟知的预训练模型，相关链接:</p>
<ul>
  <li><a href="https://arxiv.org/abs/1706.03762" target="_blank">论文地址</a></li>
  <li><a href="https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.999.0.0&amp;vd_source=64c99329fc39a0e3f42825a4c837e2a5" target="_blank">论文精读</a></li>
  <li><a href="https://github.com/tensorflow/tensor2tensor" target="_blank">代码</a></li>
</ul>

<h1 id="2-introduction">2. Introduction</h1>
<p>最开始Transformer是为了解决<strong>机器翻译</strong>的任务，Transformer没出来前，解决机器翻译主要的手段是RNN和Encoder-Decoder架构，比如说Seq2Seq。RNN的缺点是依赖时间顺序，第一个方面是并行度比较低，第二个方面是如果时序比较长，那么前期的时序信息在后期可能会丢掉。Transformer的特点有: 自注意力、多头注意力。</p>

<h1 id="3-模型架构">3. 模型架构</h1>
<p>模型预览图:</p>

<center><img src="../assets/img/posts/20221115/1.jpg" /></center>

<p>Transformer时序维度始终是512，BatchNorm和LayerNorm的区别: BN是针对一个batch的特征来做平均和方差，LN是针对batch中一个样本的所有特征做均值和方差。一般的Transformer能接受的序列长度最好不超过512，超过这个长度可能会导致硬件算不动</p>

<h2 id="31-注意力">3.1. 注意力</h2>
<p>注意力有三要素，query、key、value。对于每个query，计算与所有其他key的compatibility作为value的权重。Transformer使用的计算compatibility的方式是Scaled Dot-Product Attention，也是应用最多的计算方式:</p>

<p>
\begin{equation}
\text{Attention}(Q,K,V)=softmax(\frac{QK^T}{d_k})V
\end{equation}
</p>

<p>结合图片理解:</p>
<center><img src="../assets/img/posts/20221115/2.jpg" /></center>

<p>讲道理来说，对于时间步t的查询$q_t$而言，它不应该能看到时间步t后的序列，但是在计算attention的时候却可以看到所有的序列，所以需要有mask来保证序列在预测过程中看不见时间步t之后的序列</p>

<h2 id="32-多头注意力">3.2. 多头注意力</h2>
<p>多头注意力的出发点是让高维的序列能有h次学习机会，这样有机会能学习到更多的信息，假设有h个头，那么维度就需要投影到512/h，然后再做注意力，然后将h个头concatenate，这样就可以实现多头注意力</p>
<center><img src="../assets/img/posts/20221115/3.jpg" /></center>

<p>在decoder里，query是当前的输入，k和v都是encoder的输出</p>

<h2 id="33-position-wise-feed-forward-networks">3.3. Position-wise Feed-Forward Networks</h2>
<p>位置前馈神经网络，其实就是一个基于每个时间步的MLP，由于Transformer里每个时间步都做了attention，所以它包含了序列的信息，所以可以直接针对每个时间步做MLP</p>

<h2 id="34-embedding-and-softmax">3.4. Embedding and softmax</h2>
<p>encoder的embedding层和decoder的embedding以及decoder中softmax前的线性层是共享权重的，其中embedding层的权重需要乘以&amp;\sqrt{d_{model}}&amp;(为了保证数值相似)。embedding层最大的作用就是将vocab_size映射到模型的维度</p>

<h2 id="35-positional-encoding">3.5. Positional Encoding</h2>
<p>位置编码，因为注意力机制其实是没有考虑序列的位置信息的，它计算的是query和key的距离，所以在输入模型前需要加上位置编码来保证序列的每一步都包含其位置信息</p>

<p>
\begin{equation}
\begin{aligned}
\text{PE}_{(pos,2i)} &amp; = sin(pos/10000^{2i/d_{model}}) \\
\text{PE}_{(pos,2i+1)} &amp; = cos(pos/10000^{2i/d_{model}})
\end{aligned}
\end{equation}
</p>

<p>Transformer的位置编码的做法是利用sin和cos对不同的时间步进行编码，也就是说Transformer的做法是不训练位置编码，作者的解释是相较于训练的位置编码，两者结果相似。</p>

<h1 id="4-数据集和实验">4. 数据集和实验</h1>
<p>Transformer解决的是机器翻译任务，数据集为WMT 2014 English-German dataset，分词的时候采取BPE，并且英语和德语共用一个词库，这样保证了encoder和decoer的embedding层一致</p>

<p>学习率的设置:</p>
<p>
\begin{equation}
lrate=d_{model}^{-0.5}\cdot min(step\_num^{-0.5}, step\_num\cdot warmup\_steps^{-1.5})
\end{equation}
</p>

<p>技巧方面使用了很多的dropout和label smoothing，label smoothing就是让softmax去逼近设置的$\epsilon_{ls}$。</p>

<p>实验结果</p>
<center><img src="../assets/img/posts/20221115/4.jpg" /></center>]]></content><author><name>Quehry</name></author><category term="Transformer" /><category term="Paper Reading" /><summary type="html"><![CDATA[notes for Transformer]]></summary></entry><entry><title type="html">Vision Language Model</title><link href="http://localhost:4000/Vision-Language-Model.html" rel="alternate" type="text/html" title="Vision Language Model" /><published>2022-11-14T00:00:00+08:00</published><updated>2022-11-14T00:00:00+08:00</updated><id>http://localhost:4000/Vision-Language-Model</id><content type="html" xml:base="http://localhost:4000/Vision-Language-Model.html"><![CDATA[<h1 id="相关链接">相关链接</h1>
<ul>
  <li><a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#no-training" target="_blank">lils blog</a></li>
  <li><a href="https://theaisummer.com/vision-language-models/#vision-language-tasks" target="_blank">AI Summer</a></li>
</ul>

<h1 id="vlm">VLM</h1>
<p>VLM，即vision language model，旨在用语言模型获得视觉信息。lilian将VLM分为了四种，分别是:</p>

<ol>
  <li>利用嵌入层获得图片特征，然后与词元特征聚合后一起训练，代表性的模型有VisualBERT、SimVLM和CM3</li>
  <li>将训练好的图片嵌入层直接用于模型，这些图片嵌入层是frozen的，即整体模型在训练时不改变图片嵌入层的权重，代表性的模型有CLIP</li>
  <li>利用注意力机制将视觉信息融入到语言模型中，代表模型有VisualGPT，VC-GPT，MERLOT，Flamingo，Coca</li>
  <li>直接combine视觉和语言模型，不加以训练，代表性模型有MAGiC，PICa，Socratic Models</li>
</ol>

<h1 id="任务">任务</h1>
<p>VLM能实现的任务可以分为三类:</p>
<ol>
  <li>生成任务:
    <ul>
      <li>Visual QA: 给一张图片和一个问题，模型根据图片信息返回答案</li>
      <li>Visual Captioning: 给定图片，生成字幕</li>
      <li>Visual Commonsense Reasoning: 给定图片，推断出图片的common-sense information</li>
      <li>Visual Generation: 给定文本输入，生成图片</li>
    </ul>
  </li>
  <li>分类任务:
    <ul>
      <li>Multimodal Affective Computing: 多模态版本的情感分析</li>
      <li>Natural Language for Visual Reasoning: 给定一张图片和一段陈述，判断陈述是否正确</li>
    </ul>
  </li>
  <li>找回任务(retrieval task):
    <ul>
      <li>Visual Retrieval: 通过文本描述恢复图片</li>
      <li>Vision-Language Navigation: 通过自然语言的指令来对agent进行导航</li>
      <li>Multimodal Machine Translation: 将一种语言翻译成另一种语言，附带视觉信息</li>
    </ul>
  </li>
</ol>

<h1 id="bert-like架构">BERT-like架构</h1>
<p>鉴于BERT在NLP领域的兴起，不同模态领域里也出现了BERT-like的架构，代表性的模型有VisualBERT，ViLBERT，PixelBERT等</p>

<h1 id="contrastive-learning">contrastive learning</h1>
<p>自从CLIP出现后，大家发现用对比学习的方法能很好地连接起vision和language的信息，类似的模型有ALIGN和FLORENCE</p>

<h1 id="vlm-论文">VLM 论文</h1>
<ul>
  <li><a href="https://paperswithcode.com/methods/category/vision-and-language-pre-trained-models" target="_blank">paperswithcode</a></li>
</ul>

<h1 id="vlm-最新的论文">VLM 最新的论文</h1>]]></content><author><name>Quehry</name></author><category term="overview" /><summary type="html"><![CDATA[an overview in domain VLM]]></summary></entry><entry><title type="html">Vision Transformer</title><link href="http://localhost:4000/ViT.html" rel="alternate" type="text/html" title="Vision Transformer" /><published>2022-11-14T00:00:00+08:00</published><updated>2022-11-14T00:00:00+08:00</updated><id>http://localhost:4000/ViT</id><content type="html" xml:base="http://localhost:4000/ViT.html"><![CDATA[<h1 id="1-vit简介">1. ViT简介</h1>
<p>ViT的全称是Vision Transformer，模型最大的特点就是把NLP领域的Transformer迁移到CV领域，模型在图像识别等多个CV任务上表现超越了卷积神经网络，相关链接:</p>

<ul>
  <li><a href="https://arxiv.org/abs/2010.11929" target="_blank">论文地址</a></li>
  <li><a href="https://www.bilibili.com/video/BV15P4y137jb/?spm_id_from=333.788&amp;vd_source=64c99329fc39a0e3f42825a4c837e2a5" target="_blank">论文精读视频</a></li>
  <li><a href="https://github.com/google-research/vision_transformer" target="_blank">开源代码</a></li>
</ul>

<h1 id="2-introduction">2. Introduction</h1>
<p>限制Transformer在CV领域发挥的一个要素就是序列长度的问题，如果把图像的每个像素点看成token，那么对于一张中分辨率的图片224*224而言，序列长度为50k，Bert模型处理的序列长度只有512，所以将Transformer应用到CV领域的一个难点就是序列长度过长。ViT于是提出了将图片分割成16*16的patch的做法，这样就可以大大减少序列的长度，不改变Transformer架构的情况下直接应用，取得了很好的结果，说明了transformer确实能在CV领域能取得很好的效果。</p>

<h1 id="3-模型">3. 模型</h1>
<p>模型预览图:</p>

<center><img src="../assets/img/posts/20221114/1.jpg" /></center>

<p>为了缩短序列的长度，作者将图片分为了很多个patch，patch的大小可以是16*16，每个patch的表示为每个像素点的灰度值排列，那么一张图片patch的形状为num_patchs*768，其中768=16*16*3。得到patch的表示后，输入线性投影层(就是一个线性层，形状可以是768*768)。为了和transformer保持一致，ViT也采用了位置编码和特殊编码&lt;cls&gt;的使用，位置编码采用的是1D位置编码，位置编码和特殊编码都可学习，patch的线性投影表示和位置编码是直接加在一起，这样便得到了图像的token，其余步骤和Bert类似，ViT的训练是有监督训练，用特殊编码&lt;cls&gt;来做预测</p>

<h1 id="4实验和数据集">4.实验和数据集</h1>
<p>数据集使用了ImageNet-1k、ImageNet-21k和Google自家的JFT数据集</p>

<p>和BERT一样，ViT也根据模型使用的参数和patch的大小不同，分为了以下几种:</p>
<center><img src="../assets/img/posts/20221114/2.jpg" /></center>
<p>比如ViT-L/16代表Large ViT with patch size 16*16</p>

<p>实验结果</p>
<center><img src="../assets/img/posts/20221114/3.jpg" /></center>
<p>实验的metrics是预训练模型微调后的accuracy</p>

<center><img src="../assets/img/posts/20221114/4.jpg" /></center>
<p>这张图表明ViT在数据集大的情况下会表现更好</p>]]></content><author><name>Quehry</name></author><category term="Transformer" /><category term="Paper Reading" /><summary type="html"><![CDATA[notes for ViT]]></summary></entry><entry><title type="html">ROX learning</title><link href="http://localhost:4000/ROS.html" rel="alternate" type="text/html" title="ROX learning" /><published>2022-10-30T00:00:00+08:00</published><updated>2022-10-30T00:00:00+08:00</updated><id>http://localhost:4000/ROS</id><content type="html" xml:base="http://localhost:4000/ROS.html"><![CDATA[]]></content><author><name>Quehry</name></author><category term="notes" /><summary type="html"><![CDATA[学习ROS]]></summary></entry><entry><title type="html">linux command</title><link href="http://localhost:4000/Linux-Command.html" rel="alternate" type="text/html" title="linux command" /><published>2022-10-24T00:00:00+08:00</published><updated>2022-10-24T00:00:00+08:00</updated><id>http://localhost:4000/Linux-Command</id><content type="html" xml:base="http://localhost:4000/Linux-Command.html"><![CDATA[<h1 id="0-简介">0. 简介</h1>
<p>该博客主要记录了linux常用命令，便于后续使用时查找命令，其余的一些博客:</p>
<ul>
  <li><a href="https://docs.rockylinux.org/books/admin_guide/03-commands" target="_blank">linux command documentation</a></li>
  <li><a href="https://blog.csdn.net/weixin_49851451/article/details/125821580?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-125821580.nonecase&amp;spm=1018.2226.3001.4187" target="_blank">中文博客</a></li>
  <li><a href="https://blog.csdn.net/xulong_08/article/details/81463054" target="_blank">linux相关</a></li>
</ul>

<h1 id="1-zip和unzip">1. zip和unzip</h1>
<ul>
  <li>压缩:
    <pre><code class="language-linux">zip [parameters] target.zip source
</code></pre>
    <p>常用参数有:</p>
    <ol>
      <li>-r 将指定的目录下所有的文件和子目录一并压缩，一般就选择当前目录的文件名即可</li>
      <li>-x 压缩时排除某一文件夹或文件，注意要加引号，因为是字符串，比如<code class="language-plaintext highlighter-rouge">-x "/root/autodl-tmp/txt2img_algorithms/datasets/*"</code></li>
    </ol>
  </li>
  <li>解压
    <pre><code class="language-linux">unzip [parameters] source.zip
</code></pre>
    <p>常用参数有:</p>
    <ol>
      <li>-l 查看zip文件中包含什么</li>
      <li>-t 检查压缩文件是否有问题</li>
      <li>-d 压缩到指定目录</li>
    </ol>
  </li>
</ul>]]></content><author><name>Quehry</name></author><category term="notes" /><summary type="html"><![CDATA[记录Linux常见命令]]></summary></entry><entry><title type="html">算法练习心得</title><link href="http://localhost:4000/Algorithm-Practice.html" rel="alternate" type="text/html" title="算法练习心得" /><published>2022-10-21T00:00:00+08:00</published><updated>2022-10-21T00:00:00+08:00</updated><id>http://localhost:4000/Algorithm-Practice</id><content type="html" xml:base="http://localhost:4000/Algorithm-Practice.html"><![CDATA[<h1 id="简介">简介</h1>
<p>这篇博客记录了做算法题过程的一些心得，目前的想法是先把<a href="http://cxsjsxmooc.openjudge.cn/" target="_blank">网课的习题</a>做完提交完，然后学习C语言面向对象的知识，然后去刷题平台上去刷题</p>

<h1 id="openjudge">Openjudge</h1>
<ul>
  <li><a href="http://cxsjsxmooc.openjudge.cn/2022t2fall/" target="_blank">题目链接</a></li>
  <li>iostream头文件包含了cin与cout</li>
  <li><code class="language-plaintext highlighter-rouge">int &amp;a=b</code>中&amp;的用法是C++中的引用用法，变量的引用就是变量的别名，这样就可以实现在函数中向实参传递值</li>
  <li><code class="language-plaintext highlighter-rouge">~</code>的用法之一是按位取反运算，即数的每一位都取反，<code class="language-plaintext highlighter-rouge">^</code>的用法之一是按位异或运算</li>
  <li>stdio.h是C语言的标准库，包含了C语言常用的输入输出函数，比如文件的读写函数fopen/fclose，格式化输入输出函数scanf/printf，为了适配C++，变成了cstdio</li>
  <li>strlen()是string.h的一个函数，可以返回字符串的长度，原理是读到结束字符<strong>\0</strong>后停止</li>
  <li>字符串变量可以通过cin读取键盘输入的字符串，对于由0/1组成的字符串而言，可以考虑用整型存储，然后通过按位运算对整型的每一个bit进行操作</li>
  <li><code class="language-plaintext highlighter-rouge">memcpy()</code>是cstring中的一个函数，使用时需要引用cstring头文件，memcpy(char * a, char * b, int c)表示将字符串b拷贝到字符串a，c表示字符串的大小</li>
  <li>可以将递归嵌套在循环中完成穷举</li>
  <li>全排列的实现有固定的套路: 对排列的每一位进行循环，如果有用过的元素，就标记一下，在下次选取的时候不考虑该元素，这样按着顺序选元素就可以实现全排列，具体见<a href="http://cxsjsxmooc.openjudge.cn/2022t2fall/003/" target="_blank">问题3</a>，全排列也是一个穷举的过程</li>
  <li>定义字符串或者一维数组时，最好还是把元素个数给大一点</li>
  <li>在循环中可以通过bool值来控制某一个元素取还是不取，比如004中的加号可以由布尔值控制</li>
  <li><code class="language-plaintext highlighter-rouge">cin.peek()</code>可以读取当前输入的字符且不取走，<code class="language-plaintext highlighter-rouge">cin.get()</code>可以读取当前输入的字符并且取走</li>
  <li><code class="language-plaintext highlighter-rouge">n = scanf("%c", &amp;c)</code>，如果当前输入停止，则n=EOF</li>
  <li><code class="language-plaintext highlighter-rouge">while(cin &gt;&gt; a)</code>可以实现对某种类型的变量a的持续输入</li>
  <li>用二分法解决问题时，端点尽量最优化设计，并且除了取中点外，还可以通过左端点逐步加1/右端点逐步减1来寻找最优点</li>
  <li>关于PI的数值，可以通过acos(-1.0)取值</li>
  <li><code class="language-plaintext highlighter-rouge">#include&lt;iomanip&gt;</code>引用的是iomanip库，主要用于控制输入输出流的精度，比如<code class="language-plaintext highlighter-rouge">setprecision(int n)</code>用于控制输出流浮点数的精度，整数n代表有效数字个数(四舍五入)，使用<code class="language-plaintext highlighter-rouge">cout&lt;&lt;fixed&lt;&lt;setprecision(n)</code>可以保留浮点数的n位有效小数</li>
  <li>分治，或者说归并的思想，就是对每一步分而治之，确定每一小步的操作后，递归到归并的最小单元进行操作，常见的分治算法为归并排序与快速排序</li>
  <li>利用<code class="language-plaintext highlighter-rouge">#include&lt;fstream&gt;</code>可以进行file的读写，步骤为: 实例化<code class="language-plaintext highlighter-rouge">ifstream infile</code>-&gt;打开文件<code class="language-plaintext highlighter-rouge">infile.open("in.txt")</code>-&gt;读文件<code class="language-plaintext highlighter-rouge">infile.getline(...)</code>-&gt;关闭文件<code class="language-plaintext highlighter-rouge">infile.close()</code></li>
  <li><code class="language-plaintext highlighter-rouge">#include &lt;algorithm&gt;</code>中的方法<code class="language-plaintext highlighter-rouge">max_element</code>可以返回数组的最大值所在的地址，<code class="language-plaintext highlighter-rouge">*max_element</code>可以返回数组的最大值</li>
  <li><code class="language-plaintext highlighter-rouge">scanf("%s", a)</code>可以直接把字符串读入a</li>
  <li><code class="language-plaintext highlighter-rouge">char *</code>比<code class="language-plaintext highlighter-rouge">int *</code>占用内存少</li>
  <li>为了节省内存，可以试试将数组降维</li>
  <li>结构体的构造函数和普通函数的定义类似，只不过函数名变成了结构体的名称</li>
  <li>直接对字符<code class="language-plaintext highlighter-rouge">'0'</code>进行转型不会得到0</li>
  <li><code class="language-plaintext highlighter-rouge">memset(array, int, size)</code>一般用来对数组进行初始化，初始化的值为int</li>
  <li>结构体中重载运算符: <code class="language-plaintext highlighter-rouge">type operator+(...){}</code>其中operator和运算符一起出现可以视为函数名，type为返回值类型</li>
  <li>更改结构体元素前三思</li>
  <li><code class="language-plaintext highlighter-rouge">ostream &amp; operator &lt;&lt;(ostream &amp; o, ...){}</code>可以让自己定义的结构体能用cout输出</li>
  <li>int的取值范围为-2147483648 ~ 2147483647，long就是int，long long的取值范围为-9223372036854775807 ~ 9223372036854775807</li>
  <li>动规的方向很重要，有时候从0开始递归简单，有时候从最后递归简单</li>
  <li><code class="language-plaintext highlighter-rouge">memset()</code>是<code class="language-plaintext highlighter-rouge">#include &lt;cstring&gt;</code>的方法</li>
  <li>深搜可以和动规结合</li>
  <li>针对路径问题，在发现某一条路走不通后需要恢复该节点的访问状态，以便其他路径能访问该节点</li>
  <li>队列的使用方法: <code class="language-plaintext highlighter-rouge">queue&lt;T&gt; q</code>来初始化队列q，<code class="language-plaintext highlighter-rouge">q.front()</code>来得到队列的头且不删除，<code class="language-plaintext highlighter-rouge">q.pop()</code>删除队列的头，<code class="language-plaintext highlighter-rouge">q.push(T)</code>向队列q的队尾添加元素</li>
  <li>考虑一个节点的访问状态时一定要考虑仔细</li>
  <li>结构体的构造函数:</li>
</ul>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">struct</span> <span class="nc">node</span><span class="p">{</span>
    <span class="kt">int</span> <span class="n">data</span><span class="p">;</span>
    <span class="n">string</span> <span class="n">str</span><span class="p">;</span>
    <span class="kt">char</span> <span class="n">x</span><span class="p">;</span>
    <span class="c1">// 自定义构造函数</span>
    <span class="kt">void</span> <span class="n">init</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">,</span> <span class="n">string</span> <span class="n">b</span><span class="p">,</span> <span class="kt">char</span> <span class="n">c</span><span class="p">){</span>
        <span class="k">this</span><span class="o">-&gt;</span><span class="n">data</span> <span class="o">=</span> <span class="n">a</span><span class="p">;</span>
        <span class="k">this</span><span class="o">-&gt;</span><span class="n">str</span> <span class="o">=</span> <span class="n">b</span><span class="p">;</span>
        <span class="k">this</span><span class="o">-&gt;</span><span class="n">x</span> <span class="o">=</span> <span class="n">c</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="c1">// 无参数构造函数, 这里顺序随意, 初始化时须以定义顺序初始化, 建议这里也以定义顺序书写</span>
    <span class="n">node</span><span class="p">()</span><span class="o">:</span> <span class="n">data</span><span class="p">(),</span> <span class="n">str</span><span class="p">(),</span> <span class="n">x</span><span class="p">(){}</span>
    <span class="c1">// 有参数构造函数</span>
    <span class="c1">// 在建立结构体数组时, 如果只写带参数的构造函数将会出现数组无法初始化的错误, 须先写无参数构造函数</span>
    <span class="n">node</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">,</span> <span class="n">string</span> <span class="n">b</span><span class="p">,</span> <span class="kt">char</span> <span class="n">c</span><span class="p">)</span> <span class="o">:</span><span class="n">data</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">str</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="n">x</span><span class="p">(</span><span class="n">c</span><span class="p">){}</span>
<span class="p">};</span>
</code></pre></div></div>

<ul>
  <li>考虑深搜问题的状态时，最好考虑一下如此设置的话是否方便剪枝</li>
  <li>在深搜的过程中可以根据终点和当前节点的位置关系来改变深搜的顺序</li>
  <li>考虑剪枝时，可以考虑多种剪枝方法一起放到dfs中尝试</li>
  <li>广搜其实就是给节点分层了，从第一层开始按层搜索，这样只要找到可行的解就是最优解，具体来说就是利用队列实现，每加入一个新节点就往队尾加入它的子节点，这样便可以实现逐层搜索</li>
  <li>如果需要输出广搜过程的节点，那么需要用自定义的数组来实现，用头和尾指数来实现数组的添加和指数的移动，每个节点包含上一个节点的指数</li>
  <li>用int或者bool数组来读取cin时，如果没有空格，可能会读取到不想要的结果</li>
  <li>如果有需要用数字代表字符串的情况，可以考虑用switch来实现，比如下面的情况:</li>
</ul>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">switch</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">mov</span><span class="p">)</span> 
<span class="p">{</span>
	<span class="k">case</span> <span class="mi">1</span><span class="p">:</span> <span class="c1">//FILL</span>
		<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"FILL("</span> <span class="o">&lt;&lt;</span> <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">src</span> <span class="o">&lt;&lt;</span> <span class="s">")"</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
		<span class="k">break</span><span class="p">;</span>
	<span class="k">case</span> <span class="mi">2</span><span class="p">:</span> <span class="c1">//DROP	</span>
		<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"DROP("</span> <span class="o">&lt;&lt;</span> <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">src</span> <span class="o">&lt;&lt;</span> <span class="s">")"</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
		<span class="k">break</span><span class="p">;</span>
	<span class="k">case</span> <span class="mi">3</span><span class="p">:</span>
		<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"POUR("</span> <span class="o">&lt;&lt;</span> <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">src</span> <span class="o">&lt;&lt;</span> <span class="s">","</span> <span class="o">&lt;&lt;</span> <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">dest</span> <span class="o">&lt;&lt;</span> <span class="s">")"</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
		<span class="k">break</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<ul>
  <li>优先队列的用法:
    <ul>
      <li>引用: <code class="language-plaintext highlighter-rouge">#include &lt;queue&gt;</code></li>
      <li>实例化: <code class="language-plaintext highlighter-rouge">priority_queue&lt;T&gt; q</code>, 默认为降序排列，数据类型就是T，如果是自定义结构的话，需要重载小于运算符</li>
      <li>方法: 和队列类似，.pop()是删除头部元素，.push()是加入元素</li>
    </ul>
  </li>
  <li>贪心算法就是每一步都考虑最优的情况</li>
  <li>有时候基本上一摸一样的想法和代码，但是我的代码在openjudge跑不通，不知道为什么</li>
</ul>

<h1 id="leecode热题hot100及oop习题">leecode热题HOT100及OOP习题</h1>
<ul>
  <li><a href="https://leetcode.cn/problem-list/2cktkvj/" target="_blank">网页链接</a></li>
  <li>动规的时候如果要使用滑动窗口来减少内存使用量，一定要注意是顺序遍历还是逆序遍历比较合理，一般来说逆序遍历会比较合理</li>
  <li>如果不规定数组的大小的时候，可以这么初始化数组<code class="language-plaintext highlighter-rouge">int[]={1,2,3}</code></li>
  <li>一个问题如果从正面不好进行搜索的话，可以尝试从反方向进行搜索，比如戳气球的问题，从正面开始搜索不好弄，因为每次戳气球都会改变气球的相对位置，但如果从反方向开始搜索，看成填充气球，那么会简单很多</li>
  <li>动规的循环方向很重要</li>
  <li><code class="language-plaintext highlighter-rouge">cin.getline(字符数组名,字符个数,结束字符)</code>可以读取长度为字符个数的输入到字符数组，如果结束字符缺省，则默认为<code class="language-plaintext highlighter-rouge">\0</code></li>
  <li><code class="language-plaintext highlighter-rouge">strtok(char s[],const char * delim)</code>可以将字符串s进行分解，delim为分隔符字符，如果传入的是一个字符串，那么字符串的每一个字符均为分隔符，返回一个分解完毕的字符串，如果要再次调用，需要将s设置为<code class="language-plaintext highlighter-rouge">NULL</code></li>
  <li><code class="language-plaintext highlighter-rouge">char * strchr(const char * s, char c)</code>这个函数表示在字符串s中查找字符c，返回字符c第一次出现在字符串s的位置，返回值指向这个位置，如果找不到，就返回NULL</li>
  <li><code class="language-plaintext highlighter-rouge">sscanf</code>通常用来解析并转换字符串</li>
  <li>类的构造函数一定要加分号</li>
  <li>可以通过<code class="language-plaintext highlighter-rouge">char c : str</code>的方式遍历一个字符串，常用于for循环中</li>
  <li><code class="language-plaintext highlighter-rouge">unordered_set</code>是c++ 11 为STL标准库新添加的容器，称作无序容器或哈希容器，<code class="language-plaintext highlighter-rouge">unordered_set</code>不会对存储的数据进行排序，容器内存储的元素互不相同，添加元素的方法是<code class="language-plaintext highlighter-rouge">insert()</code></li>
  <li>vector中<code class="language-plaintext highlighter-rouge">push_back()</code>与<code class="language-plaintext highlighter-rouge">emplace_back()</code>的关系: 两者都是在向量的尾部添加元素，但是<code class="language-plaintext highlighter-rouge">emplace_back()</code>会调用构造函数原地构造，不会触发复制构造函数和移动构造，更快</li>
  <li><code class="language-plaintext highlighter-rouge">auto</code>是C++语言存储类型，初始化可为任何表达式</li>
  <li><code class="language-plaintext highlighter-rouge">.size()</code>可以返回vector的大小</li>
  <li><code class="language-plaintext highlighter-rouge">.size()</code>也可以返回<code class="language-plaintext highlighter-rouge">string</code>的大小，<code class="language-plaintext highlighter-rouge">str[i]</code>可以访问字符串的第i个字符</li>
  <li><code class="language-plaintext highlighter-rouge">str.substr(a,b)</code>可以返回字符串的子字符串，从a到b，左闭右开</li>
</ul>]]></content><author><name>Quehry</name></author><category term="algorithm" /><summary type="html"><![CDATA[记录刷算法题过程中的心得]]></summary></entry><entry><title type="html">Latent Diffusion Model</title><link href="http://localhost:4000/Latent-Diffusion-Model.html" rel="alternate" type="text/html" title="Latent Diffusion Model" /><published>2022-10-16T00:00:00+08:00</published><updated>2022-10-16T00:00:00+08:00</updated><id>http://localhost:4000/Latent-Diffusion-Model</id><content type="html" xml:base="http://localhost:4000/Latent-Diffusion-Model.html"><![CDATA[<!-- TOC -->

<ul>
  <li><a href="#1-ldm简介">1. LDM简介</a></li>
  <li><a href="#2-模型">2. 模型</a></li>
</ul>

<!-- /TOC -->

<h1 id="1-ldm简介">1. LDM简介</h1>
<p>Latent Diffusion Model(LDM)是Diffusion Model的改进版本。扩散模型相比于之前的生成模型而言，已经能取得非常好的效果，但是扩散模型有个特点，贵。不仅训练贵，而且推理也很贵。为了节省空间和资源，LDM将反向扩散过程在隐空间中进行，而不是之前的逐像素进行反向扩散(因为隐空间的维度比原始图片要小，能让训练和推断变得不那么贵)。LDM与扩散模型最大的区别就是隐空间，模型在github上开源，stable diffusion也是基于latent diffusion进行实现</p>
<ul>
  <li><a href="https://arxiv.org/abs/2112.10752" target="_blank">LDM</a></li>
  <li><a href="https://github.com/CompVis/latent-diffusion" target="_blank">github</a></li>
</ul>

<h1 id="2-模型">2. 模型</h1>
<p>LDM的模型如下图所示:</p>

<center><img src="../assets/img/posts/20221016/2.jpg" /></center>

<ul>
  <li>前向扩散: 输入x首先经过压缩后得到隐空间表达z，正常的前向扩散得到$z_T$</li>
  <li>反向扩散: 隐空间的随机噪声$z_T$经过T步去噪U-Net后得到z，然后z经过解压得到原始大小的图片$\tilde{x}$</li>
</ul>

<p>我们关心的是模型反向扩散的过程，关于图片压缩和解压的模型，作者尝试了VAE和VQVAE，返现VAE的表现稍微好一点，所以选择了VAE作为antoencoder。与一般的扩散模型一样，LDM也可以做条件生成，LDM采取的策略是classifier-free guidance，具体实现方法是将预处理好的y与U-Net的每一个中间表达都应用注意力机制，其中查询是U-Net的中间层表达，key和value是预处理好的y。预处理的encoder与y相关，如果y是文本，那么encoder可以是clip的text encoder，也可以是transformer-based encoder</p>

<center><img src="../assets/img/posts/20221016/3.jpg" /></center>]]></content><author><name>Quehry</name></author><category term="notes" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">扩散模型</title><link href="http://localhost:4000/Diffusion-Model.html" rel="alternate" type="text/html" title="扩散模型" /><published>2022-10-12T00:00:00+08:00</published><updated>2022-10-12T00:00:00+08:00</updated><id>http://localhost:4000/Diffusion-Model</id><content type="html" xml:base="http://localhost:4000/Diffusion-Model.html"><![CDATA[<!-- TOC -->

<ul>
  <li><a href="#1-扩散模型简介">1. 扩散模型简介</a></li>
  <li><a href="#2-模型">2. 模型</a>
    <ul>
      <li><a href="#21-前向扩散">2.1. 前向扩散</a></li>
      <li><a href="#22-反向扩散过程">2.2. 反向扩散过程</a></li>
      <li><a href="#23-损失函数">2.3. 损失函数</a></li>
      <li><a href="#24-ddpm中给出的训练与采样生成过程">2.4. DDPM中给出的训练与采样(生成)过程</a></li>
      <li><a href="#25-小结">2.5. 小结</a></li>
    </ul>
  </li>
  <li><a href="#3-一些技巧和主要网络结构">3. 一些技巧和主要网络结构</a>
    <ul>
      <li><a href="#31-\beta_t和\sigma_\theta的取值">3.1. $\beta_t$和$\Sigma_\theta$的取值</a></li>
      <li><a href="#32-加速扩散模型采样的技巧">3.2. 加速扩散模型采样的技巧</a></li>
      <li><a href="#33-u-net">3.3. U-Net</a></li>
    </ul>
  </li>
  <li><a href="#4-条件生成">4. 条件生成</a>
    <ul>
      <li><a href="#41-classifier-guided-diffusion">4.1. Classifier Guided Diffusion</a></li>
      <li><a href="#42-classifier-free-guidance">4.2. Classifier-Free Guidance</a></li>
      <li><a href="#43-scale-up-generation-resolution-and-quality">4.3. Scale up Generation Resolution and Quality</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h1 id="1-扩散模型简介">1. 扩散模型简介</h1>
<p>扩散模型(Diffusion Model)是深度生成模型中的SOTA，相比于GAN、VAE、Flow-based这些生成模型而言，扩散模型可以取得更好的效果。扩散模型受非平衡热力学启发，它定义了一条多时间步的马尔可夫链来逐步给图片添加噪声，如果时间步够大，最终图片会变成纯噪声，扩散模型的目的是学习反向的扩散过程，也就是输入随机噪声，能返回一张图片，相比于之前提到的各种生成模型而言，扩散模型具有相对固定的学习步骤，同时隐变量维度更高(和输入数据同样的维度)</p>

<center><img src="../assets/img/posts/20221012/2.jpg" /></center>

<p>扩散模型的早在2015年便提出了(<a href="https://arxiv.org/abs/1503.03585" target="_blank">论文链接</a>)，但在当时没有引起广泛的关注，直到2019年<a href="https://arxiv.org/abs/1907.05600" target="_blank">NCSN</a>和2020年<a href="https://arxiv.org/abs/2006.11239" target="_blank">DDPM</a>的出现才将扩散模型引入了新高度，2022年火爆的text2image模型GLIDE、DALLE2、Latent Diffusion、Imagen的相继提出，让扩散模型火出了圈，这篇博客将对扩散模型的前向计算、反向训练、训练、生成步骤及其数学原理做详细的整理，会列出很多数学公式，同时该博客也参考了很多相关资料，这里我一并列出</p>

<ul>
  <li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" target="_blank">Lil blog, 一篇整理相当详尽的博客，也是我主要的参考对象</a></li>
  <li><a href="https://huggingface.co/blog/annotated-diffusion" target="_blank">huggingface的一篇解释简单明了的博客</a></li>
  <li><a href="https://zhuanlan.zhihu.com/p/525106459" target="_blank">知乎上一篇中文博客</a></li>
  <li><a href="https://arxiv.org/abs/2006.11239" target="_blank">DDPM</a></li>
  <li><a href="https://arxiv.org/abs/2209.00796" target="_blank">一篇综述</a></li>
  <li><a href="https://arxiv.org/abs/2102.09672" target="_blank">Improved DDPM</a></li>
  <li><a href="https://arxiv.org/abs/2105.05233" target="_blank">Diffusion Models Beat GANs</a></li>
  <li><a href="https://arxiv.org/abs/2112.10741" target="_blank">GLIDE</a></li>
  <li><a href="https://arxiv.org/abs/2106.15282" target="_blank">Cascaded Diffusion Model</a></li>
</ul>

<h1 id="2-模型">2. 模型</h1>
<h2 id="21-前向扩散">2.1. 前向扩散</h2>
<p>从原始数据分布中采样$x_0$, 假设$x_0\sim q(x)$，前向扩散过程就是在每一个时间步都加上一个高斯噪声，这样就可以从最初的$x_0$生成长度为T的噪声序列$x_1, x_2, x_3,…, x_T$，每一步都用variance schedule$\beta_t$控制，其中$\beta_t\in (0, 1)$，每一步的后验分布(预定义好的)为:</p>

<p>
\begin{equation}
q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I}) \quad
q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) = \prod^T_{t=1} q(\mathbf{x}_t \vert \mathbf{x}_{t-1})
\end{equation}
</p>

<p>这个前向传播的过程中有一个非常好的性质，就是我们可以在任意时间步采样得到$x_t$，为了实现这个技巧，我们需要用到reparameterization技巧(该技巧也在VAE中出现过)，重参数化技巧的本质就是将随机采样的z通过引入高斯噪声$\epsilon$变成确定性的z，也就是上面的$x_t$可以表示为$x_t=\sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}\epsilon$，这样有利于梯度的逆传播，那么我们可以推出以下公式:</p>

<p>
\begin{equation}
\begin{aligned}
\mathbf{x}_t 
&amp;= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1 - \alpha_t}\boldsymbol{\epsilon}_{t-1} \\
&amp;= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\boldsymbol{\epsilon}}_{t-2} \\
&amp;= \dots \\
&amp;= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon} \\
q(\mathbf{x}_t \vert \mathbf{x}_0) &amp;= \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})
\end{aligned}
\end{equation}
</p>

<p>其中$\epsilon_t$都是均值为0，方差为1的高斯噪声，$\alpha_t=1-\beta_t$, $\bar{\alpha_t}=\prod_{i=1}^t\alpha_i$, 注:两个均值相同高斯噪声可以合并成一个高斯噪声，方差为之前方差的平方和开根号，一般来说，$\beta_1&lt;\beta_2&lt;…&lt;\beta_T$</p>

<h2 id="22-反向扩散过程">2.2. 反向扩散过程</h2>

<center><img src="../assets/img/posts/20221012/3.jpg" /></center>

<p>如果我们可以将前向传播的过程反向，那么我们就可以获得后验分布$q(x_{t-1}|x_{t})$，那么我们就可以利用马尔科夫链的性质，输入高斯噪声，然后获得生成的照片，但是，我们无法高效地得到$q(x_{t-1}|x_{t})$，于是我们希望学习出分布$p_\theta$来模拟后验分布$q(x_{t-1}|x_{t})$，由于前向扩散的过程中我们假设后验分布是高斯分布，所以这里我们也假设$p_\theta$是高斯分布，于是我们有:</p>

<p>
\begin{equation}
p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T) \prod^T_{t=1} p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) \quad
p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t))
\end{equation}
</p>

<p>其中分布$p_\theta$中的均值$\mu$和方差$\Sigma$与时间步t和输入$x_t$有关</p>

<p>虽然我们不知道$q(x_{t-1}|x_t)$的分布情况，但是我们可以知道$q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)$的分布情况，推导过程如下:</p>

<p style="font-size: 14px">
\begin{equation}
\begin{aligned}
q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right) &amp;=q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}, \mathbf{x}_0\right) \frac{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_0\right)}{q\left(\mathbf{x}_t \mid \mathbf{x}_0\right)} \\
&amp; \propto \exp \left(-\frac{1}{2}\left(\frac{\left(\mathbf{x}_t-\sqrt{\alpha_t} \mathbf{x}_{t-1}\right)^2}{\beta_t}+\frac{\left(\mathbf{x}_{t-1}-\sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0\right)^2}{1-\bar{\alpha}_{t-1}}-\frac{\left(\mathbf{x}_t-\sqrt{\bar{\alpha}_t} \mathbf{x}_0\right)^2}{1-\bar{\alpha}_t}\right)\right) \\
&amp;=\exp \left(-\frac{1}{2}\left(\frac{\mathbf{x}_t^2-2 \sqrt{\alpha_t} \mathbf{x}_t \mathbf{x}_{t-1}+\alpha_t \mathbf{x}_{t-1}^2}{\beta_t}+\frac{\mathbf{x}_{t-1}^2-2 \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0 \mathbf{x}_{t-1}+\bar{\alpha}_{t-1} \mathbf{x}_0^2}{1-\bar{\alpha}_{t-1}}-\frac{\left(\mathbf{x}_t-\sqrt{\bar{\alpha}_t} \mathbf{x}_0\right)^2}{1-\bar{\alpha}_t}\right)\right) \\
&amp;=\exp \left(-\frac{1}{2}\left(\left(\frac{\alpha_t}{\beta_t}+\frac{1}{1-\bar{\alpha}_{t-1}}\right) \mathbf{x}_{t-1}^2-\left(\frac{2 \sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t+\frac{2 \sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha}_{t-1}} \mathbf{x}_0\right) \mathbf{x}_{t-1}+C\left(\mathbf{x}_t, \mathbf{x}_0\right)\right)\right)
\end{aligned}
\end{equation}
</p>

<p>其中函数$C(x_t, x_0)$与$x_{t-1}$无关，根据上述式子我们可以得出$q(x_{t-1}|x_t,x_0)$满足正态分布，均值和标准差分别为$\tilde{\mu_t}$和$\tilde{\beta_t}$，表达式分别为:</p>

<p>
\begin{equation}
\begin{aligned}
\tilde{\beta}_t 
&amp;= 1/(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}) 
= 1/(\frac{\alpha_t - \bar{\alpha}_t + \beta_t}{\beta_t(1 - \bar{\alpha}_{t-1})})
= \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t \\
\tilde{\boldsymbol{\mu}}_t (\mathbf{x}_t, \mathbf{x}_0)
&amp;= (\frac{\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1} }}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0)/(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}) \\
&amp;= (\frac{\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1} }}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0) \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t \\
&amp;= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \mathbf{x}_0\\
\end{aligned}
\end{equation}
</p>

<p>于是最终可以得到$q(x_{t-1}|x_t,x_0)$:</p>

<p>
\begin{equation}
q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \tilde{\boldsymbol{\mu}}(\mathbf{x}_t, \mathbf{x}_0), \tilde{\beta}_t \mathbf{I})
\end{equation}
</p>

<p>在根据马尔可夫链我们有: $\mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t)$，注意这里的$\epsilon_t$并不是任意的一个噪声，而是让$x_0$变成$x_t$的噪声，那么$\tilde{\mu_t}$可以表示为:</p>

<p>
\begin{equation}
\begin{aligned}
\tilde{\boldsymbol{\mu}}_t
&amp;= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t) \\
&amp;= \frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \Big)
\end{aligned}
\end{equation}
</p>

<h2 id="23-损失函数">2.3. 损失函数</h2>
<p>和VAE类似，也可以用Variational Lower Bound来最大边缘似然函数$p_\theta(x_0)$:</p>

<p>
\begin{equation}
\begin{aligned}
- \log p_\theta(\mathbf{x}_0) 
&amp;\leq - \log p_\theta(\mathbf{x}_0) + D_\text{KL}(q(\mathbf{x}_{1:T}\vert\mathbf{x}_0) \| p_\theta(\mathbf{x}_{1:T}\vert\mathbf{x}_0) ) \\
&amp;= -\log p_\theta(\mathbf{x}_0) + \mathbb{E}_{\mathbf{x}_{1:T}\sim q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T}) / p_\theta(\mathbf{x}_0)} \Big] \\
&amp;= -\log p_\theta(\mathbf{x}_0) + \mathbb{E}_q \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} + \log p_\theta(\mathbf{x}_0) \Big] \\
&amp;= \mathbb{E}_q \Big[ \log \frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \\
\text{Let }L_\text{VLB} 
&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \Big[ \log \frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \geq - \mathbb{E}_{q(\mathbf{x}_0)} \log p_\theta(\mathbf{x}_0)
\end{aligned}
\end{equation}
</p>

<p>由于这里取了-log，所以目标变成了最小化VLB损失函数，经过一系列漫长的推导，我们可以得到(中间步骤其后就是用马尔科夫链和贝叶斯定理把条件概率拆开):</p>

<p>
\begin{equation}
\begin{aligned}
L_\text{VLB} &amp;= L_T + L_{T-1} + \dots + L_0 \\
\text{where } L_T &amp;= D_\text{KL}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T)) \\
L_t &amp;= D_\text{KL}(q(\mathbf{x}_t \vert \mathbf{x}_{t+1}, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_t \vert\mathbf{x}_{t+1})) \text{ for }1 \leq t \leq T-1 \\
L_0 &amp;= - \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)
\end{aligned}
\end{equation}
</p>

<p>参数化损失函数中的$L_t$: 反向扩散的目标是用神经网络来拟合后验分布$p_\theta(x_{t-1} \vert x_t) = N(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$，根据损失函数$L_t$可知，反向扩散训练的目标是: 给定t和$x_t$, $\mu_\theta$的结果和$\tilde{\mu_t}$更接近，因为任意$x_t$在给定$x_0$的情况下都可以求出，我们可以参数化高斯噪声，把反向扩散的目标变成让$\epsilon_t$和$\epsilon_\theta$更接近</p>

<p>
\begin{equation}
\begin{aligned}
\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) &amp;= \frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \Big) \\
\end{aligned}
\end{equation}
</p>

<p>损失函数$L_t$为:</p>

<p style="font-size: 20px">
\begin{equation}
\begin{aligned}
L_t 
&amp;= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{1}{2 \| \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t) \|^2_2} \| \tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0) - \boldsymbol{\mu}_\theta(\mathbf{x}_t, t) \|^2 \Big] \\
&amp;= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{1}{2  \|\boldsymbol{\Sigma}_\theta \|^2_2} \| \frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \Big) - \frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\boldsymbol{\epsilon}}_\theta(\mathbf{x}_t, t) \Big) \|^2 \Big] \\
&amp;= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{ (1 - \alpha_t)^2 }{2 \alpha_t (1 - \bar{\alpha}_t) \| \boldsymbol{\Sigma}_\theta \|^2_2} \|\boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2 \Big] \\
&amp;= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{ (1 - \alpha_t)^2 }{2 \alpha_t (1 - \bar{\alpha}_t) \| \boldsymbol{\Sigma}_\theta \|^2_2} \|\boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t, t)\|^2 \Big] 
\end{aligned}
\end{equation}
</p>

<h2 id="24-ddpm中给出的训练与采样生成过程">2.4. DDPM中给出的训练与采样(生成)过程</h2>
<center><img src="../assets/img/posts/20221012/4.jpg" /></center>

<p>训练过程:</p>
<ol>
  <li>采样一个$x_0$</li>
  <li>任选一个时间t</li>
  <li>随机采样一个高斯噪声$\epsilon$</li>
  <li>计算损失函数的梯度，更新参数$\theta$</li>
</ol>

<p>采样过程:</p>
<ol>
  <li>采样一个高斯噪声$x_T$</li>
  <li>从时间T开始，每一步采样一个高斯噪声z，利用重参数化，得到上一步的$x_{t-1}$，重复T次，最终得到生成的$x_0$</li>
</ol>

<p>注意这里还没有给出$\epsilon_\theta(x_t, t)$的网络结构，DDPM使用U-Net作为其网络结构(后面会具体展开)</p>

<h2 id="25-小结">2.5. 小结</h2>
<p>简单来说，扩散模型的前向扩散过程都是定义好的马尔可夫链，每一步都需要使用重参数化技巧来添加噪声，这里每一步的后验分布的参数都是预定义好的。反向扩散过程就是用噪声生成原始图片的过程，和VAE类似，用分布$p_\theta(x_{t-1}|x_t)$来拟合真实的后验分布$q(x_{t-1}|x_t)$，所以生成过程最重要的就是训练出合适的分布来拟合，通过VLB和重参数化的技巧，最终可以把训练过程看成给一个高斯噪声，拟合成前向扩散的噪声。这里的网络结构一般使用的是U-Net</p>

<h1 id="3-一些技巧和主要网络结构">3. 一些技巧和主要网络结构</h1>
<h2 id="31-beta_t和sigma_theta的取值">3.1. $\beta_t$和$\Sigma_\theta$的取值</h2>
<p>关于$\beta_t$的取值，DDPM的做法是$\beta_1=10^{-4}$到$\beta_T=0.02$线性取值，这样的扩散模型取得的效果不算最好，<a href="https://arxiv.org/abs/2102.09672" target="_blank">Improved DDPM</a>提出了一种新的取值方法:</p>

<p>
\begin{equation}
\beta_t = \text{clip}(1-\frac{\bar{\alpha}_t}{\bar{\alpha}_{t-1}}, 0.999) \quad\bar{\alpha}_t = \frac{f(t)}{f(0)}\quad\text{where }f(t)=\cos\Big(\frac{t/T+s}{1+s}\cdot\frac{\pi}{2}\Big)
\end{equation}
</p>

<p>关于$\Sigma_\theta$的取值方法，DDPM采用固定的$\Sigma_\theta$(不学习)，可以取$\beta_t$或者$\tilde{\beta_t}=\frac{1 - \bar{\alpha_{t-1}}}{1 - \bar{\alpha_t}} \cdot \beta_t$，Improved DDPM采用可学习的$\Sigma_\theta$参数，利用线性插值的方法:</p>

<p>
\begin{equation}
\boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t) = \exp(\mathbf{v} \log \beta_t + (1-\mathbf{v}) \log \tilde{\beta}_t)
\end{equation}
</p>

<p>由于损失函数中没有关于$\Sigma_\theta$的梯度，所以需要对损失函数进行一点更改</p>

<h2 id="32-加速扩散模型采样的技巧">3.2. 加速扩散模型采样的技巧</h2>
<p>DDPM的作者对比了扩散模型和其他生成模型的生成速度，发现DDPM的生成速度远小于其他生成模型，有一些加速模型采样的技巧:</p>
<ol>
  <li>缩短采样步骤，比如每隔T/S步才采样一次</li>
  <li><a href="https://arxiv.org/abs/2010.02502" target="_blank">DDIM</a>论文里提出的技巧，在采样过程中只需要采样一个子集的步骤便可做生成</li>
  <li><a href="https://arxiv.org/abs/2112.10752" target="_blank">Latent Diffusion Model</a>论文提出让扩散过程在隐空间中进行，而不是在像素空间中进行，这样可以让训练代价更小，推断过程更快，后续会整理LDM论文</li>
</ol>

<h2 id="33-u-net">3.3. U-Net</h2>
<p>很多扩散模型的噪声网络结构都是基于<a href="https://arxiv.org/pdf/1505.04597.pdf" target="_blank">U-Net</a>，U-Net的网络结构如下图:</p>

<center><img src="../assets/img/posts/20221012/5.jpg" /></center>

<p>模型可以分为两个部分，左边用于特征的抽取，右边部分用于上采样，由于网络结构酷似字母U而得名。U-Net网络结构又可以看成AutoEncoder的结构，它的bottleneck就是中间的低纬度特征表示，U-Net要保证输出的噪声和输入的噪声有相同的维度，是一个自回归模型。DDPM使用的是PixelCNN++的backbone，也就是基于Wide Resnet的U-Net，也就是说encoder和decoder之间是残差连接，输入$x_t$返回噪声(残差思想)</p>

<h1 id="4-条件生成">4. 条件生成</h1>
<p>条件生成就是conditioned generation，通过输入额外的conditioning information来生成图片，比如一段提示词或者生成图片的类别</p>

<h2 id="41-classifier-guided-diffusion">4.1. Classifier Guided Diffusion</h2>
<p>博客上讲解的关于classifier guidance的部分不太详尽，于是我去翻看了Classifier Guidance的论文<a href="https://arxiv.org/abs/2105.05233" target="_blank">Diffusion Models Beat GANs</a>:</p>

<p>classifier guidance的思路来源于GAN模型的条件生成，将这种条件生成应用于扩散模型后，发现效果非常好。作者提出可以训练一个分类器$p_\phi(y|x_t, t)$，然后把$\nabla_{x_t} \log p_\phi\left(y \mid x_t, t\right)$的加到总的梯度公式里面，来指导扩散模型<strong>采样的过程</strong>偏向于生成类别为y的图片</p>

<p>没有classifier guidance之前的反向扩散分布函数为: $p_\theta(x_t|x_{t+1})$，但是有了classifier guidance之后，反向扩散的后验分布函数变成了:</p>

<p>
\begin{equation}
p_{\theta, \phi}(x_t|x_{t+1}, y) = Zp_\theta(x_t|x_{t+1})p_\phi(y|x_t)
\end{equation}
</p>

<p>其中Z是正则化的常数，接下来我们需要化简上面这个公式，首先我们知道$p_\theta(x_t|x_{t+1})$本质上就是正态分布:</p>

<p>
\begin{equation}
p_\theta(x_t|x_{t+1})=\mathcal{N}(\mu, \Sigma)
\end{equation}
</p>

<p>然后我们对$log_\phi p(y|x_t)$在$x=\mu$处进行泰勒展开:</p>

<p>
\begin{equation}
\begin{aligned}
\log p_\phi(y \mid x_t) &amp; \approx log p_\phi (y \mid x_t) \mid _{x_t=\mu}+(x_t-\mu)\nabla_{x_t}logp_\phi(y \mid x_t)\mid _{x_t=\mu} \\
&amp;=(x_t-\mu)g+C_1\\
\end{aligned}
\end{equation}
</p>

<p>这里$g=\nabla_{x_t}logp_\phi(y \mid x_t)\mid _{x_t=\mu}$</p>

<p style="font-size: 18px">
\begin{equation}
\begin{aligned}
\log \left(p_\theta\left(x_t \mid x_{t+1}\right) p_\phi\left(y \mid x_t\right)\right) &amp; \approx-\frac{1}{2}\left(x_t-\mu\right)^T \Sigma^{-1}\left(x_t-\mu\right)+\left(x_t-\mu\right) g+C_2 \\
&amp;=-\frac{1}{2}\left(x_t-\mu-\Sigma g\right)^T \Sigma^{-1}\left(x_t-\mu-\Sigma g\right)+\frac{1}{2} g^T \Sigma g+C_2 \\
&amp;=-\frac{1}{2}\left(x_t-\mu-\Sigma g\right)^T \Sigma^{-1}\left(x_t-\mu-\Sigma g\right)+C_3 \\
&amp;=\log p(z)+C_4, z \sim \mathcal{N}(\mu+\Sigma g, \Sigma)
\end{aligned}
\end{equation}
</p>

<p>那么反向扩散过程就可以看成均值为$\mu+\Sigma g$，方差为$\Sigma$的正态分布，那么我们有以下采样算法:</p>

<center><img src="../assets/img/posts/20221012/6.jpg" /></center>

<p>另一种思路是修改正态分布中的噪声函数，原本的梯度为:</p>

<p>
\begin{equation}
\nabla _{x_t} log p_\theta (x_t) = - \frac{1}{\sqrt{1-\overline{\alpha_t}}} \epsilon_\theta (x_t)
\end{equation}
</p>

<p>修改反向扩散函数后的梯度为:</p>

<p>
\begin{equation}
\begin{aligned}
\nabla _{x_t} log (p_\theta (x_t) p_\phi (y \mid x_t))  &amp;= \nabla _{x_t} log p_\theta (x_t) + \nabla _{x_t} log p_\phi (y \mid x_t) \\
&amp;= - \frac{1}{\sqrt{1-\overline{\alpha_t}}} \epsilon_\theta (x_t) + \nabla _{x_t} log p_\phi (y \mid x_t)
\end{aligned}
\end{equation}
</p>

<p>那么根据梯度，我们可以定义一个新的噪声预测函数$\hat{\epsilon}$:</p>

<p>
\begin{equation}
\hat{\epsilon(x_t)} := \epsilon_\theta(x_t) - \sqrt{1-\overline{\alpha_t}} \nabla _{x_t} log p_\phi(y \mid x_t)
\end{equation}
</p>

<p>该方法对应的算法为:</p>

<center><img src="../assets/img/posts/20221012/7.jpg" /></center>

<h2 id="42-classifier-free-guidance">4.2. Classifier-Free Guidance</h2>
<p>上一小节提到的classifier guidance的技巧是需要单独使用一个分类器(参与训练或者不参与训练的情况都有)来获得$x_t$的类别，根据不同的class可以使用不同的分类器，比如resnet可以进行图片类别的guidance，CLIP可以进行文本的guidance等等。如果我们没有这个单独的分类器，我们也可以利用classifier-free guidance的技巧来实现条件生成，同样地，为了更详尽地了解这个技巧，我去翻看了<a href="https://arxiv.org/abs/2112.10741" target="_blank">GLIDE</a>论文中关于classifier-free guidance的介绍</p>

<p>classifier-free guidance并不需要模型去单独给出一个分类器，而是将条件生成与非条件生成都用同一个函数表示，即$\epsilon(x_t\mid y)$，如果我们希望这个函数表示非条件生成，那么我们只需要将y替换成空集即可，在训练过程中，我们以相同的概率随机替换y为空集。采样时，反向扩散函数为$\epsilon_\theta(x_t \mid y)$和$\epsilon_\theta(x_t \mid \emptyset)$的线性插值:</p>

<p>
\begin{equation}
\hat{\epsilon_\theta}(x_t \mid y)=\epsilon_\theta(x_t\mid \emptyset) + s \cdot (\epsilon_\theta(x_t\mid y)-\epsilon_\theta(x_t\mid \emptyset))
\end{equation}
</p>

<p>式子中的s是guidance scale，s越大代表生成的图片越靠近y，guidance-free的技巧出现后，大家发现它的效果非常好，于是后续的模型基本上都运用了该技巧，比如GLIDE、DALLE2、Imagen</p>

<h2 id="43-scale-up-generation-resolution-and-quality">4.3. Scale up Generation Resolution and Quality</h2>
<p>为了生成更高质量和更高分辨率的图片，可以将扩散模型与超分辨率的技术相结合，论文<a href="https://arxiv.org/abs/2106.15282" target="_blank">Cascaded Diffusion Model</a>中提出用层级式的扩散模型来做图片的超分辨率生成，模型的结构如下图:</p>

<center><img src="../assets/img/posts/20221012/8.jpg" /></center>

<p>模型由三个子模型组成，分别是一个基础的扩散模型和两个超分辨率扩散模型，注意这里的每个子模型都需要输入类别，超分辨率子模型还需要输入上一个子模型的低分辨率结果，这些子模型都是conditional的。超分辨率扩散模型与普通的扩散模型的区别是损失函数和反向扩散函数不同，具体来说就是U-Net的结构不同，超分辨率的U-Net的输出维度比输入维度要大，而且输入为低分辨率的图片、高分辨率的图片、类别:</p>

<center><img src="../assets/img/posts/20221012/9.jpg" /></center>

<p>一个two-stage的cascaded模型的训练算法为:</p>

<center><img src="../assets/img/posts/20221012/10.jpg" /></center>

<p>一个two-stage的cascaded模型的采样算法为:</p>

<center><img src="../assets/img/posts/20221012/11.jpg" /></center>]]></content><author><name>Quehry</name></author><category term="notes" /><summary type="html"><![CDATA[collect and arrange information and principle of diffusion model]]></summary></entry></feed>