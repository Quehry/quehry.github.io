<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-10-08T23:10:58+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Quehry</title><subtitle>Artificial Intelligence trends and concepts made easy.</subtitle><author><name>Quehry</name></author><entry><title type="html">AutoEncoder系列整理</title><link href="http://localhost:4000/AutoEncoder-Series.html" rel="alternate" type="text/html" title="AutoEncoder系列整理" /><published>2022-09-27T00:00:00+08:00</published><updated>2022-09-27T00:00:00+08:00</updated><id>http://localhost:4000/AutoEncoder-Series</id><content type="html" xml:base="http://localhost:4000/AutoEncoder-Series.html"><![CDATA[<h1 id="1-ae简介">1. AE简介</h1>
<p>AutoEncoder，即AE，自编码器，是一类在半监督学习和非监督学习中使用的人工神经网络，其功能是通过将输入信息作为学习目标，对输入信息进行表征学习(representation learning)，编码其实就是特征表示</p>

<p>半监督学习(semi-supervised learning)的训练数据一部分是有标签的，另一部分没有标签，而没标签的数量一般大于有标签数据的数量</p>

<p>自编码器的原理如下图所示，encoder首先读取input，将输入转换成高效的内部表示(code)，然后再由decoder输出输入数据的类似物</p>
<center><img src="../assets/img/posts/20221008/1.jpg" /></center>

<p>自编码器属于自监督学习的范畴，算法把输入作为监督信号来学习，encoder的作用其实就是对输入向量进行特征降维，常见的降维算法有主成分分析法PCA，但PCA本质上是一种线性变换，提取特征的能力有限</p>

<p>自编码器利用神经网络来学习输入的特征表达，AE利用数据x本身作为监督信号来指导神经网络的训练，即希望神经网络能学到映射$f_\theta$:x-&gt;x</p>
<center><img src="../assets/img/posts/20221008/2.jpg" /></center>
<p>把网络切分为两个部分，前面的子网络尝试学习映射关系$g_{\theta1}:x-&gt;z$，后面的子网络尝试学习映射关系$h_{\theta2}:z-&gt;x$，即编码器和解码器，编码器和解码器共同完成了输入数据x的编码、解码过程，把整个网络模型叫做AutoEncoder，模型根据输出与输入的距离函数作为损失函数来优化AE，随机梯度下降</p>

<p>假设输入为x，中间层为y，最终输出为z，那么y=s(Wx+b)，s是激活函数，z=s(W’y+b’)</p>

<p>接下来我将整理AE家族的一些模型，有DAE、VAE、VQVAE、MAE，当然不可能涵盖所有的模型，尽量介绍一些使用较多的模型</p>

<h1 id="2-dae">2. DAE</h1>
<p>通过Auto-Encoder得到的模型往往存在过拟合的风险，为了学习到较鲁棒的特征，可以在网络的输入层引入随机噪声，这种方法称为降噪自编码器(Denoising autoencoder, DAE)，为了更了解模型的原理和架构，我去阅读了DAE的<a href="https://dl.acm.org/doi/abs/10.1145/1390156.1390294" target="_blank">论文</a></p>

<p>作者的想法是让网络从corrputed的输入还原出原始输入，通过这个方法来提高模型的鲁棒性。corrputed的方法: 对于每一个输入x，随机选取$v_d$个元素置零，其他的部分保持不变，那么网络的目标就变成了对这些位置进行填空(fill-in)，这和BERT中Masked LM的思想差不多</p>
<center><img src="../assets/img/posts/20221008/3.jpg" /></center>
<p>论文的其他部分着重介绍为什么这种denoising的思想有用以及背后的数学原理</p>

<h1 id="3-vae">3. VAE</h1>]]></content><author><name>Quehry</name></author><category term="notes" /><summary type="html"><![CDATA[Information about autoencoder series]]></summary></entry><entry><title type="html">GAN</title><link href="http://localhost:4000/GAN.html" rel="alternate" type="text/html" title="GAN" /><published>2022-09-27T00:00:00+08:00</published><updated>2022-09-27T00:00:00+08:00</updated><id>http://localhost:4000/GAN</id><content type="html" xml:base="http://localhost:4000/GAN.html"><![CDATA[<h1 id="目录">目录</h1>

<!-- TOC -->

<ul>
  <li><a href="#目录">目录</a></li>
  <li><a href="#1-博客简介">1. 博客简介</a></li>
  <li><a href="#2-gan简介">2. GAN简介</a></li>
  <li><a href="#3-adversarial-nets">3. Adversarial nets</a></li>
  <li><a href="#4-理论原理">4. 理论原理</a></li>
  <li><a href="#5-其他">5. 其他</a></li>
</ul>

<!-- /TOC -->

<h1 id="1-博客简介">1. 博客简介</h1>
<p>GAN的全称是generative adversarial nets，是Goodfellow于2014年提出的新的生成模型框架，这种全新的生成模型框架有很多应用和变种，这篇博客主要介绍最开始的GAN的原理和论文整理，这里阅读的论文不是最终版(区别在于related work不同)，下面列出一些链接</p>
<ul>
  <li><a href="https://arxiv.org/abs/1406.2661" target="_blank">论文链接</a></li>
  <li><a href="https://www.bilibili.com/video/BV1rb4y187vD/?spm_id_from=333.788&amp;vd_source=64c99329fc39a0e3f42825a4c837e2a5" target="_blank">李沐讲解</a></li>
</ul>

<h1 id="2-gan简介">2. GAN简介</h1>
<p>GAN是一种全新的生成模型框架，它包含两个部分，生成模型G和辨别模型D，G的作用是捕捉数据的分布，D的作用是辨别数据来源于真实数据分布还是G生成的数据分布。生成模型训练过程就是让D犯错的可能性更高。GAN框架其实就是一个minmax game，如果G和D都是MLP的话，那么整个系统可以用逆传播机制训练。GAN的作者举了一个简单的例子介绍模型训练过程，生成模型可以看成印假钞的团伙，辨别模型可以看成警察，双方都在训练中提升自己的能力，最终希望达到的效果是警察无法分辨出一张假钞是真币还是假币。论文只介绍了一种特殊情况，就是G和D都是MLP的情况，作者把这种情况称为Adversarial nets</p>

<h1 id="3-adversarial-nets">3. Adversarial nets</h1>
<p>为了让生成模型学习到分布$p_g$(分布尽量和原始数据x的分布一致)，需要定义输入噪音的先验分布$p_z(z)$，$G(z;\theta_g)$表示噪音z输入生成模型的结果，G是一个可微分的函数，这里是MLP，参数为$\theta_g$。$D(x;\theta_d)$表示输入x后的辨别模型的结果，输出是一个标量，表示x来自于真实数据分布的概率。</p>

<p>也就是说，D和G的价值函数V(G, D)可表示为:</p>
<center><img src="../assets/img/posts/20220927/2.jpg" /></center>
<p>辨别模型D的目标是最大化价值函数的值，D(x)的取值在0-1之间，所以价值函数越大说明辨别模型D的效果越好，生成模型G的目标是最小化价值函数的值。GAN训练生成模型和辨别模型的过程为:</p>
<center><img src="../assets/img/posts/20220927/3.jpg" /></center>
<p>绿色的线是生成模型，蓝色虚线是辨别模型，黑色的散点线是原始数据分布</p>

<h1 id="4-理论原理">4. 理论原理</h1>
<p>算法原理由下面这一张图片展示:</p>
<center><img src="../assets/img/posts/20220927/4.jpg" /></center>
<p>在每个迭代周期的每个批量中，我们有m个取自先验分布的噪音z，其中z$\sim$ $p_g(z)$和m个取自真实分布的x，其中x$\sim$ $p_{data}(x)$，先训练辨别器D，沿着梯度上升的方向更新参数，然后在沿着log(1-D(G($z^{(i)}$)))的梯度下降的方向更新参数。</p>

<p>接下来介绍了一些命题和证明和一些定理，证实了GAN用到的价值函数和目标函数的可行性</p>

<h1 id="5-其他">5. 其他</h1>
<ul>
  <li>GAN在刚提出的时候还是有很多缺点的，比如模型还是比较难训练的，但是后续有很多很多的工作来优化原始的GAN模型，所以GAN更像是抛出了一个引子，让后续模型来优化它</li>
  <li>GAN本质上就是左右手互博，目标函数设计的也很好</li>
</ul>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[arrange notes]]></summary></entry><entry><title type="html">重要时间点及事件</title><link href="http://localhost:4000/Important-Event.html" rel="alternate" type="text/html" title="重要时间点及事件" /><published>2022-09-26T00:00:00+08:00</published><updated>2022-09-26T00:00:00+08:00</updated><id>http://localhost:4000/Important-Event</id><content type="html" xml:base="http://localhost:4000/Important-Event.html"><![CDATA[<!-- 2022.9.23 报9月工资 -->]]></content><author><name>Quehry</name></author><category term="daily" /><summary type="html"><![CDATA[for query]]></summary></entry><entry><title type="html">T5模型</title><link href="http://localhost:4000/T5%E6%A8%A1%E5%9E%8B.html" rel="alternate" type="text/html" title="T5模型" /><published>2022-09-19T00:00:00+08:00</published><updated>2022-09-19T00:00:00+08:00</updated><id>http://localhost:4000/T5%E6%A8%A1%E5%9E%8B</id><content type="html" xml:base="http://localhost:4000/T5%E6%A8%A1%E5%9E%8B.html"><![CDATA[<!-- TOC -->

<ul>
  <li><a href="#1-t5简介">1. T5简介</a></li>
  <li><a href="#2-读论文">2. 读论文</a>
    <ul>
      <li><a href="#21-introduction">2.1. Introduction</a></li>
      <li><a href="#22-setup">2.2. Setup</a>
        <ul>
          <li><a href="#221-model">2.2.1. Model</a></li>
          <li><a href="#222-the-colossai-clean-crawled-corpusc4">2.2.2. THE Colossai Clean Crawled Corpus(C4)</a></li>
          <li><a href="#223-downstream-tasks">2.2.3. Downstream Tasks</a></li>
          <li><a href="#224-input-and-output-format">2.2.4. Input and Output Format</a></li>
        </ul>
      </li>
      <li><a href="#23-experiments">2.3. Experiments</a>
        <ul>
          <li><a href="#231-baseline">2.3.1. Baseline</a>
            <ul>
              <li><a href="#2311-model">2.3.1.1. Model</a></li>
              <li><a href="#2312-training">2.3.1.2. Training</a></li>
              <li><a href="#2313-vocabulary">2.3.1.3. Vocabulary</a></li>
              <li><a href="#2314-unsupervised-objective">2.3.1.4. Unsupervised Objective</a></li>
              <li><a href="#2315-baseline-performance">2.3.1.5. Baseline Performance</a></li>
            </ul>
          </li>
          <li><a href="#232-architectures">2.3.2. Architectures</a>
            <ul>
              <li><a href="#2321-model-structures">2.3.2.1. Model Structures</a></li>
              <li><a href="#2322-comparing-different-model-structures">2.3.2.2. Comparing Different Model Structures</a></li>
              <li><a href="#2323-objectives">2.3.2.3. Objectives</a></li>
              <li><a href="#2324-results">2.3.2.4. Results</a></li>
            </ul>
          </li>
          <li><a href="#233-unsupervised-objectives">2.3.3. Unsupervised Objectives</a></li>
          <li><a href="#234-pre-training-data-set">2.3.4. Pre-training Data Set</a></li>
          <li><a href="#235-training-strategy">2.3.5. Training Strategy</a></li>
          <li><a href="#236-scaling">2.3.6. Scaling</a></li>
          <li><a href="#237-putting-it-all-together">2.3.7. Putting It All Together</a></li>
        </ul>
      </li>
      <li><a href="#24-reflection">2.4. Reflection</a></li>
    </ul>
  </li>
  <li><a href="#3-个人总结">3. 个人总结</a></li>
</ul>

<!-- /TOC -->

<h1 id="1-t5简介">1. T5简介</h1>
<p>T5的全称是text-to-text transfer transformer，是google于2019年推出的NLP领域的大型预训练模型，T5模型将NLP领域的任务均看成text to text类型，在众多任务的表现十分优异，模型本身的结构就是transformer的encoder-decoder结构，但是预训练目标以及其他细节有所区别</p>

<p>相关链接:</p>
<ul>
  <li><a href="https://arxiv.org/abs/1910.10683" target="_blank">论文</a></li>
  <li><a href="https://github.com/google-research/text-to-text-transfer-transformer" target="_blank">github</a></li>
  <li><a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" target="_blank">google博客</a></li>
  <li><a href="https://huggingface.co/docs/transformers/model_doc/t5" target="_blank">huggingface文档</a></li>
</ul>

<h1 id="2-读论文">2. 读论文</h1>
<p>摘要: 将所有的以文本为基础的语言任务变成text to text格式的任务，论文比较了不同的预训练目标、架构、无标签数据集、迁移方式在NLU任务上的表现。论文还新建了数据集C4，T5模型在很多benchmark上能做到SOTA，包括总结、QA、文本分类等。此外，T5模型和C4数据集均开源</p>

<h2 id="21-introduction">2.1. Introduction</h2>
<p>把所有的文本处理问题看成”text-to-text”问题，也即输入一段文本，输出一段文本。</p>

<center><img src="../assets/img/posts/20220919/2.jpg" /></center>

<h2 id="22-setup">2.2. Setup</h2>
<h3 id="221-model">2.2.1. Model</h3>
<p>Transformer架构一开始用于机器翻译任务，自注意力可以看成将一段序列的每个词元替换成其他词元的加权平均。T5模型的架构和Transformer的encoder-decoder结构基本一致，区别在于T5模型去除了层归一偏差，将层归一化放在残差路径外，使用了一种不同的位置嵌入方案。</p>

<h3 id="222-the-colossai-clean-crawled-corpusc4">2.2.2. THE Colossai Clean Crawled Corpus(C4)</h3>
<p>这一部分主要介绍了C4数据集的相关内容。Common Crawl是一个公开的<a href="https://commoncrawl.org/" target="_blank">数据集网站</a>，它可以提供从网页爬取的文本，但是这些文本数据存在很多问题，论文提出了以下的几种方法来让数据集更clean:</p>
<ul>
  <li>只保留以终点符号(即句点，感叹号，问号或引号)结尾的行</li>
  <li>丢弃少于五个句子的page，只保留超过3个单词的句子</li>
  <li>删除任何包含有在<a href="https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words" target="_blank">List-of-Dirty</a>网站中出现的单词的网页</li>
  <li>删除包含Javascript的行</li>
  <li>删除出现“lorem ipsum”短语的page</li>
  <li>删除所有包含大括号的页面</li>
  <li>对数据集进行重复数据删除，当连续的三句话重复出现时只保留一个</li>
  <li>使用langdetect工具过滤掉非英文的页面</li>
</ul>

<h3 id="223-downstream-tasks">2.2.3. Downstream Tasks</h3>
<p>T5模型为了测量总体的语言学习能力，在很多benchmark上测试性能，比如机器翻译、QA、摘要总结、文本分类。在GLUE和SuperGLUE上测试文本分类能力，在CNN/Daily Mail上测试摘要总结能力，在SQuAD上测试QA能力…</p>

<h3 id="224-input-and-output-format">2.2.4. Input and Output Format</h3>
<p>正如在introduction中提及的一样，论文将所有的task看成text-to-text格式。这种框架为预训练和微调提供了一致的训练目标。模型用极大似然目标训练(教师强制)。为了区分不同任务，给input前加上task-specific前缀。比如为英翻德加上前缀“translate English to German: ”，论文附录里有各种任务的前缀与相关处理方法。</p>

<h2 id="23-experiments">2.3. Experiments</h2>
<p>论文搭建模型的出发点是比较不同的预训练目标、模型架构、无标签数据集等方面，从中选择表现最好的部分组成T5模型。每次只改变baseline的一部分，其余部分保持不变。BERT不太好做生成任务，比如机器翻译和摘要总结</p>

<h3 id="231-baseline">2.3.1. Baseline</h3>
<p>也即基准</p>
<h4 id="2311-model">2.3.1.1. Model</h4>
<p>模型选用Transformer的Encoder-Decoder架构，相比于只使用Encoder来说，该架构在分类和生成任务上取得更好的效果</p>

<h4 id="2312-training">2.3.1.2. Training</h4>
<p>所有的任务都是text-to-text类型，这让作者能用极大似然法和交叉熵损失来训练模型，优化器选择AdaFactor。在测试阶段，选用概率最高的词元作为输出。在预训练阶段，采用逆平方根学习率策略，即学习率会随着迭代周期下降。预训练阶段，模型迭代524288步。在微调阶段，模型迭代262144步，同时使用固定的学习率。</p>

<h4 id="2313-vocabulary">2.3.1.3. Vocabulary</h4>
<p>由于模型任务包含了翻译任务，所以词表不仅包含了英语词汇，还包括德语、法语和罗马尼亚语词汇。词表是预定义的，所以模型输出不会出现超出词表的词汇</p>

<h4 id="2314-unsupervised-objective">2.3.1.4. Unsupervised Objective</h4>
<p>模型预训练过程需要无标签的数据。过往的预训练模型训练过程都采用masked language modeling(denosing objectives)作为预训练目标，大家发现这种处理方式能取得很好的结果。对于去噪目标，模型需要预测被遮掩的词元。借鉴于BERT的经验，模型随机采样并选择丢弃了15%的词元(作为masked)，并且连续的掩蔽词元只被一个sentinel词元替代。下面展示了一个掩蔽的例子</p>

<center><img src="../assets/img/posts/20220919/3.jpg" /></center>

<h4 id="2315-baseline-performance">2.3.1.5. Baseline Performance</h4>
<p>展示了baseline模型在不同benchmark上的表现，不同的benchmark使用不同的指标</p>

<center><img src="../assets/img/posts/20220919/4.jpg" /></center>

<h3 id="232-architectures">2.3.2. Architectures</h3>
<p>比较不同框架在benchmark上的表现</p>

<h4 id="2321-model-structures">2.3.2.1. Model Structures</h4>
<p>作者选择了三种不同的架构进行对比，第一种架构是传统的Transformer的encoder-decoder架构，第二种是language modeling(encoder)架构，BERT用的就是这个架构，下一步的输出依赖于前一步的预测，第三种是Prefix Language Model，为text-to-text任务提供任务的前缀，比如翻译任务就是加上前缀translate English to German:</p>

<center><img src="../assets/img/posts/20220919/5.jpg" /></center>

<h4 id="2322-comparing-different-model-structures">2.3.2.2. Comparing Different Model Structures</h4>
<p>比较了不同模型的层数，参数和FLOPS</p>

<h4 id="2323-objectives">2.3.2.3. Objectives</h4>
<p>除了架构的区别外，还比较了不同预训练目标带来的区别，比如使用Denosing Objectives时，LM架构需要把输入和输出连接起来进行连续的预测，使用LM目标时，LM架构需要从头预测到尾</p>

<h4 id="2324-results">2.3.2.4. Results</h4>
<p>直接看表格，可以发现第一种encoder-decoder架构的表现最好</p>

<center><img src="../assets/img/posts/20220919/6.jpg" /></center>

<h3 id="233-unsupervised-objectives">2.3.3. Unsupervised Objectives</h3>
<p>本章从以下几个角度比较Unsupervised Objectives，实验得出结论，选取BERT-style，Corruption Strategies选择Replace spans，Corruption rate选择15%，Corrupted span length选择对每个词元都决定是否corrupted(独立)，也即i.i.d.，这样得出的效果最好</p>

<center><img src="../assets/img/posts/20220919/7.jpg" /></center>

<h3 id="234-pre-training-data-set">2.3.4. Pre-training Data Set</h3>
<p>最终选择了全size的C4数据集作为预训练数据集</p>

<h3 id="235-training-strategy">2.3.5. Training Strategy</h3>
<p>比较了微调的不同方案、比较了多任务同时训练和单任务训练的效果，最终发现baseline的效果最好，即预训练加下游任务微调</p>

<h3 id="236-scaling">2.3.6. Scaling</h3>
<p>尝试了扩大模型规模的几种方式，最后发现baseline选择的预训练规模是最合适的，使用较大的模型可能会使下游的微调和推断变得更加昂贵</p>

<h3 id="237-putting-it-all-together">2.3.7. Putting It All Together</h3>
<p>这一部分介绍了模型最终的一些调整内容</p>
<ul>
  <li>预训练目标: 掩蔽片段平均长度为3，同时掩蔽比率为15%</li>
  <li>更长的训练过程: C4数据集够大，让训练过程可以不用重复数据，因此增加批量大小、增加训练步数会更好</li>
  <li>模型大小: 有好几个版本的T5模型，Base、Small、Large、3B and 11B</li>
  <li>多任务预训练: 使用多任务预训练会为下游任务带来好处
展示一下最终的效果</li>
</ul>

<center><img src="../assets/img/posts/20220919/8.jpg" /></center>

<h2 id="24-reflection">2.4. Reflection</h2>
<p>这一部分总结了模型的创新部分，同时提出了模型的缺点以及展望</p>

<h1 id="3-个人总结">3. 个人总结</h1>
<p>T5模型是google继bert之后推出的一个大型预训练模型，先说说T5模型的特点，T5模型的架构是transformer的encoder-decoder架构，预训练数据集选用google自制的C4数据集，数据集也相当大，作者希望做出一个大统一的预训练模型，所以采用text-to-text任务类型也是它的一大特点，具体来说就是把所有的NLP任务变成输入一段文本，模型输出一段文本的形式，模型的预训练目标也很有特色，采用了类似bert的掩蔽预训练目标。论文做了很多很多很贵的实验，对比了很多方面，最后得到了这个模型，论文里的实验都说明的很详细，同时它也刷了很多榜，比如GLUE等，效果是比之前的预训练模型都好，是google财大气粗的表现。</p>

<p>模型在github上开源，在tensorflow上可以直接实现</p>]]></content><author><name>Quehry</name></author><category term="paper" /><summary type="html"><![CDATA[arrange notes]]></summary></entry><entry><title type="html">技术分享会笔记(关于text2image)</title><link href="http://localhost:4000/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB%E4%BC%9A%E7%AC%94%E8%AE%B0.html" rel="alternate" type="text/html" title="技术分享会笔记(关于text2image)" /><published>2022-09-19T00:00:00+08:00</published><updated>2022-09-19T00:00:00+08:00</updated><id>http://localhost:4000/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB%E4%BC%9A%E7%AC%94%E8%AE%B0</id><content type="html" xml:base="http://localhost:4000/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB%E4%BC%9A%E7%AC%94%E8%AE%B0.html"><![CDATA[]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[note]]></summary></entry><entry><title type="html">动手学深度学习</title><link href="http://localhost:4000/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html" rel="alternate" type="text/html" title="动手学深度学习" /><published>2022-09-05T00:00:00+08:00</published><updated>2022-09-05T00:00:00+08:00</updated><id>http://localhost:4000/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0</id><content type="html" xml:base="http://localhost:4000/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html"><![CDATA[<!-- TOC -->

<ul>
  <li><a href="#0-简介">0. 简介</a></li>
  <li><a href="#1-预备知识">1. 预备知识</a></li>
  <li><a href="#2-线性神经网络">2. 线性神经网络</a></li>
  <li><a href="#3-多层感知机">3. 多层感知机</a></li>
  <li><a href="#5-卷积神经网络">5. 卷积神经网络</a></li>
  <li><a href="#6-现代卷积神经网络">6. 现代卷积神经网络</a></li>
  <li><a href="#7-循环神经网络">7. 循环神经网络</a></li>
  <li><a href="#8-现代循环神经网络">8. 现代循环神经网络</a></li>
  <li><a href="#9-注意力机制">9. 注意力机制</a></li>
  <li><a href="#13-自然语言处理-预训练">13. 自然语言处理: 预训练</a></li>
  <li><a href="#14-自然语言处理-应用">14. 自然语言处理: 应用</a></li>
</ul>

<!-- /TOC -->

<h1 id="0-简介">0. 简介</h1>
<ul>
  <li>《动手学深度学习》的笔记</li>
  <li>各种链接:
    <ul>
      <li><a href="https://space.bilibili.com/1567748478" target="_blank">bilibili</a></li>
      <li><a href="https://zh-v2.d2l.ai/index.html" target="_blank">book_zh</a></li>
      <li><a href="https://d2l.ai/index.html" target="_blank">book_en</a></li>
    </ul>
  </li>
  <li>这本书笼统的介绍了深度学习所需要的各种知识，从线性神经网络开始讲起，然后到CNN，最后到RNN，介绍了CV和NLP领域的较新的网络结构。同时这本书不止有理论内容，每一小节都有代码实践内容，可以边写代码边了解知识，同时bilibili上有李沐老师的网课配合学习，很适合初学者进行学习。</li>
</ul>

<h1 id="1-预备知识">1. 预备知识</h1>
<p>这一章主要介绍了深度学习的一些前置知识，这里对比较重要的点做备注</p>
<ul>
  <li>张量(Tensor)包含了一维张量(向量)和二维张量(矩阵)</li>
  <li>torch中A*B是哈达玛积，表示矩阵元素按元素相乘</li>
  <li>torch.dot()是点积</li>
  <li>torch.cat(…, dim=0)表示在行上延伸，比如(3, 4)和(3, 4)变成(6, 4)</li>
  <li>A.sum(axis=0)表示把每一列的数据都相加，比如(5, 4)变成(4)</li>
  <li>范数是norm，L1范数为每个元素的绝对值相加，L2范数为元素的平方和开根号，torch中默认L2范数，一般也是L2范数用的最多</li>
  <li>梯度: 连接多元函数的所有偏导数:</li>
</ul>

<center><img src="../assets/img/posts/20220905/2.jpg" /></center>

<ul>
  <li>梯度是一个向量</li>
  <li>常用的梯度计算公式:</li>
</ul>

<center><img src="../assets/img/posts/20220905/3.jpg" /></center>

<ul>
  <li>torch中自动求导的步骤:
    <ul>
      <li>第一步 为x分配内存空间: x.requires_grad_(True)</li>
      <li>第二步 链式反向传播，希望求哪个函数的梯度，就对那个函数反向传播，比如y.backward()</li>
      <li>第三步 求x的梯度，x.grad，如果我们需要重新求梯度，需要清零梯度，x.grad.zero_()</li>
      <li>注意torch中只能对标量输出求梯度，所以常见操作是sum</li>
    </ul>
  </li>
  <li>标量对向量的偏导是向量，向量对向量的偏导是矩阵</li>
  <li>贝叶斯公式: P(A|B)P(B)=P(B|A)P(A)</li>
</ul>

<h1 id="2-线性神经网络">2. 线性神经网络</h1>
<p>本章主要介绍了线性回归网络和softmax回归网络，接下来是一些笔记</p>
<ul>
  <li>随机梯度下降和梯度下降的区别: 梯度下降一般而言是针对所有的样本而言，而随机梯度下降是针对单个样本而言，同样地，小批量随机梯度下降是针对一个批量的样本而言</li>
  <li>可以调整但是在训练过程中不更新的参数叫做超参数</li>
  <li>极大似然法: $\theta$是需要估计的值，在写似然函数时只需要把$\theta$看成参数，最大化似然函数即$\theta$的估计值</li>
  <li>每个输入与每个输出相连的层成为全连接层</li>
  <li>with torch.no_grad()的作用是让输出结果之后不构建计算图</li>
  <li>本章的训练过程: 计算y的预测值-&gt;计算损失函数-&gt;累加loss并反向传播(记得每个批量在梯度更新前需要清零梯度并反向传播loss)-&gt;更新参数</li>
  <li>训练过程中重要的组成部分: 数据迭代器、损失函数、优化器(updater/trainer)、网络(记得初始化参数)</li>
  <li>softmax为分类服务，softmax本质上是将输出规范成概率数值，方便选取预测概率最大的类作为预测类:</li>
</ul>

<center><img src="../assets/img/posts/20220905/4.jpg" /></center>

<ul>
  <li>分类的标签可以用独热编码定义</li>
  <li>网络模型用nn.sequential()定义</li>
  <li>softmax回归的损失函数可以用极大似然法推出，普通的极大似然法是最大化似然函数，但是在这里我们加上-log就变成了最小化损失函数</li>
  <li>softmax回归的损失函数是交叉熵损失:</li>
</ul>

<center><img src="../assets/img/posts/20220905/5.jpg" /></center>

<h1 id="3-多层感知机">3. 多层感知机</h1>
<p>本小节主要介绍了多层感知机的实现以及面对各种问题的解决方法，比如解决过拟合的权重衰退(weight decay)和暂退法(dropout)，解决梯度爆炸与消失的Xavier初始化。</p>
<ul>
  <li>激活函数的作用是将线性网络变成非线性，常见的有ReLU、Sigmoid、tanh</li>
  <li>ReLU: max(x, 0)</li>
  <li>Sigmoid: $\frac{1}{1+e^{-x}}$</li>
  <li>tanh: $\frac{1-e^{-2x}}{1+e^{-2x}}$</li>
  <li>在torch中可以用@来简单表示矩阵乘法</li>
  <li>用nn.Sequential()来实例化网络时，nn.ReLU()单独算一层</li>
  <li>过拟合问题可以用正则化技术解决，比如权重衰退</li>
  <li>权重衰退就是L2正则化，它在计算损失函数时增加了权重的惩罚项，比如L($\omega$, b)+$\frac{\lambda}{2}$||$\omega$||，其中$\lambda$是超参数</li>
  <li>torch框架中把权重衰退放在优化器的实例化中(torch.optim)，只需要将weight_decay的超参数输入即可</li>
  <li>暂退法(Dropout): 在前向传播中，计算每一内部层的同时注入噪音，就好像在训练过程中丢弃了一些神经元</li>
  <li>中间层活性值:</li>
</ul>

<center><img src="../assets/img/posts/20220905/6.jpg" /></center>

<ul>
  <li>只有在训练过程中才有权重衰退和暂退法</li>
  <li>在torch中简单实现dropout的方法: 在构建net时将nn.Dropout(dropout)加入nn.Sequential()，其中dropout作为丢弃概率输入Dropout中</li>
  <li>网络架构顺序: linear-&gt;relu-&gt;dropout</li>
  <li>torch中实现tensor对tensor求梯度的方法是在backward()里面加入torch.ones_like()</li>
  <li>不正常的参数初始化可能会导致梯度爆炸和梯度消失</li>
  <li>Xavier初始化是解决梯度爆炸和消失的好手段</li>
</ul>

<h1 id="5-卷积神经网络">5. 卷积神经网络</h1>
<p>这章主要介绍了CNN的基础知识，包括卷积计算以及汇聚层和简单的卷积神经网络LeNet</p>
<ul>
  <li>卷积运算即互相关运算，卷积核函数沿着输入矩阵滑动计算，一般的卷积层除了核运算外，还需要加上偏置</li>
</ul>

<center><img src="../assets/img/posts/20220905/7.jpg" /></center>

<ul>
  <li>二维卷积层的输入格式: (批量大小, 通道数, 高, 宽)，卷积层又被称为特征映射(feature map)</li>
  <li>感受野(Receptive Field)的定义是卷积神经网络每一层输出的特征图上的像素点在输入图片上映射的区域大小，也就是一个像素点对应的上一层图像的区域大小</li>
  <li>填充(padding)与步幅(stride):
    <ul>
      <li>填充的作用是在输入图像的边界填充元素(通常为0)，添加$p_h$行与$p_w$列，基本是一半在左一半在右</li>
      <li>一般在定义卷积层nn.Convd()时可以加上填充与步幅</li>
      <li>步幅包括垂直步幅$S_h$与水平步幅$S_w$</li>
      <li>在经过卷积层后，二维图像变成了$[(n_h - k_h + p_h + 1)/S_h, (n_w - k_w + p_w + 1)/S_w]$</li>
    </ul>
  </li>
  <li>多通道输入，只需把各通道输出结果加起来即可</li>
  <li>多通道输出，为每个输出通道创建一个卷积核函数$(c_i, k_h, k_w)$，假设输入通道个数$c_i$，输出通道个数为$c_o$，那么卷积核形状为$(c_o, c_i, k_h, k_w)$</li>
  <li>torch.stack(): 沿一个新维度对输入张量进行连接</li>
  <li>汇聚层(pooling)包括最大汇聚层和平均汇聚层，汇聚层是直接返回输入图像的一个小窗口的最大值或者平均值</li>
  <li>汇聚层没有可学习的参数</li>
  <li>汇聚层同样有填充与步幅，默认情况下步幅与窗口大小相同，nn.MaxPool2d()</li>
  <li>每个卷积块的基本单元是: 卷积层-&gt;激活函数-&gt;汇聚层</li>
  <li>nn.Conv2d(1, 6, kernel_size=5)其中1表示输入通道数，6表示输出通道数</li>
  <li>在CNN的最后都需要连接全连接层来变成预测类别</li>
  <li>在训练过程中如果想好好利用GPU，那么需要将网络的参数与数据集数据传入GPU，具体方法是net.to(device)、X,y.to(device)</li>
  <li>多输入多输出通道的图片示例:</li>
</ul>

<center><img src="../assets/img/posts/20220905/40.jpg" /></center>

<h1 id="6-现代卷积神经网络">6. 现代卷积神经网络</h1>
<p>这一节主要介绍了CNN的各种网络的发展历程，LeNet之后，于2012年出现深度CNN网络AlexNet，之后出现了NiN与VGG，然后是GoogleNet，之后有很大进步的网络是ResNet，直到现在，ResNet用的也很多，残差思想也持续影响后续模型的搭建</p>
<ul>
  <li>促进CV有更深的网络的两大关键因素: 数据与硬件(主要是GPU)</li>
  <li>AlexNet(深度卷积网络)于2012年ImageNet挑战赛上夺冠，第一次学习到的特征超越了手工设计的特征</li>
  <li>相比于LeNet而言，AlexNet更深、激活函数用ReLU、对训练数据进行了增广</li>
  <li>VGG(使用块的网络): VGG块由一系列卷积层(包含ReLU)+汇聚层组成，VGG网络由VGG块组成</li>
  <li>NiN(网络中的网络): NiN块结构是卷积层(含ReLU)+两个1x1卷积层(相当于全连接层)，NiN网络结构是: NiN块+汇聚层+NiN块+汇聚层+…+平均汇聚层</li>
  <li>GoogleNet(含并行连结的网络)是google花费了很多money实验出的网络，特点是参数值特殊，参数以及网络结构都是经过了很多实验得出的结果</li>
  <li>GoogleNet中基本的卷积块被称为Inception块，Inception块的架构如下图所示:</li>
</ul>
<center><img src="../assets/img/posts/20220905/41.jpg" /></center>
<ul>
  <li>GoogleNet架构: 卷积块+Inception块+最大汇聚层+Inception块+最大汇聚层+Inception块+平均汇聚层+全连接层</li>
  <li>批量规范化(Batch Normalization)是一种trick，可加速深层网络的收敛速度</li>
  <li>正则化在深度学习中非常重要</li>
  <li>对于一个批量来说，首先规范化输入(减去其均值并除以标准差)，再应用比例系数与比例偏移，就是对当前批量进行了批量规范化</li>
</ul>
<center><img src="../assets/img/posts/20220905/42.jpg" /></center>
<p>上述式子中x是输入，$\hat{\mu}_B$是这个批量的均值，$\hat{\sigma}_B$是批量的标准差，$\gamma$是比例系数，$\beta$是比例偏移，$\gamma$与$\beta$与x的形状相同，是需要学习的参数</p>
<ul>
  <li>应用于全连接层的BN: $h=\Phi(BN(Wx+b))$</li>
  <li>应用于卷积层的BN: 在每个输出通道的m*p*q个元素上同时执行BN</li>
  <li>可以发现，BN的作用位置为权重层后，激活函数前</li>
  <li>BN在训练和预测时有所不同，在预测时，直接使用模型传入的移动平均所得的均值与方差</li>
  <li>用pytorch架构简单实现BN: nn.BatchNorm2d(通道数)</li>
  <li>直观地说，BN可以使优化更加平滑</li>
  <li>残差网络ResNet于2015年在ImageNet上夺冠</li>
  <li>残差思想: 每个附加层都应该更容易地包含原始函数作为其元素之一，残差块不是为了学习输出f(x)，而是学习输出与输入的差别f(x)-x</li>
  <li>残差块的架构:</li>
</ul>
<center><img src="../assets/img/posts/20220905/43.jpg" /></center>
<ul>
  <li>ResNet的架构: 和GoogleNet很像，就是Inception变成了残差块，同时多了BN</li>
  <li>稠密链接网络DenseNet: 是ResNet的继承，DenseNet的输出是连结，而不是如ResNet那样的简单相加</li>
</ul>
<center><img src="../assets/img/posts/20220905/44.jpg" /></center>

<h1 id="7-循环神经网络">7. 循环神经网络</h1>
<p>这小节主要介绍了文本数据集如何制作，RNN的网络结构与实现</p>
<ul>
  <li>序列数据就是与时间相关的数据</li>
  <li>马尔可夫模型: 用定时间跨度的观测序列预测$x_t$</li>
  <li>$P(x_1,…,x_T)=\prod_{t=1}^TP(x_t|x_{t-1},…,x_{t-\tau})$</li>
  <li>一些名词: 文本序列、词元(token)、词表(vocabulary)、语料(corpus)</li>
  <li>文本预处理过程: 读取数据集成列表-&gt;将列表词元化，变成包含多行的词元列表-&gt;构建词表(词表将词元与数字对应)</li>
  <li>文本预处理中的词元可以是单词，也可以是字符，这里采用字符</li>
  <li>语言模型(language model)的目标是估计联合概率$P(x_1,…,x_T)$</li>
  <li>涉及一个、两个、三个变量的概率公式分别被称为一元语法、二元语法、三元语法</li>
  <li>zip()的作用是将可迭代对象打包成一个个元组，然后返回元组组成的列表</li>
  <li>构建文本序列数据集的两种方法:
    <ul>
      <li>随机采样: 随机选取，特征是原始序列，标签是原始序列右移一位</li>
      <li>顺序分区: 保证每个批量中子序列再原语料中相邻</li>
    </ul>
  </li>
  <li>相比与马尔可夫模型，隐变量模型更能体现过往序列的影响:
  $P(x_t|x_{t-1},…,x_1)=P(x_t|h_{t-1})$</li>
  <li>RNN的示意图以及推导公式:</li>
</ul>

<center><img src="../assets/img/posts/20220905/8.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20220905/9.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20220905/10.jpg" /></center>

<ul>
  <li>循环神经网络中循环的是H(Hidden state)</li>
  <li>度量语言模型的质量的性能度量是困惑度(perplexity): 一个序列中n个词元的交叉熵损失来衡量语言模型的质量</li>
</ul>

<center><img src="../assets/img/posts/20220905/11.jpg" /></center>

<ul>
  <li>最好的情况下，困惑度为1，最差的情况下，困惑度为无穷大</li>
  <li>独热编码将(批量大小, 时间步数)转变成(批量大小, 时间步数, 词表大小)，但为了方便计算，最终转变成(时间步数, 批量大小, 词表大小)</li>
  <li>梯度裁剪的作用是保证梯度不会爆炸</li>
</ul>

<center><img src="../assets/img/posts/20220905/12.jpg" /></center>

<ul>
  <li>RNN的网络结构与之前差别不大，只是在更新梯度前需要进行梯度裁剪</li>
  <li>隐藏状态形状: (隐藏层个数, 批量大小, 隐层参数个数)</li>
  <li>nn.RNN()返回的Y为隐层参数个数，需要再加上全连接层</li>
</ul>

<h1 id="8-现代循环神经网络">8. 现代循环神经网络</h1>
<p>这一章介绍了拥有记忆单元的LSTM模型，以及后续新的NLP任务机器翻译，介绍了数据集处理过程和编码器解码器结构的网络seq2seq，用来处理序列转换任务</p>
<ul>
  <li>长短期记忆网络LSTM(long short term memory)</li>
  <li>LSTM相较于普通的RNN多了很多元素，最主要的设计是记忆单元，它可以影响下一步的隐藏状态:</li>
</ul>

<center><img src="../assets/img/posts/20220905/13.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20220905/14.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20220905/15.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20220905/16.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20220905/17.jpg" /></center>
<p><br /></p>

<ul>
  <li>输入、输出、遗忘门均与$H_{t-1}$和$X_t$有关</li>
  <li>记忆单元C类似于隐状态，时时更新</li>
  <li>总的来说，LSTM中$H_t$与$H_{t-1}$、$X_t$、$C_t$都有关</li>
  <li>RNN的延伸: 多层与双向RNN，其中多层很好理解，就是把单向隐藏层的神经网络变成多层，双向的作用是让序列用到上下文信息，在预测下一个词元的任务中
双向RNN表现不佳，但是在NER中表现很好</li>
  <li>nn.LSTM(num_inputs, num_hiddens, num_layers)</li>
  <li>接下来的内容变成了机器翻译任务(序列转换)</li>
  <li>机器翻译中使用单词级词元化</li>
  <li>机器翻译数据集处理过程: 读取数据集-&gt;词元化列表-&gt;将数据集分割成source(源语言)与target(目标语言)-&gt;序列末端加上&lt;eos&gt;，同时针对长短不一的序列填充&lt;pad&gt;与截断</li>
  <li>处理序列转换任务可以用编码器-解码器结构</li>
  <li>编码器的作用是将长短可变序列变成固定形状的状态，解码器的作用是将固定形状的状态变成长度可变序列</li>
  <li>编码器为解码器输入一个状态，在seq2seq中是编码器编码过程中的隐状态，这个隐状态既作为解码器的初始state，在每个时间步中也作为上下文变量和输入concatenate之后一起输入解码器</li>
  <li>采用嵌入层将词元进行向量化，嵌入层是一个矩阵，(词表大小，特征向量维度)</li>
  <li>编码器与解码器是两个GRU</li>
  <li>permute()可以改变张量维度的位置</li>
  <li>rnn()的输入形状一般为(num_steps, batch_size, embed_size)</li>
  <li>解码器的最后同样需要一个全连接层输出</li>
  <li>解码器的第一个输入为&lt;bos&gt;</li>
  <li>由于序列存在很多&lt;pad&gt;，计算损失时不能计算pad那一部分，可以mask这一部分，所以损失函数需要重新改一下</li>
  <li>在seq2seq训练时，解码器net的输入为cat(&lt;bos&gt;，真实序列少一时间步)，这种训练机制叫做强制教学</li>
  <li>预测的时候解码器net的输入仅为&lt;bos&gt;，用每一步的预测作为下一步的输入</li>
  <li>机器翻译的性能度量为BLEU(bilingual evaluation understudy)，可用来预测输出序列的质量，当预测序列与标签序列完全相同时，BLEU为1，公式如下:</li>
</ul>

<center><img src="../assets/img/posts/20220905/18.jpg" /></center>

<ul>
  <li>编码器的功能主要是为解码器提供上下文变量c和解码器的初始隐状态</li>
</ul>

<h1 id="9-注意力机制">9. 注意力机制</h1>
<p>这章主要介绍了注意力机制，介绍了注意力机制的组成部分，比如查询、键、值、评分函数，后面又介绍了与RNN结合的Bahdanau注意力以及自注意力和多头注意力，最后介绍了transformer</p>
<ul>
  <li>注意力机制的主要成分是查询(query)、键(key)、值(value)，q和k交互形成注意力权重，然后与v相乘得到注意力汇聚结果</li>
</ul>

<center><img src="../assets/img/posts/20220905/19.jpg" /></center>

<ul>
  <li>注意力汇聚结果计算公式:</li>
</ul>

<center><img src="../assets/img/posts/20220905/20.jpg" /></center>

<p>其中x是查询，$x_i$是key，$y_i$是value，$\alpha$的作用是将x与$x_i$之间的关系建模，且权重总和为1，有点像softmax</p>

<ul>
  <li>unsqueeze()的作用是在指定位置添加一个维度，squeeze()的作用是在指定位置删除一个维度，torch.bmm()是批量矩阵乘法</li>
  <li>评分函数a同样是对q和k的关系进行建模，q、k、v都可以是向量，而且长度可以不同</li>
</ul>

<center><img src="../assets/img/posts/20220905/21.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20220905/22.jpg" /></center>
<p><br /></p>

<ul>
  <li>这里介绍了两种评分函数: 加性注意力和缩放点积注意力
    <ul>
      <li>加性注意力: 可以处理长度不同的q与k</li>
    </ul>

    <center><img src="../assets/img/posts/20220905/23.jpg" /></center>

    <ul>
      <li>缩放点积注意力(计算效率高): 要求q与k长度相同</li>
    </ul>

    <center><img src="../assets/img/posts/20220905/24.jpg" /></center>
  </li>
  <li>Bahdanau注意力模型也是编码器解码器结构，与之前的seq2seq不同，这里的上下文变量在解码器的每一步都不相同，上下文变量$c_{t’}$与解码器的上一步隐状态有关，同时在解码器和编码器的输入位置都有嵌入层</li>
</ul>

<center><img src="../assets/img/posts/20220905/25.jpg" /></center>

<ul>
  <li>多头注意力: 对q、k、v使用线性变换得到h组不同的q-k-v来输入h个注意力汇聚层，得到h个输出，这h个输出再线性变换得到最终输出</li>
</ul>

<center><img src="../assets/img/posts/20220905/26.jpg" /></center>

<ul>
  <li>自注意力就是q-k-v都是相同的一组元素</li>
  <li>自注意力无法使用序列的位置信息，可以给输入concatenate一个位置编码，比如X∈$R^{nxd}$表示n个词元的d维嵌入，P∈$R^{nxd}$表示位置嵌入矩阵，那么X+P即输入，位置编码可以基于正弦函数和余弦函数的固定位置编码</li>
</ul>

<center><img src="../assets/img/posts/20220905/27.jpg" /></center>

<ul>
  <li>transformer模型与Bahdanau模型不同，它完全基于注意力机制来构建模型</li>
  <li>transformer每块都由多头注意力和基于位置的前馈神经网络组成，其中还有残差连接，即x+sublayer(x)，再层规范化。在解码器的注意力层中，q是上个解码器层的输出，k和v是编码器输出(每个源序列的位置的编码代表一个键值对)。基于位置的前馈神经网络，简称ffn，即两层MLP</li>
</ul>

<center><img src="../assets/img/posts/20220905/28.jpg" /></center>

<h1 id="13-自然语言处理-预训练">13. 自然语言处理: 预训练</h1>
<p>这一章主要介绍了NLP领域的预训练模型，NLP领域的预训练模型都是encoder，即用文本特征来表示词元(一般都是单词)，首先介绍了word2vec，然后介绍了全局向量的词嵌入，之后介绍了子词嵌入模型fastText与字节对编码(BPE)，之后介绍了BERT(双向Transformer编码器)</p>
<ul>
  <li>在介绍RNN模型时，介绍了用独热向量来表示词元，但是这有个很严重的缺点: 不同词的独热向量的余弦相似度为0。所以接下来会介绍很多词嵌入模型，即用一个词向量来表示单词</li>
  <li>word2vec: 将词映射到固定长度的向量，这里介绍了两种模型: 跳元模型(skip-gram)与连续词袋CBOW
    <ul>
      <li>跳元模型: 假设一个词可以用来在文本序列中生成其周围的词，对于每个索引为i的单词，可以用$u_i$与$v_i$分别表示其作为上下文词和中心词的向量，可以用softmax对生成概率进行建模，对于给定中心词$w_c$，生成上下文词$w_o$的概率为:</li>
    </ul>

    <center><img src="../assets/img/posts/20220905/29.jpg" /></center>

    <p>那么跳元模型的似然函数为(上下文窗口大小为m):</p>

    <center><img src="../assets/img/posts/20220905/30.jpg" /></center>

    <p>然后通过极大似然估计法来训练</p>
    <ul>
      <li>连续词袋: 与跳元模型相反，CBOW是基于上下文词生成中心词，连续词袋模型用$v_i$和$u_i$分别表示一个词的上下文词向量与中心词向量(与跳元模型相反)，同样用softmax建模(上下文词向量相加):</li>
    </ul>

    <center><img src="../assets/img/posts/20220905/31.jpg" /></center>

    <p>连续词袋模型的似然函数:</p>

    <center><img src="../assets/img/posts/20220905/32.jpg" /></center>
  </li>
  <li>由于词表过大，使用softmax来建模的话计算成本过大，可采用两种近似训练办法来优化: 负采样与分层softmax
    <ul>
      <li>负采样建模: 直接用内积加上激活函数来表示概率，负采样即在似然函数中加上负例(从预定义分布中采样噪声词)</li>
    </ul>

    <center><img src="../assets/img/posts/20220905/33.jpg" /></center>

    <ul>
      <li>层序softmax: 用二叉树来表示概率模型，同样使用了激活函数sigmoid，时间复杂度变低</li>
    </ul>
  </li>
  <li>接下来用负采样跳元模型训练(自监督训练)来展示word2vec的效果，数据集用PTB，语料库取自华尔街时报。数据集处理时用到了下采样方法: 高频词有概率被丢弃:</li>
</ul>

<center><img src="../assets/img/posts/20220905/34.jpg" /></center>

<p>上述式子中f($w_i$)是词在整个语料库中出现的比率，t是超参数。这样高频词就不会太影响模型效果，毕竟不太关注类似a和the与其他词共同出现的概率</p>
<ul>
  <li>在下采样与负采样完毕后，一个小批量中第i个样本包括中心词及其$n_i$个上下文词和$m_i$个噪声词，数据集还需要返回mask与label，分别用来遮掩&lt;pad&gt;与标记正例</li>
  <li>word2vec本质上其实是训练一个权重矩阵(词表大小, 嵌入维度)，就是一个nn.Embedding()。跳元模型的前向传播就是计算内积矩阵torch.bmm(embed_v, embed_u)
损失函数是带掩码的交叉熵损失</li>
  <li>跳元模型在预训练完毕后，可以用来找出语义相似的单词，即计算中心词与其余所有词的余弦相似度，计算结果最高的词即为语义最相似的单词</li>
  <li>无论是word2vec的哪个模型，都着眼于中心词与上下文词的关系</li>
  <li>全局向量的词嵌入GloVe: word2vec只考虑了局部的上下文词，GloVe则考虑了全局语料库统计来设计模型，训练GloVe是为了降低以下损失函数:</li>
</ul>

<center><img src="../assets/img/posts/20220905/35.jpg" /></center>

<p>上述式子中，$x_{ij}$是中心词i与上下文词j在一个上下文窗口出现的次数，h($x_{ij}$)是每个损失项的权重，当x小于c时，h(x)=$\frac{x}{c}^\alpha$，当x大于c时h(x)=1，$b_i$与$c_j$是可学习的偏置</p>
<ul>
  <li>子词嵌入模型: 对词的内部结构进行研究(比如dog和dogs的关系)
    <ul>
      <li>fastText模型: 每个中心词由其子词的向量之和表示(子词就是单词的某些连续字符)</li>
    </ul>

    <center><img src="../assets/img/posts/20220905/36.jpg" /></center>

    <p>其余部分与跳元模型相同</p>
    <ul>
      <li>字节对编码(Byte Pair Encoding, BPE): 一种算法，用来提取子词。BPE的本质就是对训练数据集进行统计分析，找出单词的公共符号，这些公共符号将作为划分子词的依据，对于每个单词，都将返回最长的子词划分结果(这样就可以获得任意长度的子词)</li>
    </ul>
  </li>
  <li>从大型语料库中训练的词向量可用于下游的自然语言处理任务，预训练的词向量可应用到词的类比性和相似性任务中，比如GloVe和fastText</li>
  <li>torch.topk(k)的作用是返回列表中的最大值(前k个)</li>
  <li>词相似: 利用余弦相似度</li>
  <li>词类比: a:b::c:d，比如man:woman::son:daughter，词类比任务就是给出a、b、c，找到d，即让d的词向量尽量靠近vec(c)+vec(b)-vec(a)</li>
  <li>word2vec于GloVe都是上下文无关的，即对于一个词元编码，只需要输入该词元即可</li>
  <li>NLP的六种任务: 情感分析、自然语言推断、语义角色标注、共指消解，NER和QA</li>
  <li>GPT的缺点: 自回归，单向</li>
  <li>BERT使用双向Transformer编码器来编码文本，BERT同样是预训练模型，可以基于双向上下文来表示任意词元</li>
  <li>针对不同的上下文任务，BERT需要对架构进行微调</li>
  <li>BERT可输入单个文本，也可以输入文本序列对，当输入单个文本时，BERT的输入序列是&lt;cls&gt;文本&lt;sep&gt;；当输入文本对时，BERT的输入序列是&lt;cls&gt;文本1&lt;sep&gt;文本2&lt;sep&gt;</li>
  <li>在BERT中，有三个嵌入层，分别是词元嵌入(普通的embedding)、段嵌入(两个片段序列的输入，用来区分不同的句子)和位置嵌入(可学习)，之后再把结果输入encoder中</li>
</ul>

<center><img src="../assets/img/posts/20220905/37.jpg" /></center>

<ul>
  <li>BERT通过两个预训练任务来优化双向Transformer编码器，分别是掩蔽语言模型(masked language modeling 即填空)和下一句预测(next sentence predicition)
    <ul>
      <li>掩遮语言模型: 随机掩蔽词元并使用来自双向上下文的词元以自监督的方式预测掩蔽词元。预测阶段使用单隐藏层的MLP来进行预测，输入BERT的编码结果和用于预测词元的位置，然后根据预测词元的位置得到预测词元的编码结果，然后输入mlp中得到预测结果</li>
      <li>下一句预测: 二分类任务，判断文本序列对是否是连续句子，预测时同样使用MLP，输入编码后的&lt;cls&gt;词元</li>
      <li>这两个任务在制作数据集的时候都需要加上一些负例或者噪声</li>
    </ul>
  </li>
  <li>BERT的预训练数据集有很多(针对不同的应用领域，使用不同的数据集进行训练)，最开始使用的是图书语料库和wiki</li>
  <li>BERT本质上就是一个双向Transformer编码器</li>
</ul>

<h1 id="14-自然语言处理-应用">14. 自然语言处理: 应用</h1>
<p>这一章主要介绍了如何将自然语言预处理模型应用到下游任务中，首先是传统的GloVe模型和子词嵌入模型，针对这种预训练模型，需要设计特定的网络结构在适配任务，但是BERT的出现，让下游任务应用更简单，有时候只需要加一个全连接层就行，参数也只需要微调，这一章主要介绍了两个下游任务，分别是情感分析和自然语言推断</p>
<ul>
  <li>情感分析任务(sentiment analysis): 本质上就是文本序列分类任务，数据集采用Imdb的电影评论集(评论+情感)</li>
  <li>处理长短不一的序列时，使用截断与填充来预处理数据集，可以将其变成长短一致的序列</li>
  <li>一般的预训练模型应用于下游任务的方式都是: 预训练模型(词元的文本表示embed)+架构(网络)+应用(各种任务)</li>
</ul>

<center><img src="../assets/img/posts/20220905/38.jpg" /></center>

<ul>
  <li>这里介绍了两种非BERT的架构，分别是用双向LSTM和CNN来处理情感分析任务:
    <ul>
      <li>双向LSTM: 双向LSTM的初始与最终步的隐状态连结起来作为文本序列的表示，然后连接一个全连接层，输出</li>
      <li>CNN: 这里使用了一种名为textCNN的网络架构，把文本序列看成一维图像进行处理，采用一维卷积来获得局部特征</li>
      <li>一维卷积是二维卷积的一个特例，同样是核函数沿着输入滑动。多通道输入的一维互相关等同于单输入通道的二维互相关</li>
    </ul>

    <center><img src="../assets/img/posts/20220905/39.jpg" /></center>
  </li>
  <li>自然语言推断(nature language inference, NLI)任务是文本对分类任务，对文本对进行判断，决定一个句子能否由另一个句子推断出，即假设(hypothesis)能否由前提(premise)推出</li>
  <li>两个句子的三种关系:
    <ul>
      <li>蕴涵(entailment): 假设可以从前提推出</li>
      <li>矛盾(contradiction): 假设的否定可以从前提推出(我感觉本质上就是假设不能由前提推出)</li>
      <li>中性(neutral): 所有其他的情况</li>
    </ul>
  </li>
  <li>NLI使用的数据集是斯坦福自然语言推断数据集(SNLI)</li>
  <li>接下来介绍两种进行NLI的方法，分别是使用注意力机制(包含MLP)和BERT微调:
    <ul>
      <li>使用注意力机制: 利用注意力机制将两个文本序列的词元对齐，然后比较、聚合这些信息，那么本质上就是三个步骤：注意、比较、聚合:
        <ul>
          <li>注意: 与attention机制类似</li>
          <li>比较: 比较软对齐的hypothesis与premise相比较</li>
          <li>聚合: 将比较结果concat之后送入mlp</li>
          <li>这中间涉及到很多mlp层</li>
        </ul>
      </li>
      <li>另一种方法就是利用BERT进行微调，后面会具体介绍</li>
    </ul>
  </li>
  <li>这种使用非BERT的应用都是将嵌入层的权重替换成预训练模型的权重</li>
  <li>BERT可处理的一些下游任务:
    <ul>
      <li>单文本分类(比如情感分析、句子在语法上是否可接受)</li>
      <li>文本对分类/回归(NLI、语义文本相似度)</li>
      <li>词元级任务: 比如文本标注(每个词元经过相同的全连接层后，返回词性标签)、问答(QA, 使用数据集SQuAD，给定段落与问题后，预测用段落的哪个片段进行回答(也即文本片段的开始与结束位置的预测))</li>
    </ul>
  </li>
  <li>加载bert模型时，可以将预训练好的参数直接放到定义好的网络架构中</li>
  <li>之前很多预训练模型在处理下游任务时，都需要为下游任务设定特定的框架，但是BERT却并不需要设置特定的框架，有时只需要添加一个额外的全连接层即可</li>
  <li>微调只更新部分参数</li>
</ul>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[d2l note]]></summary></entry><entry><title type="html">Web速成</title><link href="http://localhost:4000/Web%E9%80%9F%E6%88%90.html" rel="alternate" type="text/html" title="Web速成" /><published>2022-08-03T00:00:00+08:00</published><updated>2022-08-03T00:00:00+08:00</updated><id>http://localhost:4000/Web%E9%80%9F%E6%88%90</id><content type="html" xml:base="http://localhost:4000/Web%E9%80%9F%E6%88%90.html"><![CDATA[<h1 id="1-html学习">1. HTML学习</h1>

<center><img src="../assets/img/posts/20220803/2.jpg" /></center>
<p><br /></p>

<center><img src="../assets/img/posts/20220803/3.jpg" /></center>
<p><br /></p>

<center><img src="../assets/img/posts/20220803/4.jpg" /></center>

<h1 id="2-css学习">2. CSS学习</h1>

<center><img src="../assets/img/posts/20220803/5.jpg" /></center>
<p><br /></p>

<center><img src="../assets/img/posts/20220803/6.jpg" /></center>
<p><br /></p>

<center><img src="../assets/img/posts/20220803/7.jpg" /></center>
<p><br /></p>

<center><img src="../assets/img/posts/20220803/8.jpg" /></center>
<p><br /></p>

<center><img src="../assets/img/posts/20220803/9.jpg" /></center>
<p><br /></p>

<h1 id="3-js学习">3. JS学习</h1>
<center><img src="../assets/img/posts/20220803/10.jpg" /></center>
<p><br /></p>

<center><img src="../assets/img/posts/20220803/11.jpg" /></center>
<p><br /></p>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[速成HTML、CSS、JS]]></summary></entry><entry><title type="html">课程总结</title><link href="http://localhost:4000/%E5%A4%A7%E5%9B%9B%E4%B8%8B%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93.html" rel="alternate" type="text/html" title="课程总结" /><published>2022-04-29T00:00:00+08:00</published><updated>2022-04-29T00:00:00+08:00</updated><id>http://localhost:4000/%E5%A4%A7%E5%9B%9B%E4%B8%8B%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93</id><content type="html" xml:base="http://localhost:4000/%E5%A4%A7%E5%9B%9B%E4%B8%8B%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93.html"><![CDATA[<!-- TOC -->

<ul>
  <li><a href="#introduction-to-corporate-finance">Introduction to corporate finance</a>
    <ul>
      <li><a href="#课程简介">课程简介</a></li>
      <li><a href="#课程内容">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#工业科学">工业科学</a>
    <ul>
      <li><a href="#课程简介-1">课程简介</a></li>
      <li><a href="#课程内容-1">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#量子力学">量子力学</a>
    <ul>
      <li><a href="#课程简介-2">课程简介</a></li>
      <li><a href="#课程内容-2">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#统计物理">统计物理</a>
    <ul>
      <li><a href="#课程简介-3">课程简介</a></li>
      <li><a href="#课程内容-3">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#信号处理">信号处理</a>
    <ul>
      <li><a href="#课程简介-4">课程简介</a></li>
      <li><a href="#课程内容-4">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#结构力学2">结构力学2</a>
    <ul>
      <li><a href="#课程简介-5">课程简介</a></li>
      <li><a href="#课程内容-5">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#传热学heat-transfert">传热学(Heat Transfert)</a>
    <ul>
      <li><a href="#课程简介-6">课程简介</a></li>
      <li><a href="#课程内容-6">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#过程工程process-engin">过程工程(Process Engin)</a>
    <ul>
      <li><a href="#课程简介-7">课程简介</a></li>
      <li><a href="#课程内容-7">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#press">Press</a>
    <ul>
      <li><a href="#课程简介-8">课程简介</a></li>
      <li><a href="#课程内容-8">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#audiovisuel">Audiovisuel</a>
    <ul>
      <li><a href="#课程简介-9">课程简介</a></li>
      <li><a href="#课程内容-9">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#毕业设计">毕业设计</a>
    <ul>
      <li><a href="#毕设内容">毕设内容</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h1 id="introduction-to-corporate-finance">Introduction to corporate finance</h1>

<h2 id="课程简介">课程简介</h2>
<ul>
  <li>授课老师: Danielle Levi-Feunteun</li>
  <li>授课形式: 线上</li>
  <li>授课材料: 电子讲义</li>
  <li>考核形式: 每节课都有作业需要提交，算平时分，最后还有一个开卷的考试</li>
</ul>

<h2 id="课程内容">课程内容</h2>
<p>围绕着财报进行简单的介绍</p>

<h1 id="工业科学">工业科学</h1>

<h2 id="课程简介-1">课程简介</h2>
<ul>
  <li>授课老师: 付小尧</li>
  <li>授课形式: 工业科学由两部分组成，实验和正课，都在线下完成</li>
  <li>授课材料: 实验课有电子讲义，正课有讲义和TD</li>
  <li>考核形式: 实验和正课都有线下考试，各占五十</li>
</ul>

<h2 id="课程内容-1">课程内容</h2>
<p>实验一共有8节课，都在二号楼上，实验围绕PID控制原理展开，我影响比较深的有停车场栏杆控制、云台、装乒乓球、给羽毛球拍上弦</p>

<p>正课讲了一点二进制、逻辑电路</p>

<h1 id="量子力学">量子力学</h1>

<h2 id="课程简介-2">课程简介</h2>
<ul>
  <li>授课老师: 付小尧</li>
  <li>授课形式: 线下授课</li>
  <li>授课材料: 电子讲义和TD</li>
  <li>考核形式: 线上考试</li>
</ul>

<h2 id="课程内容-2">课程内容</h2>
<p>一共有8章，第一章介绍了光电效应、黑体，第二章介绍了薛定谔方程，后面的忘了</p>

<h1 id="统计物理">统计物理</h1>

<h2 id="课程简介-3">课程简介</h2>
<ul>
  <li>授课老师: Philippe Ribiere</li>
  <li>授课形式: 线下授课</li>
  <li>授课材料: 电子讲义和TD</li>
  <li>考核形式: 大作业(统计物理与人工智能的关系)</li>
</ul>

<h2 id="课程内容-3">课程内容</h2>
<p>一共四章，对量子力学里用到的统计知识进行了补充，统计物理根据对物质微观结构及微观粒子相互作用的认识，用概率统计的方法，对由大量粒子组成的宏观物体的物理性质及宏观规律作出微观解释的理论物理学分支。</p>

<h1 id="信号处理">信号处理</h1>

<h2 id="课程简介-4">课程简介</h2>
<ul>
  <li>授课老师: Antoine Roueff</li>
  <li>授课形式: 线上授课</li>
  <li>授课材料: 电子讲义、TD、TP(Matlab)</li>
  <li>考核形式: 线上考试</li>
</ul>

<h2 id="课程内容-4">课程内容</h2>
<p>没咋听</p>

<h1 id="结构力学2">结构力学2</h1>

<h2 id="课程简介-5">课程简介</h2>
<ul>
  <li>授课老师:  Olivier Bareille、黄行蓉</li>
  <li>授课形式: 线上授课</li>
  <li>授课材料: 电子讲义和TD</li>
  <li>考核形式: 大作业(做题)</li>
</ul>

<h2 id="课程内容-5">课程内容</h2>
<p>Olivier讲正课，黄老师讲TD，由于疫情原因，最终在家完成大作业</p>

<h1 id="传热学heat-transfert">传热学(Heat Transfert)</h1>

<h2 id="课程简介-6">课程简介</h2>
<ul>
  <li>授课老师: Nelson IBASETA、张堇</li>
  <li>授课形式: 线上授课</li>
  <li>授课材料: 电子讲义和TD</li>
  <li>考核形式: 线上考试</li>
</ul>

<h2 id="课程内容-6">课程内容</h2>
<p>Nelson讲正课，张老师讲TD，主要介绍了三种传热方式，分别是热传导，热对流和热辐射</p>

<h1 id="过程工程process-engin">过程工程(Process Engin)</h1>

<h2 id="课程简介-7">课程简介</h2>
<ul>
  <li>授课老师: Dominique Pareau、唐宏哲</li>
  <li>授课形式: 线上授课</li>
  <li>授课材料: 电子讲义和TD</li>
  <li>考核形式: 两个大作业，Dominique的是做题，唐老师的是设计一个蒸馏的方案</li>
</ul>

<h2 id="课程内容-7">课程内容</h2>
<p>Dominique主要讲了工业流程中各组分的物料守恒和能量守恒，唐宏哲主要讲了蒸馏</p>

<h1 id="press">Press</h1>

<h2 id="课程简介-8">课程简介</h2>
<ul>
  <li>授课老师: Vanessa</li>
  <li>授课形式: 线下和线上授课</li>
  <li>授课材料: 电子讲义</li>
  <li>考核形式: 大作业</li>
</ul>

<h2 id="课程内容-8">课程内容</h2>
<p>法语课</p>

<h1 id="audiovisuel">Audiovisuel</h1>

<h2 id="课程简介-9">课程简介</h2>
<ul>
  <li>授课老师: Fabien</li>
  <li>授课形式: 线下和线上授课</li>
  <li>授课材料: 电子讲义</li>
  <li>考核形式: 大作业</li>
</ul>

<h2 id="课程内容-9">课程内容</h2>
<p>法语课</p>

<h1 id="毕业设计">毕业设计</h1>

<h2 id="毕设内容">毕设内容</h2>
<p>大四最重要的一门课，我的毕设是设计了一个成绩预测模型，包含了风险学科预测和成绩区间预测，主要用了一些机器学习的算法和数据处理的知识，写论文花了一周时间，剩下的就是不停的答辩</p>]]></content><author><name>Quehry</name></author><category term="school" /><summary type="html"><![CDATA[记录大四下课程和课程笔记]]></summary></entry><entry><title type="html">毕设记录</title><link href="http://localhost:4000/Graduation-Project.html" rel="alternate" type="text/html" title="毕设记录" /><published>2022-04-20T00:00:00+08:00</published><updated>2022-04-20T00:00:00+08:00</updated><id>http://localhost:4000/Graduation-Project</id><content type="html" xml:base="http://localhost:4000/Graduation-Project.html"><![CDATA[<h1 id="1-数据预处理">1. 数据预处理</h1>

<h2 id="11-使用mysql对原始数据进行处理">1.1. 使用MySQL对原始数据进行处理</h2>
<ul>
  <li>目标: 生成每行为一个学生，第一列为学号，第二列到最后一列都是课程名称</li>
  <li>第一步: 创建表格
    <ol>
      <li>首先遇到的问题是创建列名时有MySQL关键字，所以对KCMC两端加上了反引号</li>
      <li>使用Group_concat时有内容长度限制，需要使用以下代码来暂时增大限制:
        <pre><code class="language-cmd">  SET GLOBAL group_concat_max_len = 4294967295;
  SET SESSION group_concat_max_len = 4294967295;
</code></pre>
      </li>
      <li>列名长度硬性要求: 不能超过64个字符，所以我采用了将英文翻译为中文的方法减少长度，有以下几门学科名称做过修改:
        <ul>
          <li>UPDATE grade_original SET KCMC = ‘网格生成方法及软件简介’ WHERE KCMC=’An Introduction to Mesh Generation Methods &amp; Software for Scientific Computing’</li>
          <li>UPDATE grade_original SET KCMC = ‘经典论文鉴赏:电磁学顶级论文精选’ WHERE KCMC=’Appreciation of Classical Papers: The Selected Top Papers in Electromagnetism’</li>
          <li>UPDATE grade_original SET KCMC = ‘动脉硬化的脆弱性评估:从体内成像到生物力学’ WHERE KCMC=’Atherosclerosis Vulnerability Assessment: From In Vivo Imaging To Biomechanics’</li>
          <li>UPDATE grade_original SET KCMC = ‘计算机建模和仿真基础:方法、技术和应用’ WHERE KCMC=’Basics of Computer-Based Modelling and Simulation: Methodologies, Technologies and Applications’</li>
          <li>UPDATE grade_original SET KCMC = ‘当代中国外交政策及其全球治理途径’ WHERE KCMC=’Contemporary Chinese Foreign Policy and Its Global Governance Approach’</li>
          <li>UPDATE grade_original SET KCMC = ‘灵活的中英文语言:成功的必要条件’ WHERE KCMC=’Elastic Language in Chinese and English: Essential for Successful’</li>
          <li>UPDATE grade_original SET KCMC = ‘自然界中的功能结构材料:从保护到传感’ WHERE KCMC=’Functional Structural Materials in Nature: From Protection to Sensing’</li>
          <li>UPDATE grade_original SET KCMC = ‘国际商法-在中国经商的法律环境’ WHERE KCMC=’International Business Law - The Legal Environment of Doing Business in China’</li>
          <li>UPDATE grade_original SET KCMC = ‘航空航天工程疲劳与损伤容限导论’ WHERE KCMC=’Introduction to Fatigue and Damage Tolerance in Aerospace Engineering’</li>
          <li>UPDATE grade_original SET KCMC = ‘模型检查定时系统导论:理论与实践’ WHERE KCMC=’Introduction to Model-Checking Timed Systems: Theory and Practice’</li>
          <li>UPDATE grade_original SET KCMC = ‘功能薄膜磁控溅射的研究现状与发展趋势’ WHERE KCMC=’Magnetron Sputtering of Functional Thin Films: Present Status and Trends’</li>
          <li>UPDATE grade_original SET KCMC = ‘材料表征热分析原理及应用’ WHERE KCMC=’Principles and Applications of Thermal Analysis for Materials Characterization’</li>
          <li>UPDATE grade_original SET KCMC = ‘从英语学习到口译翻译能力的发展:原则与策略’ WHERE KCMC=’Progression from English Study to Interpreting and Translation Competence: Principles and Strategies’</li>
          <li>一共十三门课名有做修改</li>
        </ul>
      </li>
      <li>MySQL对列数有硬性要求: 其中InnoDB引擎要求不超过1024，其余引擎不超过4096，但是我的列数一共有1425，所以我改用了MyISAM引擎</li>
      <li>MySQL命令行代码:</li>
    </ol>

    <pre><code class="language-cmd">  SELECT
  CONCAT(
      'CREATE TABLE grade_student (', GROUP_CONCAT(DISTINCT CONCAT('\`', KCMC, '\`', ' FLOAT', CHAR(10))
      SEPARATOR ','),
      ')', 'ENGINE=MyISAM DEFAULT CHARSET=gbk;')
  FROM
      grade_original

  INTO @sql;

  PREPARE stmt_name FROM @sql;
  EXECUTE stmt_name;
</code></pre>
  </li>
  <li>目前完成: 列名创建出来，XH一行填满，但是成绩没有填</li>
  <li>先放着，后面有时间再试试，先用python</li>
</ul>

<h2 id="12-利用python的pandas库对数据进行预处理">1.2 利用Python的pandas库对数据进行预处理</h2>
<ul>
  <li>首先实现了从原始的data_original转变成data_student_0: 里面数据格式如下: 每行代表一名学生，第一列为学号，第二列至最后一列列名是学科名称，数值是成绩</li>
  <li>然后实现了去除大家都有的学科，把参加课程设为1，未参加课程设为0，生成data_student_1.csv文件，用来进行聚类</li>
  <li>聚类算法: kmeans，选了4个簇，生成的结果总体上来说非常不错，但是我希望计算一下性能度量(<strong>待办</strong>)
    <ul>
      <li>更细化来说，首先我可以手动针对学生选课来确定学院，最好是通过大三或者大四的课程来确定，以免有人中途转系等等</li>
    </ul>
  </li>
  <li>聚类后生成了data_student_2.csv，里面新增了XY列，0代表国通，1代表计院，2代表航院，3代表中法</li>
  <li>然后需要针对学年进行进一步细分: 目前聚类后的结果分布如下</li>
</ul>

<center><img src="../assets/img/posts/20220420/2.jpg" /></center>]]></content><author><name>Quehry</name></author><category term="record" /><summary type="html"><![CDATA[毕设流程记录]]></summary></entry><entry><title type="html">算法基础</title><link href="http://localhost:4000/Algorithms.html" rel="alternate" type="text/html" title="算法基础" /><published>2022-04-13T00:00:00+08:00</published><updated>2022-04-13T00:00:00+08:00</updated><id>http://localhost:4000/Algorithms</id><content type="html" xml:base="http://localhost:4000/Algorithms.html"><![CDATA[<h1 id="目录">目录</h1>
<!-- TOC -->

<ul>
  <li><a href="#目录">目录</a></li>
  <li><a href="#0-简介">0. 简介</a></li>
  <li><a href="#1-第一周-枚举">1. 第一周 枚举</a>
    <ul>
      <li><a href="#11-完美立方">1.1. 完美立方</a></li>
      <li><a href="#12-生理周期">1.2. 生理周期</a></li>
      <li><a href="#13-称硬币">1.3. 称硬币</a></li>
      <li><a href="#14-熄灯问题">1.4. 熄灯问题</a></li>
    </ul>
  </li>
  <li><a href="#2-第二周-递归一">2. 第二周 递归(一)</a>
    <ul>
      <li><a href="#21-求阶乘">2.1. 求阶乘</a></li>
      <li><a href="#22-汉诺塔问题">2.2. 汉诺塔问题</a></li>
      <li><a href="#23-n皇后问题">2.3. n皇后问题</a></li>
      <li><a href="#24-逆波兰表达式求值">2.4. 逆波兰表达式求值</a></li>
    </ul>
  </li>
  <li><a href="#3-第三周-递归二">3. 第三周 递归(二)</a>
    <ul>
      <li><a href="#31-表达式求值">3.1. 表达式求值</a></li>
      <li><a href="#32-上台阶问题">3.2. 上台阶问题</a></li>
      <li><a href="#33-放苹果问题">3.3. 放苹果问题</a></li>
      <li><a href="#34-算24问题">3.4. 算24问题</a></li>
    </ul>
  </li>
  <li><a href="#4-第四周-二分算法">4. 第四周 二分算法</a>
    <ul>
      <li><a href="#41-程序或算法的时间复杂度">4.1. 程序或算法的时间复杂度</a></li>
      <li><a href="#42-二分查找的原理和实现">4.2. 二分查找的原理和实现</a></li>
      <li><a href="#43-二分法求方程的根">4.3. 二分法求方程的根</a></li>
    </ul>
  </li>
  <li><a href="#5-第五周-分治">5. 第五周 分治</a>
    <ul>
      <li><a href="#51-归并排序">5.1. 归并排序</a></li>
      <li><a href="#52-快速排序">5.2. 快速排序</a></li>
      <li><a href="#53-例题-输出前m大的数">5.3. 例题: 输出前m大的数</a></li>
      <li><a href="#54-例题-求排序的逆序数">5.4. 例题: 求排序的逆序数</a></li>
    </ul>
  </li>
  <li><a href="#6-第六周-动态规划一">6. 第六周 动态规划(一)</a>
    <ul>
      <li><a href="#61-数字三角形">6.1. 数字三角形</a></li>
      <li><a href="#62-动态规划解题的一般思路">6.2. 动态规划解题的一般思路</a></li>
      <li><a href="#63-最长上升子序列">6.3. 最长上升子序列</a></li>
      <li><a href="#64-最长公共子序列">6.4. 最长公共子序列</a></li>
      <li><a href="#65-最佳加法表达式">6.5. 最佳加法表达式</a></li>
    </ul>
  </li>
  <li><a href="#7-第七周-动态规划二">7. 第七周 动态规划(二)</a>
    <ul>
      <li><a href="#71-help-jimmy">7.1. Help Jimmy</a></li>
      <li><a href="#72-滑雪">7.2. 滑雪</a></li>
      <li><a href="#73-神奇的口袋">7.3. 神奇的口袋</a></li>
      <li><a href="#74-0-1背包问题">7.4. 0-1背包问题</a></li>
      <li><a href="#75-分蛋糕">7.5. 分蛋糕</a></li>
    </ul>
  </li>
  <li><a href="#8-第八周-深度优先搜索一">8. 第八周 深度优先搜索(一)</a>
    <ul>
      <li><a href="#81-在图上寻找路径和遍历">8.1. 在图上寻找路径和遍历</a></li>
      <li><a href="#82-图的表示方法-邻接矩阵和邻接表">8.2. 图的表示方法: 邻接矩阵和邻接表</a></li>
      <li><a href="#83-城堡问题">8.3. 城堡问题:</a></li>
      <li><a href="#84-踩方格">8.4. 踩方格</a></li>
    </ul>
  </li>
  <li><a href="#9-第九周-深度优先搜索二">9. 第九周 深度优先搜索(二)</a>
    <ul>
      <li><a href="#91-寻路问题">9.1. 寻路问题</a></li>
      <li><a href="#92-生日蛋糕">9.2. 生日蛋糕</a></li>
    </ul>
  </li>
  <li><a href="#10-第十周-广度优先搜索">10. 第十周 广度优先搜索</a>
    <ul>
      <li><a href="#101-抓住那头牛">10.1. 抓住那头牛</a></li>
      <li><a href="#102-迷宫问题">10.2. 迷宫问题</a></li>
      <li><a href="#103-鸣人和佐助">10.3. 鸣人和佐助</a></li>
      <li><a href="#104-八数码问题">10.4. 八数码问题</a></li>
    </ul>
  </li>
  <li><a href="#11-第十一周-贪心算法">11. 第十一周 贪心算法</a>
    <ul>
      <li><a href="#111-圣诞老人的礼物">11.1. 圣诞老人的礼物</a></li>
      <li><a href="#112-电影节">11.2. 电影节</a></li>
      <li><a href="#113-分配畜栏">11.3. 分配畜栏</a></li>
      <li><a href="#114-放置雷达">11.4. 放置雷达</a></li>
      <li><a href="#115-钓鱼">11.5. 钓鱼</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h1 id="0-简介">0. 简介</h1>
<ul>
  <li>课程来源于北大郭炜老师的MOOC，在中国大学MOOC平台上有网课，课程名为<a href="https://www.icourse163.org/course/PKU-1001894005?tid=1450413466" target="_blank">程序设计与算法(二)算法基础</a>，第九次开课，课程有附带的习题，该博客记录了我的随堂笔记</li>
</ul>

<h1 id="1-第一周-枚举">1. 第一周 枚举</h1>

<h2 id="11-完美立方">1.1. 完美立方</h2>
<ul>
  <li>枚举: 基于逐个尝试答案的一种问题求解策略</li>
  <li>例如: 求小于N的最小素数</li>
  <li>完美立方:</li>
</ul>

<center><img src="../assets/img/posts/20220413/1.jpg" /></center>

<ul>
  <li>解题思路:</li>
</ul>

<center><img src="../assets/img/posts/20220413/2.jpg" /></center>

<h2 id="12-生理周期">1.2. 生理周期</h2>
<ul>
  <li>题干:</li>
</ul>

<center><img src="../assets/img/posts/20220413/3.jpg" /></center>

<center><img src="../assets/img/posts/20220413/4.jpg" /></center>

<ul>
  <li>解题思路:</li>
</ul>

<center><img src="../assets/img/posts/20220413/5.jpg" /></center>

<h2 id="13-称硬币">1.3. 称硬币</h2>
<ul>
  <li>题干:</li>
</ul>

<center><img src="../assets/img/posts/20220413/6.jpg" /></center>

<center><img src="../assets/img/posts/20220413/7.jpg" /></center>

<ul>
  <li>解题思路</li>
</ul>

<center><img src="../assets/img/posts/20220413/8.jpg" /></center>

<h2 id="14-熄灯问题">1.4. 熄灯问题</h2>
<ul>
  <li>题干:</li>
</ul>

<center><img src="../assets/img/posts/20220413/9.jpg" /></center>

<center><img src="../assets/img/posts/20220413/10.jpg" /></center>

<center><img src="../assets/img/posts/20220413/11.jpg" /></center>

<ul>
  <li>解题思路:</li>
</ul>

<center><img src="../assets/img/posts/20220413/12.jpg" /></center>

<center><img src="../assets/img/posts/20220413/13.jpg" /></center>

<p>局部的思想，化繁为简</p>

<ul>
  <li>可以用0-31的十进制数来表示第一列的数据，因为其二进制数刚好对应开关的状态</li>
</ul>

<h1 id="2-第二周-递归一">2. 第二周 递归(一)</h1>
<h2 id="21-求阶乘">2.1. 求阶乘</h2>
<ul>
  <li>递归的基本概念: 一个函数调用其自身就是递归</li>
</ul>

<center><img src="../assets/img/posts/20220413/14.jpg" /></center>

<ul>
  <li>递归和普通函数调用一样是通过栈实现</li>
</ul>

<center><img src="../assets/img/posts/20220413/15.jpg" /></center>

<ul>
  <li>递归的作用
    <ul>
      <li>替代多重循环</li>
      <li>解决本来就是递归形式定义的问题</li>
      <li>将问题分解为规模更小的问题进行求解: 比如n！变成n * (n-1)</li>
    </ul>
  </li>
</ul>

<h2 id="22-汉诺塔问题">2.2. 汉诺塔问题</h2>
<ul>
  <li>任务描述:</li>
</ul>

<center><img src="../assets/img/posts/20220413/16.jpg" /></center>

<ul>
  <li>解决思路: 把盘子从A移动到C的过程分解为三个小问题，分别是移动n-1个盘子从A到B，然后移动1个盘子从A到C，最后移动n-1个盘子从B到C，这就是一个递归问题</li>
  <li>递归的核心思想是将大问题分解为规模更小的问题，同时还要保证是从n变成n-1</li>
  <li>代码实现:</li>
</ul>

<center><img src="../assets/img/posts/20220413/17.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20220413/18.jpg" /></center>
<p><br /></p>

<h2 id="23-n皇后问题">2.3. n皇后问题</h2>
<ul>
  <li>问题描述:</li>
</ul>

<center><img src="../assets/img/posts/20220413/19.jpg" /></center>

<p>用递归代替多重循环，皇后的攻击范围是横竖斜</p>
<ul>
  <li>解决思路: 同样是从第1行开始逐个往后摆放，但是这里的n是未知数，所以循环的层数不确定，这个时候就可以用递归代替循环，构造一个函数，表示从第k行开始摆放棋子，然后在循环内部判断每一列的位置是否能摆放，就是一个穷举问题了</li>
  <li>代码实现:</li>
</ul>

<center><img src="../assets/img/posts/20220413/20.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20220413/21.jpg" /></center>
<p><br /></p>
<center><img src="../assets/img/posts/20220413/22.jpg" /></center>
<p><br /></p>

<p>这个代码设计很巧妙的地方是，它会遍历所有的情况，只要是满足条件的就会输出，所以会返回所有可能的结果</p>

<h2 id="24-逆波兰表达式求值">2.4. 逆波兰表达式求值</h2>
<ul>
  <li>问题描述:</li>
</ul>

<center><img src="../assets/img/posts/20220413/23.jpg" /></center>

<ul>
  <li>输入输出例子:</li>
</ul>

<center><img src="../assets/img/posts/20220413/24.jpg" /></center>

<ul>
  <li>解决思路: 一个数也可以看成一个逆波兰表达式，那么就可以直接用递归求解，实现过程中需要边输入边递归</li>
  <li>代码实现:</li>
</ul>

<center><img src="../assets/img/posts/20220413/25.jpg" /></center>

<h1 id="3-第三周-递归二">3. 第三周 递归(二)</h1>
<h2 id="31-表达式求值">3.1. 表达式求值</h2>
<ul>
  <li>问题描述:</li>
</ul>

<center><img src="../assets/img/posts/20220413/26.jpg" /></center>

<ul>
  <li>解决思路: 
先看看表达式递归的定义</li>
</ul>
<center><img src="../assets/img/posts/20220413/27.jpg" /></center>
<center><img src="../assets/img/posts/20220413/28.jpg" /></center>
<p>即然把表达式的递归过程弄清楚了，那么只需要定义表达式、项、因子的函数即可</p>
<ul>
  <li>代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/29.jpg" /></center>
<center><img src="../assets/img/posts/20220413/30.jpg" /></center>
<center><img src="../assets/img/posts/20220413/31.jpg" /></center>
<center><img src="../assets/img/posts/20220413/32.jpg" /></center>

<h2 id="32-上台阶问题">3.2. 上台阶问题</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/33.jpg" /></center>
<ul>
  <li>输入输出样例:</li>
</ul>
<center><img src="../assets/img/posts/20220413/34.jpg" /></center>
<ul>
  <li>解决思路: 将n级台阶的走法看成n-1级台阶的走法+n-2级台阶的走法，分别代表在第一步走一阶还是两阶，这里需要设置边界条件来防止无限递归</li>
</ul>
<center><img src="../assets/img/posts/20220413/35.jpg" /></center>
<ul>
  <li>代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/36.jpg" /></center>

<h2 id="33-放苹果问题">3.3. 放苹果问题</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/37.jpg" /></center>
<ul>
  <li>解决思路: 又是计算方法的总数，那么和上台阶问题一样，用表达式来表示递归，分类讨论。假设i个苹果，k个盘子，如果k&gt;i，那么等价于把i个苹果放到i个盘子里，因为一定有k-i个盘子空着；如果k&lt;=i，那么又将问题分为有没有空盘子，如果有空盘子，那么至少有一个空盘子，表示为把i个苹果放到k-1个盘子里，如果没有空盘子，那么等价于把i-k个苹果放到k个盘子里</li>
</ul>
<center><img src="../assets/img/posts/20220413/38.jpg" /></center>
<ul>
  <li>代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/39.jpg" /></center>

<h2 id="34-算24问题">3.4. 算24问题</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/40.jpg" /></center>
<ul>
  <li>输入输出样例:</li>
</ul>
<center><img src="../assets/img/posts/20220413/41.jpg" /></center>
<ul>
  <li>判断两个浮点数是否相等，用两个浮点数的差是否小于某个值</li>
  <li>解决思路: 不论给了多少个数计算24，都需要首先计算出两个数的计算结果，这个计算过程可以是加减乘除任意，然后得到的结果再和剩下的n-1个数算24，这样就可以变成一个递归问题，边界条件是只剩一个数的时候是否是24</li>
</ul>
<center><img src="../assets/img/posts/20220413/42.jpg" /></center>
<ul>
  <li>代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/43.jpg" /></center>
<center><img src="../assets/img/posts/20220413/44.jpg" /></center>
<center><img src="../assets/img/posts/20220413/45.jpg" /></center>
<center><img src="../assets/img/posts/20220413/46.jpg" /></center>

<h1 id="4-第四周-二分算法">4. 第四周 二分算法</h1>
<h2 id="41-程序或算法的时间复杂度">4.1. 程序或算法的时间复杂度</h2>
<ul>
  <li>时间复杂度的定义:</li>
</ul>
<center><img src="../assets/img/posts/20220413/47.jpg" /></center>
<p>重点是明白程序中固定的操作是什么</p>
<ul>
  <li>复杂度有平均复杂度和最坏复杂度两种，两者可能一致，也可能不一致，一般来说只要平均复杂度不太高，算法的效率就还可以</li>
  <li>常见的时间复杂度:</li>
</ul>
<center><img src="../assets/img/posts/20220413/48.jpg" /></center>
<center><img src="../assets/img/posts/20220413/49.jpg" /></center>

<h2 id="42-二分查找的原理和实现">4.2. 二分查找的原理和实现</h2>
<ul>
  <li>首先可以看这么一个问题:</li>
</ul>
<center><img src="../assets/img/posts/20220413/50.jpg" /></center>
<ul>
  <li>二分查找的实现: 时间复杂度是O(log(n))</li>
</ul>
<center><img src="../assets/img/posts/20220413/51.jpg" /></center>
<ul>
  <li>查找比待查找数小的最大坐标的函数实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/52.jpg" /></center>
<ul>
  <li>二分查找的问题前提是序列必须是递增或者递减的，即有序的</li>
  <li>为了防止数据溢出，写中点的时候要这么写: int mid = L + (R - L) / 2</li>
  <li>整型在转型的时候是向下取整</li>
</ul>

<h2 id="43-二分法求方程的根">4.3. 二分法求方程的根</h2>
<ul>
  <li>二分法求方程的根需要方程满足一定的条件，不是所有的方程都可以用二分法求根</li>
  <li>问题描述及求解思路:</li>
</ul>
<center><img src="../assets/img/posts/20220413/53.jpg" /></center>
<ul>
  <li>代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/54.jpg" /></center>
<ul>
  <li>如果一个序列不是有序的，可以用排序算法对序列先进行排序然后二分查找</li>
</ul>

<h1 id="5-第五周-分治">5. 第五周 分治</h1>
<h2 id="51-归并排序">5.1. 归并排序</h2>
<ul>
  <li>分治的基本概念:</li>
</ul>
<center><img src="../assets/img/posts/20220413/55.jpg" /></center>
<ul>
  <li>分治的典型应用: 归并排序</li>
</ul>
<center><img src="../assets/img/posts/20220413/56.jpg" /></center>
<ul>
  <li>归并排序的思路就是先分治，然后归并，代码实现如下：</li>
</ul>
<center><img src="../assets/img/posts/20220413/57.jpg" /></center>
<center><img src="../assets/img/posts/20220413/58.jpg" /></center>
<center><img src="../assets/img/posts/20220413/59.jpg" /></center>
<ul>
  <li>归并排序的时间复杂度:</li>
</ul>
<center><img src="../assets/img/posts/20220413/60.jpg" /></center>

<h2 id="52-快速排序">5.2. 快速排序</h2>
<ul>
  <li>快速排序的思想:</li>
</ul>
<center><img src="../assets/img/posts/20220413/61.jpg" /></center>
<ul>
  <li>代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/62.jpg" /></center>
<center><img src="../assets/img/posts/20220413/63.jpg" /></center>
<center><img src="../assets/img/posts/20220413/64.jpg" /></center>
<ul>
  <li>快速排序的时间复杂度是O(nlog(n))，这是在运气不坏的情况下得出的结果(平均复杂度)，运气最坏的情况下时间复杂度为O($n^2$)(最坏复杂度)</li>
</ul>

<h2 id="53-例题-输出前m大的数">5.3. 例题: 输出前m大的数</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/65.jpg" /></center>
<ul>
  <li>解决思路:</li>
</ul>
<center><img src="../assets/img/posts/20220413/66.jpg" /></center>
<p>用分治的思想解决问题，先把前m个元素移到数组的最右边，然后在对这m个元素进行快排</p>
<ul>
  <li>具体解决方法:</li>
</ul>
<center><img src="../assets/img/posts/20220413/67.jpg" /></center>
<center><img src="../assets/img/posts/20220413/68.jpg" /></center>
<ul>
  <li>时间复杂度计算:</li>
</ul>
<center><img src="../assets/img/posts/20220413/69.jpg" /></center>

<h2 id="54-例题-求排序的逆序数">5.4. 例题: 求排序的逆序数</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/70.jpg" /></center>
<ul>
  <li>解决思路:</li>
</ul>
<center><img src="../assets/img/posts/20220413/71.jpg" /></center>
<center><img src="../assets/img/posts/20220413/72.jpg" /></center>
<p>分治一般都使用了递归</p>

<h1 id="6-第六周-动态规划一">6. 第六周 动态规划(一)</h1>
<h2 id="61-数字三角形">6.1. 数字三角形</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/73.jpg" /></center>
<ul>
  <li>输入格式:</li>
</ul>
<center><img src="../assets/img/posts/20220413/74.jpg" /></center>
<ul>
  <li>解题思路: 
看成递归问题</li>
</ul>
<center><img src="../assets/img/posts/20220413/75.jpg" /></center>
<ul>
  <li>递归程序代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/76.jpg" /></center>
<ul>
  <li>虽然说在代码逻辑这一方面，递归算法没有问题，但是这个算法的时间复杂度太高，程序很容易超时:</li>
</ul>
<center><img src="../assets/img/posts/20220413/77.jpg" /></center>
<ul>
  <li>之前的递归算法中存在过多的重复计算，如果能把每一步的计算结果保存起来，那么即可避免重复计算，算法的时间复杂度为O($n^2$)</li>
</ul>
<center><img src="../assets/img/posts/20220413/78.jpg" /></center>
<ul>
  <li>记忆递归型动规程序:</li>
</ul>
<center><img src="../assets/img/posts/20220413/79.jpg" /></center>
<p>用一个二维数组存储每一个结点的max值，那么读取到这个结点时，就可以直接获得数值，避免了重复计算</p>
<ul>
  <li>也可以用递推的思想解决问题，先把最后一行的结果计算出来，然后从下到上逐步计算，用一个双重循环解决</li>
</ul>
<center><img src="../assets/img/posts/20220413/80.jpg" /></center>
<ul>
  <li>代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/81.jpg" /></center>
<ul>
  <li>还可以对空间进行优化，因为下一层的数值在计算上一层的数值后就没有用了，那么完全不需要用一个二维数组存储maxsum，完全可以用一个一维数组存放。再进一步来说，连maxSum数组都可以不要，直接用D的第n行替代maxSum</li>
</ul>
<center><img src="../assets/img/posts/20220413/82.jpg" /></center>
<ul>
  <li>空间优化后的代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/83.jpg" /></center>

<h2 id="62-动态规划解题的一般思路">6.2. 动态规划解题的一般思路</h2>
<ul>
  <li>递归到动规的一般转化方法:</li>
</ul>
<center><img src="../assets/img/posts/20220413/84.jpg" /></center>
<ul>
  <li>动规解题的一般思路:
    <ul>
      <li>第一步: 将原问题分解为子问题</li>
    </ul>
    <center><img src="../assets/img/posts/20220413/85.jpg" /></center>
    <ul>
      <li>第二步: 确定状态</li>
    </ul>
    <center><img src="../assets/img/posts/20220413/86.jpg" /></center>
    <ul>
      <li>第三步: 确定一些初始状态的值</li>
    </ul>
    <center><img src="../assets/img/posts/20220413/87.jpg" /></center>
    <ul>
      <li>第四步: 确定状态转移方程</li>
    </ul>
    <center><img src="../assets/img/posts/20220413/88.jpg" /></center>
  </li>
  <li>能用动规解决的问题的特点:</li>
</ul>
<center><img src="../assets/img/posts/20220413/89.jpg" /></center>

<h2 id="63-最长上升子序列">6.3. 最长上升子序列</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/90.jpg" /></center>
<ul>
  <li>输入输出格式:</li>
</ul>
<center><img src="../assets/img/posts/20220413/91.jpg" /></center>
<ul>
  <li>解题思路:
    <ul>
      <li>找子问题:</li>
    </ul>
    <center><img src="../assets/img/posts/20220413/92.jpg" /></center>
    <center><img src="../assets/img/posts/20220413/93.jpg" /></center>
    <ul>
      <li>确定状态:</li>
    </ul>
    <center><img src="../assets/img/posts/20220413/94.jpg" /></center>
    <ul>
      <li>找出状态转移方程:</li>
    </ul>
    <center><img src="../assets/img/posts/20220413/95.jpg" /></center>
  </li>
  <li>代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/96.jpg" /></center>
<ul>
  <li>动规的常用两种形式:</li>
</ul>
<center><img src="../assets/img/posts/20220413/97.jpg" /></center>

<h2 id="64-最长公共子序列">6.4. 最长公共子序列</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/98.jpg" /></center>
<ul>
  <li>输入输出样例:</li>
</ul>
<center><img src="../assets/img/posts/20220413/99.jpg" /></center>
<ul>
  <li>解题思路:</li>
</ul>
<center><img src="../assets/img/posts/20220413/100.jpg" /></center>
<p>重要的还是找到一个合适的子问题与状态</p>
<ul>
  <li>状态转移方程</li>
</ul>
<center><img src="../assets/img/posts/20220413/101.jpg" /></center>
<ul>
  <li>证明一下这个递推公式是正确的:</li>
</ul>
<center><img src="../assets/img/posts/20220413/102.jpg" /></center>
<ul>
  <li>代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/103.jpg" /></center>
<center><img src="../assets/img/posts/20220413/104.jpg" /></center>

<h2 id="65-最佳加法表达式">6.5. 最佳加法表达式</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/105.jpg" /></center>
<ul>
  <li>解题思路:</li>
</ul>
<center><img src="../assets/img/posts/20220413/106.jpg" /></center>
<center><img src="../assets/img/posts/20220413/107.jpg" /></center>
<center><img src="../assets/img/posts/20220413/108.jpg" /></center>

<h1 id="7-第七周-动态规划二">7. 第七周 动态规划(二)</h1>
<h2 id="71-help-jimmy">7.1. Help Jimmy</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/109.jpg" /></center>
<center><img src="../assets/img/posts/20220413/110.jpg" /></center>
<ul>
  <li>输入输出样例:</li>
</ul>
<center><img src="../assets/img/posts/20220413/111.jpg" /></center>
<center><img src="../assets/img/posts/20220413/112.jpg" /></center>
<ul>
  <li>解题思路:</li>
</ul>
<center><img src="../assets/img/posts/20220413/113.jpg" /></center>
<p>板子的顺序其实没有关系，重要的关注点是当前板子的左侧或右侧正下方的板子是哪个板子，然后计算出从每个板子的左侧或者右侧下降需要的最短时间，也就是说这里的状态值得是不同的板子</p>
<ul>
  <li>伪代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/114.jpg" /></center>
<center><img src="../assets/img/posts/20220413/115.jpg" /></center>
<center><img src="../assets/img/posts/20220413/116.jpg" /></center>
<p>将下落点看成宽度为0的板子是一种很好的思路</p>
<ul>
  <li>时间复杂度:</li>
</ul>
<center><img src="../assets/img/posts/20220413/117.jpg" /></center>

<h2 id="72-滑雪">7.2. 滑雪</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/118.jpg" /></center>
<ul>
  <li>输入输出样例:</li>
</ul>
<center><img src="../assets/img/posts/20220413/119.jpg" /></center>
<ul>
  <li>解题思路:</li>
</ul>
<center><img src="../assets/img/posts/20220413/120.jpg" /></center>
<center><img src="../assets/img/posts/20220413/121.jpg" /></center>
<center><img src="../assets/img/posts/20220413/122.jpg" /></center>
<p>这个题目递推的顺序很奇怪，如果按照二维数组的排序顺序来递推会出现问题，这里比较好的解决思路是把点按照高度排序，因为如果高度低的点值没求出来的话，高度高的点的值一定求不出来。然后这里排完序后有两种解决思路，一种是按顺序把每个点的值根据周围四个低的点求出，另一种思路是按照顺序每次更新周围四个点的值</p>

<h2 id="73-神奇的口袋">7.3. 神奇的口袋</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/123.jpg" /></center>
<ul>
  <li>输入输出样例:</li>
</ul>
<center><img src="../assets/img/posts/20220413/124.jpg" /></center>
<ul>
  <li>当然可以用枚举的方法暴力求解</li>
  <li>也可以用递推的方法求解:</li>
</ul>
<center><img src="../assets/img/posts/20220413/125.jpg" /></center>
<p>设计一个递推的函数，代表前k个物品凑w体积的方法个数，那么在新出现这个物品时有两个选择，即选或不选</p>
<ul>
  <li>动规解法:</li>
</ul>
<center><img src="../assets/img/posts/20220413/126.jpg" /></center>
<p>用二维数组来表示状态</p>

<h2 id="74-0-1背包问题">7.4. 0-1背包问题</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/127.jpg" /></center>
<ul>
  <li>解题思路: 和上一小节的思路类似，状态同样用二维数组表示，然后找出状态转移方程求解即可</li>
</ul>
<center><img src="../assets/img/posts/20220413/128.jpg" /></center>
<center><img src="../assets/img/posts/20220413/129.jpg" /></center>
<center><img src="../assets/img/posts/20220413/130.jpg" /></center>
<p>可以用滚动数组的思想来优化空间，递推的过程是从右往左替换滚动数组的过程</p>

<h2 id="75-分蛋糕">7.5. 分蛋糕</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/131.jpg" /></center>
<ul>
  <li>输入输出样例:</li>
</ul>
<center><img src="../assets/img/posts/20220413/132.jpg" /></center>
<ul>
  <li>解题思路:</li>
</ul>
<center><img src="../assets/img/posts/20220413/133.jpg" /></center>
<p>由于这里存在高宽，所以状态需要用三维数组表示</p>
<ul>
  <li>递推公式:</li>
</ul>
<center><img src="../assets/img/posts/20220413/134.jpg" /></center>

<h1 id="8-第八周-深度优先搜索一">8. 第八周 深度优先搜索(一)</h1>
<h2 id="81-在图上寻找路径和遍历">8.1. 在图上寻找路径和遍历</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/135.jpg" /></center>
<ul>
  <li>利用这种策略，可能出现的情况:</li>
</ul>
<center><img src="../assets/img/posts/20220413/136.jpg" /></center>
<ul>
  <li>深度优先搜索(Depth-First-Search)的定义:</li>
</ul>
<center><img src="../assets/img/posts/20220413/137.jpg" /></center>
<ul>
  <li>刚才那个问题的伪代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/138.jpg" /></center>
<center><img src="../assets/img/posts/20220413/139.jpg" /></center>
<ul>
  <li>如果要记录路径，代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/140.jpg" /></center>
<center><img src="../assets/img/posts/20220413/141.jpg" /></center>

<h2 id="82-图的表示方法-邻接矩阵和邻接表">8.2. 图的表示方法: 邻接矩阵和邻接表</h2>
<ul>
  <li>邻接矩阵表示图:</li>
</ul>
<center><img src="../assets/img/posts/20220413/142.jpg" /></center>
<ul>
  <li>邻接表表示图:</li>
</ul>
<center><img src="../assets/img/posts/20220413/143.jpg" /></center>

<h2 id="83-城堡问题">8.3. 城堡问题:</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/144.jpg" /></center>
<ul>
  <li>输入输出样例:</li>
</ul>
<center><img src="../assets/img/posts/20220413/145.jpg" /></center>
<center><img src="../assets/img/posts/20220413/146.jpg" /></center>
<ul>
  <li>解题思路: 这种比较抽象的问题可以通过建模转换成相对简单的题目，把城堡的方块看成节点，然后如果两个方块连接，则连接这两个节点</li>
</ul>
<center><img src="../assets/img/posts/20220413/147.jpg" /></center>
<p>求房间个数等价于求极大连通子图个数</p>
<center><img src="../assets/img/posts/20220413/148.jpg" /></center>
<ul>
  <li>代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/149.jpg" /></center>
<center><img src="../assets/img/posts/20220413/150.jpg" /></center>

<h2 id="84-踩方格">8.4. 踩方格</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/151.jpg" /></center>
<ul>
  <li>解决思路: 用递归的思路解决问题，第一步的选择有三种，那么走n步的方案数等于这三种走法的方案和</li>
</ul>
<center><img src="../assets/img/posts/20220413/152.jpg" /></center>
<ul>
  <li>代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/153.jpg" /></center>
<center><img src="../assets/img/posts/20220413/154.jpg" /></center>

<h1 id="9-第九周-深度优先搜索二">9. 第九周 深度优先搜索(二)</h1>
<h2 id="91-寻路问题">9.1. 寻路问题</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/155.jpg" /></center>
<ul>
  <li>解题思路: 从城市1开始深度优先遍历整个图，找到能到达城市N的最优路线(在不超过开销情况下的最短路径)</li>
</ul>
<center><img src="../assets/img/posts/20220413/156.jpg" /></center>
<p>但是这种方法会超时，所以我们需要对算法进行改进(剪枝)，最容易想到的方法是最优性剪枝，但是发现这种剪枝方法还是超时了</p>
<center><img src="../assets/img/posts/20220413/157.jpg" /></center>
<p>这种剪枝方法能保存中间计算结果，效果比之前的最优性剪枝要好</p>
<ul>
  <li>代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/158.jpg" /></center>
<center><img src="../assets/img/posts/20220413/159.jpg" /></center>
<center><img src="../assets/img/posts/20220413/160.jpg" /></center>
<center><img src="../assets/img/posts/20220413/161.jpg" /></center>
<center><img src="../assets/img/posts/20220413/162.jpg" /></center>

<h2 id="92-生日蛋糕">9.2. 生日蛋糕</h2>
<ul>
  <li>下棋也是一个深度搜索的过程</li>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/163.jpg" /></center>
<p>蛋糕的高和半径是递减的</p>
<ul>
  <li>解题思路:</li>
</ul>
<center><img src="../assets/img/posts/20220413/164.jpg" /></center>
<ul>
  <li>代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/165.jpg" /></center>
<center><img src="../assets/img/posts/20220413/166.jpg" /></center>
<ul>
  <li>剪枝(剪枝在深度优先搜索中很重要)</li>
</ul>
<center><img src="../assets/img/posts/20220413/167.jpg" /></center>

<h1 id="10-第十周-广度优先搜索">10. 第十周 广度优先搜索</h1>
<h2 id="101-抓住那头牛">10.1. 抓住那头牛</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/168.jpg" /></center>
<ul>
  <li>解决思路:</li>
</ul>
<center><img src="../assets/img/posts/20220413/169.jpg" /></center>
<p>第一种想法是用深度优先搜索解决问题，让农夫尝试所有的走法，不能走重，不能往下走了就回溯</p>
<center><img src="../assets/img/posts/20220413/170.jpg" /></center>
<center><img src="../assets/img/posts/20220413/171.jpg" /></center>
<p>第二种想法是用广度优先搜索的思路解决问题，对所有的节点进行分层，广搜的优点是确保可以找到最优解，但是因为拓展出来的节点比较多，且多数节点都需要保存，因此需要的存储空间较大，<strong>用队列保存节点</strong></p>
<ul>
  <li>广搜算法:</li>
</ul>
<center><img src="../assets/img/posts/20220413/172.jpg" /></center>
<p>光看文字可能有些晦涩难懂，下面有open表和close表变化的实例</p>
<center><img src="../assets/img/posts/20220413/173.jpg" /></center>
<center><img src="../assets/img/posts/20220413/174.jpg" /></center>
<center><img src="../assets/img/posts/20220413/175.jpg" /></center>
<center><img src="../assets/img/posts/20220413/176.jpg" /></center>
<center><img src="../assets/img/posts/20220413/177.jpg" /></center>
<center><img src="../assets/img/posts/20220413/178.jpg" /></center>
<center><img src="../assets/img/posts/20220413/179.jpg" /></center>
<ul>
  <li>代码实现</li>
</ul>
<center><img src="../assets/img/posts/20220413/180.jpg" /></center>
<center><img src="../assets/img/posts/20220413/181.jpg" /></center>
<center><img src="../assets/img/posts/20220413/182.jpg" /></center>
<p>在了解广度优先搜索的概念后，代码实现的难点在于队列queue的使用，queue的实例化可以用queue&lt;T&gt;来实现，q.front()是取当前元素，q.push()是在队列尾部添加元素，q.pop()是删除头部元素</p>

<h2 id="102-迷宫问题">10.2. 迷宫问题</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/183.jpg" /></center>
<ul>
  <li>解决思路: 广搜</li>
</ul>
<center><img src="../assets/img/posts/20220413/184.jpg" /></center>
<p>这里队列不能用STL的queue实现(因为要给出最短路径)，要自己写，可以用一维数组实现</p>
<center><img src="../assets/img/posts/20220413/185.jpg" /></center>
<center><img src="../assets/img/posts/20220413/186.jpg" /></center>
<p>其实就是记录了每个节点的父节点，在达到重点的时候可以一路返回过去得到路径</p>

<h2 id="103-鸣人和佐助">10.3. 鸣人和佐助</h2>
<p>鸣人和佐助问题是迷宫问题的一个变种</p>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/187.jpg" /></center>
<ul>
  <li>解决思路:</li>
</ul>
<center><img src="../assets/img/posts/20220413/188.jpg" /></center>
<p>由于每个位置的查克拉不同也会导致状态的不同，所以状态用三个参数表示，然后根据条件拓展节点</p>
<ul>
  <li>问题变种:</li>
</ul>
<center><img src="../assets/img/posts/20220413/189.jpg" /></center>
<center><img src="../assets/img/posts/20220413/190.jpg" /></center>

<h2 id="104-八数码问题">10.4. 八数码问题</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/191.jpg" /></center>
<ul>
  <li>解决思路:</li>
</ul>
<center><img src="../assets/img/posts/20220413/192.jpg" /></center>
<p>用广度优先搜索来解决问题，优先拓展浅层节点，在逐渐深入</p>
<ul>
  <li>广度优先搜索的代码框架:</li>
</ul>
<center><img src="../assets/img/posts/20220413/193.jpg" /></center>
<ul>
  <li>这个题目的关键点在于判重，状态数目大，如何存储才能较快判断一个状态是否重复</li>
  <li>一些可能的编码方案:</li>
</ul>
<center><img src="../assets/img/posts/20220413/194.jpg" /></center>
<center><img src="../assets/img/posts/20220413/195.jpg" /></center>
<center><img src="../assets/img/posts/20220413/196.jpg" /></center>
<center><img src="../assets/img/posts/20220413/197.jpg" /></center>
<center><img src="../assets/img/posts/20220413/198.jpg" /></center>
<ul>
  <li>继续优化问题的方法: 判定八数码问题是否有解</li>
</ul>
<center><img src="../assets/img/posts/20220413/199.jpg" /></center>
<p>移动0的位置，不改变排列的奇偶性</p>
<ul>
  <li>代码实现(单向广搜，用set判重)</li>
</ul>
<center><img src="../assets/img/posts/20220413/200.jpg" /></center>
<center><img src="../assets/img/posts/20220413/201.jpg" /></center>
<center><img src="../assets/img/posts/20220413/202.jpg" /></center>
<center><img src="../assets/img/posts/20220413/203.jpg" /></center>
<center><img src="../assets/img/posts/20220413/204.jpg" /></center>
<center><img src="../assets/img/posts/20220413/205.jpg" /></center>
<center><img src="../assets/img/posts/20220413/206.jpg" /></center>
<center><img src="../assets/img/posts/20220413/207.jpg" /></center>
<ul>
  <li>其余优化问题的方法: 双向广搜、针对本题的预处理、A*算法</li>
  <li>广搜和深搜的比较:</li>
</ul>
<center><img src="../assets/img/posts/20220413/208.jpg" /></center>

<h1 id="11-第十一周-贪心算法">11. 第十一周 贪心算法</h1>
<h2 id="111-圣诞老人的礼物">11.1. 圣诞老人的礼物</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/209.jpg" /></center>
<ul>
  <li>样例输入输出:</li>
</ul>
<center><img src="../assets/img/posts/20220413/210.jpg" /></center>
<p>第一行表示箱子总数和可携带的最大重量，其余行都表示箱子的信息，第一列是价值，第二列是重量</p>
<ul>
  <li>解决思路:</li>
</ul>
<center><img src="../assets/img/posts/20220413/211.jpg" /></center>
<p>因为要携带尽可能价值高的糖果，所以可以把所有箱子的价值重量比算出来，然后排列，按顺序装糖果。这种方法就是贪心算法的思路</p>
<ul>
  <li>代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/212.jpg" /></center>
<center><img src="../assets/img/posts/20220413/213.jpg" /></center>
<p>这里实现candy的结构体时，结构体里面重载了运算符&lt;，operator是转换运算符，这样就可以直接调用sort对candies进行排序</p>
<ul>
  <li>证明这种方法是正确的:</li>
</ul>
<center><img src="../assets/img/posts/20220413/214.jpg" /></center>
<ul>
  <li>贪心算法:</li>
</ul>
<center><img src="../assets/img/posts/20220413/215.jpg" /></center>
<p>每一步行动总是按照某种指标选取最优的操作来进行</p>

<h2 id="112-电影节">11.2. 电影节</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/216.jpg" /></center>
<ul>
  <li>样例输入输出:</li>
</ul>
<center><img src="../assets/img/posts/20220413/217.jpg" /></center>
<ul>
  <li>解决思路:</li>
</ul>
<center><img src="../assets/img/posts/20220413/218.jpg" /></center>
<ul>
  <li>证明贪心算法的正确性:</li>
</ul>
<center><img src="../assets/img/posts/20220413/219.jpg" /></center>

<h2 id="113-分配畜栏">11.3. 分配畜栏</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/220.jpg" /></center>
<ul>
  <li>解决思路:</li>
</ul>
<center><img src="../assets/img/posts/20220413/221.jpg" /></center>
<center><img src="../assets/img/posts/20220413/222.jpg" /></center>
<center><img src="../assets/img/posts/20220413/223.jpg" /></center>
<p>按照奶牛的开始挤奶时间来分配奶牛的畜栏，可以用队列存储最早结束的畜栏的时间</p>
<ul>
  <li>代码实现:</li>
</ul>
<center><img src="../assets/img/posts/20220413/224.jpg" /></center>
<center><img src="../assets/img/posts/20220413/225.jpg" /></center>
<center><img src="../assets/img/posts/20220413/226.jpg" /></center>
<center><img src="../assets/img/posts/20220413/227.jpg" /></center>
<p>priority是优先队列，在优先队列中，优先级高的元素先出队列，并非按照先进先出的顺序</p>

<h2 id="114-放置雷达">11.4. 放置雷达</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/228.jpg" /></center>
<ul>
  <li>解决思路
    <ul>
      <li>首先把题目问题转换一下:</li>
    </ul>
    <center><img src="../assets/img/posts/20220413/229.jpg" /></center>
    <ul>
      <li>接下来我们通过观察得到一个重要的结论，那就是如果一个雷达可以覆盖多个雷达，那么这个雷达可以在所覆盖雷达的最右边的起点</li>
    </ul>
    <center><img src="../assets/img/posts/20220413/230.jpg" /></center>
    <ul>
      <li>贪心算法实现步骤:</li>
    </ul>
    <center><img src="../assets/img/posts/20220413/231.jpg" /></center>
    <center><img src="../assets/img/posts/20220413/232.jpg" /></center>
  </li>
</ul>

<h2 id="115-钓鱼">11.5. 钓鱼</h2>
<ul>
  <li>问题描述:</li>
</ul>
<center><img src="../assets/img/posts/20220413/233.jpg" /></center>
<ul>
  <li>解决思路:</li>
</ul>
<center><img src="../assets/img/posts/20220413/234.jpg" /></center>
<center><img src="../assets/img/posts/20220413/235.jpg" /></center>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[郭炜老师算法网课记录]]></summary></entry></feed>