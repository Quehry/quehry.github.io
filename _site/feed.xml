<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-02-14T17:30:28+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Quehry</title><subtitle>Artificial Intelligence trends and concepts made easy.</subtitle><author><name>Quehry</name></author><entry><title type="html">研一上课程总结</title><link href="http://localhost:4000/%E7%A0%94%E4%B8%80%E4%B8%8A%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93.html" rel="alternate" type="text/html" title="研一上课程总结" /><published>2022-12-05T00:00:00+08:00</published><updated>2022-12-05T00:00:00+08:00</updated><id>http://localhost:4000/%E7%A0%94%E4%B8%80%E4%B8%8A%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93</id><content type="html" xml:base="http://localhost:4000/%E7%A0%94%E4%B8%80%E4%B8%8A%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93.html"><![CDATA[<h1 id="课程目录">课程目录</h1>
<!-- TOC -->

<ul>
  <li><a href="#课程目录">课程目录</a></li>
  <li><a href="#1-material-材料">1. Material 材料</a></li>
  <li><a href="#2-deeplearning-深度学习">2. Deeplearning 深度学习</a></li>
  <li><a href="#3-intellectual-property-law-知识产权法">3. Intellectual property law 知识产权法</a></li>
  <li><a href="#4-culture-and-society-文化与社会">4. Culture and society 文化与社会</a></li>
  <li><a href="#5-新时代中国特色社会主义理论与实践">5. 新时代中国特色社会主义理论与实践</a></li>
  <li><a href="#6-organisation-theory-组织理论">6. Organisation theory 组织理论</a></li>
  <li><a href="#7-solid-state-physics-固体物理">7. Solid State Physics 固体物理</a></li>
  <li><a href="#8-simulation-optimization-仿真与优化">8. Simulation optimization 仿真与优化</a></li>
  <li><a href="#9-automatic-control-自动控制">9. Automatic control 自动控制</a></li>
  <li><a href="#10-introduction-to-complex-systems-复杂系统">10. Introduction to complex systems 复杂系统</a></li>
  <li><a href="#11-structural-mechanics-3-结构力学3">11. Structural Mechanics 3 结构力学3</a></li>
  <li><a href="#12-英文科技论文写作与学术报告">12. 英文科技论文写作与学术报告</a></li>
</ul>

<!-- /TOC -->

<h1 id="1-material-材料">1. Material 材料</h1>
<ul>
  <li>课程信息:
    <ul>
      <li>授课老师: 唐宏哲</li>
      <li>授课形式: 线下上课，课程有录屏</li>
      <li>授课材料: 没有讲义，老师上课直接在黑板上写</li>
      <li>考核信息: 大作业+考试，大作业是自由主题，但是要和上课的内容相关，考试在线下举行，时长一小时，大概四个大题</li>
    </ul>
  </li>
  <li>课程简介: 一共五节课，但一共只有四节课在讲正课的知识，第一节课讲了金属材料和高炉炼铁，炼钢和制铝，然后讲了高分子化合物。第二节课介绍了金属晶体，回想起了高中化学。第三节课讲了腐蚀与防护，我们大作业也是基于这个主题展开。第四节课讲了相平衡与相图，之前在热力学也学过类似的内容</li>
</ul>

<h1 id="2-deeplearning-深度学习">2. Deeplearning 深度学习</h1>
<ul>
  <li>课程信息:
    <ul>
      <li>授课老师: 于雷</li>
      <li>授课形式: 线下上课，课程有录屏，有上机课(做TP)</li>
      <li>授课材料: 有讲义，TP</li>
      <li>考核信息: 平时分+考试，不知道TP算不算平时分，考试在线下举行</li>
    </ul>
  </li>
  <li>课程简介: 课程全称应该是深度学习与高性能计算，第一个讲义介绍了深度学习的基本信息与发展历史。第二个讲义介绍了经典的线性模型。第三个讲义介绍了优化方法，介绍了不同的优化器。第四个讲义介绍了numpy的基本使用和matplotlib可视化方法。第五个讲义介绍了前馈神经网络，也就是多层感知机。第六个讲义介绍了卷积神经网络，这节课让我记得了池化层滑动步长和边长一样长。第七个讲义介绍了数据并行计算的方法，首先简单介绍了python中不同进程间信息传递和通信的机制，然后介绍了数据并行化计算和模型并行化计算的方法。TP1需要实现线性模型，TP2需要实现MLP，TP3需要实现卡农算法和联邦计算，其实就是利用python的multiprocessing库和队列来实现权重的更新的收发</li>
</ul>

<h1 id="3-intellectual-property-law-知识产权法">3. Intellectual property law 知识产权法</h1>
<ul>
  <li>课程信息:
    <ul>
      <li>授课老师: 徐绪辉</li>
      <li>授课形式: 线下上课，课程有录屏</li>
      <li>授课材料: 有讲义</li>
      <li>考核信息: 线上考试</li>
    </ul>
  </li>
  <li>课程简介: 简单介绍了知识产权法的相关知识，知识产权法由著作权法、商标法和专利法三部法律构成，老师上课也是按照这个流程讲的，首先讲了法律意识和法律，然后第二讲介绍了著作权法，第三讲介绍了商标法，最后一讲介绍了专利法</li>
</ul>

<h1 id="4-culture-and-society-文化与社会">4. Culture and society 文化与社会</h1>
<ul>
  <li>课程信息:
    <ul>
      <li>授课老师: Anne</li>
      <li>授课形式: 前半阶段线下上课，后半阶段由于疫情原因暂停线下上课，改为线上学习</li>
      <li>授课材料: 线下授课的时候发了纸质材料，线上授课后有讲义</li>
      <li>考核信息: 平时分+DM+小测，没有考试</li>
    </ul>
  </li>
  <li>课程简介: 熟悉的法语课程，主要介绍了法国的文化和社会情况。DM1需要回答与小说的人物共情是否是小说创作的出发点，DM2需要介绍中国人使用网络的数据，DM3需要介绍一种艺术形式，我选择了photorealism，照相写实主义，然后最后好像还得写一篇小文章</li>
</ul>

<h1 id="5-新时代中国特色社会主义理论与实践">5. 新时代中国特色社会主义理论与实践</h1>
<ul>
  <li>课程信息:
    <ul>
      <li>授课老师: 刘娜娜,王江伟,田英</li>
      <li>授课形式: 线下授课，分专题不同老师来讲课</li>
      <li>授课材料: 部分讲义</li>
      <li>考核信息: 平时分+大作业</li>
    </ul>
  </li>
  <li>课程简介: 中特课，介绍新时代中国特色社会主义理论与实践，刘娜娜老师介绍的专题是法治与法制。大作业共需要回答四个大题</li>
</ul>

<h1 id="6-organisation-theory-组织理论">6. Organisation theory 组织理论</h1>
<ul>
  <li>课程信息:
    <ul>
      <li>授课老师: Attias Daniele</li>
      <li>授课形式: 线上授课，一共四节课</li>
      <li>授课材料: 讲义</li>
      <li>考核信息: 平时分+大作业</li>
    </ul>
  </li>
  <li>课程简介: 一共三个讲义，第一个讲义介绍了如何分析公司的策略，第二个讲义介绍了有哪些策略，第三个讲义介绍了创新公司如何创造价值。一共有两个大作业，第一个大作业是分析Electorstat公司的情况，做一个ppt介绍分析的结果和建议。第二个大作业是分析FREE这个公司的情况并回答三个问题</li>
</ul>

<h1 id="7-solid-state-physics-固体物理">7. Solid State Physics 固体物理</h1>
<ul>
  <li>课程信息:
    <ul>
      <li>授课老师: 安炜，付小尧</li>
      <li>授课形式: 线下授课，疫情后线上授课</li>
      <li>授课材料: 讲义，TD</li>
      <li>考核信息: 线上考试+大作业</li>
    </ul>
  </li>
  <li>课程简介: 固体物理一共有七讲，第一讲介绍了自由费米气和理想金属，主要介绍了德鲁德模型和索莫菲尔德模型。第二讲介绍了晶体结构的相关知识，布拉伐点阵，晶系和倒空间的构建，这一部分和材料所学知识有关。第三讲介绍了晶体结构的散射与衍射，即电子散射与晶格衍射。首先讲了一个电子如何散射，然后介绍了一群电子如何散射，从这推出了晶格如何衍射以及布拉格衍射条件，最后介绍了晶格衍射的应用，比如X射线衍射法解析晶体结构。第四讲和第五讲介绍了声子的相关知识。声子就是晶格振动的简正模能量量子。晶格原子在振动，计算了不同温度下的热容。第六讲和第七讲介绍了电子的能带论，能带理论是讨论晶体中电子运动状态的一种重要近似理论。固体物理需要分析各种固体的性质，所以和化学材料相关。大作业是找一篇sci论文进行内容梳理和课上知识关联，我选择了一篇新材料的第一性原理计算的论文进行分析。</li>
</ul>

<h1 id="8-simulation-optimization-仿真与优化">8. Simulation optimization 仿真与优化</h1>
<ul>
  <li>课程信息:
    <ul>
      <li>授课老师: Pascal Laurent</li>
      <li>授课形式: 线上授课</li>
      <li>授课材料: 讲义+TD</li>
      <li>考核信息: 线上考试+大作业+平时分</li>
    </ul>
  </li>
  <li>课程简介: 相当硬核的一门课，算上准备课程，一共有9节课，第一节课介绍了微分计算，第二节课介绍了凸函数的相关性质，第三节课介绍了优化方法与下降方法，第四节课介绍了其余的优化方法，包括牛顿法和随机梯度下降，前四节课都属于优化部分。后四节课属于仿真与偏微分方程部分，第五节课介绍了基础知识，第六节课介绍了偏微分方程，第七节课介绍了有限元方法，第八节课对前面的知识进行了回顾同时介绍了项目内容。一共有两个大作业，第一个大作业需要对一个简单的分类任务进行优化，除此之外，我们还讨论了不同优化器的区别，adam optimizer和普通的sgd优化器的区别与优缺点。第二个大作业需要利用ANSYS软件对某个港口的海浪的振幅进行计算与可视化，这个项目我们组做的相对潦草。</li>
</ul>

<h1 id="9-automatic-control-自动控制">9. Automatic control 自动控制</h1>
<ul>
  <li>课程信息:
    <ul>
      <li>授课老师: Rahmani Ahmed, 马纪明, 韩亮</li>
      <li>授课形式: 线上授课+线下TP课</li>
      <li>授课材料: 讲义</li>
      <li>考核信息:</li>
    </ul>
  </li>
  <li>课程简介:</li>
</ul>

<h1 id="10-introduction-to-complex-systems-复杂系统">10. Introduction to complex systems 复杂系统</h1>
<ul>
  <li>课程信息:
    <ul>
      <li>授课老师: M.Batteux Michel</li>
      <li>授课形式: 线上授课</li>
      <li>授课材料: 讲义+PC(可以看作平时的TP)</li>
      <li>考核信息: 平时分+PC+大作业</li>
    </ul>
  </li>
  <li>课程简介: 一共八节课，每节课都有对应的PC作业。第一节课介绍了什么是复杂系统，怎样描述一个系统，PC1需要利用上课的知识描述一个系统。第二节课介绍了车间调度问题以及如何模拟一系列的离散事件，介绍了如何用Python进行模拟实验，PC2的内容是运行它给的程序，更加直观的了解如何用python进行模拟实验。第三节课简单介绍了架构系统的相关知识，比如一个架构的各种组件，架构系统的步骤等等，PC3需要利用课上的知识分析一个架构系统。第四节课介绍了如何对复杂系统的组成部分进行分类，了解各组件的相互作用，也即如何正确地组织一个复杂系统，PC4给了一个案例进行分析。第五节课介绍了如何满足复杂系统的约束。第六节课介绍了用关键路径法对复杂系统进行建模，和之前项目管理的内容类似。第七节课介绍了蒙特卡洛估计法，PC7是对英超球队的数据进行估计与分析。第八节课介绍了复杂系统的设计者可能切必须面对的三类困难。最后一个大作业相当于是对之前内容的总结与回顾，涉及到了之前讲的所有内容。</li>
</ul>

<h1 id="11-structural-mechanics-3-结构力学3">11. Structural Mechanics 3 结构力学3</h1>
<ul>
  <li>课程信息:
    <ul>
      <li>授课老师: Olivier Barelle, 黄行蓉</li>
      <li>授课形式: 线上授课</li>
      <li>授课材料: 讲义+TD</li>
      <li>考核信息: 大作业</li>
    </ul>
  </li>
  <li>课程简介: 一共四节课，第一节课回顾了有限元，模态和响应的计算方法。第二节课介绍转子动力学，第三讲介绍预应力，最后一节课介绍转子实验台。大作业的题目形式和之前类似，有公共的题目，也有分开的题目。</li>
</ul>

<h1 id="12-英文科技论文写作与学术报告">12. 英文科技论文写作与学术报告</h1>
<p>北航研究生院网课，需要刷课然后有个小考试。</p>]]></content><author><name>Quehry</name></author><category term="courses" /><summary type="html"><![CDATA[整理研一上课程信息]]></summary></entry><entry><title type="html">BLIP</title><link href="http://localhost:4000/BLIP.html" rel="alternate" type="text/html" title="BLIP" /><published>2022-12-01T00:00:00+08:00</published><updated>2022-12-01T00:00:00+08:00</updated><id>http://localhost:4000/BLIP</id><content type="html" xml:base="http://localhost:4000/BLIP.html"><![CDATA[<h1 id="basic-info">Basic Info</h1>
<ul>
  <li>论文全称: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</li>
  <li>相关链接:
    <ul>
      <li><a href="https://github.com/salesforce/BLIP" target="_blank">Github</a></li>
    </ul>
  </li>
</ul>

<h1 id="introduction">Introduction</h1>
<p>Vision Language Pre-training近些年有不错的进展，但是以往的VLP的预训练任务要么是vision-language understanding，要么是vision-language generation。BLIP在设计预训练任务时，综合考虑了这两方面的预训练任务。BLIP另一大亮点就是对数据的处理，vision-language领域的数据集，人工标注且效果好的数据不多，所以作者提出了一种CapFilt的Bootstrapping方法来过滤掉不好的标注，扩充image-text pair数据。论文主要有两大贡献:</p>
<ol>
  <li>Multimodal mixture of Encoder-Decoder(MED)，一种新的预训练框架，综合考虑了understanding任务和generation任务</li>
  <li>Captioning and Filtering(CapFilt)，一种数据集自举的方法，微调预训练的MED，并分为两个子模块，captioner用来将web获取的图片标注，filter用来将noisy的image text pair去除</li>
</ol>

<h1 id="model-architecture">Model Architecture</h1>
<p>模型的主题架构如下:</p>
<center><img src="../assets/img/posts/20221201/1.jpg" /></center>

<p>作者设计了三种不同的子架构:</p>
<ol>
  <li>unimodal encoder: 上图左侧的两个架构，针对单一的text或者image的encoder，对于text而言就是BERT的encoder，对于image而言就是ViT的encoder，同样用&lt;cls&gt;特殊编码来表示sentence的全局信息</li>
  <li>image-grounded text encoder: 上图中间的架构，针对text而言，不同之处是加上了image encoder后的cross attention，所以是image-grounded。text的开头加上特殊字符&lt;encode&gt;，代表image-text pair的多模态表示</li>
  <li>image-grounded text decoder: 上图最右侧的架构，除了和image encoder的cross-attention外，还用causal self-attention替换了bidirectional self-attention，causal attention具体是什么，可以去看<strong>causal attention for vision-language tasks</strong>这篇论文。特殊字符&lt;decode&gt;用来表示序列的开始，除了self-attention层外，和image-grounded text encoder共享参数</li>
</ol>

<p>MED这种架构的主要作用就是服务于预训练任务:</p>
<ol>
  <li>image-text contrastive loss(ITC): 应该和CLIP的任务类似，对齐特征空间中的image编码表示和text编码表示，属于vision-language understanding任务</li>
  <li>image-text matching loss(ITM): 判断vision和language是否匹配，二分类任务，属于vision-language understanding任务</li>
  <li>language modeling loss(LM): vision-language生成任务，自回归，给定序列的开头和图片的编码，输出完整的caption</li>
</ol>

<h1 id="capfilt">CapFilt</h1>
<p>正如前文所介绍，CapFilt的主要目的是筛除noisy的pair，因为从web上爬取的image-text pair质量太低。CaoFilt利用预训练的MED架构，抽取出两个子模块Captioner和Filter，Captioner就是MED的image-grounded text decoder部分，用来对web的图片进行标注，然后我们就获得了新的pair。Filter就是MED的image-grounded text encoder部分，原本那部分的预训练任务是ITM，用来当作过滤器很合适，这样就可以筛除noisy的pair，从而达到扩充数据集且数据集质量高的效果，下图清晰的介绍了这个过程:</p>
<center><img src="../assets/img/posts/20221201/2.jpg" /></center>

<h1 id="experiments-and-datasets">Experiments and datasets</h1>
<p>CapFilt的效果:</p>
<center><img src="../assets/img/posts/20221201/3.jpg" /></center>

<p>数据集:</p>
<ol>
  <li>两个人工标注数据集: COCO和VG(Visual Genome)</li>
  <li>三个从web爬取的数据集: CC(Conceptual Captions), SBU, LAION</li>
</ol>

<p>生成caption的方法:</p>
<ol>
  <li>nucleus sampling: 选取每个词元时，只要累计的概率超过了一个阈值，那么就作为一种选取方式</li>
  <li>beam search: 文本生成的常用手段，具体来说就是定义一个束宽k，选取每个词元时，都保留概率最高的k个选择，最后选出概率高的输出</li>
</ol>

<p>作者比较了这两种生成caption的方法，认为nucleus sampling会取得更好的结果，但是可能会生成奇怪的caption，如果是为了生成safe的caption，可以用beam search</p>
<center><img src="../assets/img/posts/20221201/4.png" /></center>

<h1 id="downstream-tasks">Downstream tasks</h1>
<ol>
  <li>image-text retrieval: 包含了image-to-text retrieval(TR)和text-to-image retrieval(IR)任务，即图片检索文本和文本检索图片任务，BLIP达到了SOTA，度量指标是Recall@k，在信息检索领域，代表查询图像对应标签在检索标签列表的前topk中出现的次数于真值标签列表个数的比例</li>
</ol>
<center><img src="../assets/img/posts/20221201/5.jpg" /></center>

<ol>
  <li>image captioning: 图片标注，用MED的LM部分实现，指标有CIDEr, SPICE, BLEU@4</li>
</ol>
<center><img src="../assets/img/posts/20221201/6.jpg" /></center>

<ol>
  <li>Visual Question Answering(VQA): 给定问题和图片，生成answer，这里作者把它看成生成任务，图片经过image encoder，问题经过question encoder，然后特殊字符&lt;decode&gt;输入answer encoder生成answer:</li>
</ol>
<center><img src="../assets/img/posts/20221201/7.jpg" /></center>

<ol>
  <li>Natural Language Visual Reasoning(NLVR): 判断一段话是否是在形容一对图片，为了适应这个任务，同样需要对训练框架进行调整:</li>
</ol>
<center><img src="../assets/img/posts/20221201/8.jpg" /></center>

<p>3和4的实验结果:</p>
<center><img src="../assets/img/posts/20221201/9.jpg" /></center>]]></content><author><name>Quehry</name></author><category term="paper reading" /><category term="notes" /><category term="multi-modality" /><category term="vision-language" /><summary type="html"><![CDATA[论文阅读]]></summary></entry><entry><title type="html">DreamBooth</title><link href="http://localhost:4000/DreamBooth.html" rel="alternate" type="text/html" title="DreamBooth" /><published>2022-11-28T00:00:00+08:00</published><updated>2022-11-28T00:00:00+08:00</updated><id>http://localhost:4000/DreamBooth</id><content type="html" xml:base="http://localhost:4000/DreamBooth.html"><![CDATA[<h1 id="dreambooth简介">DreamBooth简介</h1>
<p>DreamBooth是Google团队继Imagen后研发的针对Subject进行定制化训练的finetune方法，只需要同一个物体(动物、人、物体)的3-5张图片和prompt，就可以微调出一个专属的模型，这个模型可以生成输入物体的各种姿势，也可以将这个物体融入到景观中。DreamBooth本质上微调了Unet和TextEncoder，效果图如下:</p>

<center><img src="../assets/img/posts/20221128/1.jpg" /></center>

<p>相关链接:</p>
<ul>
  <li><a href="https://dreambooth.github.io/" target="_blank">DreamBooth</a></li>
</ul>

<h1 id="application">Application</h1>
<ol>
  <li>Recontextualization:</li>
</ol>
<center><img src="../assets/img/posts/20221128/2.jpg" /></center>

<ol>
  <li>Art Renditions</li>
</ol>
<center><img src="../assets/img/posts/20221128/3.jpg" /></center>

<ol>
  <li>Expression Manipulation</li>
</ol>
<center><img src="../assets/img/posts/20221128/4.jpg" /></center>

<ol>
  <li>Novel View Synthesis</li>
</ol>
<center><img src="../assets/img/posts/20221128/5.jpg" /></center>

<ol>
  <li>Accessorization</li>
</ol>
<center><img src="../assets/img/posts/20221128/6.jpg" /></center>

<ol>
  <li>Property Modification</li>
</ol>
<center><img src="../assets/img/posts/20221128/7.jpg" /></center>]]></content><author><name>Quehry</name></author><category term="paper reading" /><category term="notes" /><category term="image generation" /><category term="finetune" /><summary type="html"><![CDATA[论文阅读]]></summary></entry><entry><title type="html">C++面向对象程序设计</title><link href="http://localhost:4000/Object-Oriented-Programming.html" rel="alternate" type="text/html" title="C++面向对象程序设计" /><published>2022-11-20T00:00:00+08:00</published><updated>2022-11-20T00:00:00+08:00</updated><id>http://localhost:4000/Object-Oriented-Programming</id><content type="html" xml:base="http://localhost:4000/Object-Oriented-Programming.html"><![CDATA[<h1 id="1-简介">1. 简介</h1>
<p>网课程序设计与算法(三):C++面向对象程序设计的随堂笔记，授课平台是中国大学MOOC，授课老师是北大的郭炜老师，开课次数是第十五次开课。程序设计与算法是一个系列的课程，C++面向对象程序设计是该系列的最后一门课程。课程文件有每节课的讲义及习题(openjudge上)，相关链接:</p>
<ul>
  <li><a href="https://www.icourse163.org/learn/PKU-1002029030?tid=1469134446#/learn/content" target="_blank">课程链接，开课阶段过了可能会失效</a></li>
  <li><a href="http://cxsjsxmooc.openjudge.cn/2022t3fall/" target="_blank">openjudge习题</a></li>
</ul>

<h1 id="2-随堂笔记">2. 随堂笔记</h1>
<h2 id="21-第一章-从c到c">2.1. 第一章 从C到C++</h2>
<h3 id="211-引用">2.1.1. 引用</h3>
<ul>
  <li>引用的概念: <code class="language-plaintext highlighter-rouge">int &amp; a = b</code></li>
</ul>

<center><img src="../assets/img/posts/20221120/2.jpg" /></center>

<ul>
  <li>引用的一些注意事项</li>
</ul>

<center><img src="../assets/img/posts/20221120/3.jpg" /></center>

<ul>
  <li>C语言中，形参的值改变不会影响实参的值</li>
  <li>C语言中交换两个数的值的函数swap的写法:</li>
</ul>

<center><img src="../assets/img/posts/20221120/4.png" /></center>

<ul>
  <li>C++中有了引入的概念后，swap函数便可以这么写:</li>
</ul>

<center><img src="../assets/img/posts/20221120/5.jpg" /></center>

<ul>
  <li>除此之外，引用还可以作为函数的返回值，目前还不知道具体作用，后续会了解</li>
</ul>

<center><img src="../assets/img/posts/20221120/6.jpg" /></center>

<ul>
  <li>常引用:</li>
</ul>

<center><img src="../assets/img/posts/20221120/7.jpg" /></center>

<ul>
  <li>不能通过常引用来修改其引用内容:</li>
</ul>

<center><img src="../assets/img/posts/20221120/8.jpg" /></center>

<ul>
  <li><code class="language-plaintext highlighter-rouge">const T &amp;</code> 和 <code class="language-plaintext highlighter-rouge">T &amp;</code>的相互转换:</li>
</ul>

<center><img src="../assets/img/posts/20221120/9.jpg" /></center>

<h3 id="212-const关键字">2.1.2. const关键字</h3>
<ul>
  <li>const的第一个作用就是定义常量</li>
  <li>const的第二个作用是定义常量指针:</li>
</ul>

<center><img src="../assets/img/posts/20221120/10.jpg" /></center>
<center><img src="../assets/img/posts/20221120/11.jpg" /></center>

<ul>
  <li>常量指针常作为函数的参数，可避免函数内部不小心改变参数指针所指地方的内容</li>
</ul>

<center><img src="../assets/img/posts/20221120/12.jpg" /></center>

<h3 id="213-动态内存分配">2.1.3. 动态内存分配</h3>
<ul>
  <li>用new运算符实现动态内存分配<code class="language-plaintext highlighter-rouge">P = new T</code>:</li>
</ul>

<center><img src="../assets/img/posts/20221120/13.jpg" /></center>

<ul>
  <li>用new运算符分配一个数组:</li>
</ul>

<center><img src="../assets/img/posts/20221120/14.jpg" /></center>

<ul>
  <li>用new动态分配的内存空间，一定要用delete运算符进行释放，<code class="language-plaintext highlighter-rouge">delete 指针</code>:</li>
</ul>

<center><img src="../assets/img/posts/20221120/15.jpg" /></center>

<ul>
  <li>如果定义了一个数组的动态空间，delete的时候一定要加上中括号:</li>
</ul>

<center><img src="../assets/img/posts/20221120/16.jpg" /></center>

<h3 id="214-内联函数">2.1.4. 内联函数</h3>
<ul>
  <li>内联函数的主要作用是加速简单的函数的调用，所以一般是用于相对简单的函数，在前面加上<code class="language-plaintext highlighter-rouge">inline</code>关键字:</li>
</ul>

<center><img src="../assets/img/posts/20221120/17.jpg" /></center>

<center><img src="../assets/img/posts/20221120/18.jpg" /></center>

<ul>
  <li>函数重载: 名字相同但参数个数或参数类型不同</li>
</ul>

<center><img src="../assets/img/posts/20221120/19.png" /></center>

<ul>
  <li>C++中，定义函数的时候可以让<strong>最右边</strong>的连续若干个参数有缺省值，那么调用函数的时候，相应位置不写参数，参数就是缺省值:</li>
</ul>

<center><img src="../assets/img/posts/20221120/20.jpg" /></center>

<ul>
  <li>函数的缺省参数的作用:</li>
</ul>

<center><img src="../assets/img/posts/20221120/21.jpg" /></center>

<h2 id="22-第二章-类和对象基础">2.2. 第二章 类和对象基础</h2>
<h3 id="221-类和对象的基本概念">2.2.1. 类和对象的基本概念</h3>
<ul>
  <li>在面向对象编程之前，设计一个程序的方法是结构化程序设计(structured programming)，具体来说就是数据结构+算法</li>
  <li>结构化程序设计有很多不足，结构化的程序，在规模庞大时，会变得难以理解，难以扩充，难以查错，难以重用</li>
  <li>软件业的目标时更快、更正确、更经济地建立软件</li>
  <li>面向对象程序设计可以很好的解决上述问题</li>
  <li>面向对象的程序=类+类+..+类</li>
  <li>面向对象的程序设计具有抽象、封装、继承、多态四个基本特点</li>
  <li>类有成员变量和成员函数，成员变量和成员函数统称为类的成员，类看上去就像带函数的结构</li>
  <li>一个类的例子:</li>
</ul>

<center><img src="../assets/img/posts/20221120/22.jpg" /></center>

<ul>
  <li>类在实例化之后就是一个对象，通过类，可以定义变量，类定义出来的变量，也称为类的实例，就是我们所说的对象</li>
  <li>对象所占用的内存空间大小等于所有成员变量的大小之和</li>
  <li>和结构体类似，既可以通过<code class="language-plaintext highlighter-rouge">对象名.成员名</code>的方法来访问类的成员变量和成员函数，也可以通过<code class="language-plaintext highlighter-rouge">指针-&gt;成员名</code>来访问指针指向的类的成员变量:</li>
</ul>

<center><img src="../assets/img/posts/20221120/23.jpg" /></center>

<ul>
  <li>C++中，也可以通过引用的方法来访问类的成员，因为引用之后，两者就等价了</li>
  <li>类的成员函数和类的定义可以分开写，可以在类的内部声明类的成员函数，然后在类的外部通过<code class="language-plaintext highlighter-rouge">类名::类的成员函数</code>来定义:</li>
</ul>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">CRectangle</span><span class="o">::</span><span class="n">Area</span><span class="p">(){</span>
    <span class="k">return</span> <span class="n">w</span> <span class="o">*</span> <span class="n">h</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>
<ul>
  <li>在类的定义中，用下列访问关键字来说明类成员可被访问的范围(private,public,protected):</li>
</ul>

<center><img src="../assets/img/posts/20221120/24.jpg" /></center>

<ul>
  <li>如果类的某个成员前面没有上述关键字，则缺省地被认为是私有成员:</li>
</ul>

<center><img src="../assets/img/posts/20221120/25.jpg" /></center>

<ul>
  <li>类成员的可访问范围:</li>
</ul>

<center><img src="../assets/img/posts/20221120/26.jpg" /></center>

<ul>
  <li>设置私有成员的机制，叫做<strong>隐藏</strong>，隐藏的目的是强制对成员变量的访问一定要通过成员函数进行，那么以后成员变量的类型等属性修改后，只需要更改成员函数即可</li>
  <li><code class="language-plaintext highlighter-rouge">strcpy(char * a, char* b)</code>可以将字符串b的值赋给字符串a</li>
  <li>析构函数(destructor)与构造函数相反，当对象结束其生命周期，如对象所在的函数已调用完毕时，系统自动执行析构函数。比如在建立对象时使用<code class="language-plaintext highlighter-rouge">new</code>开辟了一片内存空间，<code class="language-plaintext highlighter-rouge">delete</code>会自动调用析构函数后释放内存</li>
  <li>成员函数的重载及参数缺省: 成员函数可以重载，也可以带缺省的参数</li>
</ul>

<center><img src="../assets/img/posts/20221120/27.jpg" /></center>

<ul>
  <li>使用重载的时候一定要注意避免函数重载时的二义性</li>
</ul>

<h3 id="222-构造函数">2.2.2. 构造函数</h3>
<ul>
  <li>构造函数是成员函数的一种，名字与类名相同，不能有返回值，可以有参数，作用是对对象实例化时进行初始化，如果定义类的时候没写构造函数，则编译器默认生成一个无参数的构造函数，默认的构造函数不做任何操作</li>
</ul>

<center><img src="../assets/img/posts/20221120/28.jpg" /></center>

<ul>
  <li>构造函数的作用是执行必要的初始化工作，有个构造函数，就不必再写初始化函数，也不用担心忘记调用初始化函数</li>
  <li>对象实例化的方法:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">Complex c1</code></li>
      <li><code class="language-plaintext highlighter-rouge">Complex c1(3,4)</code></li>
      <li><code class="language-plaintext highlighter-rouge">Complex * pc = new Complex</code></li>
      <li><code class="language-plaintext highlighter-rouge">Complex * pc = new Complex(3,4)</code></li>
    </ul>
  </li>
  <li>构造函数也可以重载</li>
  <li>利用类来构造数组时，如果声明数组的时候没有传参，那么默认无参数，如果传递了参数，那么就认为是有参数的构造函数</li>
</ul>

<center><img src="../assets/img/posts/20221120/29.png" /></center>
<center><img src="../assets/img/posts/20221120/30.jpg" /></center>

<ul>
  <li>构造函数最好是public的</li>
  <li>new完之后一定记得delete</li>
  <li>用指针来实例化对象的时候一定要开辟内存空间，不然不知道这个指针指向哪里</li>
</ul>

<h3 id="223-复制构造函数">2.2.3. 复制构造函数</h3>
<ul>
  <li>复制构造函数的概念:</li>
</ul>

<center><img src="../assets/img/posts/20221120/31.jpg" /></center>

<ul>
  <li>复制构造函数的一个例子:</li>
</ul>

<center><img src="../assets/img/posts/20221120/32.jpg" /></center>

<ul>
  <li>如果定义自己的复制构造函数，则默认的复制构造函数不会存在</li>
  <li>复制构造函数起作用的三种情况:
    <ul>
      <li>用一个对象去初始化同类的另一个对象时</li>
      <li>如果某函数有一个参数是类A的对象(不是类A的引用)，那么该函数被调用时，类A的复制构造函数将被调用</li>
    </ul>

    <center><img src="../assets/img/posts/20221120/33.jpg" /></center>

    <ul>
      <li>如果函数的返回值是类A的对象时，则函数返回时，A的复制构造函数将被调用</li>
    </ul>

    <center><img src="../assets/img/posts/20221120/34.png" /></center>
  </li>
  <li>对象间赋值并不导致复制构造函数被调用，会直接把a的成员变量的值赋给b</li>
</ul>

<center><img src="../assets/img/posts/20221120/35.png" /></center>

<ul>
  <li>如果某个函数的参数包含类A的对象时，调用函数时会调用类A的复制构造函数，调用开销比较大，这时可以将函数的参数变成常量引用<code class="language-plaintext highlighter-rouge">const Complex &amp;</code>，这样形参的改变不会影响实参</li>
</ul>

<center><img src="../assets/img/posts/20221120/36.png" /></center>

<h3 id="224-类型转换构造函数">2.2.4. 类型转换构造函数</h3>
<ul>
  <li>类型转换构造函数的概念:</li>
</ul>

<center><img src="../assets/img/posts/20221120/37.png" /></center>

<ul>
  <li>例子:</li>
</ul>

<center><img src="../assets/img/posts/20221120/38.jpg" /></center>

<ul>
  <li><code class="language-plaintext highlighter-rouge">float a = int b</code>其实就是定义对象a的时候调用了float类的类型转换构造函数(显式)</li>
  <li>关键字<code class="language-plaintext highlighter-rouge">explicit</code>用于只有一个参数的构造函数，表明构造函数是显式的:</li>
</ul>

<center><img src="../assets/img/posts/20221120/39.jpg" /></center>

<ul>
  <li>析构函数的概念:</li>
</ul>

<center><img src="../assets/img/posts/20221120/40.jpg" /></center>

<ul>
  <li>例子:</li>
</ul>

<center><img src="../assets/img/posts/20221120/41.jpg" /></center>

<ul>
  <li>类的数组在生命周期结束时，会调用每个元素的析构函数</li>
  <li>析构函数在对象作为函数的返回值返回后被调用(下面这个例子中析构函数被调用了三次):</li>
</ul>

<center><img src="../assets/img/posts/20221120/42.jpg" /></center>

<h3 id="225-构造函数和析构函数调用时机">2.2.5. 构造函数和析构函数调用时机</h3>
<ul>
  <li>构造函数和析构函数什么时候会被调用? 看一个例子:</li>
</ul>

<center><img src="../assets/img/posts/20221120/43.jpg" /></center>

<ul>
  <li>静态局部变量在函数消亡时不会消失</li>
</ul>

<center><img src="../assets/img/posts/20221120/44.jpg" /></center>

<ul>
  <li>复制构造函数在不同编译器里不同的表现:</li>
</ul>

<center><img src="../assets/img/posts/20221120/45.jpg" /></center>
<center><img src="../assets/img/posts/20221120/46.jpg" /></center>
<center><img src="../assets/img/posts/20221120/47.png" /></center>

<h2 id="23-类和对象提高">2.3. 类和对象提高</h2>
<h3 id="231-this指针">2.3.1. this指针</h3>
<ul>
  <li>new返回的是指针</li>
  <li>this指针的作用就是指向成员函数所作用的对象</li>
  <li>非静态成员函数中可以直接使用this来代表指向该函数作用的对象的指针:</li>
</ul>

<center><img src="../assets/img/posts/20221120/48.jpg" /></center>

<ul>
  <li>静态成员函数中不能使用this指针，因为静态成员函数并不具体作用于某个对象，因此，静态成员函数的真实的参数的个数，就是程序中写出的参数个数</li>
</ul>

<h3 id="232-静态成员变量">2.3.2. 静态成员变量</h3>
<ul>
  <li>静态成员: 在声明前加了<code class="language-plaintext highlighter-rouge">static</code>关键字的成员，包含了静态成员变量和静态成员函数:</li>
</ul>

<center><img src="../assets/img/posts/20221120/49.jpg" /></center>

<ul>
  <li>普通的成员变量每个对象各有一份，而静态成员变量一共就一份，为所有对象所共享，<code class="language-plaintext highlighter-rouge">sizeof</code>运算符不会计算静态成员变量</li>
</ul>

<center><img src="../assets/img/posts/20221120/50.jpg" /></center>

<ul>
  <li>访问静态成员的方法(静态成员并不需要通过对象就可以访问):</li>
</ul>

<center><img src="../assets/img/posts/20221120/51.jpg" /></center>

<ul>
  <li>静态成员变量本质上就是针对于某个类的<strong>全局变量</strong>，哪怕一个对象都不存在，类的静态成员变量也存在，静态成员函数本质上就是<strong>全局函数</strong></li>
</ul>

<center><img src="../assets/img/posts/20221120/52.jpg" /></center>

<ul>
  <li>例子:</li>
</ul>

<center><img src="../assets/img/posts/20221120/53.jpg" /></center>

<center><img src="../assets/img/posts/20221120/54.jpg" /></center>

<center><img src="../assets/img/posts/20221120/55.jpg" /></center>

<ul>
  <li>静态成员变量必须在定义类的文件中对静态变量进行一次说明或初始化:</li>
</ul>

<center><img src="../assets/img/posts/20221120/56.jpg" /></center>

<ul>
  <li>注意: 在静态成员函数中，不能访问非静态成员变量，也不能调用非静态成员函数</li>
</ul>

<center><img src="../assets/img/posts/20221120/57.jpg" /></center>

<ul>
  <li>上述的写法其实有缺陷，因为在定义对象的时候可能会调用复制构造函数，这些临时对象在生成时不会增加totalnumber，但是在消亡时会减少totalnumber，会出错:</li>
</ul>

<center><img src="../assets/img/posts/20221120/58.jpg" /></center>

<ul>
  <li>解决方法就是定义一个复制构造函数:</li>
</ul>

<center><img src="../assets/img/posts/20221120/59.jpg" /></center>

<h3 id="233-成员对象和封闭类">2.3.3. 成员对象和封闭类</h3>
<ul>
  <li>有成员对象的类叫封闭类</li>
  <li>定义构造函数时可以利用初始化列表来初始化成员变量: <code class="language-plaintext highlighter-rouge">CTyre(int w, int r):radius(r),width(w){};</code></li>
  <li>封闭类的例子:</li>
</ul>

<center><img src="../assets/img/posts/20221120/60.png" /></center>

<ul>
  <li>如果CCar类不定义构造函数，则编译器不明白CCar类的成员对象如何初始化</li>
  <li>任何生成封闭类对象的语句，都要让编译器明白，对象中的成员对象是如何初始化的</li>
</ul>

<center><img src="../assets/img/posts/20221120/61.png" /></center>

<ul>
  <li>封闭类构造函数和析构函数的执行顺序:</li>
</ul>

<center><img src="../assets/img/posts/20221120/62.jpg" /></center>

<ul>
  <li>如果封闭类的对象是用默认的复制构造函数初始化的，那么封闭类的成员对象也会用复制构造函数初始化:</li>
</ul>

<center><img src="../assets/img/posts/20221120/63.jpg" /></center>

<h3 id="234-常量对象和常量成员函数">2.3.4. 常量对象和常量成员函数</h3>
<ul>
  <li>如果不希望某个对象的值发生改变，则定义该对象的时候可以在前面加上<code class="language-plaintext highlighter-rouge">const</code>关键字</li>
  <li>在类的成员函数声明<strong>后</strong>加上<code class="language-plaintext highlighter-rouge">const</code>关键字，则该成员函数为常量成员函数，常量成员函数在执行期间不应该修改其所作用的对象</li>
</ul>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Sample</span><span class="p">{</span>
    <span class="nl">public:</span>
        <span class="kt">int</span> <span class="n">value</span><span class="p">;</span>
        <span class="kt">void</span> <span class="n">GetValue</span><span class="p">()</span> <span class="k">const</span><span class="p">;</span>
        <span class="kt">void</span> <span class="n">func</span><span class="p">(){};</span>
<span class="p">}</span>
<span class="kt">void</span> <span class="n">Sample</span><span class="o">::</span><span class="n">GetValue</span><span class="p">()</span> <span class="k">const</span><span class="p">{</span>
    <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c1">// wrong</span>
    <span class="n">func</span><span class="p">();</span> <span class="c1">//wrong</span>
<span class="p">}</span>
</code></pre></div></div>
<ul>
  <li>常量成员函数和不加<code class="language-plaintext highlighter-rouge">const</code>的成员函数算重载关系</li>
  <li>对象作为函数的参数时，生成该参数需要调用复制构造函数，效率比较低，用指针作为参数，代码又不好看，那么解决方法就是用对象的引用作为参数</li>
</ul>

<h3 id="235-友元">2.3.5. 友元</h3>
<ul>
  <li>友元(friends)分为友元函数和友元类两种</li>
  <li>友元函数: 加上<code class="language-plaintext highlighter-rouge">friend</code>关键字就成为了友元函数，一个类的友元函数可以访问该类的私有成员</li>
</ul>

<center><img src="../assets/img/posts/20221120/64.jpg" /></center>
<center><img src="../assets/img/posts/20221120/65.jpg" /></center>

<ul>
  <li>可以将一个类的成员函数说明为另一个类的友元</li>
  <li>友元类: 如果A是B的友元类，那么A的成员函数可以访问B的私有成员</li>
</ul>

<center><img src="../assets/img/posts/20221120/66.jpg" /></center>

<ul>
  <li>友元类之间的关系不能传递，不能继承</li>
</ul>

<h2 id="24-运算符重载">2.4. 运算符重载</h2>
<h3 id="241-运算符重载的基本概念">2.4.1. 运算符重载的基本概念</h3>
<ul>
  <li>基本运算符只能支持c++基本的数据类型的运算</li>
</ul>
<center><img src="../assets/img/posts/20221120/67.jpg" /></center>

<ul>
  <li>运算符重载的意义:</li>
</ul>
<center><img src="../assets/img/posts/20221120/68.jpg" /></center>

<ul>
  <li>运算符重载的形式:</li>
</ul>
<center><img src="../assets/img/posts/20221120/69.jpg" /></center>

<ul>
  <li>具体形式:</li>
</ul>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">返回值类型</span> <span class="k">operator</span> <span class="err">运算符</span><span class="p">(</span><span class="err">形参表</span><span class="p">){</span>

<span class="p">}</span>
</code></pre></div></div>

<ul>
  <li>运算符重载实例:</li>
</ul>
<center><img src="../assets/img/posts/20221120/70.png" /></center>

<ul>
  <li>运算符重载作为不同函数时参数的个数不一样，作为成员函数时，不用包含自身</li>
</ul>

<h3 id="242-赋值运算符的重载">2.4.2. 赋值运算符的重载</h3>
<ul>
  <li>赋值运算符=的重载，相当于一种方便的初始化手段，只能作为成员函数重载</li>
</ul>
<center><img src="../assets/img/posts/20221120/71.jpg" /></center>

<ul>
  <li>一个例子:</li>
</ul>
<center><img src="../assets/img/posts/20221120/72.png" /></center>

<ul>
  <li>深拷贝和浅拷贝</li>
</ul>
<center><img src="../assets/img/posts/20221120/73.png" /></center>
<center><img src="../assets/img/posts/20221120/74.jpg" /></center>

<ul>
  <li>解决方法:</li>
</ul>
<center><img src="../assets/img/posts/20221120/75.jpg" /></center>

<ul>
  <li>但是还会存在问题:</li>
</ul>
<center><img src="../assets/img/posts/20221120/76.jpg" /></center>

<ul>
  <li>为什么赋值运算符的返回值类型是<code class="language-plaintext highlighter-rouge">String &amp;</code>？</li>
</ul>
<center><img src="../assets/img/posts/20221120/77.jpg" /></center>

<ul>
  <li>复制构造函数:</li>
</ul>
<center><img src="../assets/img/posts/20221120/78.jpg" /></center>

<h3 id="243-运算符重载为友元">2.4.3. 运算符重载为友元</h3>
<ul>
  <li>有时候会出现比较特殊的情况，需要将运算符重载函数写在类外，同时还需要访问类的私有成员，这时候就可以让友元函数发挥作用</li>
</ul>
<center><img src="../assets/img/posts/20221120/79.jpg" /></center>
<center><img src="../assets/img/posts/20221120/80.jpg" /></center>
<center><img src="../assets/img/posts/20221120/81.jpg" /></center>

<h3 id="244-可变长数组的实现">2.4.4. 可变长数组的实现</h3>
<ul>
  <li>希望这个可变长数组实现的功能:</li>
</ul>
<center><img src="../assets/img/posts/20221120/82.png" /></center>

<ul>
  <li>因为我们希望实现动态分配内存来存储数组元素，那么需要一个指针成员变量</li>
  <li>取下标的操作<code class="language-plaintext highlighter-rouge">[]</code>也可以重载:</li>
</ul>
<center><img src="../assets/img/posts/20221120/83.jpg" /></center>

<ul>
  <li>构造函数的实现:</li>
</ul>
<center><img src="../assets/img/posts/20221120/84.png" /></center>
<ul>
  <li>析构函数和赋值运算符重载:</li>
</ul>
<center><img src="../assets/img/posts/20221120/85.png" /></center>
<center><img src="../assets/img/posts/20221120/86.jpg" /></center>

<ul>
  <li><code class="language-plaintext highlighter-rouge">push_back()</code>比较麻烦的实现: 每次重新分配内存空间</li>
</ul>
<center><img src="../assets/img/posts/20221120/87.jpg" /></center>

<h3 id="245-流插入运算符的重载">2.4.5. 流插入运算符的重载</h3>
<ul>
  <li><code class="language-plaintext highlighter-rouge">cout</code>是ostream类的对象</li>
</ul>
<center><img src="../assets/img/posts/20221120/88.jpg" /></center>

<ul>
  <li>为了实现<code class="language-plaintext highlighter-rouge">cout&lt;&lt;5&lt;&lt;'this'</code>这样连续输出的功能，重载<code class="language-plaintext highlighter-rouge">&lt;&lt;</code>时返回值为<code class="language-plaintext highlighter-rouge">ostream &amp;</code>:</li>
</ul>
<center><img src="../assets/img/posts/20221120/89.png" /></center>

<ul>
  <li>一个例子:</li>
</ul>
<center><img src="../assets/img/posts/20221120/90.jpg" /></center>
<center><img src="../assets/img/posts/20221120/91.jpg" /></center>

<ul>
  <li>重载<code class="language-plaintext highlighter-rouge">&gt;&gt;</code>运算符:</li>
</ul>
<center><img src="../assets/img/posts/20221120/92.jpg" /></center>
<center><img src="../assets/img/posts/20221120/93.jpg" /></center>
<center><img src="../assets/img/posts/20221120/94.jpg" /></center>

<h3 id="246-类型转换符的重载">2.4.6. 类型转换符的重载</h3>
<ul>
  <li>类型运算符也可以重载，比如说</li>
</ul>
<center><img src="../assets/img/posts/20221120/95.png" /></center>

<ul>
  <li>类型转换运算符重载时不需要写返回值类型</li>
</ul>

<h3 id="247-自增自减运算符的重载">2.4.7. 自增、自减运算符的重载</h3>
<ul>
  <li>为了区分自增运算符的前置和后置，C++规定前置运算符用成员函数重载，后置运算符用普通函数重载，多一个没用的参数</li>
</ul>
<center><img src="../assets/img/posts/20221120/96.png" /></center>

<ul>
  <li>一个例子:</li>
</ul>
<center><img src="../assets/img/posts/20221120/97.jpg" /></center>
<center><img src="../assets/img/posts/20221120/98.jpg" /></center>

<ul>
  <li>后置++/–会返回一个临时的对象</li>
  <li>运算符重载的注意事项:</li>
</ul>
<center><img src="../assets/img/posts/20221120/99.jpg" /></center>]]></content><author><name>Quehry</name></author><category term="course" /><category term="notes" /><summary type="html"><![CDATA[郭炜网课，C++的面向对象的程序设计]]></summary></entry><entry><title type="html">Bert</title><link href="http://localhost:4000/Bert.html" rel="alternate" type="text/html" title="Bert" /><published>2022-11-17T00:00:00+08:00</published><updated>2022-11-17T00:00:00+08:00</updated><id>http://localhost:4000/Bert</id><content type="html" xml:base="http://localhost:4000/Bert.html"><![CDATA[<h1 id="1-bert简介">1. Bert简介</h1>
<p>BERT是GOOGLE团队于2018年提出的模型，在NLP领域里有着举足轻重的地位，BERT开创了NLP领域的预训练时代。BERT其实就是Transformer的编码器。BERT的双向性体现在MLM任务上，该任务可以让模型在得到全局的信息之后再去做预测</p>
<ul>
  <li><a href="https://arxiv.org/abs/1810.04805" target="_blank">论文地址</a></li>
  <li><a href="https://www.bilibili.com/video/BV1PL411M7eQ/?spm_id_from=333.999.0.0&amp;vd_source=64c99329fc39a0e3f42825a4c837e2a5" target="_blank">论文精度</a></li>
  <li><a href="https://github.com/google-research/bert" target="_blank">开源代码</a></li>
</ul>

<h1 id="2-相关工作">2. 相关工作</h1>
<p>在BERT之前，将预训练的自然语言表示应用到下游任务有两种方法，一种是feature-based，代表模型就是ELMo，另一种是finetune，代表模型是GPT。这两种模型都是基于语言模型实现损失函数，也就是单向的，Bert提出了用掩码语言模型来做损失函数，这样既可以学到从左往右的信息，也可以学到从右往左的信息</p>

<h1 id="3-模型架构">3. 模型架构</h1>
<p>模型主体架构就是Transformer的编码器</p>

<h2 id="31-输入表示">3.1. 输入表示</h2>
<p>由于BERT预训练的数据集很大，所以BERT的采用WordPiece的方法来获得词元，WordPiece常见的方法有BPE，这样处理后有词表大小为30000。除此之外，BERT还有特殊的词元&lt;cls&gt;和&lt;sep&gt;，分别表示序列的整体信息和序列对的信息，嵌入层:</p>

<center><img src="../assets/img/posts/20221117/1.jpg" /></center>

<p>Segment Embedding就是句段的信息，如果有两个句子，那么输入为0或者1</p>

<h2 id="32-预训练的task">3.2. 预训练的Task</h2>
<p>预训练任务有两个，掩码语言模型(MLM)和下一句预测(NSP)，MLM主要为了学习词元级别的信息，NSP为了学习序列对的信息，主要为了服务QA等任务。掩码语言模型就是随机替换或者掩蔽某一词元，具体来说，选择15%的词元来做掩蔽，10%的几率不替换，10%的几率替换成随机词元，80%的几率替换成[Mask]词元。</p>

<h1 id="4-预训练数据集和实验">4. 预训练数据集和实验</h1>
<p>预训练数据集为: BooksCorpus和English Wikipedia</p>

<p>做了很多很多实验，刷了很多榜，GLUE，SQuAD，SWAG，针对不同的问题，需要给模型加上输出层做微调，其中很多任务都使用特殊字符&lt;cls&gt;来做预测</p>

<p>Bert-Base有110M参数，Bert-Large有340M个参数</p>]]></content><author><name>Quehry</name></author><category term="Bert" /><category term="Paper Reading" /><category term="NLP" /><summary type="html"><![CDATA[notes for Bert]]></summary></entry><entry><title type="html">Transformer</title><link href="http://localhost:4000/Transformer.html" rel="alternate" type="text/html" title="Transformer" /><published>2022-11-15T00:00:00+08:00</published><updated>2022-11-15T00:00:00+08:00</updated><id>http://localhost:4000/Transformer</id><content type="html" xml:base="http://localhost:4000/Transformer.html"><![CDATA[<h1 id="1-transformer简介">1. Transformer简介</h1>
<p>2017年Google团队提出的完全基于attention机制的神经网络结构，是一种全新的网络架构，影响力十分大，后续的BERT, ViT等多个熟知的预训练模型都是由Transformer变体而来，相关链接:</p>
<ul>
  <li><a href="https://arxiv.org/abs/1706.03762" target="_blank">论文地址</a></li>
  <li><a href="https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.999.0.0&amp;vd_source=64c99329fc39a0e3f42825a4c837e2a5" target="_blank">论文精读</a></li>
  <li><a href="https://github.com/tensorflow/tensor2tensor" target="_blank">代码</a></li>
  <li><a href="https://theaisummer.com/attention/" target="_blank">一篇介绍注意力机制的博客</a></li>
</ul>

<h1 id="2-introduction">2. Introduction</h1>
<p>最开始Transformer是为了解决<strong>机器翻译</strong>的任务，Transformer没出来前，解决机器翻译主要的手段是RNN和Encoder-Decoder架构，比如说Seq2Seq。RNN的缺点是依赖时间顺序，第一个方面是并行度比较低，第二个方面是如果时序比较长，那么前期的时序信息在后期可能会丢掉。Transformer的特点有: 自注意力、多头注意力。</p>

<h1 id="3-模型架构">3. 模型架构</h1>
<p>模型预览图:</p>

<center><img src="../assets/img/posts/20221115/1.jpg" /></center>

<p>Transformer时序维度始终是512，BatchNorm和LayerNorm的区别: BN是针对一个batch的特征来做平均和方差，LN是针对batch中一个样本的所有特征做均值和方差。一般的Transformer能接受的序列长度最好不超过512，超过这个长度可能会导致硬件算不动</p>

<h2 id="31-注意力">3.1. 注意力</h2>
<p>注意力有三要素，query、key、value。对于每个query，计算与其他所有key的compatibility作为value的权重。Transformer使用的计算compatibility的方式是Scaled Dot-Product Attention，也是应用最多的计算方式:</p>

<p>
\begin{equation}
\text{Attention}(Q,K,V)=softmax(\frac{QK^T}{d_k})V
\end{equation}
</p>

<p>结合图片理解:</p>
<center><img src="../assets/img/posts/20221115/10.png" /></center>

<center><img src="../assets/img/posts/20221115/2.jpg" /></center>

<p>讲道理来说，对于时间步t的查询$q_t$而言，它不应该能看到时间步t后的序列，但是在计算attention的时候却可以看到所有的序列，所以需要有mask来保证序列在预测过程中看不见时间步t之后的序列</p>

<h2 id="32-多头注意力">3.2. 多头注意力</h2>
<p>多头注意力的出发点是让高维的序列能有h次学习机会，这样有机会能学习到更多的信息，假设有h个头，那么维度就需要投影到512/h，然后再做注意力，然后将h个头concatenate，这样就可以实现多头注意力</p>
<center><img src="../assets/img/posts/20221115/3.jpg" /></center>

<p>在decoder里，query是当前的输入，k和v都是encoder的输出</p>

<h2 id="33-position-wise-feed-forward-networks">3.3. Position-wise Feed-Forward Networks</h2>
<p>位置前馈神经网络，其实就是一个基于每个时间步的MLP，由于Transformer里每个时间步都做了attention，所以它包含了序列的信息，所以可以直接针对每个时间步做MLP</p>

<h2 id="34-embedding-and-softmax">3.4. Embedding and softmax</h2>
<p>encoder的embedding层和decoder的embedding以及decoder中softmax前的线性层是共享权重的，其中embedding层的权重需要乘以$\sqrt{d_{model}}$(为了保证数值相似)。embedding层最大的作用就是将vocab_size映射到模型的维度</p>

<h2 id="35-positional-encoding">3.5. Positional Encoding</h2>
<p>位置编码，因为注意力机制其实是没有考虑序列的位置信息的，它计算的是query和key的距离，所以在输入模型前需要加上位置编码来保证序列的每一步都包含其位置信息，pos表示第几个单词，2i表示对于偶数列，2i+1表示奇数列</p>

<p>
\begin{equation}
\begin{aligned}
\text{PE}_{(pos,2i)} &amp; = sin(pos/10000^{2i/d_{model}}) \\
\text{PE}_{(pos,2i+1)} &amp; = cos(pos/10000^{2i/d_{model}})
\end{aligned}
\end{equation}
</p>

<p>Transformer的位置编码的做法是利用sin和cos对不同的时间步进行编码，也就是说Transformer的做法是不训练位置编码，作者的解释是相较于训练的位置编码，两者结果相似。</p>

<h1 id="4-数据集和实验">4. 数据集和实验</h1>
<p>Transformer解决的是机器翻译任务，数据集为WMT 2014 English-German dataset，分词的时候采取BPE，并且英语和德语共用一个词库，这样保证了encoder和decoer的embedding层一致</p>

<p>学习率的设置:</p>
<p>
\begin{equation}
lrate=d_{model}^{-0.5}\cdot min(step\_num^{-0.5}, step\_num\cdot warmup\_steps^{-1.5})
\end{equation}
</p>

<p>技巧方面使用了很多的dropout和label smoothing，label smoothing就是让softmax去逼近设置的$\epsilon_{ls}$。</p>

<p>实验结果</p>
<center><img src="../assets/img/posts/20221115/4.jpg" /></center>

<h1 id="5-为什么注意力机制有用">5. 为什么注意力机制有用？</h1>
<h2 id="51-回顾一下seq2seq">5.1. 回顾一下seq2seq</h2>
<p>seq2seq是典型的encoder-decoder架构，解决机器翻译问题，下面两张图清晰地介绍了seq2seq的基本架构及原理:</p>
<center><img src="../assets/img/posts/20221115/5.png" /></center>

<center><img src="../assets/img/posts/20221115/6.png" /></center>
<p>其中的RNN块可以替换成LSTM块或者GRU块，可以取得更好的结果</p>

<h2 id="52-seq2seq的缺陷">5.2. seq2seq的缺陷</h2>
<p>中间表达z不能编码输入序列所有时间步的信息，也就是常说的bottleneck problem(瓶颈问题)，RNN会忘记稍远的时间步的信息，所以当序列长度稍长时，seq2seq进行翻译任务时表现会有所下滑，除此之外，RNN还有一个广为人知的缺陷，梯度消失问题，前面时间步的信息会在后续变得越来越不重要。</p>
<center><img src="../assets/img/posts/20221115/7.png" /></center>

<p>为了解决这个问题，attention出现了，保证了我们能看到序列的每一个时间步的信息，然后pay attention to最重要的部分</p>

<h2 id="53-attention">5.3. attention</h2>
<p>attention本质上就是加权的value，特殊之处在于这个权重是一个可学习的函数。下面这个图展示了attention的对应关系，可以发现l’homme在man的权重最高。</p>
<center><img src="../assets/img/posts/20221115/8.png" /></center>

<p>不同的注意力alignment score function:</p>
<center><img src="../assets/img/posts/20221115/9.png" /></center>

<p>attention让我们能注意到整个句段的代价是更复杂的计算，局部注意力(local attention)就是舍去了部分单词的权重</p>

<h2 id="54-self-attention">5.4. self-attention</h2>
<p>Transformer中绕不开的部分，自己对自己进行注意力计算</p>

<h2 id="55-attention的优点">5.5. attention的优点</h2>
<p>除了attention能看到序列的所有单词这一优势外，attention还有两个优点，解决了梯度消失问题，极大的可解释性</p>]]></content><author><name>Quehry</name></author><category term="Transformer" /><category term="Paper Reading" /><summary type="html"><![CDATA[notes for Transformer]]></summary></entry><entry><title type="html">Vision Transformer</title><link href="http://localhost:4000/ViT.html" rel="alternate" type="text/html" title="Vision Transformer" /><published>2022-11-14T00:00:00+08:00</published><updated>2022-11-14T00:00:00+08:00</updated><id>http://localhost:4000/ViT</id><content type="html" xml:base="http://localhost:4000/ViT.html"><![CDATA[<h1 id="1-vit简介">1. ViT简介</h1>
<p>ViT的全称是Vision Transformer，模型最大的特点就是把NLP领域的Transformer迁移到CV领域，模型在图像识别等多个CV任务上表现超越了卷积神经网络，相关链接:</p>

<ul>
  <li><a href="https://arxiv.org/abs/2010.11929" target="_blank">论文地址</a></li>
  <li><a href="https://www.bilibili.com/video/BV15P4y137jb/?spm_id_from=333.788&amp;vd_source=64c99329fc39a0e3f42825a4c837e2a5" target="_blank">论文精读视频</a></li>
  <li><a href="https://github.com/google-research/vision_transformer" target="_blank">开源代码</a></li>
</ul>

<h1 id="2-introduction">2. Introduction</h1>
<p>限制Transformer在CV领域发挥的一个要素就是序列长度的问题，如果把图像的每个像素点看成token，那么对于一张中分辨率的图片224*224而言，序列长度为50k，Bert模型处理的序列长度只有512，所以将Transformer应用到CV领域的一个难点就是序列长度过长。ViT于是提出了将图片分割成16*16的patch的做法，这样就可以大大减少序列的长度，不改变Transformer架构的情况下直接应用，取得了很好的结果，说明了transformer确实能在CV领域能取得很好的效果。</p>

<h1 id="3-模型">3. 模型</h1>
<p>模型预览图:</p>

<center><img src="../assets/img/posts/20221114/1.jpg" /></center>

<p>为了缩短序列的长度，作者将图片分为了很多个patch，patch的大小可以是16*16，每个patch的表示为每个像素点的灰度值排列，那么一张图片patch的形状为num_patchs*768，其中768=16*16*3。得到patch的表示后，输入线性投影层(就是一个线性层，形状可以是768*768)。为了和transformer保持一致，ViT也采用了位置编码和特殊编码&lt;cls&gt;的使用，位置编码采用的是1D位置编码，位置编码和特殊编码都可学习，patch的线性投影表示和位置编码是直接加在一起，这样便得到了图像的token，其余步骤和Bert类似，ViT的训练是有监督训练，用特殊编码&lt;cls&gt;来做预测</p>

<h1 id="4实验和数据集">4.实验和数据集</h1>
<p>数据集使用了ImageNet-1k、ImageNet-21k和Google自家的JFT数据集</p>

<p>和BERT一样，ViT也根据模型使用的参数和patch的大小不同，分为了以下几种:</p>
<center><img src="../assets/img/posts/20221114/2.jpg" /></center>
<p>比如ViT-L/16代表Large ViT with patch size 16*16</p>

<p>实验结果</p>
<center><img src="../assets/img/posts/20221114/3.jpg" /></center>
<p>实验的metrics是预训练模型微调后的accuracy</p>

<center><img src="../assets/img/posts/20221114/4.jpg" /></center>
<p>这张图表明ViT在数据集大的情况下会表现更好</p>]]></content><author><name>Quehry</name></author><category term="Transformer" /><category term="Paper Reading" /><summary type="html"><![CDATA[notes for ViT]]></summary></entry><entry><title type="html">Vision Language Model</title><link href="http://localhost:4000/Vision-Language-Model.html" rel="alternate" type="text/html" title="Vision Language Model" /><published>2022-11-14T00:00:00+08:00</published><updated>2022-11-14T00:00:00+08:00</updated><id>http://localhost:4000/Vision-Language-Model</id><content type="html" xml:base="http://localhost:4000/Vision-Language-Model.html"><![CDATA[<h1 id="相关链接">相关链接</h1>
<ul>
  <li><a href="https://lilianweng.github.io/posts/2022-06-09-vlm/#no-training" target="_blank">lils blog</a></li>
  <li><a href="https://theaisummer.com/vision-language-models/#vision-language-tasks" target="_blank">AI Summer</a></li>
</ul>

<h1 id="vlm">VLM</h1>
<p>VLM，即vision language model，旨在用语言模型获得视觉信息。lilian将VLM分为了四种，分别是:</p>

<ol>
  <li>利用嵌入层获得图片特征，然后与词元特征聚合后一起训练，代表性的模型有VisualBERT、SimVLM和CM3</li>
  <li>将训练好的图片嵌入层直接用于模型，这些图片嵌入层是frozen的，即整体模型在训练时不改变图片嵌入层的权重，代表性的模型有CLIP</li>
  <li>利用注意力机制将视觉信息融入到语言模型中，代表模型有VisualGPT，VC-GPT，MERLOT，Flamingo，Coca</li>
  <li>直接combine视觉和语言模型，不加以训练，代表性模型有MAGiC，PICa，Socratic Models</li>
</ol>

<h1 id="任务">任务</h1>
<p>VLM能实现的任务可以分为三类:</p>
<ol>
  <li>生成任务:
    <ul>
      <li>Visual QA: 给一张图片和一个问题，模型根据图片信息返回答案</li>
      <li>Visual Captioning: 给定图片，生成字幕</li>
      <li>Visual Commonsense Reasoning: 给定图片，推断出图片的common-sense information</li>
      <li>Visual Generation: 给定文本输入，生成图片</li>
    </ul>
  </li>
  <li>分类任务:
    <ul>
      <li>Multimodal Affective Computing: 多模态版本的情感分析</li>
      <li>Natural Language for Visual Reasoning: 给定一张图片和一段陈述，判断陈述是否正确</li>
    </ul>
  </li>
  <li>找回任务(retrieval task):
    <ul>
      <li>Visual Retrieval: 通过文本描述恢复图片</li>
      <li>Vision-Language Navigation: 通过自然语言的指令来对agent进行导航</li>
      <li>Multimodal Machine Translation: 将一种语言翻译成另一种语言，附带视觉信息</li>
    </ul>
  </li>
</ol>

<h1 id="bert-like架构">BERT-like架构</h1>
<p>鉴于BERT在NLP领域的兴起，不同模态领域里也出现了BERT-like的架构，代表性的模型有VisualBERT，ViLBERT，PixelBERT等</p>

<h1 id="contrastive-learning">contrastive learning</h1>
<p>自从CLIP出现后，大家发现用对比学习的方法能很好地连接起vision和language的信息，类似的模型有ALIGN和FLORENCE</p>

<h1 id="vlm-论文">VLM 论文</h1>
<ul>
  <li><a href="https://paperswithcode.com/methods/category/vision-and-language-pre-trained-models" target="_blank">paperswithcode</a></li>
</ul>

<h1 id="vlm-最新的论文">VLM 最新的论文</h1>]]></content><author><name>Quehry</name></author><category term="overview" /><summary type="html"><![CDATA[an overview in domain VLM]]></summary></entry><entry><title type="html">ROX learning</title><link href="http://localhost:4000/ROS.html" rel="alternate" type="text/html" title="ROX learning" /><published>2022-10-30T00:00:00+08:00</published><updated>2022-10-30T00:00:00+08:00</updated><id>http://localhost:4000/ROS</id><content type="html" xml:base="http://localhost:4000/ROS.html"><![CDATA[<h1 id="1-简介">1. 简介</h1>
<p>这篇博客主要记录学习ROS的笔记，内容来源于:</p>
<ul>
  <li><a href="https://www.bilibili.com/video/BV1zt411G7Vn/?spm_id_from=333.337.search-card.all.click&amp;vd_source=64c99329fc39a0e3f42825a4c837e2a5" target="_blank">bili古月居</a></li>
  <li><a href="https://www.udemy.com/course/ros-navigation/learn/lecture/11785182#overview" target="_blank">udemy</a></li>
  <li><a href="https://github.com/guyuehome/ros_21_tutorials" target="_blank">古月居课件及源码</a></li>
</ul>

<h1 id="2-古月居">2. 古月居</h1>

<h2 id="21-ros安装">2.1. ROS安装</h2>
<p>根据<a href="http://wiki.ros.org/noetic/Installation/Ubuntu" target="_blank">ROS-wiki</a>的说明一步步进行安装</p>

<h2 id="22-ros是什么">2.2. ROS是什么</h2>
<ul>
  <li>ROS发展史:</li>
</ul>

<center><img src="../assets/img/posts/20221030/2.jpg" /></center>

<ul>
  <li>ROS能提高机器人研发中软件的复用率</li>
</ul>

<center><img src="../assets/img/posts/20221030/3.png" /></center>

<h2 id="23-ros的核心概念">2.3. ROS的核心概念</h2>
<ul>
  <li>节点(Node)是ROS中的执行单元:
    <ul>
      <li>执行具体任务的进程、独立运行的可执行文件</li>
      <li>不同节点可使用不同的编程语言，分布式运行在不同的主机</li>
      <li>节点在系统的名称必须唯一</li>
    </ul>
  </li>
  <li>节点管理器(ROS Master)是ROS的控制中心:
    <ul>
      <li>为节点提供命名和注册服务</li>
      <li>跟踪和记录话题/服务通信，辅助节点相互查找、建立连接</li>
      <li>为节点提供参数</li>
    </ul>
  </li>
</ul>

<center><img src="../assets/img/posts/20221030/4.png" /></center>

<ul>
  <li>话题通信机制(异步):
    <ul>
      <li>话题(Topic)
        <ul>
          <li>节点间用来传输数据的重要总线</li>
          <li>使用发布/订阅模型，数据由发布者传输到订阅者，同一个话题的订阅者或发布者可以不唯一</li>
        </ul>
      </li>
      <li>消息(Message)
        <ul>
          <li>具有一定类型的数据结构</li>
          <li>使用编程语言无关的.msg文件定义，编译过程中生成对应的代码文件</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<center><img src="../assets/img/posts/20221030/5.png" /></center>

<ul>
  <li>服务通信机制(同步):
    <ul>
      <li>服务(Service)
        <ul>
          <li>使用客户端/服务器模型，客户端发送请求数据，服务器完成处理后返回应答数据</li>
          <li>使用编程语言无关的.srv文件定义请求和应答数据结构</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<center><img src="../assets/img/posts/20221030/6.png" /></center>

<ul>
  <li>话题和服务通信的区别:</li>
</ul>

<center><img src="../assets/img/posts/20221030/7.png" /></center>

<ul>
  <li>参数-全局共享字典:
    <ul>
      <li>可通过网络访问共享、多变量字典</li>
      <li>节点使用此服务器来存储和检索运行时的参数</li>
      <li>不适合存储动态配置的数据</li>
    </ul>
  </li>
</ul>

<center><img src="../assets/img/posts/20221030/8.png" /></center>

<ul>
  <li>文件系统</li>
</ul>

<center><img src="../assets/img/posts/20221030/9.png" /></center>

<h2 id="24-ros命令行工具">2.4. ROS命令行工具</h2>
<ul>
  <li>
    <p>常用命令: rostopic/rosservice/rosnode/rosparam/rosmsg/rossrv</p>
  </li>
  <li>
    <p>rqt_graph可以查看系统的节点和通信机制</p>
  </li>
</ul>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rqt_graph
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">rosnode list</code>可以查看节点的列表</li>
  <li><code class="language-plaintext highlighter-rouge">rosnode info /turtlesim</code>可以查看turtlesim这个节点的信息</li>
  <li><code class="language-plaintext highlighter-rouge">rostopic list</code>可以查看话题的列表</li>
  <li><code class="language-plaintext highlighter-rouge">rostopic pub /turtlesim/cmd_vel ...</code>可以使用cmd_vel话题发送消息</li>
  <li><code class="language-plaintext highlighter-rouge">rosmsg show geometry_msgs/Twist</code>可以查看某个消息的信息</li>
  <li><code class="language-plaintext highlighter-rouge">rosservice list</code>可以查看ROS中服务的列表</li>
  <li><code class="language-plaintext highlighter-rouge">rosservice call ...</code>可以调用某个服务</li>
</ul>

<center><img src="../assets/img/posts/20221030/10.png" /></center>

<ul>
  <li><code class="language-plaintext highlighter-rouge">rosbag record ...</code>可以记录某一个话题</li>
  <li><code class="language-plaintext highlighter-rouge">rosbag play ...</code>可以通过保存的bag文件来复现该话题</li>
</ul>

<center><img src="../assets/img/posts/20221030/11.png" /></center>

<h2 id="25-创建工作空间与功能包">2.5. 创建工作空间与功能包</h2>
<ul>
  <li>工作空间是一个存放工程开发相关文件的文件夹</li>
</ul>

<center><img src="../assets/img/posts/20221030/12.png" /></center>

<ul>
  <li>创建工作空间的步骤</li>
</ul>

<center><img src="../assets/img/posts/20221030/13.png" /></center>

<ul>
  <li><code class="language-plaintext highlighter-rouge">catkin_make install</code>创建install空间</li>
  <li>创建功能包</li>
</ul>

<center><img src="../assets/img/posts/20221030/14.png" /></center>

<h2 id="26-publisher的编程实现">2.6. Publisher的编程实现</h2>
<ul>
  <li>话题模型</li>
</ul>

<center><img src="../assets/img/posts/20221030/15.png" /></center>

<ul>
  <li>首先创建功能包</li>
</ul>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ~/catkiin_ws/src
catkin_create_pkg learning_topic roscpp rospy std_msgs geometry_msgs turtlesim
</code></pre></div></div>

<ul>
  <li>使用c++创建publisher</li>
</ul>

<center><img src="../assets/img/posts/20221030/16.png" /></center>

<ul>
  <li>创建publisher时需要声明消息类型，话题的名称，创建publisher完毕后，即可创建消息来让publisher发布</li>
  <li>配置publisher代码编译的规则(在cmakelists中)</li>
</ul>

<center><img src="../assets/img/posts/20221030/17.png" /></center>

<ul>
  <li>设置需要编译的代码和生成的可执行文件/设置链接库</li>
  <li>编译并运行发布者</li>
</ul>

<center><img src="../assets/img/posts/20221030/18.png" /></center>

<ul>
  <li>python实现publisher</li>
</ul>

<center><img src="../assets/img/posts/20221030/19.png" /></center>

<h2 id="27-subscriber的实现">2.7. Subscriber的实现</h2>
<ul>
  <li>python实现subsriber</li>
</ul>

<center><img src="../assets/img/posts/20221030/20.png" /></center>

<h2 id="28-话题消息的定义与使用">2.8. 话题消息的定义与使用</h2>
<ul>
  <li>自定义话题消息
    <ul>
      <li>定义msg文件</li>
      <li>在package.xml中添加功能包的依赖</li>
      <li>在CMakeLists中添加编译选项</li>
      <li>编译生成语言相关文件</li>
    </ul>
  </li>
</ul>

<center><img src="../assets/img/posts/20221030/21.png" /></center>

<ul>
  <li>python实现topic</li>
</ul>

<center><img src="../assets/img/posts/20221030/22.png" /></center>

<ul>
  <li>定义publisher和subscriber(需要声明topic和消息)</li>
</ul>

<h2 id="29-client实现">2.9. client实现</h2>
<ul>
  <li>服务模型(C/S模型):</li>
</ul>

<center><img src="../assets/img/posts/20221030/23.png" /></center>

<ul>
  <li>python实现client</li>
</ul>

<center><img src="../assets/img/posts/20221030/24.png" /></center>

<h2 id="210-server实现">2.10. server实现</h2>
<ul>
  <li>python实现server:</li>
</ul>

<center><img src="../assets/img/posts/20221030/25.png" /></center>

<h2 id="211-服务数据的定义与使用">2.11. 服务数据的定义与使用</h2>
<ul>
  <li>自定义服务数据</li>
</ul>

<center><img src="../assets/img/posts/20221030/26.png" /></center>

<ul>
  <li>服务模型</li>
</ul>

<center><img src="../assets/img/posts/20221030/27.png" /></center>

<ul>
  <li>首先生成server节点，然后创建service，client在发现service后创建并发送数据，server在接受数据后进入callback返回request</li>
</ul>

<h2 id="212-参数的使用与编程方法">2.12. 参数的使用与编程方法</h2>
<ul>
  <li>参数模型</li>
</ul>

<h1 id="3-udemy课程">3. Udemy课程</h1>
<h2 id="31-course-overview">3.1. Course Overview</h2>
<h3 id="311-installation-of-turtlebot3">3.1.1. Installation of turtlebot3</h3>
<ul>
  <li>这小节主要介绍如何安装turtlebot3:</li>
  <li>按照udemy的讲义上一步步安装，首先安装ros-noetic-navigation和ros-noetic-slam-gmapping，然后拷贝github上的代码到本地编译turtlebot3，同样的做法安装turtlebot3-simulation</li>
</ul>

<h3 id="312-turtlebot3-simulation-environments">3.1.2. Turtlebot3 SImulation Environments</h3>
<ul>
  <li><code class="language-plaintext highlighter-rouge">roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch</code> 打开一个空白的空间，里面只有一辆小车</li>
  <li><code class="language-plaintext highlighter-rouge">roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch</code> 启动后，可以键入wasd来操作小车</li>
  <li><code class="language-plaintext highlighter-rouge">roslaunch turtlebot3_gazebo tuetlebot3_world.launch</code> 里面有几根柱子和小车</li>
  <li><code class="language-plaintext highlighter-rouge">roslaunch turtlebot3_gazebo turtlebot3_house.launch</code> 房间</li>
  <li><code class="language-plaintext highlighter-rouge">roslaunch turtlebot3_gazebo turtlebot3_gazebo_rviz.launch</code> ros中很重要的可视化软件Rviz，关于turtlebot3有适配版本，可以可视化小车的激光雷达和摄像头</li>
  <li><code class="language-plaintext highlighter-rouge">roslaunch turtlebot3_slam turtlebot3_slam.launch slam_methods:=gmapping</code> slam，可视化在Rviz上，slam的算法有很多，这里采用gmapping的方法</li>
</ul>

<h2 id="32-ros-navigation-demo">3.2. ROS Navigation Demo</h2>
<ul>
  <li>这章展示了ROS Navigation的一个demo，首先打开house，然后<code class="language-plaintext highlighter-rouge">roslaunch turtlebot3_navigation turtlebot3_navigation.launch map_file=...</code>来打开Navigation，在Rviz中展示，标定好终点后，它会自动规划路径到达终点</li>
  <li>frame就是不同的参考系</li>
</ul>

<h2 id="33-2d-frames-transformations-and-localization">3.3. 2D Frames, Transformations and Localization</h2>

<h2 id="34-3d-frames-transformations-and-localization">3.4. 3D Frames, Transformations and Localization</h2>

<h2 id="35-orientation-in-3d-space">3.5. Orientation in 3D Space</h2>

<h2 id="36-the-tf-package-frame-transformation-and-localization-in-ros-explained">3.6. The TF Package: Frame, Transformation and Localization in ROS Explained</h2>
<ul>
  <li>TF是一个让用户随时间跟踪多个参考系的功能包，它使用一种树型数据结构，根据时间缓冲并维护多个参考系之间的坐标变换关系，可以帮助用户在任意时间，将点，向量等数据的坐标，在两个参考系中完成坐标变换:</li>
</ul>
<center><img src="../assets/img/posts/20221030/28.png" /></center>

<ul>
  <li>机器人的模型用一个XML文件定义，使用URDF(unified robot description format)这个语言去描述机器人的Frame和Transformation。其中Frame连接在关节上，在URDF中用link表示，不同关节的相对位置用Transformation表示，可以在ROS wiki上了解更详细的信息</li>
  <li>TF package nodes(TF功能包提供了一些有用的ROS nodes):</li>
</ul>
<center><img src="../assets/img/posts/20221030/29.png" /></center>

<ul>
  <li>可以通过订阅odom话题来获得odom的child frame:base_footprint的position和orientation，odom: topic represents a pose based on a odometry information</li>
  <li><code class="language-plaintext highlighter-rouge">rosrun tf2_tools view_frames</code>来获得当前场景下的frames和transformation的总体情况，会在当前目录生成一个pdf文件</li>
  <li>robot相对于map的参考系叫做amcl_pose</li>
  <li><code class="language-plaintext highlighter-rouge">rosrun tf tf_echo frame1 frame2</code>可以获得两个frame之间translation和rotation</li>
  <li><code class="language-plaintext highlighter-rouge">rosrun tf static_transform_publisher 1 2 3 0.1 0.2 0.3 frame_a frame_b 10</code>可以在两个frame之间创造一个transformation</li>
  <li>可以定义节点来广播frame_a到frame_b的transformation，也可以定义节点来收听frame_a到frame_b的transformation</li>
  <li>总的来说，TF功能包能获得当前场景下的frame和transformation信息，同时也可以自定义frame之间的transformation</li>
</ul>

<h2 id="37-map-based-navigation">3.7. Map-based navigation</h2>
<ul>
  <li>robot navigation表示机器人自动避障的能力，有两种主要的导航方法，第一种方法是map-based navigation，预加载好地图的相关信息，另一种方法是reactive navigation，不会预加载地图的相关信息，只会利用传感器感知周围环境</li>
  <li>navigation可以分为三步，localization，mapping，path planning:</li>
</ul>
<center><img src="../assets/img/posts/20221030/30.png" /></center>

<ul>
  <li>mapping的主要方法是SLAM(simultaneous localization and mapping):</li>
</ul>
<center><img src="../assets/img/posts/20221030/31.png" /></center>

<ul>
  <li>navigation的主要功能包:</li>
</ul>
<center><img src="../assets/img/posts/20221030/32.png" /></center>

<ul>
  <li>turtlebot3 slam demo:</li>
</ul>
<center><img src="../assets/img/posts/20221030/33.png" /></center>

<ul>
  <li>地图的表示方法: occupancy grid map，一张地图由网格状的cell组成，一个cell可以是empty或者occupied:</li>
</ul>
<center><img src="../assets/img/posts/20221030/34.png" /></center>
<center><img src="../assets/img/posts/20221030/36.png" /></center>

<ul>
  <li>gmapping是slam的一种方法，还有其他方法，比如cartographer，hector_slam:</li>
</ul>
<center><img src="../assets/img/posts/20221030/35.png" /></center>

<ul>
  <li>通过激光雷达的数据判断哪些地方有障碍物</li>
  <li>SLAM生成的地图的质量由odometry和laser scanner决定，odometry就是里程计，衡量机器人从初始姿态到终点位姿的一个标准，如果要实现机器人的定位与导航，就需要知道机器人行进了多少距离，往哪个方向行进的</li>
  <li>通过SLAM生成的地图可以保存下来，通过指令<code class="language-plaintext highlighter-rouge">rosrun map_server map_saver -f ~/tb3_house_map</code>来保存地图到指定路径</li>
  <li>保存的地图由两个文件组成，<code class="language-plaintext highlighter-rouge">.pgm</code>和<code class="language-plaintext highlighter-rouge">.yaml</code>，其中<code class="language-plaintext highlighter-rouge">.pgm</code>就是一张地图的图片，灰色的区域表示未知的区域，黑色的区域表示障碍物，白色的区域表示自由区域，<code class="language-plaintext highlighter-rouge">.yaml</code>存储了地图的metadata，比如原点，地图图片的路径，阈值等</li>
  <li>如何通过SLAM生成的地图对小车进行导航(Navigation):
    <ul>
      <li>首先告诉小车自己在哪里，通过Rviz的2D pose estimation实现</li>
      <li>然后确定小车需要到达的终点</li>
      <li>motion planner algorithm告诉小车如何到达终点</li>
    </ul>
  </li>
</ul>
<center><img src="../assets/img/posts/20221030/37.png" /></center>

<ul>
  <li>Map-based navigation命令行: <code class="language-plaintext highlighter-rouge">roslaunch turtlebot3_navigation turtlebot3_navigation.launch map_file:=...</code></li>
</ul>
<center><img src="../assets/img/posts/20221030/38.png" /></center>

<ul>
  <li>通过程序实现map-based navigation，无需通过Rviz实现</li>
  <li>为什么出现小车后退重新尝试的行为(recovery behavior):</li>
</ul>
<center><img src="../assets/img/posts/20221030/39.png" /></center>

<ul>
  <li>如何让机器人支持ROS的Navigation模块</li>
</ul>
<center><img src="../assets/img/posts/20221030/40.png" /></center>

<ul>
  <li>Robot setup：
    <ul>
      <li>安装ROS</li>
      <li>必须有tf transform</li>
      <li>URDF</li>
      <li>sensor information</li>
      <li>odometry information</li>
      <li>base controller</li>
      <li>mapping</li>
    </ul>
  </li>
</ul>

<h2 id="38-navigation-stack-tuning">3.8. Navigation Stack Tuning</h2>
<ul>
  <li>控制Navigation的最大速度/加速度和最小速度/加速度，可以通过更改local planner的yaml文件，对于turtlebot3而言，这个文件在<code class="language-plaintext highlighter-rouge">turtlebot3_navigation/param</code>里</li>
  <li>如何获得小车的最大速度:</li>
</ul>
<center><img src="../assets/img/posts/20221030/41.png" /></center>

<ul>
  <li>如何获得小车的最大加速度:</li>
</ul>
<center><img src="../assets/img/posts/20221030/42.png" /></center>

<ul>
  <li>Global Path Planner Tuning, 微调全局路径规划参数，全局路径规划需要考虑地图信息来找到一条没有障碍的路径</li>
</ul>
<center><img src="../assets/img/posts/20221030/43.png" /></center>

<ul>
  <li>ROS中有三个global planner：</li>
</ul>
<center><img src="../assets/img/posts/20221030/44.png" /></center>

<ul>
  <li>Carrot Planner的示意图:</li>
</ul>
<center><img src="../assets/img/posts/20221030/45.png" /></center>

<ul>
  <li>NAVFN 和 global_planner</li>
</ul>
<center><img src="../assets/img/posts/20221030/46.png" /></center>

<ul>
  <li>global_planner中可以调节的参数的例子:</li>
</ul>
<center><img src="../assets/img/posts/20221030/47.png" /></center>

<ul>
  <li>local path planner的相关知识，局部路径规划会在执行全局路径规划给出的路径的同时，通过传感器获得的局部信息来躲避动态的障碍。</li>
</ul>
<center><img src="../assets/img/posts/20221030/48.png" /></center>

<ul>
  <li>DWA algorithm(Dynamic Window Approach 属于local path planning的一种):
    <ul>
      <li>离散地采样几种方向和速度</li>
      <li>对于每一种采样，都预测一下如何按照这种方式前进会发生什么</li>
      <li>对于每一种结果，使用一种评价标准进行打分</li>
      <li>选取最高评分的前进方法</li>
    </ul>
  </li>
</ul>
<center><img src="../assets/img/posts/20221030/49.png" /></center>

<ul>
  <li>微调DWA算法的参数，微调simulation time这一参数</li>
</ul>
<center><img src="../assets/img/posts/20221030/50.png" /></center>

<ul>
  <li>不同DWA参数的影响</li>
</ul>
<center><img src="../assets/img/posts/20221030/51.png" /></center>

<ul>
  <li>动手实现微调DWA的simulation time参数对navigation的影响(视频在notion上有)</li>
</ul>]]></content><author><name>Quehry</name></author><category term="notes" /><summary type="html"><![CDATA[学习ROS]]></summary></entry><entry><title type="html">linux command</title><link href="http://localhost:4000/Linux-Command.html" rel="alternate" type="text/html" title="linux command" /><published>2022-10-24T00:00:00+08:00</published><updated>2022-10-24T00:00:00+08:00</updated><id>http://localhost:4000/Linux-Command</id><content type="html" xml:base="http://localhost:4000/Linux-Command.html"><![CDATA[<h1 id="0-简介">0. 简介</h1>
<p>该博客主要记录了linux常用命令，便于后续使用时查找命令，其余的一些博客:</p>
<ul>
  <li><a href="https://docs.rockylinux.org/books/admin_guide/03-commands" target="_blank">linux command documentation</a></li>
  <li><a href="https://blog.csdn.net/weixin_49851451/article/details/125821580?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-125821580.nonecase&amp;spm=1018.2226.3001.4187" target="_blank">中文博客</a></li>
  <li><a href="https://blog.csdn.net/xulong_08/article/details/81463054" target="_blank">linux相关</a></li>
</ul>

<h1 id="1-zip和unzip">1. zip和unzip</h1>
<ul>
  <li>压缩:
    <pre><code class="language-linux">zip [parameters] target.zip source
</code></pre>
    <p>常用参数有:</p>
    <ol>
      <li>-r 将指定的目录下所有的文件和子目录一并压缩，一般就选择当前目录的文件名即可</li>
      <li>-x 压缩时排除某一文件夹或文件，注意要加引号，因为是字符串，比如<code class="language-plaintext highlighter-rouge">-x "/root/autodl-tmp/txt2img_algorithms/datasets/*"</code></li>
    </ol>
  </li>
  <li>解压
    <pre><code class="language-linux">unzip [parameters] source.zip
</code></pre>
    <p>常用参数有:</p>
    <ol>
      <li>-l 查看zip文件中包含什么</li>
      <li>-t 检查压缩文件是否有问题</li>
      <li>-d 压缩到指定目录</li>
    </ol>
  </li>
</ul>]]></content><author><name>Quehry</name></author><category term="notes" /><summary type="html"><![CDATA[记录Linux常见命令]]></summary></entry></feed>