<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-07-27T16:52:35+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Quehry</title><subtitle>Artificial Intelligence trends and concepts made easy.</subtitle><author><name>Quehry</name></author><entry><title type="html">课程总结</title><link href="http://localhost:4000/%E5%A4%A7%E5%9B%9B%E4%B8%8B%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93.html" rel="alternate" type="text/html" title="课程总结" /><published>2022-04-29T00:00:00+08:00</published><updated>2022-04-29T00:00:00+08:00</updated><id>http://localhost:4000/%E5%A4%A7%E5%9B%9B%E4%B8%8B%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93</id><content type="html" xml:base="http://localhost:4000/%E5%A4%A7%E5%9B%9B%E4%B8%8B%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93.html"><![CDATA[<!-- TOC -->

<ul>
  <li><a href="#introduction-to-corporate-finance">Introduction to corporate finance</a>
    <ul>
      <li><a href="#课程简介">课程简介</a></li>
      <li><a href="#课程内容">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#工业科学">工业科学</a>
    <ul>
      <li><a href="#课程简介-1">课程简介</a></li>
      <li><a href="#课程内容-1">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#量子力学">量子力学</a>
    <ul>
      <li><a href="#课程简介-2">课程简介</a></li>
      <li><a href="#课程内容-2">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#统计物理">统计物理</a>
    <ul>
      <li><a href="#课程简介-3">课程简介</a></li>
      <li><a href="#课程内容-3">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#信号处理">信号处理</a>
    <ul>
      <li><a href="#课程简介-4">课程简介</a></li>
      <li><a href="#课程内容-4">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#结构力学2">结构力学2</a>
    <ul>
      <li><a href="#课程简介-5">课程简介</a></li>
      <li><a href="#课程内容-5">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#传热学heat-transfert">传热学(Heat Transfert)</a>
    <ul>
      <li><a href="#课程简介-6">课程简介</a></li>
      <li><a href="#课程内容-6">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#过程工程process-engin">过程工程(Process Engin)</a>
    <ul>
      <li><a href="#课程简介-7">课程简介</a></li>
      <li><a href="#课程内容-7">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#press">Press</a>
    <ul>
      <li><a href="#课程简介-8">课程简介</a></li>
      <li><a href="#课程内容-8">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#audiovisuel">Audiovisuel</a>
    <ul>
      <li><a href="#课程简介-9">课程简介</a></li>
      <li><a href="#课程内容-9">课程内容</a></li>
    </ul>
  </li>
  <li><a href="#毕业设计">毕业设计</a>
    <ul>
      <li><a href="#毕设内容">毕设内容</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h1 id="introduction-to-corporate-finance">Introduction to corporate finance</h1>

<h2 id="课程简介">课程简介</h2>
<ul>
  <li>授课老师: Danielle Levi-Feunteun</li>
  <li>授课形式: 线上</li>
  <li>授课材料: 电子讲义</li>
  <li>考核形式: 每节课都有作业需要提交，算平时分，最后还有一个开卷的考试</li>
</ul>

<h2 id="课程内容">课程内容</h2>
<p>围绕着财报进行简单的介绍</p>

<h1 id="工业科学">工业科学</h1>

<h2 id="课程简介-1">课程简介</h2>
<ul>
  <li>授课老师: 付小尧</li>
  <li>授课形式: 工业科学由两部分组成，实验和正课，都在线下完成</li>
  <li>授课材料: 实验课有电子讲义，正课有讲义和TD</li>
  <li>考核形式: 实验和正课都有线下考试，各占五十</li>
</ul>

<h2 id="课程内容-1">课程内容</h2>
<p>实验一共有8节课，都在二号楼上，实验围绕PID控制原理展开，我影响比较深的有停车场栏杆控制、云台、装乒乓球、给羽毛球拍上弦</p>

<p>正课讲了一点二进制、逻辑电路</p>

<h1 id="量子力学">量子力学</h1>

<h2 id="课程简介-2">课程简介</h2>
<ul>
  <li>授课老师: 付小尧</li>
  <li>授课形式: 线下授课</li>
  <li>授课材料: 电子讲义和TD</li>
  <li>考核形式: 线上考试</li>
</ul>

<h2 id="课程内容-2">课程内容</h2>
<p>一共有8章，第一章介绍了光电效应、黑体，第二章介绍了薛定谔方程，后面的忘了</p>

<h1 id="统计物理">统计物理</h1>

<h2 id="课程简介-3">课程简介</h2>
<ul>
  <li>授课老师: Philippe Ribiere</li>
  <li>授课形式: 线下授课</li>
  <li>授课材料: 电子讲义和TD</li>
  <li>考核形式: 大作业(统计物理与人工智能的关系)</li>
</ul>

<h2 id="课程内容-3">课程内容</h2>
<p>一共四章，对量子力学里用到的统计知识进行了补充，统计物理根据对物质微观结构及微观粒子相互作用的认识，用概率统计的方法，对由大量粒子组成的宏观物体的物理性质及宏观规律作出微观解释的理论物理学分支。</p>

<h1 id="信号处理">信号处理</h1>

<h2 id="课程简介-4">课程简介</h2>
<ul>
  <li>授课老师: Antoine Roueff</li>
  <li>授课形式: 线上授课</li>
  <li>授课材料: 电子讲义、TD、TP(Matlab)</li>
  <li>考核形式: 线上考试</li>
</ul>

<h2 id="课程内容-4">课程内容</h2>
<p>没咋听</p>

<h1 id="结构力学2">结构力学2</h1>

<h2 id="课程简介-5">课程简介</h2>
<ul>
  <li>授课老师:  Olivier Bareille、黄行蓉</li>
  <li>授课形式: 线上授课</li>
  <li>授课材料: 电子讲义和TD</li>
  <li>考核形式: 大作业(做题)</li>
</ul>

<h2 id="课程内容-5">课程内容</h2>
<p>Olivier讲正课，黄老师讲TD，由于疫情原因，最终在家完成大作业</p>

<h1 id="传热学heat-transfert">传热学(Heat Transfert)</h1>

<h2 id="课程简介-6">课程简介</h2>
<ul>
  <li>授课老师: Nelson IBASETA、张堇</li>
  <li>授课形式: 线上授课</li>
  <li>授课材料: 电子讲义和TD</li>
  <li>考核形式: 线上考试</li>
</ul>

<h2 id="课程内容-6">课程内容</h2>
<p>Nelson讲正课，张老师讲TD，主要介绍了三种传热方式，分别是热传导，热对流和热辐射</p>

<h1 id="过程工程process-engin">过程工程(Process Engin)</h1>

<h2 id="课程简介-7">课程简介</h2>
<ul>
  <li>授课老师: Dominique Pareau、唐宏哲</li>
  <li>授课形式: 线上授课</li>
  <li>授课材料: 电子讲义和TD</li>
  <li>考核形式: 两个大作业，Dominique的是做题，唐老师的是设计一个蒸馏的方案</li>
</ul>

<h2 id="课程内容-7">课程内容</h2>
<p>Dominique主要讲了工业流程中各组分的物料守恒和能量守恒，唐宏哲主要讲了蒸馏</p>

<h1 id="press">Press</h1>

<h2 id="课程简介-8">课程简介</h2>
<ul>
  <li>授课老师: Vanessa</li>
  <li>授课形式: 线下和线上授课</li>
  <li>授课材料: 电子讲义</li>
  <li>考核形式: 大作业</li>
</ul>

<h2 id="课程内容-8">课程内容</h2>
<p>法语课</p>

<h1 id="audiovisuel">Audiovisuel</h1>

<h2 id="课程简介-9">课程简介</h2>
<ul>
  <li>授课老师: Fabien</li>
  <li>授课形式: 线下和线上授课</li>
  <li>授课材料: 电子讲义</li>
  <li>考核形式: 大作业</li>
</ul>

<h2 id="课程内容-9">课程内容</h2>
<p>法语课</p>

<h1 id="毕业设计">毕业设计</h1>

<h2 id="毕设内容">毕设内容</h2>
<p>大四最重要的一门课，我的毕设是设计了一个成绩预测模型，包含了风险学科预测和成绩区间预测，主要用了一些机器学习的算法和数据处理的知识，写论文花了一周时间，剩下的就是不停的答辩</p>]]></content><author><name>Quehry</name></author><category term="school" /><summary type="html"><![CDATA[记录大四下课程和课程笔记]]></summary></entry><entry><title type="html">毕设记录</title><link href="http://localhost:4000/Graduation-Project.html" rel="alternate" type="text/html" title="毕设记录" /><published>2022-04-20T00:00:00+08:00</published><updated>2022-04-20T00:00:00+08:00</updated><id>http://localhost:4000/Graduation-Project</id><content type="html" xml:base="http://localhost:4000/Graduation-Project.html"><![CDATA[<h1 id="1-数据预处理">1. 数据预处理</h1>

<h2 id="11-使用mysql对原始数据进行处理">1.1. 使用MySQL对原始数据进行处理</h2>
<ul>
  <li>目标: 生成每行为一个学生，第一列为学号，第二列到最后一列都是课程名称</li>
  <li>第一步: 创建表格
    <ol>
      <li>首先遇到的问题是创建列名时有MySQL关键字，所以对KCMC两端加上了反引号</li>
      <li>使用Group_concat时有内容长度限制，需要使用以下代码来暂时增大限制:
        <pre><code class="language-cmd">  SET GLOBAL group_concat_max_len = 4294967295;
  SET SESSION group_concat_max_len = 4294967295;
</code></pre>
      </li>
      <li>列名长度硬性要求: 不能超过64个字符，所以我采用了将英文翻译为中文的方法减少长度，有以下几门学科名称做过修改:
        <ul>
          <li>UPDATE grade_original SET KCMC = ‘网格生成方法及软件简介’ WHERE KCMC=’An Introduction to Mesh Generation Methods &amp; Software for Scientific Computing’</li>
          <li>UPDATE grade_original SET KCMC = ‘经典论文鉴赏:电磁学顶级论文精选’ WHERE KCMC=’Appreciation of Classical Papers: The Selected Top Papers in Electromagnetism’</li>
          <li>UPDATE grade_original SET KCMC = ‘动脉硬化的脆弱性评估:从体内成像到生物力学’ WHERE KCMC=’Atherosclerosis Vulnerability Assessment: From In Vivo Imaging To Biomechanics’</li>
          <li>UPDATE grade_original SET KCMC = ‘计算机建模和仿真基础:方法、技术和应用’ WHERE KCMC=’Basics of Computer-Based Modelling and Simulation: Methodologies, Technologies and Applications’</li>
          <li>UPDATE grade_original SET KCMC = ‘当代中国外交政策及其全球治理途径’ WHERE KCMC=’Contemporary Chinese Foreign Policy and Its Global Governance Approach’</li>
          <li>UPDATE grade_original SET KCMC = ‘灵活的中英文语言:成功的必要条件’ WHERE KCMC=’Elastic Language in Chinese and English: Essential for Successful’</li>
          <li>UPDATE grade_original SET KCMC = ‘自然界中的功能结构材料:从保护到传感’ WHERE KCMC=’Functional Structural Materials in Nature: From Protection to Sensing’</li>
          <li>UPDATE grade_original SET KCMC = ‘国际商法-在中国经商的法律环境’ WHERE KCMC=’International Business Law - The Legal Environment of Doing Business in China’</li>
          <li>UPDATE grade_original SET KCMC = ‘航空航天工程疲劳与损伤容限导论’ WHERE KCMC=’Introduction to Fatigue and Damage Tolerance in Aerospace Engineering’</li>
          <li>UPDATE grade_original SET KCMC = ‘模型检查定时系统导论:理论与实践’ WHERE KCMC=’Introduction to Model-Checking Timed Systems: Theory and Practice’</li>
          <li>UPDATE grade_original SET KCMC = ‘功能薄膜磁控溅射的研究现状与发展趋势’ WHERE KCMC=’Magnetron Sputtering of Functional Thin Films: Present Status and Trends’</li>
          <li>UPDATE grade_original SET KCMC = ‘材料表征热分析原理及应用’ WHERE KCMC=’Principles and Applications of Thermal Analysis for Materials Characterization’</li>
          <li>UPDATE grade_original SET KCMC = ‘从英语学习到口译翻译能力的发展:原则与策略’ WHERE KCMC=’Progression from English Study to Interpreting and Translation Competence: Principles and Strategies’</li>
          <li>一共十三门课名有做修改</li>
        </ul>
      </li>
      <li>MySQL对列数有硬性要求: 其中InnoDB引擎要求不超过1024，其余引擎不超过4096，但是我的列数一共有1425，所以我改用了MyISAM引擎</li>
      <li>MySQL命令行代码:</li>
    </ol>

    <pre><code class="language-cmd">  SELECT
  CONCAT(
      'CREATE TABLE grade_student (', GROUP_CONCAT(DISTINCT CONCAT('\`', KCMC, '\`', ' FLOAT', CHAR(10))
      SEPARATOR ','),
      ')', 'ENGINE=MyISAM DEFAULT CHARSET=gbk;')
  FROM
      grade_original

  INTO @sql;

  PREPARE stmt_name FROM @sql;
  EXECUTE stmt_name;
</code></pre>
  </li>
  <li>目前完成: 列名创建出来，XH一行填满，但是成绩没有填</li>
  <li>先放着，后面有时间再试试，先用python</li>
</ul>

<h2 id="12-利用python的pandas库对数据进行预处理">1.2 利用Python的pandas库对数据进行预处理</h2>
<ul>
  <li>首先实现了从原始的data_original转变成data_student_0: 里面数据格式如下: 每行代表一名学生，第一列为学号，第二列至最后一列列名是学科名称，数值是成绩</li>
  <li>然后实现了去除大家都有的学科，把参加课程设为1，未参加课程设为0，生成data_student_1.csv文件，用来进行聚类</li>
  <li>聚类算法: kmeans，选了4个簇，生成的结果总体上来说非常不错，但是我希望计算一下性能度量(<strong>待办</strong>)
    <ul>
      <li>更细化来说，首先我可以手动针对学生选课来确定学院，最好是通过大三或者大四的课程来确定，以免有人中途转系等等</li>
    </ul>
  </li>
  <li>聚类后生成了data_student_2.csv，里面新增了XY列，0代表国通，1代表计院，2代表航院，3代表中法</li>
  <li>然后需要针对学年进行进一步细分: 目前聚类后的结果分布如下</li>
</ul>

<center><img src="../assets/img/posts/20220420/2.jpg" /></center>]]></content><author><name>Quehry</name></author><category term="record" /><summary type="html"><![CDATA[毕设流程记录]]></summary></entry><entry><title type="html">算法基础</title><link href="http://localhost:4000/Algorithms.html" rel="alternate" type="text/html" title="算法基础" /><published>2022-04-13T00:00:00+08:00</published><updated>2022-04-13T00:00:00+08:00</updated><id>http://localhost:4000/Algorithms</id><content type="html" xml:base="http://localhost:4000/Algorithms.html"><![CDATA[<h1 id="目录">目录</h1>

<h1 id="1-第一周-枚举">1. 第一周 枚举</h1>

<h2 id="11-完美立方">1.1. 完美立方</h2>
<ul>
  <li>枚举: 基于逐个尝试答案的一种问题求解策略</li>
  <li>例如: 求小于N的最小素数</li>
  <li>完美立方:</li>
</ul>

<center><img src="../assets/img/posts/20220413/1.jpg" /></center>

<ul>
  <li>解题思路:</li>
</ul>

<center><img src="../assets/img/posts/20220413/2.jpg" /></center>

<h2 id="12-生理周期">1.2. 生理周期</h2>
<ul>
  <li>题干:</li>
</ul>

<center><img src="../assets/img/posts/20220413/3.jpg" /></center>

<center><img src="../assets/img/posts/20220413/4.jpg" /></center>

<ul>
  <li>解题思路:</li>
</ul>

<center><img src="../assets/img/posts/20220413/5.jpg" /></center>

<h2 id="13-称硬币">1.3. 称硬币</h2>
<ul>
  <li>题干:</li>
</ul>

<center><img src="../assets/img/posts/20220413/6.jpg" /></center>

<center><img src="../assets/img/posts/20220413/7.jpg" /></center>

<ul>
  <li>解题思路</li>
</ul>

<center><img src="../assets/img/posts/20220413/8.jpg" /></center>

<h2 id="14-熄灯问题">1.4. 熄灯问题</h2>
<ul>
  <li>题干:</li>
</ul>

<center><img src="../assets/img/posts/20220413/9.jpg" /></center>

<center><img src="../assets/img/posts/20220413/10.jpg" /></center>

<center><img src="../assets/img/posts/20220413/11.jpg" /></center>

<ul>
  <li>解题思路:</li>
</ul>

<center><img src="../assets/img/posts/20220413/12.jpg" /></center>

<center><img src="../assets/img/posts/20220413/13.jpg" /></center>

<p>局部的思想，化繁为简</p>

<ul>
  <li>可以用0-31的十进制数来表示第一列的数据，因为其二进制数刚好对应开关的状态</li>
</ul>

<h1 id="2-第二周-递归">2. 第二周 递归</h1>
<h2 id="21-求阶乘">2.1. 求阶乘</h2>
<ul>
  <li>递归的基本概念: 一个函数调用其自身就是递归</li>
</ul>

<center><img src="../assets/img/posts/20220413/14.jpg" /></center>

<ul>
  <li>递归和普通函数调用一样是通过栈实现</li>
</ul>

<center><img src="../assets/img/posts/20220413/15.jpg" /></center>

<ul>
  <li>递归的作用
    <ul>
      <li>替代多重循环</li>
      <li>解决本来就是递归形式定义的问题</li>
      <li>将问题分解为规模更小的问题进行求解: 比如n！变成n * (n-1)</li>
    </ul>
  </li>
</ul>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[郭炜老师算法网课记录]]></summary></entry><entry><title type="html">C语言程序设计</title><link href="http://localhost:4000/C-Language.html" rel="alternate" type="text/html" title="C语言程序设计" /><published>2022-02-25T00:00:00+08:00</published><updated>2022-02-25T00:00:00+08:00</updated><id>http://localhost:4000/C-Language</id><content type="html" xml:base="http://localhost:4000/C-Language.html"><![CDATA[<h1 id="目录">目录</h1>

<!-- TOC -->

<ul>
  <li><a href="#目录">目录</a></li>
  <li><a href="#1-第一章-c语言快速入门">1. 第一章 C语言快速入门</a>
    <ul>
      <li><a href="#11-信息在计算机中的表示">1.1. 信息在计算机中的表示</a>
        <ul>
          <li><a href="#111-用0和1表示各种信息">1.1.1. 用0和1表示各种信息</a></li>
          <li><a href="#112-十进制到二进制的互相转换">1.1.2. 十进制到二进制的互相转换</a></li>
          <li><a href="#113-k进制小数">1.1.3. K进制小数</a></li>
          <li><a href="#114-十六进制数到二进制数的相互转换">1.1.4. 十六进制数到二进制数的相互转换</a></li>
        </ul>
      </li>
      <li><a href="#12-c语言快速入门">1.2. C语言快速入门</a></li>
      <li><a href="#13-变量和数据类型初探">1.3. 变量和数据类型初探</a>
        <ul>
          <li><a href="#131-什么是变量">1.3.1. 什么是变量</a></li>
          <li><a href="#132-变量的命名规则">1.3.2. 变量的命名规则</a></li>
          <li><a href="#133-c的基本数据类型">1.3.3. C++的基本数据类型</a></li>
        </ul>
      </li>
      <li><a href="#14-变量和数据类型进阶">1.4. 变量和数据类型进阶</a>
        <ul>
          <li><a href="#141-数据类型的自动转换">1.4.1. 数据类型的自动转换</a></li>
        </ul>
      </li>
      <li><a href="#15-常量">1.5. 常量</a>
        <ul>
          <li><a href="#151-整型常量">1.5.1. 整型常量</a></li>
          <li><a href="#152-字符型常量">1.5.2. 字符型常量</a></li>
          <li><a href="#153-符号常量">1.5.3. 符号常量</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#2-第二章-输入输出和基本运算">2. 第二章 输入输出和基本运算</a>
    <ul>
      <li><a href="#21-输入输出进阶">2.1. 输入输出进阶</a>
        <ul>
          <li><a href="#211-输入输出控制符">2.1.1. 输入输出控制符</a></li>
          <li><a href="#212-用scanf读入不同类型的变量">2.1.2. 用scanf读入不同类型的变量</a></li>
          <li><a href="#213-控制printf输出整数的宽度">2.1.3. 控制printf输出整数的宽度</a></li>
          <li><a href="#214-用c的cout进行输出">2.1.4. 用C++的cout进行输出</a></li>
          <li><a href="#215-用c的cin进行输入">2.1.5. 用C++的cin进行输入</a></li>
        </ul>
      </li>
      <li><a href="#22-算术运算符和算术表达式">2.2. 算术运算符和算术表达式</a>
        <ul>
          <li><a href="#221-赋值运算符">2.2.1. 赋值运算符</a></li>
          <li><a href="#222-算术运算符">2.2.2. 算术运算符</a></li>
          <li><a href="#223-模运算">2.2.3. 模运算</a></li>
          <li><a href="#224-自增运算符-">2.2.4. 自增运算符 ++</a></li>
        </ul>
      </li>
      <li><a href="#23-关系运算符和逻辑表达式">2.3. 关系运算符和逻辑表达式</a>
        <ul>
          <li><a href="#231-关系运算符">2.3.1. 关系运算符</a></li>
          <li><a href="#232-逻辑运算符和逻辑表达式">2.3.2. 逻辑运算符和逻辑表达式</a></li>
        </ul>
      </li>
      <li><a href="#24-其他运算符及运算符优先级">2.4. 其他运算符及运算符优先级</a>
        <ul>
          <li><a href="#241-强制类型转换运算符">2.4.1. 强制类型转换运算符</a></li>
          <li><a href="#242-部分运算符的优先级">2.4.2. 部分运算符的优先级</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#3-分支语句和循环语句">3. 分支语句和循环语句</a>
    <ul>
      <li><a href="#31-if语句">3.1. if语句</a>
        <ul>
          <li><a href="#311-条件分支结构">3.1.1. 条件分支结构</a></li>
          <li><a href="#312-if语句">3.1.2. if语句</a></li>
        </ul>
      </li>
      <li><a href="#32-switch语句">3.2. switch语句</a></li>
      <li><a href="#33-for循环">3.3. for循环</a>
        <ul>
          <li><a href="#331-for循环语句">3.3.1. for循环语句</a></li>
        </ul>
      </li>
      <li><a href="#34-while循环和do-while循环">3.4. while循环和do while循环</a>
        <ul>
          <li><a href="#341-while循环">3.4.1. while循环</a></li>
          <li><a href="#342-do-while循环">3.4.2. do while循环</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#4-第四章-循环综合应用">4. 第四章 循环综合应用</a>
    <ul>
      <li><a href="#41-break语句和continue语句">4.1. break语句和continue语句</a>
        <ul>
          <li><a href="#411-break语句">4.1.1. break语句</a></li>
          <li><a href="#412-continue语句">4.1.2. continue语句</a></li>
        </ul>
      </li>
      <li><a href="#42-oj输入数据的处理">4.2. OJ输入数据的处理</a>
        <ul>
          <li><a href="#421-scanf表达式的值">4.2.1. scanf表达式的值</a></li>
          <li><a href="#422-用freopen重定向输入">4.2.2. 用freopen重定向输入</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#5-第五章-数组">5. 第五章 数组</a>
    <ul>
      <li><a href="#51-数组">5.1. 数组</a></li>
      <li><a href="#52-筛法求素数">5.2. 筛法求素数</a></li>
      <li><a href="#53-数组初始化">5.3. 数组初始化</a></li>
      <li><a href="#54-数组越界">5.4. 数组越界</a></li>
      <li><a href="#55-二维数组">5.5. 二维数组</a></li>
    </ul>
  </li>
  <li><a href="#6-第六章-函数和位运算">6. 第六章 函数和位运算</a>
    <ul>
      <li><a href="#61-函数">6.1. 函数</a></li>
      <li><a href="#62-递归初步">6.2. 递归初步</a></li>
      <li><a href="#63-库函数和头文件">6.3. 库函数和头文件</a></li>
      <li><a href="#64-位运算">6.4. 位运算</a>
        <ul>
          <li><a href="#641-按位与">6.4.1. 按位与&amp;</a></li>
          <li><a href="#642-按位或\">6.4.2. 按位或|</a></li>
          <li><a href="#643-按位异或^">6.4.3. 按位异或^</a></li>
          <li><a href="#644-按位非">6.4.4. 按位非~</a></li>
          <li><a href="#645-左移运算符">6.4.5. 左移运算符«</a></li>
          <li><a href="#646-右移运算符">6.4.6. 右移运算符»</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#7-第七章-字符串">7. 第七章 字符串</a>
    <ul>
      <li><a href="#71-字符串的形式和存储">7.1. 字符串的形式和存储</a></li>
      <li><a href="#72-输入字符串">7.2. 输入字符串</a></li>
      <li><a href="#73-字符串库函数">7.3. 字符串库函数</a></li>
    </ul>
  </li>
  <li><a href="#8-第八章-指针一">8. 第八章 指针(一)</a>
    <ul>
      <li><a href="#81-指针的基本概念和用法">8.1. 指针的基本概念和用法</a></li>
      <li><a href="#82-指针的意义和相互赋值">8.2. 指针的意义和相互赋值</a></li>
      <li><a href="#83-指针的运算">8.3. 指针的运算</a></li>
      <li><a href="#84-指针作为函数参数">8.4. 指针作为函数参数</a></li>
      <li><a href="#85-指针和数组">8.5. 指针和数组</a></li>
    </ul>
  </li>
  <li><a href="#9-第九章-指针二">9. 第九章 指针(二)</a>
    <ul>
      <li><a href="#91-指针和二维数组指向指针的指针">9.1. 指针和二维数组、指向指针的指针</a></li>
      <li><a href="#92-指针和字符串">9.2. 指针和字符串</a></li>
      <li><a href="#93-字符串库函数">9.3. 字符串库函数</a></li>
      <li><a href="#94-void指针和内存操作函数">9.4. void指针和内存操作函数</a></li>
      <li><a href="#95-函数指针">9.5. 函数指针</a></li>
    </ul>
  </li>
  <li><a href="#10-第十章-程序结构和简单算法">10. 第十章 程序结构和简单算法</a>
    <ul>
      <li><a href="#101-结构">10.1. 结构</a></li>
      <li><a href="#102-全局变量局部变量静态变量">10.2. 全局变量、局部变量、静态变量</a></li>
      <li><a href="#103-变量的作用域和生存周期">10.3. 变量的作用域和生存周期</a></li>
      <li><a href="#104-选择排序和插入排序">10.4. 选择排序和插入排序</a>
        <ul>
          <li><a href="#1041-选择排序">10.4.1. 选择排序</a></li>
          <li><a href="#1042-插入排序">10.4.2. 插入排序</a></li>
        </ul>
      </li>
      <li><a href="#105-冒泡排序">10.5. 冒泡排序</a></li>
      <li><a href="#106-程序或算法的时间复杂度">10.6. 程序或算法的时间复杂度</a></li>
    </ul>
  </li>
  <li><a href="#11-第十一章-文件读写">11. 第十一章 文件读写</a>
    <ul>
      <li><a href="#111-文件读写概述">11.1. 文件读写概述</a>
        <ul>
          <li><a href="#1111-打开文件的函数">11.1.1. 打开文件的函数</a></li>
        </ul>
      </li>
      <li><a href="#112-文本文件读写">11.2. 文本文件读写</a>
        <ul>
          <li><a href="#1121-文本文件读写">11.2.1. 文本文件读写</a></li>
          <li><a href="#1121-文本文件读写另一种函数">11.2.1. 文本文件读写(另一种函数)</a></li>
        </ul>
      </li>
      <li><a href="#113-二进制文件读写概述">11.3. 二进制文件读写概述</a>
        <ul>
          <li><a href="#1131-文件的读写指针">11.3.1. 文件的读写指针</a></li>
          <li><a href="#1132-二进制文件读写">11.3.2. 二进制文件读写</a></li>
        </ul>
      </li>
      <li><a href="#114-创建和读取二进制文件">11.4. 创建和读取二进制文件</a></li>
      <li><a href="#115-修改二进制文件">11.5. 修改二进制文件</a></li>
      <li><a href="#116-文件拷贝程序">11.6. 文件拷贝程序</a></li>
    </ul>
  </li>
  <li><a href="#12-c的stl">12. C++的STL</a>
    <ul>
      <li><a href="#121-stl排序算法sort">12.1. STL排序算法sort</a></li>
      <li><a href="#122-stl二分查找算法">12.2. STL二分查找算法</a>
        <ul>
          <li><a href="#1221-用binary_search进行二分查找">12.2.1. 用binary_search进行二分查找</a></li>
          <li><a href="#1222-用lower_bound二分查找下界">12.2.2. 用lower_bound二分查找下界</a></li>
          <li><a href="#1223-用upper_bound二分查找上界">12.2.3. 用upper_bound二分查找上界</a></li>
        </ul>
      </li>
      <li><a href="#123-multiset">12.3. multiset</a></li>
      <li><a href="#124-set">12.4. set</a></li>
      <li><a href="#125-multimap">12.5. multimap</a></li>
      <li><a href="#126-map">12.6. map</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h1 id="1-第一章-c语言快速入门">1. 第一章 C语言快速入门</h1>

<h2 id="11-信息在计算机中的表示">1.1. 信息在计算机中的表示</h2>

<h3 id="111-用0和1表示各种信息">1.1.1. 用0和1表示各种信息</h3>
<ul>
  <li>计算机中的所有信息都是用0、1表示</li>
  <li>二进制数的一位，称为一个比特(bit), 简写b</li>
  <li>八个二进制位称为一个字节(byte), 简写B</li>
  <li>1KB, 1MB, 1GB, 1TB</li>
  <li>ASCII编码方案：用8个连续的0或1来表示一个字母数字和标点符号，一共有256种不同的组合</li>
</ul>

<h3 id="112-十进制到二进制的互相转换">1.1.2. 十进制到二进制的互相转换</h3>
<ul>
  <li>十进制数是数的十进制表示形式的简称</li>
  <li>短除法，每次除以进制，余数就是这个进制的最小位数</li>
</ul>

<h3 id="113-k进制小数">1.1.3. K进制小数</h3>
<ul>
  <li>K进制小数和整数的定义类似，只不过变成了K的负次方，比如小数后的第一位是$K^{-1}$</li>
</ul>

<h3 id="114-十六进制数到二进制数的相互转换">1.1.4. 十六进制数到二进制数的相互转换</h3>

<center><img src="assets/img/posts/20220225/2.jpg" /></center>

<h2 id="12-c语言快速入门">1.2. C语言快速入门</h2>

<ul>
  <li>空格也是一个字符</li>
  <li>C语言中输入输出: scanf、printf</li>
  <li>程序的注释:
    <ul>
      <li>多行注释: /* …  */</li>
      <li>单行注释: //</li>
    </ul>
  </li>
</ul>

<h2 id="13-变量和数据类型初探">1.3. 变量和数据类型初探</h2>

<h3 id="131-什么是变量">1.3.1. 什么是变量</h3>
<ul>
  <li>变量就是一个代号, 程序运行时系统会自动为变量分配内存空间，于是变量就代表了系统分配的那片内存空间</li>
  <li>变量有名字和类型两种属性，变量的类型决定了一个变量占用多少个字节</li>
  <li>变量的定义要在使用之前</li>
  <li>一个变量不能定义两次</li>
</ul>

<h3 id="132-变量的命名规则">1.3.2. 变量的命名规则</h3>
<ul>
  <li>变量不能以数字开头</li>
  <li>变量只能由大小写字母、数字和下划线组成</li>
  <li>变量名不能和C++系统预留的一些保留字重复</li>
</ul>

<h3 id="133-c的基本数据类型">1.3.3. C++的基本数据类型</h3>

<center><img src="assets/img/posts/20220225/3.jpg" /></center>

<ul>
  <li>float的取值范围是绝对值的范围</li>
  <li>整型、实数型、布尔型、字符型</li>
  <li>用sizeof()可以返回数据类型所占的字节数</li>
  <li>变量在定义的时候可以给它指定一个初始值</li>
</ul>

<h2 id="14-变量和数据类型进阶">1.4. 变量和数据类型进阶</h2>

<ul>
  <li>整型可以分为有符号整型和无符号整型</li>
  <li>有符号整数的表示方式
    <ul>
      <li>将最左边的位看作符号位，符号位为0表示非负数，其绝对值就是除去符号位以外的部分</li>
      <li>符号位为1，则表示是负数，其绝对值是除符号位意外的部分<strong>取反</strong>后加1</li>
      <li>将一个负整数转化为有符号整数是: 符号位取1，其余部分取该负整数的绝对值的二进制表示取反加1</li>
    </ul>
  </li>
</ul>

<h3 id="141-数据类型的自动转换">1.4.1. 数据类型的自动转换</h3>
<ul>
  <li>有些不同的数据类型之间是相容的，可以相互赋值</li>
  <li>int a = 11.34, 其实就是a=11</li>
  <li>整型数据也可以转换为字符型数据, 但只会留下最右边的一个字节</li>
  <li>看一个例子</li>
</ul>

<center><img src="assets/img/posts/20220225/4.jpg" /></center>

<h2 id="15-常量">1.5. 常量</h2>

<h3 id="151-整型常量">1.5.1. 整型常量</h3>
<ul>
  <li>十六进制整型常量以0x开头</li>
  <li>一个十六进制位正好对应四个二进制位</li>
  <li>0开头的是八进制数</li>
</ul>

<h3 id="152-字符型常量">1.5.2. 字符型常量</h3>
<ul>
  <li>字符型常量表示一个字符，用单引号括起来</li>
  <li>字符型常量和变量都占一个字节，内部存放的是ASCII编码</li>
  <li>小写字母的ASCII编码比大写字母大</li>
  <li>字符型常量中有一部分以‘\’开头, 被称为转义字符</li>
</ul>

<center><img src="assets/img/posts/20220225/5.jpg" /></center>

<ul>
  <li>字符串常量用双引号括起来，字符常量用单引号括起来</li>
</ul>

<h3 id="153-符号常量">1.5.3. 符号常量</h3>
<ul>
  <li>为了阅读和修改方便, 常用一个由字母和数字组成的符号来代表某个常量</li>
  <li>#define 常量名 常量值</li>
  <li>尽量多用符号常量，少用数值常量</li>
</ul>

<h1 id="2-第二章-输入输出和基本运算">2. 第二章 输入输出和基本运算</h1>

<h2 id="21-输入输出进阶">2.1. 输入输出进阶</h2>

<h3 id="211-输入输出控制符">2.1.1. 输入输出控制符</h3>
<ul>
  <li>在printf和scanf中可以使用以%开头的控制符，指明要输入和输出的数据类型</li>
  <li>常用的格式控制符</li>
</ul>

<center><img src="assets/img/posts/20220225/6.jpg" /></center>

<h3 id="212-用scanf读入不同类型的变量">2.1.2. 用scanf读入不同类型的变量</h3>
<ul>
  <li>输入字符时，不会跳过空格</li>
  <li>如果在输入中有scanf中出现的非控制字符，则这些字符会被跳过</li>
</ul>

<h3 id="213-控制printf输出整数的宽度">2.1.3. 控制printf输出整数的宽度</h3>
<ul>
  <li>比如用%nd和%0nd控制输出整型的长度</li>
  <li>用%.nf控制输出浮点数的精度</li>
</ul>

<h3 id="214-用c的cout进行输出">2.1.4. 用C++的cout进行输出</h3>
<ul>
  <li>cout « …</li>
  <li>endl可以进行换行</li>
</ul>

<h3 id="215-用c的cin进行输入">2.1.5. 用C++的cin进行输入</h3>
<ul>
  <li>cin » …</li>
  <li>cin、cout的速度比printf、scanf慢，输入输出数据量达到时候用后者</li>
  <li>一个程序不要同时使用cout和printf</li>
</ul>

<h2 id="22-算术运算符和算术表达式">2.2. 算术运算符和算术表达式</h2>

<h3 id="221-赋值运算符">2.2.1. 赋值运算符</h3>
<ul>
  <li>a += b 等同于 a = a + b</li>
</ul>

<h3 id="222-算术运算符">2.2.2. 算术运算符</h3>
<ul>
  <li>加减乘除</li>
  <li>%表示取余数</li>
  <li>两个整数进行加减乘都可能导致计算结果超出了结果类型所能表示的范围，这种情况就是溢出</li>
  <li>如果溢出，则直接丢弃溢出的部分</li>
  <li>有时计算的最终结果似乎不会溢出，但中间结果可能溢出，这也会导致程序出错</li>
  <li>解决溢出的办法是尽量使用高精度的数据类型</li>
  <li>除法的结果，类型和操作数中精度高的类型相同</li>
</ul>

<h3 id="223-模运算">2.2.3. 模运算</h3>
<ul>
  <li>求余数的运算符%也称为模运算符，两个操作数都是整数类型</li>
</ul>

<h3 id="224-自增运算符-">2.2.4. 自增运算符 ++</h3>
<ul>
  <li>自增运算符有前置用法和后置用法</li>
  <li>前置用法: ++ a 表示将a的值加1，表达式返回a+1后的值</li>
  <li>后置用法: a ++ 表示将a的值加1，表达式返回值为a加1前的值</li>
</ul>

<h2 id="23-关系运算符和逻辑表达式">2.3. 关系运算符和逻辑表达式</h2>

<h3 id="231-关系运算符">2.3.1. 关系运算符</h3>
<ul>
  <li>一共有六种关系运算符用于数值的比较</li>
  <li>比较的结果是bool类型</li>
</ul>

<center><img src="assets/img/posts/20220225/7.jpg" /></center>

<h3 id="232-逻辑运算符和逻辑表达式">2.3.2. 逻辑运算符和逻辑表达式</h3>
<ul>
  <li>逻辑运算符用于表达式的逻辑操作，有&amp;&amp;、||、!这三种，操作结果为true或false</li>
  <li>逻辑表达式是短路运算，即对逻辑表达式的计算在整个表达式的值已经能够断定的时候停止</li>
</ul>

<h2 id="24-其他运算符及运算符优先级">2.4. 其他运算符及运算符优先级</h2>

<h3 id="241-强制类型转换运算符">2.4.1. 强制类型转换运算符</h3>
<ul>
  <li>(int)、(char)这样的运算符就是强制将操作数转换为指定类型</li>
</ul>

<h3 id="242-部分运算符的优先级">2.4.2. 部分运算符的优先级</h3>

<center><img src="assets/img/posts/20220225/8.jpg" /></center>

<h1 id="3-分支语句和循环语句">3. 分支语句和循环语句</h1>

<h2 id="31-if语句">3.1. if语句</h2>

<h3 id="311-条件分支结构">3.1.1. 条件分支结构</h3>
<ul>
  <li>有时候我们希望满足一个条件执行一种语句，另一个条件执行另一种语句</li>
</ul>

<h3 id="312-if语句">3.1.2. if语句</h3>
<ul>
  <li>if语句可以没有else if，也可以没有else</li>
  <li>如果语句组只有一条语句，则不需要{}</li>
  <li>if语句可以嵌套</li>
  <li>else总是和离它最近的if配对，加一个花括号可以解决这个问题</li>
</ul>

<h2 id="32-switch语句">3.2. switch语句</h2>

<center><img src="assets/img/posts/20220225/9.jpg" /></center>

<ul>
  <li>可以没有default语句</li>
  <li>注意常量表达式不能带变量</li>
</ul>

<h2 id="33-for循环">3.3. for循环</h2>

<h3 id="331-for循环语句">3.3.1. for循环语句</h3>

<center><img src="assets/img/posts/20220225/10.jpg" /></center>

<ul>
  <li>注意是先执行语句组然后执行表达式3</li>
  <li>表达式1和表达式3都可以是用逗号连接的若干个表达式</li>
  <li>for循环可以嵌套，形成多重for循环</li>
  <li>for语句括号里面的表达式1、表达式2、表达式3可以任何一个都不写，但是分号必须保留</li>
</ul>

<h2 id="34-while循环和do-while循环">3.4. while循环和do while循环</h2>

<h3 id="341-while循环">3.4.1. while循环</h3>

<center><img src="assets/img/posts/20220225/11.jpg" /></center>

<h3 id="342-do-while循环">3.4.2. do while循环</h3>
<ul>
  <li>如果希望循环至少要执行一次，那么可以用do while循环</li>
</ul>

<center><img src="assets/img/posts/20220225/12.jpg" /></center>

<h1 id="4-第四章-循环综合应用">4. 第四章 循环综合应用</h1>

<h2 id="41-break语句和continue语句">4.1. break语句和continue语句</h2>

<h3 id="411-break语句">4.1.1. break语句</h3>
<ul>
  <li>break语句出现在循环体中，其作用是跳出循环</li>
  <li>在多重循环中，break语句只能跳出直接包含它的那一重循环</li>
</ul>

<h3 id="412-continue语句">4.1.2. continue语句</h3>
<ul>
  <li>continue可以出现在循环体中，其作用是立即结束本次循环，并回到循环开头判断是否要进行下一次循环</li>
</ul>

<h2 id="42-oj输入数据的处理">4.2. OJ输入数据的处理</h2>

<h3 id="421-scanf表达式的值">4.2.1. scanf表达式的值</h3>
<ul>
  <li>scanf()表达式其实是有返回值的，返回值为int类型，表示成功读入的变量个数</li>
  <li>scamf()值为EOF则说明输入数据已经结束</li>
  <li>ctrl+z表示输入结束</li>
  <li>这样就可以处理五结束标记的OJ题目输入</li>
</ul>

<h3 id="422-用freopen重定向输入">4.2.2. 用freopen重定向输入</h3>
<ul>
  <li>调试程序时，每次运行程序都要输入测试数据，太麻烦</li>
  <li>可以将测试数据存入文件，然后用freopen将输入由键盘重定向为文件，则运行程序时不再需要输入数据</li>
</ul>

<h1 id="5-第五章-数组">5. 第五章 数组</h1>

<h2 id="51-数组">5.1. 数组</h2>
<ul>
  <li>数组可以用来表示类型相同的元素的集合，集合的名字就是数组名</li>
  <li>数组可以用来表达类型相同的元素的集合，集合的名字就是数组名</li>
  <li>一维数组的定义方法如下：</li>
</ul>

<center><img src="assets/img/posts/20220225/13.jpg" /></center>

<ul>
  <li>元素个数必须是常量或常量表达式</li>
  <li>sizeof()可以访问数组所占字节</li>
  <li>数组名代表数组的地址</li>
  <li>数组一般不要定义在main里面，尤其是大数组</li>
</ul>

<h2 id="52-筛法求素数">5.2. 筛法求素数</h2>
<ul>
  <li>之前我们判断一个数n是不是素数，使用2到根号n之间的所有整数去除n，也就是穷举</li>
  <li>筛法：把2到n中所有的数都列出来，然后从2开始，先划掉n内所有2的倍数，然后每次从下一个剩下的数开始，划掉其n内的所有倍数，最后剩下的数就是素数</li>
  <li>筛法会稍微快一点，用空间换时间</li>
  <li>代码如下：</li>
</ul>

<center><img src="assets/img/posts/20220225/14.jpg" /></center>

<h2 id="53-数组初始化">5.3. 数组初始化</h2>
<ul>
  <li>在定义一个一维数组的同时，可以给数组中的元素赋初值</li>
</ul>

<center><img src="assets/img/posts/20220225/15.jpg" /></center>

<ul>
  <li>如果在定义数组的时候，如给全部元素赋值，则可以不给出数组元素的个数</li>
  <li>可以用数组取代复杂分支结构</li>
  <li>使用string须包含头文件<string></string></li>
</ul>

<h2 id="54-数组越界">5.4. 数组越界</h2>
<ul>
  <li>数组元素的下标，可以是任何整数，可以是负数，也可以大于数组的元素个数，不会导致编译错误</li>
  <li>但如果将越界写入了别的变量的内存空间，就很有可能出错</li>
</ul>

<h2 id="55-二维数组">5.5. 二维数组</h2>
<ul>
  <li>二维数组的定义：</li>
</ul>

<center><img src="assets/img/posts/20220225/16.jpg" /></center>

<ul>
  <li>二维数组的访问可以直接用下标访问</li>
  <li>二维数组的初始化也是用{}</li>
  <li>二维数组初始化时，如果对每行都进行初始化，则不用写行数或列数</li>
</ul>

<h1 id="6-第六章-函数和位运算">6. 第六章 函数和位运算</h1>

<h2 id="61-函数">6.1. 函数</h2>
<ul>
  <li>函数可以实现某一功能，当程序中需要使用该项功能时，只需要写一条语句，调用实现该功能的函数即可</li>
  <li>函数的定义:</li>
</ul>

<center><img src="assets/img/posts/20220225/17.jpg" /></center>

<ul>
  <li>函数的调用: 函数名(参数1, 参数2…)</li>
  <li>函数中至少含有一个return, 如果函数的类型为void, 则用return;</li>
  <li>定义函数的参数叫做形参, 调用函数时的参数叫做实参</li>
  <li>函数的定义一般在调用之前</li>
  <li>但是函数的调用语句前面有函数的声明即可，不一定要有定义</li>
</ul>

<center><img src="assets/img/posts/20220225/18.jpg" /></center>

<ul>
  <li>C/C++程序从main函数开始</li>
  <li>函数的形参是实参的一个拷贝，形参的改变一般不会影响到实参</li>
  <li>一维数组作为形参时不用写出元素的个数，这时候形参的改变会影响实参</li>
  <li>二维数组作为形参时，必须写明数组有多少列，不用写明有多少行</li>
</ul>

<h2 id="62-递归初步">6.2. 递归初步</h2>
<ul>
  <li>一个函数，自己调用自己，就是递归</li>
  <li>递归函数得有终止条件</li>
</ul>

<h2 id="63-库函数和头文件">6.3. 库函数和头文件</h2>
<ul>
  <li>头文件&lt;cmath&gt;中包含很多数学库函数的声明</li>
  <li>库函数的定义一般在.lib文件中</li>
  <li>库函数: C/C++标准规定, 编译器自带的函数</li>
  <li>头文件: C++编译器提供许多头文件, 比如: iostream、cmath、string</li>
  <li>头文件内部包含很多库函数的声明以及其他信息, 比如cin、cout的定义</li>
</ul>

<center><img src="assets/img/posts/20220225/19.jpg" /></center>

<center><img src="assets/img/posts/20220225/20.jpg" /></center>

<h2 id="64-位运算">6.4. 位运算</h2>
<ul>
  <li>位运算: 用于对整数类型变量中的某一位(bit)或者若干位进行操作</li>
  <li>C++提供了六种位运算符来进行位运算操作</li>
</ul>

<center><img src="assets/img/posts/20220225/21.jpg" /></center>

<h3 id="641-按位与">6.4.1. 按位与&amp;</h3>
<ul>
  <li>比如表达式(21 &amp; 18)的结果是16</li>
  <li>通常用来将某变量中的某些位清0且同时保留其他位不变</li>
</ul>

<h3 id="642-按位或">6.4.2. 按位或|</h3>
<ul>
  <li>比如“21|18”的结果是23</li>
  <li>按位或运算通常用来将某些变量中的某些位置1且保留其他位不变</li>
</ul>

<h3 id="643-按位异或">6.4.3. 按位异或^</h3>
<ul>
  <li>异或是逻辑运算, 如果两个值相同返回0, 如果两个值不同返回1</li>
  <li>异或运算通常用来将某变量中的某些位取反</li>
  <li>异或运算的特点:</li>
</ul>

<center><img src="assets/img/posts/20220225/22.jpg" /></center>

<h3 id="644-按位非">6.4.4. 按位非~</h3>
<ul>
  <li>按位非运算符~是单目运算符，其功能是将操作数中的二进制位0变成1，1变成0</li>
</ul>

<h3 id="645-左移运算符">6.4.5. 左移运算符«</h3>
<ul>
  <li>9 « 4 表示将9的二进制表示左移4位</li>
</ul>

<h3 id="646-右移运算符">6.4.6. 右移运算符»</h3>
<ul>
  <li>右移时，移出最右边的位就被丢弃</li>
  <li>对于有符号数，在右移时，符号位将一起移动，并且大多数C++编译器规定，如果圆符号位为1，则右移时高位就补充1，原符号位为0，则右移时高位就补充0</li>
</ul>

<h1 id="7-第七章-字符串">7. 第七章 字符串</h1>

<h2 id="71-字符串的形式和存储">7.1. 字符串的形式和存储</h2>
<ul>
  <li>字符串常量占据内存的字节数等于字符串中字符数目加1，多出来的是结尾字符’\0’</li>
  <li>空串”“也是合法的字符串常量</li>
  <li>包含’\0’字符的一维char数组，就是一个字符串，其中存放的字符串即为’\0’前面的字符组成</li>
  <li>可以给一维数组这么赋值: char title[] = “Prison Break”</li>
  <li>‘\0’可以视为字符数组结束标志</li>
</ul>

<h2 id="72-输入字符串">7.2. 输入字符串</h2>
<ul>
  <li>用scanf也可以将字符串读入字符数组</li>
  <li>scanf会自动添加结尾’\0’</li>
  <li>scanf读入到空格为止</li>
  <li>scanf(“%s”, line) 不用取地址符</li>
  <li>读入一行到字符串组: cin.getline(char buf[], int bufsize), 读入一行，自动添加’\0’, 回车换行符不会写入buf, 但是会从输入流中去掉</li>
  <li>也可以用gets(char buf[])来读入一行到字符数组，回车换行符不会写入buf，但是会从输入流中去掉，可能导致数组越界</li>
</ul>

<h2 id="73-字符串库函数">7.3. 字符串库函数</h2>
<ul>
  <li>使用字符串库函数需要 #include &lt;cstring&gt;</li>
  <li>形参为char []类型，则实参可以是char数组或字符串常量</li>
  <li>字符串拷贝 strcpy(char[] dest, char[] src) 拷贝src到dest</li>
  <li>字符串比较大小 int strcmp(char[] s1, char[] s2) 是根据字符的ASCII码值进行比较，大写字母比小写字母小</li>
  <li>求字符串长度 int strlen(char[] s)</li>
  <li>字符串拼接 strcat(char[] s1, char[] s2) 将s2拼接到s1后面</li>
  <li>字符串转成大写 strupr(char [])</li>
  <li>字符串转成小写 strlwr(char [])</li>
</ul>

<center><img src="assets/img/posts/20220225/23.jpg" /></center>

<h1 id="8-第八章-指针一">8. 第八章 指针(一)</h1>

<h2 id="81-指针的基本概念和用法">8.1. 指针的基本概念和用法</h2>
<ul>
  <li>指针也称作指针变量，大小为4个字节(或8个字节)的变量，其内容代表一个内存地址</li>
  <li>通过指针，能够对该指针指向的内存区域进行读写</li>
  <li>指针的定义: 类型名 * 指针变量名</li>
  <li>比如: int * p = (int *) 40000</li>
  <li>p指向地址40000，地址p就是地址40000</li>
  <li>* p就代表地址40000开始处的若干个字节的内容</li>
  <li>我们可以通过指针访问其指向的内存空间</li>
</ul>

<center><img src="assets/img/posts/20220225/24.jpg" /></center>

<ul>
  <li>指针定义总结</li>
</ul>

<center><img src="assets/img/posts/20220225/25.jpg" /></center>

<ul>
  <li>指针用法，一般是让指针指向一个变量的地址</li>
</ul>

<center><img src="assets/img/posts/20220225/26.jpg" /></center>

<h2 id="82-指针的意义和相互赋值">8.2. 指针的意义和相互赋值</h2>
<ul>
  <li>有了指针，就有了<strong>自由访问内存空间</strong>的手段</li>
  <li>不同类型的指针，如果不经过强制类型转换，不能直接互相赋值</li>
</ul>

<h2 id="83-指针的运算">8.3. 指针的运算</h2>
<ul>
  <li>两个同类型的指针变量，可以比较大小</li>
  <li>两个同类型的指针变量，可以相减</li>
</ul>

<center><img src="assets/img/posts/20220225/27.jpg" /></center>

<ul>
  <li>指针变量加减一个整数的结果是指针</li>
</ul>

<center><img src="assets/img/posts/20220225/28.jpg" /></center>

<ul>
  <li>指针变量可以自增自减</li>
  <li>指针可以用下标运算符[]进行运算</li>
</ul>

<center><img src="assets/img/posts/20220225/29.jpg" /></center>

<h2 id="84-指针作为函数参数">8.4. 指针作为函数参数</h2>
<ul>
  <li>地址0不能访问，指向地址0的指针就是空指针</li>
  <li>可以用NULL关键字对任何类型的指针进行赋值，NULL实际上就是整数0.值为NULL的指针就是空指针</li>
  <li>指针可以作为条件表达式使用，如果指针的值为NULL，则相当于为假，值不为NULL，就相当于为真</li>
</ul>

<h2 id="85-指针和数组">8.5. 指针和数组</h2>
<ul>
  <li>数组的名字是一个指针常量，指向数组的起始地址</li>
</ul>

<center><img src="assets/img/posts/20220225/30.jpg" /></center>

<ul>
  <li>作为函数形参时， T *p与 T p[] 等价</li>
</ul>

<h1 id="9-第九章-指针二">9. 第九章 指针(二)</h1>

<h2 id="91-指针和二维数组指向指针的指针">9.1. 指针和二维数组、指向指针的指针</h2>

<center><img src="assets/img/posts/20220225/31.jpg" /></center>

<ul>
  <li>二维数组的每一行都是一维数组，也就是指针</li>
  <li>指向指针的指针:</li>
</ul>

<center><img src="assets/img/posts/20220225/32.jpg" /></center>

<h2 id="92-指针和字符串">9.2. 指针和字符串</h2>
<ul>
  <li>字符串常量的类型就是char *</li>
  <li>字符数组名的类型也是char *</li>
</ul>

<h2 id="93-字符串库函数">9.3. 字符串库函数</h2>

<ul>
  <li>字符串操作库函数</li>
</ul>

<center><img src="assets/img/posts/20220225/33.jpg" /></center>

<center><img src="assets/img/posts/20220225/34.jpg" /></center>

<ul>
  <li>这些字符串操作库函数都需要include&lt;cstring&gt;</li>
</ul>

<h2 id="94-void指针和内存操作函数">9.4. void指针和内存操作函数</h2>
<ul>
  <li>void指针: void * p</li>
  <li>可以用任何类型的指针对void指针进行赋值或初始化</li>
  <li>对于void指针，*p没有定义，++p、–p，p += n、p+n、p-n均无定义</li>
  <li>内存操作库函数memset</li>
</ul>

<center><img src="assets/img/posts/20220225/35.jpg" /></center>

<ul>
  <li>内存操作库函数memcpy</li>
</ul>

<center><img src="assets/img/posts/20220225/36.jpg" /></center>

<h2 id="95-函数指针">9.5. 函数指针</h2>
<ul>
  <li>程序运行期间，每个函数都会占用一段连续的内存空间。而函数名就是该函数所占内存区域的起始地址(也称入口地址)</li>
  <li>我们可以将函数的入口地址赋给一个指针变量，使该指针变量指向该函数，然后通过指针变量就可以调用这个函数，这种指向函数的指针变量称为函数指针</li>
  <li>定义形式:</li>
</ul>

<center><img src="assets/img/posts/20220225/37.jpg" /></center>

<ul>
  <li>使用方法:</li>
</ul>

<center><img src="assets/img/posts/20220225/38.jpg" /></center>

<ul>
  <li>函数指针和qsort库函数</li>
</ul>

<center><img src="assets/img/posts/20220225/39.jpg" /></center>

<center><img src="assets/img/posts/20220225/40.jpg" /></center>

<ul>
  <li>pfcompare是比较函数</li>
</ul>

<h1 id="10-第十章-程序结构和简单算法">10. 第十章 程序结构和简单算法</h1>

<h2 id="101-结构">10.1. 结构</h2>
<ul>
  <li>在现实问题中，常常需要用一组不同类型的数据来描述一个事物</li>
  <li>C++允许程序员自己定义新的数据类型。因此针对“学生”这种事物，可以定义一种新名为Student的数据类型，一个student类型的变量就能描述一个学生的全部信息，同理，还可以定义数据类型worker来表示工人</li>
  <li>结构(struct): 用struct关键字来定义一个结构，也就定义了一个新的数据类型</li>
</ul>

<center><img src="assets/img/posts/20220225/41.jpg" /></center>

<ul>
  <li>student即成为自定义的类型的名字，可以用来定义变量</li>
  <li>两个同类型的结构变量，可以相互赋值，结构变量之间不能用比较运算符进行计算</li>
  <li>一般来说，一个结构变量所占的内存空间的大小就是结构中所有成员变量大小之和</li>
  <li>一个结构的成员变量可以是任何类型的，包括可以是另一个结构类型</li>
  <li>结构的成员变量可以是指向本结构类型的变量的指针</li>
</ul>

<center><img src="assets/img/posts/20220225/42.jpg" /></center>

<ul>
  <li>访问结构变量的成员变量: 一个结构变量的成员变量完全可以和一个普通变量一样来使用，也可以取得其地址</li>
  <li>结构变量名.成员变量名</li>
  <li>结构变量可以在定义时进行初始化:(使用花括号和逗号)</li>
</ul>

<center><img src="assets/img/posts/20220225/43.jpg" /></center>

<ul>
  <li>结构数组也可以定义，就是把结构体名字看作变量类型使用</li>
  <li>指向结构变量的指针，通过指针访问其指向的结构变量的成员变量</li>
</ul>

<center><img src="assets/img/posts/20220225/44.jpg" /></center>

<h2 id="102-全局变量局部变量静态变量">10.2. 全局变量、局部变量、静态变量</h2>
<ul>
  <li>定义在函数内部的变量叫<strong>局部变量</strong>(函数的形参也是局部变量)</li>
  <li>定义在所有函数的外面的变量叫做<strong>全局变量</strong></li>
  <li>全局变量在所有函数中均可以使用，局部变量只能在定义它的内部函数中使用</li>
  <li><strong>静态变量</strong>: 全局变量都是静态变量，局部变量定义时如果前面加了static关键字，则该变量也成为静态变量</li>
  <li>静态变量在整个程序运行期间都是固定不变的</li>
  <li>局部变量在函数每次调用时地址都可能不同</li>
  <li>如果未明确初始化，则静态变量会被自动初始化为全0，局部非静态变量的值则随机</li>
  <li>静态变量只初始化一次，也就是下次调用函数的时候不进行初始化</li>
</ul>

<h2 id="103-变量的作用域和生存周期">10.3. 变量的作用域和生存周期</h2>
<ul>
  <li>变量名、函数名、类型名统称为标识符</li>
  <li>一个标识符能够起作用的范围，叫做该标识符的作用域</li>
  <li>使用标识符的语句，必须出现在它们的声明或者定义之后</li>
  <li>在单文件的程序中，结构、函数和全局变量的作用域是其定义所在的整个文件</li>
  <li>函数的形参的作用域是整个函数</li>
  <li>局部变量的作用域，是从定义它的语句开始，到包含它的最内层的那一对大括号{}的右大括号为止</li>
  <li>for循环里定义的循环控制变量，其作用域是整个for循环</li>
  <li>同名标识符的作用域，可能一个被另一个包含，则在小的作用域里，作用域大的那个标识符被屏蔽，不起作用</li>
  <li>所谓变量的生存期，值的是在此期间，变量占有内存空间，其占有的内存空间只能归它使用，不会用来存放别的东西</li>
  <li>而变量的生存期终止，就意味着该变量不再占有内存空间，它原来占有的内存空间，随时可能被派作他用</li>
  <li>全局变量的生存期，从程序被装入内存开始，到整个程序结束</li>
  <li>静态局部变量的生存期，从定义它的语句第一次被执行开始，直到程序结束</li>
  <li>函数形参的生存期从函数执行开始，到函数返回时结束，非静态局部变量的生存期，从执行到定义它的语句开始，一旦程序执行了它的作用域之外，其生存期即告终止</li>
</ul>

<h2 id="104-选择排序和插入排序">10.4. 选择排序和插入排序</h2>

<h3 id="1041-选择排序">10.4.1. 选择排序</h3>
<ul>
  <li>排序问题: 编程接收键盘输入的若干个整数，排序后从小到大输出，先输入一个整数n，表明有n个整数需要排序，接下来再输入待排序的n个整数</li>
  <li>选择排序:</li>
</ul>

<center><img src="assets/img/posts/20220225/45.jpg" /></center>

<center><img src="assets/img/posts/20220225/46.jpg" /></center>

<ul>
  <li>选择最小的整数，与第i位的整数更换位置</li>
</ul>

<h3 id="1042-插入排序">10.4.2. 插入排序</h3>

<center><img src="assets/img/posts/20220225/47.jpg" /></center>

<center><img src="assets/img/posts/20220225/48.jpg" /></center>

<ul>
  <li>插入排序就是将数组分为有序和无序，每次让无序最左边的元素与有序分别比较，插入到合适的位置</li>
</ul>

<h2 id="105-冒泡排序">10.5. 冒泡排序</h2>

<center><img src="assets/img/posts/20220225/49.jpg" /></center>

<center><img src="assets/img/posts/20220225/50.jpg" /></center>

<ul>
  <li>冒泡排序同样是将数组分为有序和无序两组，无序在左边，有序在右边，每次将无序部分两两比较，较大的在右边</li>
  <li>上面三种简单排序算法，都要做$n^2$量级次数的比较，其中n是元素个数</li>
  <li>而比较好的排序算法，如快速排序，归并排序等，只需要做$n*log_2n$量级次数的比较</li>
</ul>

<h2 id="106-程序或算法的时间复杂度">10.6. 程序或算法的时间复杂度</h2>
<ul>
  <li>一个程序或算法的时间效率，也称为时间复杂度，有时简称复杂度</li>
  <li>复杂度常用大的字母O和小写字母n来表示，比如O(n), n代表问题的规模</li>
  <li>复杂度也有平均复杂度和最坏复杂度两种，两种可能一致，也可能不一致</li>
  <li>如果复杂度是多个n的函数之和，则只关心随n的增长增长得最快的那个函数</li>
</ul>

<center><img src="assets/img/posts/20220225/51.jpg" /></center>

<ul>
  <li>一些例子</li>
</ul>

<center><img src="assets/img/posts/20220225/52.jpg" /></center>

<h1 id="11-第十一章-文件读写">11. 第十一章 文件读写</h1>

<h2 id="111-文件读写概述">11.1. 文件读写概述</h2>
<ul>
  <li>二进制文件: 本质上所有文件都是0、1串，因此都是二进制文件。但是一般将内容不是文字，记事本打开看是乱码的文件，称为二进制文件</li>
  <li>文本文件: 内容是文字，用记事本打开能看到文字的文件</li>
  <li>文件读写相关函数在头文件cstdio中声明: #include &lt;cstdio&gt;</li>
  <li>fopen函数打开文件，返回FILE * 指针，指向和文件相关的一个FILE变量，FILE是一个struct</li>
  <li>文件读写结束后，一定要fclose关闭文件，否则可能导致数据没被保存，或者无法打开其他文件</li>
  <li>一些读写函数都需要FILE *指针进行</li>
</ul>

<center><img src="assets/img/posts/20220225/53.jpg" /></center>

<h3 id="1111-打开文件的函数">11.1.1. 打开文件的函数</h3>

<center><img src="assets/img/posts/20220225/54.jpg" /></center>

<ul>
  <li>打开文件的模式</li>
</ul>

<center><img src="assets/img/posts/20220225/55.jpg" /></center>

<ul>
  <li>二进制打开和文本打开的区别:</li>
</ul>

<center><img src="assets/img/posts/20220225/56.jpg" /></center>

<ul>
  <li>主要是二进制打开的话会有换行符的区别，最好还是用二进制打开</li>
</ul>

<center><img src="assets/img/posts/20220225/57.jpg" /></center>

<ul>
  <li>文件名的绝对路径和相对路径:</li>
</ul>

<center><img src="assets/img/posts/20220225/58.jpg" /></center>

<h2 id="112-文本文件读写">11.2. 文本文件读写</h2>

<h3 id="1121-文本文件读写">11.2.1. 文本文件读写</h3>

<center><img src="assets/img/posts/20220225/59.jpg" /></center>

<ul>
  <li>我们希望写一个文件读写程序:</li>
</ul>

<center><img src="assets/img/posts/20220225/60.jpg" /></center>

<h3 id="1121-文本文件读写另一种函数">11.2.1. 文本文件读写(另一种函数)</h3>
<ul>
  <li>fgets是读取一行</li>
</ul>

<center><img src="assets/img/posts/20220225/61.jpg" /></center>

<ul>
  <li>读取整个文本文件并输出</li>
</ul>

<center><img src="assets/img/posts/20220225/62.jpg" /></center>

<ul>
  <li>fputs是输出一行</li>
</ul>

<center><img src="assets/img/posts/20220225/63.jpg" /></center>

<h2 id="113-二进制文件读写概述">11.3. 二进制文件读写概述</h2>

<h3 id="1131-文件的读写指针">11.3.1. 文件的读写指针</h3>

<center><img src="assets/img/posts/20220225/64.jpg" /></center>

<ul>
  <li>这都是C语言读写的规则</li>
</ul>

<center><img src="assets/img/posts/20220225/65.jpg" /></center>

<ul>
  <li>fseek的作用是将读写指针定位到距离origin位置offset字节处</li>
</ul>

<h3 id="1132-二进制文件读写">11.3.2. 二进制文件读写</h3>
<ul>
  <li>用fread进行二进制读文件</li>
</ul>

<center><img src="assets/img/posts/20220225/66.jpg" /></center>

<ul>
  <li>用fgetc进行二进制读文件</li>
</ul>

<center><img src="assets/img/posts/20220225/67.jpg" /></center>

<ul>
  <li>
    <p>fgetc是用来读取一个字节</p>
  </li>
  <li>
    <p>用fwrite二进制写文件</p>
  </li>
</ul>

<center><img src="assets/img/posts/20220225/68.jpg" /></center>

<ul>
  <li>用fputc二进制写文件</li>
</ul>

<center><img src="assets/img/posts/20220225/69.jpg" /></center>

<h2 id="114-创建和读取二进制文件">11.4. 创建和读取二进制文件</h2>
<ul>
  <li>用二进制文件存学生信息比用文本方式存的好处: 可能节约空间、便于快速读取、改单个学生信息</li>
</ul>

<h2 id="115-修改二进制文件">11.5. 修改二进制文件</h2>
<ul>
  <li>用r+b打开文件既读又写时，如果做了读操作，则做写操作之前一定要用fssek重新定位文件读写指针</li>
</ul>

<h2 id="116-文件拷贝程序">11.6. 文件拷贝程序</h2>
<ul>
  <li>文件拷贝程序mycopy示例</li>
</ul>

<center><img src="assets/img/posts/20220225/70.jpg" /></center>

<center><img src="assets/img/posts/20220225/71.jpg" /></center>

<center><img src="assets/img/posts/20220225/72.jpg" /></center>

<h1 id="12-c的stl">12. C++的STL</h1>

<h2 id="121-stl排序算法sort">12.1. STL排序算法sort</h2>
<ul>
  <li>STL: standard template library 标准模板库</li>
  <li>包含一些常用的算法如排序查找，还有常用的数据结构如可变长数组、链表、字典等</li>
  <li>要使用其中的算法，需要#include &lt;algorithm&gt;</li>
  <li>用sort进行排序(用法一)</li>
</ul>

<center><img src="assets/img/posts/20220225/73.jpg" /></center>

<ul>
  <li>用sort进行排序(用法二)</li>
</ul>

<center><img src="assets/img/posts/20220225/74.jpg" /></center>

<ul>
  <li>用sort进行排序(用法三)，用自定义的排序规则对任何类型T的数组进行排序</li>
</ul>

<center><img src="assets/img/posts/20220225/75.jpg" /></center>

<ul>
  <li>几个自定义排序规则例子</li>
</ul>

<center><img src="assets/img/posts/20220225/76.jpg" /></center>

<center><img src="assets/img/posts/20220225/77.jpg" /></center>

<h2 id="122-stl二分查找算法">12.2. STL二分查找算法</h2>

<h3 id="1221-用binary_search进行二分查找">12.2.1. 用binary_search进行二分查找</h3>
<ul>
  <li>用法一:</li>
</ul>

<center><img src="assets/img/posts/20220225/78.jpg" /></center>

<ul>
  <li>用法二:</li>
</ul>

<center><img src="assets/img/posts/20220225/79.jpg" /></center>

<h3 id="1222-用lower_bound二分查找下界">12.2.2. 用lower_bound二分查找下界</h3>
<ul>
  <li>用法一:</li>
</ul>

<center><img src="assets/img/posts/20220225/80.jpg" /></center>

<ul>
  <li>用法二:</li>
</ul>

<center><img src="assets/img/posts/20220225/81.jpg" /></center>

<h3 id="1223-用upper_bound二分查找上界">12.2.3. 用upper_bound二分查找上界</h3>
<ul>
  <li>用法一:</li>
</ul>

<center><img src="assets/img/posts/20220225/82.jpg" /></center>

<ul>
  <li>用法二:</li>
</ul>

<center><img src="assets/img/posts/20220225/83.jpg" /></center>

<h2 id="123-multiset">12.3. multiset</h2>
<ul>
  <li>STL中的平衡二叉树数据结构</li>
  <li>有时需要在大量增加、删除数据的同时，还要进行大量数据的查找</li>
  <li>可以使用平衡二叉树数据结构存放数据，体现在STL中，就是以下四种排序容器: multiset、set、multimap、map</li>
  <li>multiset的用法:</li>
</ul>

<center><img src="assets/img/posts/20220225/84.jpg" /></center>

<ul>
  <li>multiset上的迭代器</li>
</ul>

<center><img src="assets/img/posts/20220225/85.jpg" /></center>

<center><img src="assets/img/posts/20220225/86.jpg" /></center>

<ul>
  <li>自定义排序规则的multiset用法:</li>
</ul>

<center><img src="assets/img/posts/20220225/87.jpg" /></center>

<center><img src="assets/img/posts/20220225/88.jpg" /></center>

<h2 id="124-set">12.4. set</h2>
<ul>
  <li>set和multiset的区别在于容器里面不能有重复元素</li>
  <li>set插入元素可能不成功</li>
  <li>pair模板的用法</li>
</ul>

<center><img src="assets/img/posts/20220225/89.jpg" /></center>

<ul>
  <li>set的例子</li>
</ul>

<center><img src="assets/img/posts/20220225/90.jpg" /></center>

<center><img src="assets/img/posts/20220225/91.jpg" /></center>

<h2 id="125-multimap">12.5. multimap</h2>
<ul>
  <li>multimap容器里面的元素，都是pair形式的</li>
</ul>

<center><img src="assets/img/posts/20220225/92.jpg" /></center>

<ul>
  <li>multimap的应用</li>
</ul>

<center><img src="assets/img/posts/20220225/93.jpg" /></center>

<ul>
  <li>代码实现细节</li>
</ul>

<center><img src="assets/img/posts/20220225/94.jpg" /></center>

<center><img src="assets/img/posts/20220225/95.jpg" /></center>

<center><img src="assets/img/posts/20220225/96.jpg" /></center>

<center><img src="assets/img/posts/20220225/97.jpg" /></center>

<h2 id="126-map">12.6. map</h2>
<ul>
  <li>map和multimap的区别: 不能有关键字重复的元素, 可以使用[], 下标为关键字, 返回值为first和关键字相同的元素的second</li>
  <li>插入元素可能失败</li>
</ul>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[网课记录]]></summary></entry><entry><title type="html">机器学习</title><link href="http://localhost:4000/Machine-Learning.html" rel="alternate" type="text/html" title="机器学习" /><published>2021-12-22T00:00:00+08:00</published><updated>2021-12-22T00:00:00+08:00</updated><id>http://localhost:4000/Machine-Learning</id><content type="html" xml:base="http://localhost:4000/Machine-Learning.html"><![CDATA[<h1 id="目录">目录</h1>

<!-- TOC -->

<ul>
  <li><a href="#目录">目录</a></li>
  <li><a href="#1-第1章-绪论">1. 第1章 绪论</a></li>
  <li><a href="#2-第2章-模型评估与选择">2. 第2章 模型评估与选择</a>
    <ul>
      <li><a href="#21-思维导图">2.1. 思维导图</a></li>
      <li><a href="#22-经验误差与过拟合">2.2. 经验误差与过拟合</a></li>
      <li><a href="#23-评估方法">2.3. 评估方法</a>
        <ul>
          <li><a href="#231-留出法">2.3.1. 留出法</a></li>
          <li><a href="#232-交叉验证法">2.3.2. 交叉验证法</a></li>
          <li><a href="#233-自助法">2.3.3. 自助法</a></li>
        </ul>
      </li>
      <li><a href="#24-性能度量">2.4. 性能度量</a>
        <ul>
          <li><a href="#241-错误率与精度">2.4.1. 错误率与精度</a></li>
          <li><a href="#242-查准率查全率与f1">2.4.2. 查准率、查全率与F1</a></li>
          <li><a href="#243-roc与auc">2.4.3. ROC与AUC</a></li>
          <li><a href="#244-代价敏感错误率与代价曲线">2.4.4. 代价敏感错误率与代价曲线</a></li>
        </ul>
      </li>
      <li><a href="#25-比较检验">2.5. 比较检验</a>
        <ul>
          <li><a href="#251-假设检验">2.5.1. 假设检验</a></li>
          <li><a href="#252-交叉验证t检验">2.5.2. 交叉验证t检验</a></li>
          <li><a href="#253-mcnemar检验">2.5.3. McNemar检验</a></li>
          <li><a href="#254-friedman检验与nemenyi后续检验">2.5.4. Friedman检验与Nemenyi后续检验</a></li>
        </ul>
      </li>
      <li><a href="#26-偏差与方差">2.6. 偏差与方差</a></li>
    </ul>
  </li>
  <li><a href="#3-第3章-线性模型">3. 第3章 线性模型</a>
    <ul>
      <li><a href="#31-思维导图">3.1. 思维导图</a></li>
      <li><a href="#32-基本形式">3.2. 基本形式</a></li>
      <li><a href="#33-线性回归">3.3. 线性回归</a></li>
      <li><a href="#34-对数几率回归">3.4. 对数几率回归</a></li>
      <li><a href="#35-线性判别分析">3.5. 线性判别分析</a></li>
      <li><a href="#36-多分类学习">3.6. 多分类学习</a></li>
      <li><a href="#37-类别不平衡问题">3.7. 类别不平衡问题</a></li>
    </ul>
  </li>
  <li><a href="#4-第4章-决策树">4. 第4章 决策树</a>
    <ul>
      <li><a href="#41-思维导图">4.1. 思维导图</a>
        <ul>
          <li><a href="#411-章节导图">4.1.1. 章节导图</a></li>
          <li><a href="#412-如何生成一棵决策树">4.1.2. 如何生成一棵决策树</a></li>
        </ul>
      </li>
      <li><a href="#42-基本流程">4.2. 基本流程</a></li>
      <li><a href="#43-划分选择">4.3. 划分选择</a>
        <ul>
          <li><a href="#431-信息增益">4.3.1. 信息增益</a></li>
          <li><a href="#432-增益率">4.3.2. 增益率</a></li>
          <li><a href="#433-基尼指数">4.3.3. 基尼指数</a></li>
        </ul>
      </li>
      <li><a href="#44-剪枝处理">4.4. 剪枝处理</a>
        <ul>
          <li><a href="#441-预剪枝">4.4.1. 预剪枝</a></li>
          <li><a href="#442-后剪枝">4.4.2. 后剪枝</a></li>
        </ul>
      </li>
      <li><a href="#45-连续与缺失值">4.5. 连续与缺失值</a>
        <ul>
          <li><a href="#451-连续值处理">4.5.1. 连续值处理</a></li>
          <li><a href="#452-缺失值处理">4.5.2. 缺失值处理</a></li>
        </ul>
      </li>
      <li><a href="#46-多变量决策树">4.6. 多变量决策树</a></li>
      <li><a href="#47-阅读材料">4.7. 阅读材料</a></li>
    </ul>
  </li>
  <li><a href="#5-第5章-神经网络">5. 第5章 神经网络</a>
    <ul>
      <li><a href="#50-思维导图">5.0. 思维导图</a></li>
      <li><a href="#51-神经元模型">5.1. 神经元模型</a></li>
      <li><a href="#52-感知机与多层网络">5.2. 感知机与多层网络</a></li>
      <li><a href="#53-误差逆传播算法">5.3. 误差逆传播算法</a></li>
      <li><a href="#54-全局最小与局部极小">5.4. 全局最小与局部极小</a></li>
      <li><a href="#55-其他常见神经网络">5.5. 其他常见神经网络</a>
        <ul>
          <li><a href="#551-rbf网络">5.5.1. RBF网络</a></li>
          <li><a href="#552-art网络">5.5.2. ART网络</a></li>
          <li><a href="#553-som网络">5.5.3. SOM网络</a></li>
          <li><a href="#554-级联相关网络">5.5.4. 级联相关网络</a></li>
          <li><a href="#555-elman网络">5.5.5. Elman网络</a></li>
          <li><a href="#556-boltzmann机">5.5.6. Boltzmann机</a></li>
        </ul>
      </li>
      <li><a href="#56-深度学习">5.6. 深度学习</a></li>
    </ul>
  </li>
  <li><a href="#6-第6章-支持向量机">6. 第6章 支持向量机</a>
    <ul>
      <li><a href="#60-思维导图">6.0. 思维导图</a></li>
      <li><a href="#61-间隔与支持向量">6.1. 间隔与支持向量</a></li>
      <li><a href="#62-对偶问题">6.2. 对偶问题</a></li>
      <li><a href="#63-核函数">6.3. 核函数</a></li>
      <li><a href="#64-软间隔与正则化">6.4. 软间隔与正则化</a></li>
      <li><a href="#65-支持向量回归">6.5. 支持向量回归</a></li>
      <li><a href="#66-核方法">6.6. 核方法</a></li>
    </ul>
  </li>
  <li><a href="#7-第7章-贝叶斯分类器">7. 第7章 贝叶斯分类器</a>
    <ul>
      <li><a href="#70-思维导图">7.0. 思维导图</a></li>
      <li><a href="#71-贝叶斯决策论">7.1. 贝叶斯决策论</a></li>
      <li><a href="#72-极大似然估计">7.2. 极大似然估计</a></li>
      <li><a href="#73-朴素贝叶斯分类器">7.3. 朴素贝叶斯分类器</a></li>
      <li><a href="#74-半朴素贝叶斯分类器">7.4. 半朴素贝叶斯分类器</a></li>
      <li><a href="#75-贝叶斯网">7.5. 贝叶斯网</a>
        <ul>
          <li><a href="#751-结构">7.5.1. 结构</a></li>
          <li><a href="#752-学习">7.5.2. 学习</a></li>
          <li><a href="#753-推断">7.5.3. 推断</a></li>
        </ul>
      </li>
      <li><a href="#76-em算法">7.6. EM算法</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h1 id="1-第1章-绪论">1. 第1章 绪论</h1>

<h1 id="2-第2章-模型评估与选择">2. 第2章 模型评估与选择</h1>

<h2 id="21-思维导图">2.1. 思维导图</h2>

<center><img src="../assets/img/posts/20211222/16.jpg" /></center>

<h2 id="22-经验误差与过拟合">2.2. 经验误差与过拟合</h2>
<p><strong>定义：</strong></p>
<ul>
  <li><strong>错误率(error rate)</strong>: 如果m个样本中有a个样本分类错误，则错误率E=a/m。</li>
  <li><strong>精度(accuracy)</strong>：精度=1-错误率。</li>
  <li><strong>训练误差</strong>：学习器在训练集上的误差称为训练误差或者经验误差。</li>
  <li><strong>泛化误差(generalization error)</strong>：学习器在新样本上的误差称为泛化误差。</li>
  <li><strong>过拟合(overfitting)</strong>：当学习器把训练样本学得太好了的时候，很可能把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这就会导致泛化性能下降。</li>
  <li><strong>欠拟合(underfitting)</strong>：对训练样本的一般性质尚未学好。</li>
</ul>

<h2 id="23-评估方法">2.3. 评估方法</h2>
<p>通常我们可通过实验测试来对学习器的泛化误差进行评估而做出选择，于是我们需要一个<strong>测试集(testing set)</strong>，然后以测试集上的测试误差作为泛化误差的近似。下面介绍一些常见的处理数据集D的方法(数据集D-&gt;训练集S+测试集T)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<h3 id="231-留出法">2.3.1. 留出法</h3>
<ul>
  <li>留出发(hold-out)直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T。在S上训练出模型后，用T来评估其测试误差。</li>
  <li>以采样的角度来看待数据集划分的过程，则保留类别比例的采样方式通常称为<strong>分层采样</strong>。比如1000个样本，50%的正例，50%负例，以7：3划分数据集，那么训练集包含350个正例和350个负例就是分层采样。</li>
  <li>在使用留出法评估结果时，一般要采用若干次随即划分、重复进行实验评估后取平均值作为留出法的评估结果。</li>
  <li>常用做法时将大约2/3~4/5的样本用于训练。</li>
</ul>

<h3 id="232-交叉验证法">2.3.2. 交叉验证法</h3>
<ul>
  <li><strong>交叉验证法(cross validation)</strong>先将数据集D划分为k个大小相似的互斥子集，每个子集D<sub>i</sub>都尽可能保持数据分布的一致性。然后每次用k-1个自己的并集作为训练集，余下的那个子集作为测试集，这样就可以获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。</li>
  <li>交叉验证也称为k折交叉验证。k的常见取值有10、5、20。</li>
  <li><strong>留一法</strong>就是k=m，其中数据集D有m个样本。</li>
</ul>

<h3 id="233-自助法">2.3.3. 自助法</h3>
<ul>
  <li><strong>自助法(bootstrapping)</strong>：给定包含m个样本的数据集D，每次随机从D中挑选一个样本，将其拷贝放入D’, 然后再将该样本放回最初的数据集D中，这个过程重复执行m次后，我们获得了包含m个样本的数据集D’。我们将D’作为训练集，D\D’作为测试集。</li>
  <li>不难发现大概有36.8%(m趋于无限大时)的样本在m次采样中始终不被采到。</li>
</ul>

<center>$\lim\limits_{m\rightarrow\infty}(1-\frac{1}{m})^m = \frac{1}{e} ≈ 0.368$</center>

<ul>
  <li>缺点：自助法产生的数据集改变了初始数据集的分布，这样会引入估计偏差。因此，在初始数据量足够时，留出法和交叉验证法更常用一些。</li>
</ul>

<h2 id="24-性能度量">2.4. 性能度量</h2>
<ul>
  <li>对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是<strong>性能度量(performance measure)</strong>。</li>
  <li>在预测任务中，给定D = {(x<sub>1</sub>,y<sub>1</sub>), (x<sub>2</sub>,y<sub>2</sub>)…(x<sub>m</sub>,y<sub>m</sub>)}, 其中y<sub>i</sub>是x<sub>i</sub>的真实标记。学习器f。</li>
  <li><strong>均方误差(mean squared error)</strong>：回归任务最常用的性能度量是均方误差：</li>
</ul>

<center>$E(f;D)=\frac{1}{m}\sum_1^m(f(x_i)-y_i)^2$</center>

<p>接下来我将介绍<strong>分类任务</strong>中常用的性能度量</p>

<h3 id="241-错误率与精度">2.4.1. 错误率与精度</h3>
<p>本章开头提到了错误率和精度，这是分类任务中最常用的两种性能度量，既适用于二分类任务，也适用于多分类任务。</p>

<h3 id="242-查准率查全率与f1">2.4.2. 查准率、查全率与F1</h3>

<ul>
  <li>针对二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例(true positive)、假正例(false positive)、真反例(true negative)、假反例(false negative)，分别记为TP、FP、TN、FN。</li>
  <li>混淆矩阵(confusion matrix)</li>
</ul>

<table>
  <tbody>
    <tr>
      <td>真\预</td>
      <td>正例</td>
      <td>反例</td>
    </tr>
    <tr>
      <td>正例</td>
      <td>TP</td>
      <td>FN</td>
    </tr>
    <tr>
      <td>反例</td>
      <td>FP</td>
      <td>TN</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>查准率(precision)，记为P，它表示选择的好瓜中有多少是真正的好瓜</li>
</ul>

<center>$P=\frac{TP}{TP+FP}$</center>

<ul>
  <li>查全率(recall)，记为R，它表示好瓜中有多少被选出来了</li>
</ul>

<center>$R=\frac{TP}{TP+FN}$</center>

<ul>
  <li>
    <p>一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。</p>
  </li>
  <li>
    <p>P-R曲线：在很多情形下，我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器认为最可能是正例的样本，排在最后的则是学习器认为最不可能是正例的样本，按此顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率、查准率，也得到了P-R图。</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/2.jpg" /></center>

<ul>
  <li>
    <p>如果一个学习器的P-R曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者。如果两者曲线相交，则可以通过比较平衡点(break-even-point)来比较，越大越好。</p>
  </li>
  <li>
    <p>F1度量：F1综合考虑了查准率和查全率，是他们的调和平均</p>
  </li>
</ul>

<center>$F1=\frac{2*P*R}{P+R}$</center>

<ul>
  <li>F1的一般形式：有时候我们希望赋予查准率和查重率不同的权重：</li>
</ul>

<center>$F_\beta=\frac{(1+\beta^2)*P*R}{\beta^2*P+R}$</center>

<p>其中β大于1表示查全率有更大影响</p>

<ul>
  <li>
    <p>有时候我们有多个二分类混淆矩阵，我们希望在这n个混淆矩阵上综合考虑查准率和查全率，那么我们有以下的度量</p>
  </li>
  <li>
    <p>宏查准率macro-P，宏查全率macro-R，宏F1：</p>
  </li>
</ul>

<center>$macro-P=\frac{1}{n}\sum_1^nP_i$</center>

<center>$macro-R=\frac{1}{n}\sum_1^nR_i$</center>

<ul>
  <li>微查准率micro-P，微查全率micro-P，微F1：</li>
</ul>

<p>对TP、FP、TN、FN进行平均</p>

<center>$micro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}$</center>

<center>$micro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}$</center>

<h3 id="243-roc与auc">2.4.3. ROC与AUC</h3>
<ul>
  <li>
    <p>很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为反类。</p>
  </li>
  <li>
    <p>ROC曲线是从排序角度来出发研究学习器的泛化性能的工具。ROC的全称是Receiver Operating Characteristic。</p>
  </li>
  <li>
    <p>与P-R曲线类似，我们根据学习器的预测结果进行排序，按此顺序逐个把样本作为正例进行预测，计算出真正例率TPR(true positive rate)与假正例率FPR(false positive rate)并以它们分别为横纵坐标画出ROC曲线</p>
  </li>
</ul>

<center>$TPR=\frac{TP}{TP+FN}$</center>

<center>$FPR=\frac{FP}{FP+TN}$</center>

<center><img src="../assets/img/posts/20211222/3.jpg" /></center>

<ul>
  <li>
    <p>同样的，如果一个学习器的ROC曲线被另一个学习器完全包住，则认为后者的性能优于前者。若两个曲线相交，则比较曲线下的面积AUC(area under curve)</p>
  </li>
  <li>
    <p>形式化的看，AUC考虑的是样本预测的排序质量，那么我们可以定义一个排序损失l<sub>rank</sub></p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/4.jpg" /></center>

<center>AUC=1-l<sub>rank</sub></center>

<h3 id="244-代价敏感错误率与代价曲线">2.4.4. 代价敏感错误率与代价曲线</h3>
<ul>
  <li>有时候将正例误判断为负例与将负例误判断为正例所带来的后果不一样，我们赋予它们非均等代价</li>
  <li>代价矩阵(cost matrix)</li>
</ul>

<table>
  <tbody>
    <tr>
      <td>真\预</td>
      <td>第0类</td>
      <td>第1类</td>
    </tr>
    <tr>
      <td>第0类</td>
      <td>0</td>
      <td>cost<sub>01</sub></td>
    </tr>
    <tr>
      <td>第1类</td>
      <td>cost<sub>10</sub></td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<p>其中cost<sub>ij</sub>表示将第i类样本预测为第j类样本的代价</p>

<ul>
  <li>代价敏感错误率</li>
</ul>

<center><img src="../assets/img/posts/20211222/5.jpg" /></center>

<ul>
  <li>在非均等代价下，ROC曲线不能直接反映出学习器的期望总代价，而代价曲线则可达到此目的，横轴是正例概率代价，纵轴是归一化代价</li>
</ul>

<center><img src="../assets/img/posts/20211222/6.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/7.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/8.jpg" /></center>

<h2 id="25-比较检验">2.5. 比较检验</h2>
<p>我们有了实验评估方法与性能度量，是否就可以对学习器的性能进行评估比较了呢？实际上，机器学习中性能比较这件事比大家想象的要复杂很多，我们需要考虑多种因素的影响，下面介绍一些对学习器性能进行比较的方法，本小节默认以错误率作为性能度量。(但我觉得似乎同一个数据集下就可以比较)</p>

<h3 id="251-假设检验">2.5.1. 假设检验</h3>
<ul>
  <li>假设检验中的假设是对学习器泛化错误率分布的某种猜想或者判断，例如“ε=ε<sub>0</sub>”这样的假设</li>
  <li>现实任务中我们并不知道学习器的泛化错误率，我们只能获知其测试错误率$\hat{\epsilon}$，但直观上，两者接近的可能性比较大，因此可根据测试错误率估推出泛化错误率的分布。</li>
  <li>泛化错误率为$\epsilon$的学习器被测得测试错误率为$\hat{\epsilon}$的概率：</li>
</ul>

<center><img src="../assets/img/posts/20211222/9.jpg" /></center>

<ul>
  <li>我们发现$\epsilon$符合二项分布</li>
</ul>

<center><img src="../assets/img/posts/20211222/10.jpg" /></center>

<ul>
  <li>
    <p>二项检验：我们可以使用<strong>二项检验(binomial test)</strong>来对“$\epsilon$&lt;0.3”这样的假设进行检验，即在$\alpha$显著度下，$1-\alpha$置信度下判断假设是否成立。</p>
  </li>
  <li>
    <p>t检验：我们也可以用t检验(t-test)来检验。</p>
  </li>
  <li>
    <p>上面介绍的都是针对单个学习器泛化性能的假设进行检验</p>
  </li>
</ul>

<h3 id="252-交叉验证t检验">2.5.2. 交叉验证t检验</h3>

<ul>
  <li>
    <p>对于两个学习器A和B，若我们使用k折交叉验证法得到的测试错误率分别是$\epsilon_1^A$, $\epsilon_2^A$…$\epsilon_k^A$和$\epsilon_1^B$, $\epsilon_2^B$…$\epsilon_k^B$。其中$\epsilon_i^A$和$\epsilon_i^B$是在相同第i折训练集上得到的结果。则可以用k折交叉验证“成对t检验”来进行比较检验。</p>
  </li>
  <li>
    <p>我们这里的基本思想是若两个学习器的性能相同，则它们使用相同的训练测试集得到的错误率应该相同，即$\epsilon_i^A=\epsilon_1^B$</p>
  </li>
  <li>
    <p>$\Delta_i$ = $\epsilon_i^A$ - $\epsilon_i^B$，然后对$\Delta$进行分析</p>
  </li>
</ul>

<h3 id="253-mcnemar检验">2.5.3. McNemar检验</h3>
<ul>
  <li>对于二分类问题，使用留出法不仅可以估计出学习器A和B的测试错误率，还可获得两学习器分类结果的差别，即两者都正确、都错误…的样本数</li>
</ul>

<center><img src="../assets/img/posts/20211222/11.jpg" /></center>

<ul>
  <li>若我们假设两学习器性能相同，则应有e<sub>01</sub>=e<sub>10</sub>，那么变量|e<sub>01</sub>-e<sub>10</sub>|应该服从正态分布/卡方分布，然后用McNemar检验</li>
</ul>

<h3 id="254-friedman检验与nemenyi后续检验">2.5.4. Friedman检验与Nemenyi后续检验</h3>

<ul>
  <li>
    <p>交叉验证t检验与McNemar检验都是在一个数据集上比较两个算法的性能，但是很多时候，我们会在一组数据集上比较多个算法。</p>
  </li>
  <li>
    <p>当有多个算法参与比较时，一种做法是在每个数据集上本别列出两两比较的结果。另一种方法则是基于算法排列的<strong>Friedman检验</strong></p>
  </li>
  <li>
    <p>假定我们用D<sub>1</sub>, D<sub>2</sub>, D<sub>3</sub>, D<sub>4</sub>四个数据集对算法A、B、C进行比较。首先，使用留出法或交叉验证法得到每个算法在每个数据集上的测试结果。然后在每个数据集上根据测试性能由好到坏排序，并赋予序值1,2,…若算法的测试性能相同，则平分序值。</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/12.jpg" /></center>

<ul>
  <li>
    <p>然后使用Fredman检验来判断这些算法是否性能都相同，若相同，那么它们的平均序值应当相同。r<sub>i</sub>表示第i个算法的平均序值，那么它的均值和方差应该满足…</p>
  </li>
  <li>
    <p>若“所有算法的性能相同”这个假设被拒绝，则说明算法的性能显著不同，这时需要后续检验(post-hoc test)来进一步区分各算法，常用的有Nemenyi后续检验</p>
  </li>
  <li>
    <p>Nemenyi检验计算出平均序值差别的临界值域</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/13.jpg" /></center>

<ul>
  <li>在表中找到k=3时q<sub>0.05</sub>=2.344，根据公式计算出临界值域CD=1.657，由表中的平均序值可知A与B算法的差距，以及算法B与C的差距均未超过临界值域，而算法A与C的差距超过临界值域，因此检验结果认为算法A与C的性能显著不同，而算法A与B以及算法B与C的性能没有显著差别。</li>
</ul>

<center><img src="../assets/img/posts/20211222/14.jpg" /></center>

<h2 id="26-偏差与方差">2.6. 偏差与方差</h2>

<ul>
  <li>
    <p>对学习算法除了通过实验估计其泛化性能，人们往往还希望了解它“为什么”具有这样的性能。偏差-方差分解是解释学习算法泛化性能的一种重要工具</p>
  </li>
  <li><strong>偏差</strong>度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力</li>
  <li><strong>方差</strong>度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响</li>
  <li>
    <p><strong>噪声</strong>则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度</p>
  </li>
  <li>一般来说，偏差与方差是有冲突的，也就是偏差大的方差小，偏差小的方差大</li>
</ul>

<center><img src="../assets/img/posts/20211222/15.jpg" /></center>

<h1 id="3-第3章-线性模型">3. 第3章 线性模型</h1>
<h2 id="31-思维导图">3.1. 思维导图</h2>

<center><img src="../assets/img/posts/20211222/42.jpg" /></center>

<h2 id="32-基本形式">3.2. 基本形式</h2>

<ul>
  <li>
    <p>给定由d个属性描述的示例$x=(x_1;x_2;…x_d)$，这是一个列向量，其中$x_i$是$x$在第i个属性上的取值。</p>
  </li>
  <li>
    <p><strong>线性模型(linear model)</strong>试图学得一个通过属性线性组合来进行预测的函数：</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/17.jpg" /></center>

<ul>
  <li>向量形式：</li>
</ul>

<center>$f(x)=\omega^Tx+b$</center>

<p>其中$\omega=(\omega_1;\omega_2…\omega_d)$</p>

<ul>
  <li>
    <p>当$\omega$和b学得后，模型就得以确定</p>
  </li>
  <li>
    <p>线性模型的优点：形式简单，易于建模，良好的可解释性(comprehensibility)</p>
  </li>
</ul>

<h2 id="33-线性回归">3.3. 线性回归</h2>
<ul>
  <li>
    <p>对离散属性的处理，若属性值存在“序”的关系，可通过连续化将其转化为连续值，比如高、矮变成1、0；2.若不存在序的关系，可转化为k维向量。</p>
  </li>
  <li>
    <p>均方误差是回归任务中最常用的性能度量，试图让均方误差最小化：</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/18.jpg" /></center>

<ul>
  <li>
    <p>均方误差有非常好的几何意义，它对应了欧氏距离。基于均方误差最小化来进行模型求解的方法称为<strong>最小二乘法(least square method)</strong>。在线性回归中，最小二乘法就是试图找到一条直线，使得样本到直线上的欧氏距离之和最小</p>
  </li>
  <li>
    <p>首先观察一个属性值的情况。求解$\omega$和$b$使得均方误差最小化的过程，称为线性回归的最小二乘“参数估计”，我们令均方误差分别对$\omega$和$b$求导令其为零，可以得到最优解的<strong>闭式解(closed-form)</strong>,即解析解</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/19.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/20.jpg" /></center>

<ul>
  <li>更一般的情况是d个属性，称其为“多元线性回归”，同样的步骤，只是$\omega$变成向量形式，自变量写成m*(d+1)的矩阵形式，m对应了m个样本，d+1对应了d个属性和偏置。同样的求导为0然后得出$\hat{\omega}$最优解的闭式解，其中$\hat{\omega}=(\omega;b)$。当$X^TX$为满秩矩阵<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>或正定矩阵时，有唯一的解：</li>
</ul>

<center><img src="../assets/img/posts/20211222/21.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/22.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/23.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/24.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/25.jpg" /></center>

<ul>
  <li>
    <p>然而现实任务中往往不是满秩矩阵，例如在许多任务中我们会遇到大量的变量其数目甚至超过样例数。那么我们会求出$\hat{\omega}=(\omega;b)$的多个解，它们都能使均方误差最小化。选择哪一个解作为输出与学习算法的归纳偏好决定，常见的做法是<strong>引入正则化(regularization)</strong></p>
  </li>
  <li>
    <p><strong>广义线性模型(generalized linear model)</strong>:</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/26.jpg" /></center>

<center>$g(y)=\omega^Tx+b$</center>

<p>其中g()单调可微，被称为联系函数。比如当g()=ln()时称为对数线性回归</p>

<h2 id="34-对数几率回归">3.4. 对数几率回归</h2>
<ul>
  <li>
    <p>上一节讨论的是线性回归进行回归学习。那么如果我们要做的是分类任务应该怎么改变？我们只需要找到一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。</p>
  </li>
  <li>
    <p>考虑二分类问题，那么我们可以用单位阶跃函数/Heaviside函数联系y与z，其中y是真是标记，z是预测值，$z=\omega^Tx+b$</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/27.jpg" /></center>

<ul>
  <li>
    <p>但是它有个问题就是不连续，所以我们希望找到能在一定程度上近似它的替代函数。</p>
  </li>
  <li>
    <p><strong>对数几率函数(logistic function)</strong>就是一个替代函数：</p>
  </li>
</ul>

<center>$y=\frac{1}{1+e^{-z}}$</center>

<center><img src="../assets/img/posts/20211222/28.jpg" /></center>

<ul>
  <li>那么这个式子可以转化为：可以把y视为样本x作为正例的可能性，则1-y是其反例的可能性，两者的比值称为几率(odds)：</li>
</ul>

<center><img src="../assets/img/posts/20211222/29.jpg" /></center>

<ul>
  <li>若将y视为类后验概率估计。则式子可以重写为：</li>
</ul>

<center><img src="../assets/img/posts/20211222/30.jpg" /></center>

<ul>
  <li>接下来我们可以通过<strong>极大似然法(maximum likelihood method)</strong>来估计$\omega$和$b$。给定数据集，对数似然函数<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>为：</li>
</ul>

<center><img src="../assets/img/posts/20211222/31.jpg" /></center>

<p>即每个样本属于其真实标记的概率越大越好。</p>

<ul>
  <li>推导过程：</li>
</ul>

<center><img src="../assets/img/posts/20211222/32.jpg" /></center>

<p>上面有个式子应该有问题，(3.26)应该是</p>

<center>$p(y_i|x_i;\omega,b) = p_1(\hat{x_i};\beta)^{y_i}p_0(\hat{x_i};\beta)^{1-y_i}$</center>

<p>因为$\beta$是高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法如<strong>梯度下降法(gradient descent method)</strong>和牛顿法都可以求得最优解</p>

<h2 id="35-线性判别分析">3.5. 线性判别分析</h2>

<ul>
  <li>
    <p>线性判别分析(Linear Discriminant Analysis，简称LDA)是一种经典的线性学习方法。</p>
  </li>
  <li>
    <p>LDA的思想非常朴素: 给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离;在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/33.jpg" /></center>

<ul>
  <li>令$X_i$、$\mu_i$、$\Sigma_i$分别表示第i类示例的集合、均值向量<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>、协方差矩阵<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>。</li>
</ul>

<ul>
  <li>欲使同类样例的投影点尽可能接近，可以让同类样例投影点的协方差尽可能小，即$\omega^T\Sigma_0\omega+\omega^T\Sigma_1\omega$尽可能小;而欲使异类样例的投影点尽可能远离，可以让类中心之间的距离尽可能大，即$||\omega^T\mu_0-\omega^T\mu_1||$尽可能大:</li>
</ul>

<center><img src="../assets/img/posts/20211222/34.jpg" /></center>

<ul>
  <li>剩余推导过程：</li>
</ul>

<center><img src="../assets/img/posts/20211222/35.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/36.jpg" /></center>

<ul>
  <li>值得一提的是，LDA可从贝时斯决策理论的角度来阐释，并可证明，当两类数据同先验、满足高斯分布且协方差相等时，LDA可达到最优分类。</li>
</ul>

<h2 id="36-多分类学习">3.6. 多分类学习</h2>
<ul>
  <li>
    <p>现实中常遇到多分类学习任务，有些二分类学习方法可直接推广到多分类，但在更多情形下，我们是基于一些基本策略，利用二分类学习器来解决多分类问题。</p>
  </li>
  <li>
    <p>不失一般性，考虑N个类别$C_1$、$C_2$…$C_N$，多分类学习的基本思路是”<strong>拆解法</strong>”，即将多分类任务拆为若干个二分类任务求解。具体来说，先对问题进行拆分，然后为拆出的每个二分类任务训练一个分类器;在测试时，对这些分类器的预测结果进行集成以获得最终的多分类结果。</p>
  </li>
  <li>
    <p>这里我们着重介绍如何拆分，最经典的拆分策略有三种：一对一(One vs One)、一对其余(One vs Rest)、多对多(Many vs Many)</p>
  </li>
  <li>
    <p>一对一：将这N个类别两两配对，从而产生 N(N-1)/2个二分类任务。在测试阶段，新样本将同时提交给所有分类器，于是我们将得到N(N-1)/2个分类结果，最终结果可通过投票产生:即把被预测得最
多的类别作为最终分类结果。</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/37.jpg" /></center>

<ul>
  <li>一对其余：OvR则是每次将一个类的样例作为正例，所有其他类的样例作为反例来训练N个分类器。在测试时若仅有一个分类器预测为正类，则对应的类别标记作为最终分类结果。若有多个分类器预测为正类，则通常考虑各分类器的预测置信度，选择置信度最大的类别标记作为分类结果。</li>
</ul>

<center><img src="../assets/img/posts/20211222/38.jpg" /></center>

<ul>
  <li>
    <p>多对多MvM是每次将若干个类作为正类，若干个其他类作为反类。这里我们介绍一种最常用的MvM技术：<strong>纠错输出码(ECOC)</strong></p>
  </li>
  <li>
    <p>ECOC是将编码的思想引入类别拆分，主要分为两步：</p>
  </li>
</ul>

<p>  1.编码： 对N个类别做M次划分，每次划分将一部分类别划为正类，一部分划为反类，从而形成一个二分类训练集;这样一共产生M个训练集，可训练出M个分类器</p>

<p>  2.解码：:M个分类器分别对测试样本进行预测，这些预测标记组成一个编码.将这个预测编码与每个类别各自的编码进行比较，返回其中距离最小的类别作为最终预测结果。</p>

<ul>
  <li>类别划分通过<strong>编码矩阵</strong>指定。编码矩阵有多种形式，常见的有二元码和三元码，前者将每个类别分别指定为正类和反类，后者在正、反类之外，还可指定“停用类”</li>
</ul>

<center><img src="../assets/img/posts/20211222/39.jpg" /></center>

<h2 id="37-类别不平衡问题">3.7. 类别不平衡问题</h2>

<ul>
  <li>
    <p>前面介绍的分类学习方法都有一个共同的基本假设：即不同类别的训练样例数目相当。如果不同类别的样例数差别很大，会对学习过程造成困扰。</p>
  </li>
  <li>
    <p><strong>类别不平衡(class-imbalance)</strong>就是指分类任务中不同类别的训练样例数目差别很大的情况。</p>
  </li>
  <li>
    <p><strong>再缩放(rescaling)</strong>是类别不平衡中的一个基本策略：比如在最简单的二分类问题中，我们假设y大于0.5为正例，y小于0.5为负例，但是在类别不平衡时，我们可以改变阈值来达到再平衡：</p>
  </li>
</ul>

<p>  将</p>

<center><img src="../assets/img/posts/20211222/40.jpg" /></center>

<p>  变成</p>

<center><img src="../assets/img/posts/20211222/41.jpg" /></center>

<ul>
  <li>现有的解决类别不平衡的技术大体上有三类做法(这里我们均假设正例样本少):</li>
</ul>

<p>  1.第一类是直接对训练集里的反类样例进行”欠采样” (undersampling)，即去除一些反例使得正、反例数日接近，然后再进行学习;</p>

<p>  2.第二类是对训练集里的正类样例进行”过采样” (oversampling)，即增加一些正例使得正、反例数目接近，然后再进行学习;</p>

<p>  3.第三类则是直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将上面的公式(改变阈值)嵌入到其决策过程中，称为”阔值移动” (threshold-moving)</p>

<ul>
  <li>需注意的是，过采样法不能简单地对初始正例样本进行重复来样，否则会招致严重的过拟合；过采样法的代表性算法SMOTE是通过对训练集里的正例进行插值来产生额外的正例.另一方面，欠采样法若随机丢弃反例可能丢失一些重要信息;欠采样法的代表性算法EasyEnsemble则是利用集成学习机制，将反例划分为若干个集合供不同学习器使用，这样对每个学习器来看都进行了欠采样，但在全局来看却不会丢失重要信息。</li>
</ul>

<h1 id="4-第4章-决策树">4. 第4章 决策树</h1>
<h2 id="41-思维导图">4.1. 思维导图</h2>
<h3 id="411-章节导图">4.1.1. 章节导图</h3>

<center><img src="../assets/img/posts/20211222/61.jpg" /></center>

<h3 id="412-如何生成一棵决策树">4.1.2. 如何生成一棵决策树</h3>

<center><img src="../assets/img/posts/20211222/62.jpg" /></center>

<h2 id="42-基本流程">4.2. 基本流程</h2>
<ul>
  <li>决策树是基于树结构来进行决策，其中包含一个根结点，多个内部结点和多个叶结点</li>
  <li>叶结点对应决策结果，其他每个结点都对应一个属性测试</li>
  <li>每个结点包含的样本集合根据属性测试被划分到子结点中，那么根结点包含样本全集</li>
  <li>决策树学习的目的是为了产生一棵泛化能力强的决策树</li>
  <li>决策树的生成是一个递归过程，下面这张图展示了递归的过程：</li>
</ul>

<center><img src="../assets/img/posts/20211222/43.jpg" /></center>

<p>对于每个结点，首先判断该结点的样本集是否属于同一个类别C，如果是，则将该结点标记为C类叶结点。再判断该结点的样本集的属性值是否完全相同(或者是否为空集)，如果是，则将该结点标记为D类叶结点，其中D类是这些样本中最多的类别。如果该结点即不是<strong>同属于一个类别</strong>也不是<strong>属性值取值相同</strong>，那么则需要继续划分，选择一个最优的划分属性$a_*$,创建新的分支，对于每个分支结点首先判断是否为空，如果为空则判定为E类叶结点，其中E类是父结点中类别最多的类。如果子结点不为空则递归。</p>

<h2 id="43-划分选择">4.3. 划分选择</h2>
<p>可以发现生成决策树最关键的步骤就是选择最优划分属性，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的<strong>纯度</strong>越来越高。我们有很多指标来确定选择哪一个属性作为最优划分选择，下面将分别介绍：</p>

<h3 id="431-信息增益">4.3.1. 信息增益</h3>
<ul>
  <li><strong>信息熵(information entropy)</strong>是度量样本集合纯度最常用的一种指标。下面是信息熵的定义公式：</li>
</ul>

<center><img src="../assets/img/posts/20211222/44.jpg" /></center>

<p>  其中$p_k$表示第k类样本在样本集D中所占比例，信息熵越小表示D的纯度越高。</p>

<ul>
  <li>假设离散属性a有V个可能的取值${a^1,a^2…a^V}$,那么我们可以计算出在使用a作为划分属性前后的信息熵差别，也就是<strong>信息增益(information gain)</strong>：</li>
</ul>

<center><img src="../assets/img/posts/20211222/45.jpg" /></center>

<p>  对每一个子结点$D^v$都赋予权重同时相加。</p>

<ul>
  <li>
    <p>著名的ID3决策树学习算法就是以信息增益作为准则来选择划分属性，我们希望找到信息增益最大的属性。</p>
  </li>
  <li>
    <p>书上使用信息增益划分的例子：</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/46.jpg" /></center>

<h3 id="432-增益率">4.3.2. 增益率</h3>
<ul>
  <li>
    <p>信息增益对可取值数目较多的属性有所偏好，比如我们使用编号这一属性来划分，每一个编号都只有一个样本，那么信息增益肯定增大了，但是决策树的泛化能力显然下降了。</p>
  </li>
  <li>
    <p><strong>增益率(gain ratio)</strong>，我们通过对信息增益除以IV来平衡属性数目带来的影响，增益率的定义如下：</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/47.jpg" /></center>

<p>  IV(intrinsic value)的定义如下：</p>

<center><img src="../assets/img/posts/20211222/48.jpg" /></center>

<p>  IV是属性a的固有值，属性a可取的数值数目越多，那么IV就越大</p>

<ul>
  <li>但是增益率也有问题，那就是对于可取值数目较少的属性有偏好，所以著名的C4.5决策树算法并不是直接使用增益率，而是先从候选划分属性中找出信息增益高于平均水平的属性，然后再从中选择增益率最高的属性</li>
</ul>

<h3 id="433-基尼指数">4.3.3. 基尼指数</h3>
<ul>
  <li>CART决策树使用基尼指数来选择划分属性</li>
  <li>数据集D的纯度定义如下</li>
</ul>

<center><img src="../assets/img/posts/20211222/49.jpg" /></center>

<p>  直观来说，Gini反映了从数据集随便抽取两个样本，它们类别不一致概率</p>

<ul>
  <li><strong>基尼指数</strong>定义如下：</li>
</ul>

<center><img src="../assets/img/posts/20211222/50.jpg" /></center>

<p>  很明显，我们希望基尼指数越小越好，所以我们选择基尼指数最小的属性最为最优划分属性。</p>

<h2 id="44-剪枝处理">4.4. 剪枝处理</h2>
<ul>
  <li>不难发现，上面对于属性的划分很容易过拟合，所以针对过拟合现象，决策树选择<strong>剪枝(pruning)</strong>来对付过拟合</li>
  <li>剪枝就是去掉一些分支来降低过拟合的风险，剪枝可以分为预剪枝和后剪枝</li>
  <li><strong>预剪枝(prepruning)</strong>:在决策树生成的过程中，对每个结点在划分前进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分</li>
  <li><strong>后剪枝(postpruning)</strong>:从训练集生成了一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能的提升，则将该子树替换为叶结点。</li>
</ul>

<h3 id="441-预剪枝">4.4.1. 预剪枝</h3>
<ul>
  <li>如何判断决策树泛化性能？可以使用留出法预留一部分数据用作验证集进行性能评估，性能度量可以用之前介绍的那些，本小节使用精度作为性能度量</li>
  <li>预剪枝生成的决策树：</li>
</ul>

<center><img src="../assets/img/posts/20211222/51.jpg" /></center>

<ul>
  <li>可以发现预剪枝显著减少了分支的数量，这样可以减少决策树的训练时间开销。但是这样也有一个问题，就是有些分支的当前划分虽然不能提升泛化性能，但是后续划分却有可能导致性能显著提高，这样就带来了欠拟合的风险</li>
</ul>

<h3 id="442-后剪枝">4.4.2. 后剪枝</h3>
<ul>
  <li>首先生成决策树，然后对每个结点进行评估是否需要剪枝</li>
</ul>

<center><img src="../assets/img/posts/20211222/52.jpg" /></center>

<ul>
  <li>虽然后剪枝决策树的欠拟合风险小，泛化性能也往往优于预剪枝决策树，但是后剪枝的时间开销大</li>
</ul>

<h2 id="45-连续与缺失值">4.5. 连续与缺失值</h2>
<h3 id="451-连续值处理">4.5.1. 连续值处理</h3>
<ul>
  <li>到目前为止仅讨论了基于离散属性来生成决策树，但是现实学习任务中通常会遇到连续属性</li>
  <li>很明显连续属性不能根据连续属性的可取值来对结点进行划分，我们需要用到<strong>连续属性离散化</strong>的技术，最简单的策略是<strong>二分法</strong></li>
  <li>给定样本集D和和连续属性a，假定a在D上出现了n个不同的取值，将这些值从小到大排序，然后每次选择每两个数的中位数t作为划分点，那么连续值就可以当作离散值来处理了，分出的子样本集分别记作$D_t^+$和$D_t^-$</li>
</ul>

<center><img src="../assets/img/posts/20211222/53.jpg" /></center>

<ul>
  <li>划分结果：</li>
</ul>

<center><img src="../assets/img/posts/20211222/54.jpg" /></center>

<ul>
  <li>需要注意的是：<strong>连续属性在划分后并不会被丢失，后续划分仍然可以使用</strong></li>
</ul>

<h3 id="452-缺失值处理">4.5.2. 缺失值处理</h3>
<ul>
  <li>在实际数据中一般都有很多缺失值，所以我们需要考虑如何对含有缺失值的数据进行学习</li>
  <li>给几个定义：$\tilde{D}$表示D中属性a上没有缺失值的样本子集，假设属性值a可取值{$a^1$,$a^2$…$a^V$}, $\tilde{D}^v$表示$\tilde{D}$中属性值a取值为$a^v$的子集，$\tilde{D}_k$表示样本子集，我们为每个样本赋予权重$\omega_x$(决策树开始阶段，根结点中权重初始化为1)并定义：</li>
</ul>

<center><img src="../assets/img/posts/20211222/55.jpg" /></center>

<ul>
  <li>
    <p>直观地看，对属性a，$\rho$表示无缺失值样本所占比例，$\tilde{p}_k$表示无缺失样本中第k类样本所占的比例，$\tilde{r}_v$表示无缺失值样本在属性上取值为$a^v$所占的比例</p>
  </li>
  <li>
    <p>那么我们可以将信息增益的公式推广为</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/56.jpg" /></center>

<p> </p>

<center><img src="../assets/img/posts/20211222/57.jpg" /></center>

<ul>
  <li>
    <p>那么对于那些在该属性上缺失的值如何处理呢？分两种情况：若样本$x$在划分属性$a$上的取值己知, 则将$x$划入与其取值对应的子结点，且样本权值在于结点中保持为$\omega_x$, 若样本$x$在划分属性$a$上的取值未知，则将$x$同时划入所有子结点, 且样本权值在与属性值$a^v$对应的子结点中调整为$\tilde{r}_v*\omega_x$，直观地看，这就是让同一个样本以不同的概率划入到不同的子结点中去。</p>
  </li>
  <li>
    <p>C4.5就是使用了上述的解决方法</p>
  </li>
</ul>

<h2 id="46-多变量决策树">4.6. 多变量决策树</h2>
<ul>
  <li>若我们把每个属性视为坐标空间中的一个坐标轴，则d个属性描述的样本就对应了d维空间中的一个数据点，对样本分类则意味着在这个坐标空间中寻找不同类样本之间的分类边界，决策树生成的分类边界有个明显的特点：<strong>轴平行</strong>，即它的分类边界由若干个与坐标轴平行的分段组成</li>
</ul>

<center><img src="../assets/img/posts/20211222/58.jpg" /></center>

<ul>
  <li>这样的决策树由于要进行大量的属性测试，预测时间开销会很大，所以我们希望使用如下图红线所示的斜划分。<strong>多变量决策树</strong>就是能实现这样斜划分甚至更复杂划分的决策树</li>
</ul>

<center><img src="../assets/img/posts/20211222/59.jpg" /></center>

<ul>
  <li>以实现斜划分的决策树为例，非叶结点不再是仅对某一个属性，而是对属性的线性组合进行测试</li>
</ul>

<center><img src="../assets/img/posts/20211222/60.jpg" /></center>

<h2 id="47-阅读材料">4.7. 阅读材料</h2>
<ul>
  <li>
    <p>多变量决策树算法主要有OC1，还有一些算法试图在决策树的叶结点上嵌入神经网络，比如感知机树在每个叶结点上训练一个感知机</p>
  </li>
  <li>
    <p>有些决策树学习算法可进行<strong>“增量学习”(incrementallearning)</strong>，即在接收到新样本后可对己学得的模型进行调整，而不用完全重新学习。主要机制是通过调整分支路径上的划分属性次序来对树进行部分重构，代表性算法ID4、ID5R、ITI等。增量学习可有效地降低每次接收到新样本后的训练时间开销，但多步增量学习后的模型会与基于全部数据训练而得的模型有较大差别。</p>
  </li>
</ul>

<h1 id="5-第5章-神经网络">5. 第5章 神经网络</h1>

<h2 id="50-思维导图">5.0. 思维导图</h2>

<center><img src="../assets/img/posts/20211222/73.jpg" /></center>

<h2 id="51-神经元模型">5.1. 神经元模型</h2>
<ul>
  <li>神经网络定义: 神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应</li>
  <li>神经网络中最基本的成分是神经元模型(neuron)</li>
  <li>M-P神经元模型:</li>
</ul>

<center><img src="../assets/img/posts/20211222/63.jpg" /></center>

<ul>
  <li>在这个模型中，神经元接收来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过激活函数处理以产生神经元的输出</li>
  <li>激活函数: 有<strong>阶越函数</strong>和<strong>Sigmoid函数</strong>等等</li>
</ul>

<center><img src="../assets/img/posts/20211222/64.jpg" /></center>

<h2 id="52-感知机与多层网络">5.2. 感知机与多层网络</h2>
<ul>
  <li>感知机(perceptron)由两层神经元组成，如下图所示</li>
</ul>

<center><img src="../assets/img/posts/20211222/65.jpg" /></center>

<ul>
  <li>
    <p>权重$\omega_i$以及阈值$\theta$可通过学习得到</p>
  </li>
  <li>
    <p>感知机的学习规则非常简单，对训练样例(x, y)，若当前感知机的输出为$\hat{y}$, 则感知机权重将这样调整</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/66.jpg" /></center>

<ul>
  <li>其中$\eta$称为学习率(learning rate)</li>
  <li>感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元，其学习能力非常有限</li>
  <li>要解决非线性可分问题，需考虑使用多层功能神经元，比如下图，输入层和输出层之间的一层神经元被称为隐含层(hidden layer)，隐含层和输出层神经元都是拥有激活函数的功能神经元</li>
</ul>

<center><img src="../assets/img/posts/20211222/67.jpg" /></center>

<ul>
  <li>多层前馈神经网络(multi-layer feedforward neural networks): 每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接</li>
</ul>

<h2 id="53-误差逆传播算法">5.3. 误差逆传播算法</h2>
<ul>
  <li>误差逆传播算法(error BackPropagation)简称BP算法，可用于很多类型的神经网络</li>
</ul>

<center><img src="../assets/img/posts/20211222/68.jpg" /></center>

<ul>
  <li>误差采用均方误差:</li>
</ul>

<center><img src="../assets/img/posts/20211222/69.jpg" /></center>

<ul>
  <li>BP算法基于梯度下降(gradient descent)策略，以目标的负梯度方向对参数进行调整</li>
</ul>

<center><img src="../assets/img/posts/20211222/70.jpg" /></center>

<ul>
  <li>
    <p>学习率控制着算法每一轮迭代中的更新步长，若太大则容易振荡，太小则收敛速度又会过慢</p>
  </li>
  <li>上面介绍的标准BP算法每次仅针对<strong>一个训练样例</strong>更新连接权和阈值，我们也可以简单推出基于累积误差最小化的更新规则，就得到了累积BP算法</li>
  <li>正是由于其强大的表达能力，BP神经网络经常遭遇过拟合，其训练误差持续降低，但测试误差却可能上升，由两种策略常用来缓解BP网络的过拟合。
    <ul>
      <li>第一种策略是<strong>早停(early stopping)</strong>: 若训练集误差降低但验证集误差升高，则停止训练</li>
      <li>第二种策略是正则化，其基本思路是在误差目标函数中增加一个用于描述网络复杂度的部分，例如连接权和阈值的平方和，那么误差目标函数变成:</li>
    </ul>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/71.jpg" /></center>

<h2 id="54-全局最小与局部极小">5.4. 全局最小与局部极小</h2>
<ul>
  <li>我们常常会谈到两种最优: <strong>局部极小(local minimum)</strong>和<strong>全局最小(global minimum)</strong></li>
  <li>直观地看，局部极小解是参数空间中的某个点，其领域点的误差函数值均不小于该点的函数值</li>
  <li>全局最小解是指参数空间中所有点的误差函数值均不小于该点的误差函数值</li>
</ul>

<center><img src="../assets/img/posts/20211222/72.jpg" /></center>

<ul>
  <li>在现实任务中，人们常采用以下策略来试图跳出局部极小，从而进一步实现全局最小
    <ul>
      <li>以多组不同参数值初始化多个神经网络</li>
      <li>使用“模拟退火”(simulated annealing)技术，每一步都以一定概率接受比当前解更差的结果，从而有助于跳出局部极小</li>
      <li>随机梯度下降，即每次使用随机的样本进行误差计算</li>
    </ul>
  </li>
</ul>

<h2 id="55-其他常见神经网络">5.5. 其他常见神经网络</h2>

<h3 id="551-rbf网络">5.5.1. RBF网络</h3>
<ul>
  <li>RBF(Radial Basis Function, 径向基函数)网络是一种单隐层前馈神经网络，它使用径向基函数作为隐层神经元激活函数，而输出层则是对隐层神经元输出的线性组合</li>
  <li>具有足够多隐层神经元的RBF网络能以任意精度逼近任意连续函数</li>
</ul>

<h3 id="552-art网络">5.5.2. ART网络</h3>
<ul>
  <li>竞争型学习(competitive learning)是神经网络中一种常用的无监督学习策略，在使用该策略时，网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元的状态被抑制。这种机制亦称胜者通吃(winner-take-all)原则</li>
  <li>ART(Adaptive Resonance Theory，自适应谐振理论)网络是竞争型学习的重要代表。该网络由比较层、识别层、识别阔值和重置模块构成。其中，比较层负责接受输入样本，并将其传递给识别层神经元。识别层每个神经元对应一个模式类，神经元数目可在训练过程中动态增长以增加新的模式类</li>
</ul>

<h3 id="553-som网络">5.5.3. SOM网络</h3>
<ul>
  <li>SOM(Self-Organizing Map，自组织映射)网络是一种竞争学习型的无监督神经网络，它能将高维输入数据映射到低维空间(通常为二维) ，同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元</li>
</ul>

<center><img src="../assets/img/posts/20211222/74.jpg" /></center>

<h3 id="554-级联相关网络">5.5.4. 级联相关网络</h3>
<ul>
  <li>一般的神经网络模型通常假定网络结构是事先固定的，训练的目的是利用训练样本来确定合适的连接权、阈值等参数。与此不同，结构自适网络则将网络结构也当作学习的目标之一，并希望能在训练过程中找到最符合数据特点的网络结构-级联相关(Cascade-Correlation)网络是结构自适应网络的重要代表</li>
</ul>

<center><img src="../assets/img/posts/20211222/75.jpg" /></center>

<h3 id="555-elman网络">5.5.5. Elman网络</h3>
<ul>
  <li>与前馈神经网络不同<strong>递归神经网络(recurrent neural networks)</strong>允许网络中出现环形结构，从而可让一些神经元的输出反馈回来作为输入信号。这样的结构与信息反馈过程，使得网络在t时刻的输出状态不仅与t时刻的输入有关，还与t-1时刻的网络状态有关，从而能处理与时间有关的动态变化</li>
  <li>Elman网络是最常用的递归神经网络之一，其结构如图所示，它的结构与多层前馈网络很相似，但隐层神经元的输出被反馈回来，与下一时刻输入层神经元提供的信号一起，作为隐层神经元在下一时刻的输入。隐层神经元通常采用Sigmoid激活函数，而网络的训练则常通过推广的BP算法进行</li>
</ul>

<center><img src="../assets/img/posts/20211222/76.jpg" /></center>

<h3 id="556-boltzmann机">5.5.6. Boltzmann机</h3>
<ul>
  <li>神经网络中有一类模型是为网络状态定义一个能量(energy)，能量最小化时网络达到理想状态，而网络的训练就是在最小化这个能量函数。Boltzmann机就是一种基于能量的模型(energy-based model)，常见结构如图所示，其神经元分为两层:显层与隐层。显层用于表示数据的输入与输出，隐层则被理解为数据的内在表达</li>
</ul>

<center><img src="../assets/img/posts/20211222/77.jpg" /></center>

<h2 id="56-深度学习">5.6. 深度学习</h2>
<ul>
  <li>理论上来说，参数越多的模型复杂度越高、 “容量” (capacity) 越大，这意味着它能完成更复杂的学习任务。但一般情形下，复杂模型的训练效率低，易陷入过拟合，因此难以受到人们青睐。而随着云计算、大数据时代的到来，计算能力的大幅提高可缓解训练低效性，训练数据的大幅增加则可降低过拟合风险，因此，以”深度学习”(deep learning)为代表的复杂模型开始受到人们的关注。</li>
  <li>无监督逐层训练(unsupervised layer-wise training)是多隐层网络训练的有效手段，其基本思想是每次训练一层隐结点，训练时将上一层隐结点的输出作为输入，向本层隐结点的输出作为下一层隐结点的输入，这称为<strong>预训练(pre-training)</strong>;在预训练全部完成后，再对整个网络进行<strong>微调(fine-tuning)</strong>训练</li>
  <li>另一种节省训练开销的策略是权共享，即让一组神经元使用相同的连接权，比如CNN</li>
  <li>以往在机器学习用于现实任务时，描述样本的特征通常需由人类专家来设计，这称为<strong>特征工程(feature engineering)</strong>，深度学习则通过机器学习技术自身产生好特征</li>
  <li>神经网络是一种难以解释的黑箱模型</li>
</ul>

<h1 id="6-第6章-支持向量机">6. 第6章 支持向量机</h1>

<h2 id="60-思维导图">6.0. 思维导图</h2>

<center><img src="../assets/img/posts/20211222/134.jpg" /></center>

<h2 id="61-间隔与支持向量">6.1. 间隔与支持向量</h2>
<ul>
  <li>给定训练样本集$D={(x_1, y_1),(x_2, y_2),…,(x_m, y_m)}$, 其中$y_i\in${$-1, +1$}, 也就是一个传统的分类问题。分类学习最基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开，但能将训练样本分开的划分超平面可能有很多，如下图</li>
</ul>

<center><img src="../assets/img/posts/20211222/78.jpg" /></center>

<ul>
  <li>直观上看，应该去找位于两类训练样本正中间的划分超平面，因为该划分超平面对训练样本局部扰动的容忍性最好</li>
  <li>在样本空间中，划分超平面可通过如下线性方程来描述: $\omega^Tx+b=0$, 其中$\omega$为法向量, 决定了超平面的方向, b为位移项, 决定了超平面与原点的距离, 显然划分超平面可被法向量$\omega$和b确定</li>
  <li>样本空间中任意点x到超平面的距离可写为</li>
</ul>

<center><img src="../assets/img/posts/20211222/79.jpg" /></center>

<ul>
  <li>假设超平面能将训练样本正确分类, 即对($x_i, y_i$)$\in$D, 我们有以下结果:</li>
</ul>

<center><img src="../assets/img/posts/20211222/80.jpg" /></center>

<ul>
  <li><strong>支持向量</strong>: 距离超平面最近的几个训练样本点使得上述条件等号成立，它们就被称为支持向量(support vector), 两个异类支持向量到超平面的距离之和为<strong>间隔</strong>$\gamma$</li>
</ul>

<center><img src="../assets/img/posts/20211222/81.jpg" /></center>

<ul>
  <li>图示:</li>
</ul>

<center><img src="../assets/img/posts/20211222/82.jpg" /></center>

<ul>
  <li>欲找到具有最大间隔的划分超平面，也就是找到能满足约束的最大的$\gamma$</li>
</ul>

<center><img src="../assets/img/posts/20211222/83.jpg" /></center>

<ul>
  <li>为了最大化间隔，仅需最大化||$\omega$||$^{-1}$, 这等价于最小化||$\omega$||$^{2}$, 于是上述式子可重写为:</li>
</ul>

<center><img src="../assets/img/posts/20211222/84.jpg" /></center>

<ul>
  <li>这就是支持向量机(Support Vector Machine, SVM)的基本型</li>
</ul>

<h2 id="62-对偶问题">6.2. 对偶问题</h2>
<ul>
  <li>我们注意到SVM的基本型本身是一个凸二次规划问题<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>, 能直接用现成的优化计算包求解，但我们可以有更高效的方法</li>
</ul>

<ul>
  <li>对上述式子使用拉格朗日乘子法<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>可得到其对偶问题，则该问题的拉格朗日函数可写为</li>
</ul>

<center><img src="../assets/img/posts/20211222/91.jpg" /></center>

<ul>
  <li>其中$\alpha$=($\alpha_1$;$\alpha_2$…;$\alpha_m$), 令L($\omega$, b, $\alpha$)对$\omega$和b的偏导为零可得</li>
</ul>

<center><img src="../assets/img/posts/20211222/92.jpg" /></center>

<ul>
  <li>将这两个条件带入之前的拉格朗日函数中，即可将$\omega$和b消去，得到以下的对偶问题:</li>
</ul>

<center><img src="../assets/img/posts/20211222/93.jpg" /></center>

<center><img src="../assets/img/posts/20211222/94.jpg" /></center>

<ul>
  <li>从对偶问题中解出的$\alpha_i$是拉格朗日乘子，它恰对应着训练样本($x_i$, $y_i$), 在主问题中有不等式约束，因此上述过程需满足<strong>KTT条件</strong>, 即要求:</li>
</ul>

<center><img src="../assets/img/posts/20211222/96.jpg" /></center>

<ul>
  <li>解出$\alpha$后，求出$\omega$与b即可得到模型:</li>
</ul>

<center><img src="../assets/img/posts/20211222/95.jpg" /></center>

<ul>
  <li>于是, 对任意训练样本($x_i$, $y_i$), 总有$\alpha_i=0$或者$y_if(x_i)=1$。若$\alpha_i=0$，则该样本将不会在最终的模型的求和中出现，也就不会对f(x)有任何影响;若$\alpha_i&gt;0$, 则必有$y_if(x_i)=1$，所对应的样本点位于最大间隔边界上，是一个支持向量。这显示出支持向量机的一个重要性质:训练完成后, 大部分的训练样本都不需保留，最终模型仅与支持向量有关</li>
  <li>那么如何求解对偶问题解出$\alpha_i$呢？这是一个二次规划问题, 可使用通用的二次规划算法求解，然而，该问题的规模正比于训练样本数，这会在实际任务中造成很大的开销，为了避开这个障碍，人们通过利用问题本身的特性，提出了很多高效算法, SMO是其中一个著名的代表</li>
  <li>SMO的基本思路是先固定$\alpha_i$之外的所有参数，然后求$\alpha_i$上的极值，由于存在约束$\sum_1^m\alpha_iy_i=0$,若固定$\alpha_i$之外的其他变量，则$\alpha_i$可由其他变量导出，于是SMO每次选择两个变量$\alpha_i$和$\alpha_j$，并固定其他参数。这样，在参数初始化后，SMO不断执行如下两个步骤直至收敛
    <ul>
      <li>选取一对需更新的变量$\alpha_i$和$\alpha_j$</li>
      <li>固定$\alpha_i$和$\alpha_j$以外的参数，求解获得更新后的$\alpha_i$和$\alpha_j$</li>
    </ul>
  </li>
  <li>如何确定偏移项b呢？注意到对任意支持向量($x_s$, $y_s$)都有$y_sf(x_s)=1$:</li>
</ul>

<center><img src="../assets/img/posts/20211222/97.jpg" /></center>

<ul>
  <li>其中S为所有支持向量的下标集，理论上可以选取任意支持向量来求出b，但现实任务中常采用一种更鲁棒的做法: 使用所有支持向量求解的平均值</li>
</ul>

<center><img src="../assets/img/posts/20211222/98.jpg" /></center>

<h2 id="63-核函数">6.3. 核函数</h2>
<ul>
  <li>在本章前面的讨论中，我们假设训练样本是线性可分的，然而在现实任务中，原始样本空间内也许并不存在一个能正确划分两类样本的超平面，例如异或问题就不是线性可分的</li>
  <li>对这样的问题，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分，例如将异或问题的原始二维空间映射到一个合适的三维空间，就能找到一个合适的划分超平面。如果原始空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本可分</li>
</ul>

<center><img src="../assets/img/posts/20211222/99.jpg" /></center>

<ul>
  <li>令$\phi(x)$表示将x映射后的特征向量，于是在特征空间中划分超平面所对应的模型可表示为</li>
</ul>

<center><img src="../assets/img/posts/20211222/100.jpg" /></center>

<ul>
  <li>其中$\omega$和b是模型参数，我们有以下二次规划:</li>
</ul>

<center><img src="../assets/img/posts/20211222/101.jpg" /></center>

<ul>
  <li>其对偶问题是:</li>
</ul>

<center><img src="../assets/img/posts/20211222/102.jpg" /></center>

<center><img src="../assets/img/posts/20211222/103.jpg" /></center>

<ul>
  <li>我们观察到在对偶问题中涉及到计算$\phi(x_i)^T\phi(x_j)$, 这是样本$x_i$与$x_j$映射到特征空间之后的内积。由于特征空间维数可能很高，甚至是无穷维，因此直接计算$\phi(x_i)^T\phi(x_j)$通常是困难的，为了避开这个障碍，可以设想这样一个函数:</li>
</ul>

<center><img src="../assets/img/posts/20211222/104.jpg" /></center>

<ul>
  <li>即$x_i$与$x_j$在特征空间的内积等于它们在原始样本空间中通过函数$\kappa(·,·)$计算的结果, 有了这样的函数，我们就不必直接去计算高维甚至无穷维特征空间中的内积，于是式子可重写为</li>
</ul>

<center><img src="../assets/img/posts/20211222/105.jpg" /></center>

<ul>
  <li>求解后可得到:</li>
</ul>

<center><img src="../assets/img/posts/20211222/106.jpg" /></center>

<ul>
  <li>上面式子显示出模型最优解可通过训练样本的核函数展开，这一展式亦称<strong>支持向量展式</strong></li>
  <li>显然，若已知合适映射$\phi(·)$的具体形式，则可写出核函数$\kappa(·,·)$，但在现实任务中我们通常不知道$\phi(·)$是什么形式，那么合适的核函数是否一定存在呢？什么样的函数能做核函数呢？我们有以下定理:</li>
</ul>

<center><img src="../assets/img/posts/20211222/107.jpg" /></center>

<ul>
  <li>核函数选择成为支持向量机的最大变数，若核函数选择不合适，则意味着将样本映射到一个不合适的特征空间，很可能导致性能不佳</li>
  <li>常用核函数:</li>
</ul>

<center><img src="../assets/img/posts/20211222/108.jpg" /></center>

<ul>
  <li>此外，还可通过函数组合得到:
    <ul>
      <li>两个核函数的线性组合</li>
      <li>两个核函数的直积</li>
    </ul>

    <center><img src="../assets/img/posts/20211222/109.jpg" /></center>

    <ul>
      <li>若$\kappa_1$为核函数，则对任意函数g(x):</li>
    </ul>

    <center><img src="../assets/img/posts/20211222/110.jpg" /></center>

    <p>也是核函数</p>
  </li>
</ul>

<h2 id="64-软间隔与正则化">6.4. 软间隔与正则化</h2>
<ul>
  <li>在前面的讨论中，我们一直假定训练样本在样本空间或特征空间中是线性可分的，即存在一个超平面能将不同类的样本完全划分开。然而，在现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分</li>
  <li>缓解该问题的一个方法是允许支持向量机在一些样本上出错，为此，要引入软间隔(soft margin)的概念</li>
  <li>具体来说，前面介绍的支持向量机形式都是要求所有样本均满足约束，即所有样本必须划分正确，这称为硬间隔，而软间隔则允许某些样本不满足约束</li>
  <li>优化目标可写为:</li>
</ul>

<center><img src="../assets/img/posts/20211222/111.jpg" /></center>

<ul>
  <li>其中C&gt;0是一个常数，$l_{0/1}$是0/1损失函数</li>
</ul>

<center><img src="../assets/img/posts/20211222/112.jpg" /></center>

<ul>
  <li>显然，当C无穷大时，上面式子迫使所有样本均满足约束，当C取有限值时，式子允许一些样本不满足约束</li>
  <li>然而0/1损失函数非凸、非连续，数学性质不太好，于是人们通常用其他一些函数来替代它，下面给出三个常用的替代损失函数：</li>
</ul>

<center><img src="../assets/img/posts/20211222/113.jpg" /></center>

<ul>
  <li>若采用hinge损失，则式子变成:</li>
</ul>

<center><img src="../assets/img/posts/20211222/114.jpg" /></center>

<ul>
  <li>引入松弛变量$\xi_i$≥0，可将式子重写为</li>
</ul>

<center><img src="../assets/img/posts/20211222/115.jpg" /></center>

<center><img src="../assets/img/posts/20211222/116.jpg" /></center>

<ul>
  <li>这就是常用的软间隔支持向量机，可以看出每个样本都有一个对应的松弛变量，用以表征样本不满足约束的程度</li>
  <li>这仍是一个二次规划的问题，于是，通过拉格朗日乘子法可得到拉格朗日函数:</li>
</ul>

<center><img src="../assets/img/posts/20211222/117.jpg" /></center>

<ul>
  <li>其中$\alpha_i≥0$, $\mu_i≥0$是拉格朗日乘子</li>
  <li>偏导为0后可得到对偶问题:</li>
</ul>

<center><img src="../assets/img/posts/20211222/118.jpg" /></center>

<ul>
  <li>KTT条件:</li>
</ul>

<center><img src="../assets/img/posts/20211222/119.jpg" /></center>

<ul>
  <li>可以发现软间隔支持向量机的最终模型仅与支持向量有关</li>
  <li>我们还可以把0/1损失函数替换成别的替代损失函数以得到其他学习模型，这些模型的性质与所用的替代函数直接相关，但它们具有一个共性: 优化目标中的第一项用来描述划分超平面的间隔大小，另一项用来描述训练集上的误差</li>
</ul>

<center><img src="../assets/img/posts/20211222/120.jpg" /></center>

<ul>
  <li>其中$\Omega(f)$称为“结构风险”(structural risk)，用于描述模型f的某些性质;第二项$\sum_i^ml(f(x_i),y_i)$称为“经验风险”(empirical risk)，用于描述模型与训练数据的契合程度; C用于对二者进行折中。从经验风险最小化的角度来看，$\Omega(f)$表述了我们希望获得具有何种性质的模型(例如希望获得复杂度较小的模型)，这为引入领域知识和用户意图提供了途径; 另一方面，该信息有助于削减假设空间，从而降低了最小化训练误差的过拟合风险。从这个角度来说，上面的公式为“正则化”(regularization)问题，$\Omega(f)$称为正则化项，C则称为正则化常数。$L_p$范数(norm)是常用的正则化项，其中$L_2$范数$||\omega||_2$倾向于$\omega$的分量取值尽量均衡，即非零分量个数尽量稠密，而$L_0$范数$||\omega||_0$和$L_1$范数$||\omega||_1$则倾向于$\omega$的分量尽量稀疏，即非零分量个数尽量少</li>
</ul>

<h2 id="65-支持向量回归">6.5. 支持向量回归</h2>
<ul>
  <li>现在我们来考虑回归问题，给定训练样本$D={(x_1, y_1), (x_2, y_2), …, (x_m, y_m))}，希望学得一个形如$f(x)=\omega^Tx+b$的回归模型，使得f(x)与y尽可能接近。</li>
  <li>传统回归模型通常直接基于输出f(x)与真实输出y之间的差别来计算损失，当且仅当f(x)与y完全相同时，损失才为0。与此不同，支持向量回归(Support Vector Regression, 简称SVR)假设我们能容忍f(x)与y之间最多有$\epsilon$的偏差，如下图所示，若样本落入此间隔带，则被认为预测正确</li>
</ul>

<center><img src="../assets/img/posts/20211222/121.jpg" /></center>

<ul>
  <li>于是SVR问题可形式化为:</li>
</ul>

<center><img src="../assets/img/posts/20211222/122.jpg" /></center>

<ul>
  <li>其中C为正则化常数，$l_{\epsilon}$是$\epsilon$-不敏感损失函数:</li>
</ul>

<center><img src="../assets/img/posts/20211222/123.jpg" /></center>

<ul>
  <li>引入松弛变量$\xi_i$和$\hat{\xi_i}$，可将上述式子重写为:</li>
</ul>

<center><img src="../assets/img/posts/20211222/124.jpg" /></center>

<center><img src="../assets/img/posts/20211222/125.jpg" /></center>

<ul>
  <li>类似地，通过引入拉格朗日乘子可得到拉格朗日函数:</li>
</ul>

<center><img src="../assets/img/posts/20211222/126.jpg" /></center>

<ul>
  <li>偏导为零得到对偶问题:</li>
</ul>

<center><img src="../assets/img/posts/20211222/127.jpg" /></center>

<ul>
  <li>KTT条件:</li>
</ul>

<center><img src="../assets/img/posts/20211222/128.jpg" /></center>

<ul>
  <li>将上述条件代入，SVR的解形如:</li>
</ul>

<center><img src="../assets/img/posts/20211222/129.jpg" /></center>

<ul>
  <li>能使式子中$(\hat{\alpha}_i-\alpha_i)≠0$的样本即为SVR的支持向量，它们必落在$\epsilon$-间隔带之外</li>
  <li>由KTT条件可看出，对每个样本($x_i, y_i$)都有$(C-\alpha_i)\xi_i=0$且$\alpha_i(f(x_i)-y_i-\epsilon-\xi_i)=0$，于是在得到$\alpha_i$后，若0&lt;$\alpha_i$&lt;C, 则必有$\xi_i=0$,进而有:</li>
</ul>

<center><img src="../assets/img/posts/20211222/130.jpg" /></center>

<ul>
  <li>理论上说，可任意选取满足的$\alpha_i$来得到b，但是实践中常采用一种更鲁棒的方法: 选取多个满足0&lt;$\alpha_i$&lt;C的样本通过上述公式求解b后取平均值</li>
  <li>若考虑特征映射，则:</li>
</ul>

<center><img src="../assets/img/posts/20211222/131.jpg" /></center>

<center><img src="../assets/img/posts/20211222/132.jpg" /></center>

<h2 id="66-核方法">6.6. 核方法</h2>
<ul>
  <li>若不考虑偏移项b，则无论SVM还是SVR，学得的模型总能表示成核函数的线性组合。不仅如此，事实上我们有下面这个称为“表示定理”(representer theorem)的更一般的结论</li>
</ul>

<center><img src="../assets/img/posts/20211222/133.jpg" /></center>

<ul>
  <li>人们发展出一系列基于核函数的学习方法，统称为核方法，最常见的是通过引入核函数来将线性学习器拓展为非线性学习器</li>
  <li>支持向量机是针对二分类任务设计的，对多分类任务要进行专门的推广</li>
</ul>

<h1 id="7-第7章-贝叶斯分类器">7. 第7章 贝叶斯分类器</h1>

<h2 id="70-思维导图">7.0. 思维导图</h2>

<center><img src="../assets/img/posts/20211222/162.jpg" /></center>

<h2 id="71-贝叶斯决策论">7.1. 贝叶斯决策论</h2>
<ul>
  <li>贝叶斯决策论是概率框架下实施决策的基本方法。对于分类任务来说，在所有相关概率都已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。</li>
  <li>以多分类任务为例: 假设有N种可能的类别标记，即$Y=${$c_1, c_2,…,c_N$}, $\lambda_{ij}$是将真实标记为$c_j$的样本误分类为$c_i$所产生的损失。基于后验概率<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup>$P(c_i|x)$可获得将样本x分类为$c_i$所产生的期望损失，即在样本上的<u>条件风险</u>:</li>
</ul>

<center><img src="../assets/img/posts/20211222/135.jpg" /></center>

<ul>
  <li>我们的任务是寻找一个判定准则h: X -&gt; Y以最小化总体风险</li>
</ul>

<center><img src="../assets/img/posts/20211222/136.jpg" /></center>

<ul>
  <li>显然，对每个样本x，若h能最小化条件风险R(h(x)|x)，则总体风险R(h)也将被最小化。这就产生了贝叶斯判定准则: 为最小化总体风险，只需在每个样本上选择那个能使条件风险R(c|x)最小的类别标记，即:</li>
</ul>

<center><img src="../assets/img/posts/20211222/137.jpg" /></center>

<ul>
  <li>此时，$h^*$称为贝叶斯最优分类器，与之对应的总体风险R($h^*$)称为贝叶斯风险。1-R($h^*$)反映了分类器所能达到的最好性能。</li>
</ul>

<ul>
  <li>具体来说，若目标是最小化分类错误率，则误判损失$\lambda_{ij}$可写为:</li>
</ul>

<center><img src="../assets/img/posts/20211222/138.jpg" /></center>

<ul>
  <li>此时条件风险为:</li>
</ul>

<center><img src="../assets/img/posts/20211222/139.jpg" /></center>

<ul>
  <li>于是，最小化分类错误率的贝叶斯最优分类器为:</li>
</ul>

<center><img src="../assets/img/posts/20211222/140.jpg" /></center>

<ul>
  <li>
    <p>即对每个样本，选择能使后验概率$P(c|x)$最大的类别标记</p>
  </li>
  <li>
    <p>不难看出，欲使用贝叶斯判定准则来最小化决策风险，首先要获得后验概率$P(c|x)$。然而，在现实任务中这通常难以直接获得，从这个角度来看，机器学习所要实现的是基于有限的训练样本集尽可能准确地估计出后验概率$P(c|x)$。大体来说，主要有两种策略: 给定x，可通过直接建模$P(c|x)$来预测c，这样得到的是<u>判别式模型</u>。也可以先对联合概率分布P(x,c)建模，然后再由此获得$P(c|x)$，这样得到的是<u>生成式模型</u>。</p>
  </li>
  <li>
    <p>对于生成式模型来说，必须考虑:</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/141.jpg" /></center>

<ul>
  <li>基于贝叶斯定理，$P(c|x)$可写为:</li>
</ul>

<center><img src="../assets/img/posts/20211222/142.jpg" /></center>

<ul>
  <li>其中，P(c)是类先验概率，P(x|c)是样本x相对于类标记c的类条件概率，或称为似然(likelihood)。P(x)是用于归一化的证据因子，P(x)对所有类标记均相同。因此估计P(c|x)的问题就转化为如何基于训练数据D来估计先验概率P(c)和似然P(x|c).</li>
  <li>类先验概率P(c)表达了样本空间中各类样本所占的比例，根据大数定律，当训练集包含充足独立同分布的样本时，P(c)可通过各类样本出现的频率进行估计</li>
  <li>对类条件概率P(x|c)来说，由于它涉及关于x所有属性的联合概率，直接根据样本出现的频率来估计将会遇到严重的困难，例如样本d个属性都是二值的，则样本空间将有$2^d$种可能取值，在现实应用中，这个值往往远大于训练样本数m，因此不能用频率估计概率</li>
</ul>

<h2 id="72-极大似然估计">7.2. 极大似然估计</h2>
<ul>
  <li>估计类条件概率的一种常用策略是先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布进行参数估计。具体来说，记关于类别c的类条件概率为P(x|c)，假设P(x|c)具有确定的形式且被参数向量$\theta_c$唯一确定，则我们的任务就是利用训练集D估计参数$\theta_c$</li>
  <li>概率模型的训练过程就是参数估计的过程，统计学界的两个学派提出了不同的解决方案，频率主义学派认为参数虽然未知，但却客观存在固定值，因此可以通过优化似然函数等准则来确定参数值。本节介绍源自频率主义学派的极大似然估计<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup></li>
</ul>

<ul>
  <li>令$D_c$表示训练集D中第c类样本组成的集合，假设这些样本是独立同分布的，则参数$\theta_c$对于数据集$D_c$的似然是:</li>
</ul>

<center><img src="../assets/img/posts/20211222/143.jpg" /></center>

<ul>
  <li>对$\theta_c$进行极大似然估计，就是去寻找最大化似然$P(D_c|\theta_c)$的参数值$\hat{\theta_c}$</li>
  <li>上述似然中的连乘操作易造成下溢，通常使用对数似然(log-likelihood)</li>
</ul>

<center><img src="../assets/img/posts/20211222/144.jpg" /></center>

<ul>
  <li>此时参数$\hat{\theta_c}$的极大似然估计$\hat{\theta_c}$为</li>
</ul>

<center><img src="../assets/img/posts/20211222/145.jpg" /></center>

<ul>
  <li>例如，再连续属性情形下，假设概率密度函数p(x|c)~N($\mu_c, \sigma_c^2$)， 则参数$\mu_c$和$\sigma_c^2$的极大似然估计为:</li>
</ul>

<center><img src="../assets/img/posts/20211222/146.jpg" /></center>

<ul>
  <li>也就是说，通过极大似然法得到的正态分布均值就是样本均值，方差就是$(x-\hat{mu_c})(x-\hat{\mu_c})^T$的均值</li>
  <li>虽然这种参数化方法虽然能使类条件概率估计变得相对简单，但估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布</li>
</ul>

<h2 id="73-朴素贝叶斯分类器">7.3. 朴素贝叶斯分类器</h2>
<ul>
  <li>不难发现，基于贝叶斯公式来估计后验概率P(c|x)的主要困难在于: 类条件概率P(x|c)是所有属性上的联合概率，难以从有限的训练样本直接估计而得。为了避开这个障碍，<strong>朴素贝叶斯分类器(naïve Bayes classifier)</strong>采用了属性条件独立性假设</li>
  <li>基于这个假设，贝叶斯公式可以重写为:</li>
</ul>

<center><img src="../assets/img/posts/20211222/147.jpg" /></center>

<ul>
  <li>对于所有类别来说P(x)相同，因此基于贝叶斯判定准则有:</li>
</ul>

<center><img src="../assets/img/posts/20211222/148.jpg" /></center>

<ul>
  <li>对于P(c)和$P(x_i|c)$都用频率估计概率(对于离散属性而言)，对于连续属性则考虑概率密度函数为正态分布，参数由极大似然估计求出</li>
  <li>但是这种方法存在一个问题: 如果某个属性值在训练样本中没有出现过，那么对概率进行连乘的时候会乘上一个零，这样显然不太合理。解决方法: 可以在估计概率值时进行平滑处理，常用拉普拉斯修正。具体来说，令N表示训练集D中所有的类别数，$N_i$表示第i个属性可能的取值数，则修正为:</li>
</ul>

<center><img src="../assets/img/posts/20211222/149.jpg" /></center>

<ul>
  <li>在现实任务中朴素贝叶斯分类器有多种使用方式。例如，若任务对预测速度要求较高，则对给定训练集，可将朴素贝叶斯分类器涉及的所有概率估值事先计算好存储起来，这样在进行预测时只需”查表”即可进行判别;若任务数据更替频繁，则可采用”懒惰学习”(lazy learning)方式，先不进行任何训练，待收到预测请求时再根据当前数据集进行概率估值;若数据不断增加，则可在现有估值基础上，仅对新增样本的属性值所涉及的概率估值进行计数修正即可实现增量学习</li>
</ul>

<h2 id="74-半朴素贝叶斯分类器">7.4. 半朴素贝叶斯分类器</h2>
<ul>
  <li>朴素贝叶斯的假设在现实任务中往往难以实现，人们尝试对属性条件独立性假设进行一定程度的放松，由此产生了一类称为<strong>半朴素贝叶斯分类器</strong>的方法</li>
  <li>半朴素贝叶斯分类器的基本想法是适当考虑一部分属性间的相互依赖信息，从而既不需要进行完全联合概率，又不至于彻底忽略了比较强的属性依赖关系。<u>独依赖估计</u>(one-dependent estimator 简称ODE)是半朴素贝叶斯分类器最常用的一种策略。独依赖就是假设每个属性在类别之外最多仅依赖于一个其他属性，即:</li>
</ul>

<center><img src="../assets/img/posts/20211222/150.jpg" /></center>

<ul>
  <li>其中$pa_i$为属性$x_i$所依赖的属性，称为$x_i$的父属性，此时如果每个属性的父属性已知，则可采用之前的方法估计概率值，但是问题的关键在于如何确定每个属性的父属性，不同的做法产生不同的独依赖分类器</li>
  <li>最直接的做法是假设所有属性都依赖于同一个属性，称为<strong>超父</strong>，然后通过交叉验证等模型选择方法来确定超父属性，由此形成了SPODE(super parent ODE)方法。例如下图中，$x_1$是超父属性</li>
</ul>

<center><img src="../assets/img/posts/20211222/151.jpg" /></center>

<ul>
  <li>另一种TAN(Tree Augmented naïve Bayes)则是最大带权生成树算法的基础上，通过以下步骤将属性间依赖关系简约为如图所示的树形结构:
    <ul>
      <li>1.计算任意两个属性间的条件相互信息</li>
    </ul>

    <center><img src="../assets/img/posts/20211222/152.jpg" /></center>

    <ul>
      <li>2.以属性为结点构建完全图，任意两个结点之间边的权重设为$I(x_i,x_j)|y$</li>
      <li>3.构建此完全图的最大带权生成树，挑选根变量，将边置为有向</li>
      <li>4.加入类别结点y，增加从y到每个属性的有向边</li>
    </ul>
  </li>
</ul>

<center><img src="../assets/img/posts/20211222/153.jpg" /></center>

<ul>
  <li>TAN实际保留了强相关属性之间的依赖性</li>
  <li>AODE(averaged one-dependent estimator)是一种基于集成学习机制、更为强大的独依赖分类器，于SPODE通过模型选择确定超父属性不同，AODE尝试将每个属性作为超父来构建SPODE，然后将那些具有足够训练数据支撑的SPODE集成起来作为最终结果:</li>
</ul>

<center><img src="../assets/img/posts/20211222/154.jpg" /></center>

<ul>
  <li>与朴素贝叶斯分类器类似，AODE的训练过程也是计数，即在训练数据上对符合条件的样本进行计数的过程</li>
  <li>既然将属性条件独立性假设放松为独依赖假设可能获得泛化性能的提升，那么，能否通过考虑属性间的高阶依赖来进一步提升泛化性能呢?也就是说，将式中的属性$pa_i$替换为包含k个属性的集合<strong>$pa_i$</strong>，从而将ODE拓展为kDE。需注意的是，随着k的增加，准确估计概率$P(x_i|y,pa_i)$所需的训练样本数量将以指数级增加。因此，若训练数据非常充分，泛化性能有可能提升;但在有限样本条件下，则又陷入估计高阶联合概率的泥沼</li>
</ul>

<h2 id="75-贝叶斯网">7.5. 贝叶斯网</h2>
<ul>
  <li>贝叶斯网亦称信念图，它借助有向无环图来刻画属性之间的依赖关系，并使用条件概率表来描述属性的联合概率分布，一个贝叶斯网B由结构G和参数$\Theta$两部分组成，即$B=&lt;G,\Theta&gt;$</li>
</ul>

<center><img src="../assets/img/posts/20211222/155.jpg" /></center>

<h3 id="751-结构">7.5.1. 结构</h3>
<ul>
  <li>贝叶斯网结构有效地表达了属性间的条件独立性，给定父结点集，贝叶斯网假设每个属性与它的非后裔属性独立，于是联合概率分布定义为:</li>
</ul>

<center><img src="../assets/img/posts/20211222/156.jpg" /></center>

<ul>
  <li>上图中，$x_3$和$x_4$在给定$x_1$的取值时独立，简记为$x_3$⊥$x_4$|$x_1$</li>
  <li>下图给出了贝叶斯网的三个变量之间的典型依赖关系</li>
</ul>

<center><img src="../assets/img/posts/20211222/157.jpg" /></center>

<ul>
  <li>在顺序结构中，给定x的值，则y与z条件独立。V型结构(V-structure)亦称冲撞的结构，给定子结点$x_4$的取值，$x_1$与$x_2$必不独立，奇妙的是，若$x_4$的取值完全未知，则V型结构下$x_1$和$x_2$却是相互独立的，我们做一个简单的验证:</li>
</ul>

<center><img src="../assets/img/posts/20211222/158.jpg" /></center>

<ul>
  <li>这样的独立性称为边际独立性，记为$x_1$╨$x_2$</li>
  <li>为了分析有向图中变量间的条件独立性，可使用有向分离，将有向图变为无向图:
    <ul>
      <li>找出有向图中的所有V型结构，在V型结构的两个父结点之间加上一条无向边</li>
      <li>将所有有向边改为无向边</li>
    </ul>
  </li>
  <li>这样产生的无向图称为道德图。基于道德图能直观、迅速地找到变量间的条件独立性。假定道德图中有变量x, y和变量集合z={$z_i$}，若变量x和y能在图上被z分开，即从道德图中将变量集合z去除后，x和y分属两个连通分支，则称变量x和y被z有向分离，x⊥y|z成立</li>
</ul>

<h3 id="752-学习">7.5.2. 学习</h3>
<ul>
  <li>若网络结构已知，即属性间的依赖关系已知，则贝叶斯网的学习过程相对简单，只需要通过对训练样本计数，估计出每个结点的条件概率表即可。但在现实应用中我们往往并不知晓网络结构，于是，贝叶斯网学习的首要任务就是根据训练数据集来找出结构最”恰当”的贝叶斯网。”评分搜索”是求解这一问题的常用办法。具体来说，我们先定义一个评分函数(score function)，以此来评估贝叶斯网与训练数据的契合程度，然后基于这个评分函数来寻找结构最优的贝叶斯网。显然，评分函数引入了关于我们希望获得什么样的贝叶斯网的归纳偏好</li>
  <li>细节参见书中</li>
</ul>

<h3 id="753-推断">7.5.3. 推断</h3>
<ul>
  <li>贝叶斯网训练好之后就能用来回答”查询”(query)，即通过一些属性变量的观测值来推测其他属性变量的取值。例如在西瓜问题中，若我们观测到西瓜色泽青绿、敲声烛响、根蒂蜷缩，想知道它是否成熟、甜度如何。这样通过已知变量观测值来推测待查询变量的过程称为”推断”(inference)，已知变量观测值称为”证据” (evidence)</li>
  <li>最理想的是直接根据贝叶斯网定义的联合概率分布来精确计算后验概率，不幸的是，这样的精确推断已被证明是NP难，在现实应用中，贝叶斯网的近似推断常使用吉布斯采样来完成</li>
</ul>

<h2 id="76-em算法">7.6. EM算法</h2>
<ul>
  <li>在前面的讨论中，我们一直假设训练样本所有属性变量的值都已被观测到，即训练样本是完整的，但在现实应用中往往会遇到不完整的训练样本，在这种存在未观测变量的情形下，是否仍能对模型参数进行估计呢？未观测变量的学名是隐变量(latent variable)</li>
  <li>令X表示已观测变量集，Z表示隐变量集，$\Theta$表示模型参数，若欲对$\Theta$做极大似然估计，则应最大化对数似然:</li>
</ul>

<center><img src="../assets/img/posts/20211222/159.jpg" /></center>

<ul>
  <li>然而由于Z是隐变量，上式无法直接求解，因此我们可通过对Z计算期望，来最大化已观测数据的对数边际似然(marginal likelihood)</li>
</ul>

<center><img src="../assets/img/posts/20211222/160.jpg" /></center>

<ul>
  <li><strong>EM(Expectation-Maximization)算法</strong>是常用的估计隐变量的利器，它是一种迭代式的方法。其基本想法是:若参数$\Theta$己知，则可根据训练数据推断出最优隐变量Z的值(E步);反之，若Z的值已知，则可方便地对参数$\Theta$做极大似然估计(M步)</li>
  <li>于是，以初始值$\Theta^0$为起点，可迭代执行以下步骤直至收敛:
    <ul>
      <li>基于$\Theta^t$推断隐变量Z的期望，记为$Z^t$;</li>
      <li>基于已观测变量X和$Z^t$对参数$\Theta$做极大似然估计，记为$\Theta^{t+1}$;</li>
    </ul>
  </li>
  <li>这就是EM算法的原型，进一步，若我们不是取Z的期望，而是基于$\Theta^t$计算隐变量Z的概率分布$P(Z|X,\Theta^t)$, 则算法的步骤为:</li>
</ul>

<center><img src="../assets/img/posts/20211222/161.jpg" /></center>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>注意这里的测试集其实是验证集，我们通常把实际情况中遇到的数据集称为测试集。 <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>满秩矩阵指方阵的秩等于矩阵的行数/列数，满秩矩阵有逆矩阵且对于y=Xb有唯一的解 <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>统计学中，似然函数是一种关于统计模型参数的函数。给定输出x时，关于参数θ的似然函数L(θ|x)（在数值上）等于给定参数θ后变量X的概率：L(θ|x)=P(X=x|θ)。 <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>随机变量的期望组成的向量称为期望向量或者均值向量 <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>协方差矩阵的每个元素是各个向量元素之间的协方差。协方差就是Covariance <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>二次规划(Quadratic Programming)是一种典型的优化问题，在此类问题中，目标函数是变量的二次函数，而约束条件是变量的线性不等式，假定变量个数为d，约束条件的个数为m，则标准的二次规划问题形如: <center><img src="../assets/img/posts/20211222/85.jpg" /></center> 其中x为d维向量，Q为实对称矩阵，A为实矩阵，若Q为半正定矩阵，则目标函数为凸函数，相应的二次规划是凸二次优化问题。半正定矩阵即满足: A是n阶方阵，如果对任何非零向量X，都有X’AX≥0，其中X’表示X的转置，就称A为半正定矩阵。正定矩阵定义类似，只是把大于等于变成大于。此时若约束条件Ax≤b定义的可行域不为空，且目标函数在此可行域有下界，则该问题将有全局最小值。若Q为正定矩阵，则该问题有唯一全局最小值 <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>拉格朗日乘子法是一种寻找多元函数在一组约束下的极值的方法，通过引入拉格朗日乘子, 可将d个变量与k个约束条件的最优化问题转化为具有d+k个变量的无约束优化问题求解。首先考虑等式约束的优化问题，假定x为d维向量，欲寻找x的某个取值$x^*$, 使目标函数f(x)最小且同时满足g(x)=0的约束，从几何角度看，该问题的目标是在由方程g(x)=0确定的d-1维曲面上寻找能使目标函数f(x)最小化的点，此时不难得到如下结论: 1.对于约束曲面上的任意点x, 该点的梯度▽g(x)正交于约束曲面; 2.在最优点$x^*$，目标函数在该点的梯度▽f($x^*$)正交于约束曲面<center><img src="../assets/img/posts/20211222/86.jpg" /></center> 由此可知，在最优点$x^*$，存在$\lambda$≠0使得<center><img src="../assets/img/posts/20211222/87.jpg" /></center>$\lambda$称为拉格朗日算子，定义拉格朗日函数<center><img src="../assets/img/posts/20211222/88.jpg" /></center>现在考虑不等式约束g(x)≤0,如图所示<center><img src="../assets/img/posts/20211222/89.jpg" /></center> 此时最优点$x^*$或在g(x)&lt;0的区域中，或在边界g(x)=0上。对于g(x)&lt;0的情形，约束g(x)≤0不起作用，可直接通过条件▽f(x)=0来获得最优点，这相当于将$\lambda$置零，g(x)=0的情形类似于上面等式约束的分析。因此，在约束g(x)≤0下最小化f(x)，可转化为在如下约束下最小化拉格朗日函数: <center><img src="../assets/img/posts/20211222/90.jpg" /></center>上述约束条件称为KTT条件。一个优化问题可以从两个角度来考察，即主问题和对偶问题，主问题就是没有引入拉格朗日乘子的原始目标函数，对偶问题就是加上了拉格朗日乘子的目标函数，这个目标函数是原始目标函数的最优值的一个下界，所以我们希望找到最好的下界，也就是最大的下界，所以对偶函数是寻找最大值 <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>提到后验概率，不得不提到先验概率，先验概率是根据以往经验和分析得到的概率，而后验概率可以视为条件概率，它可以通过贝叶斯公式算出 <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>它是建立在极大似然原理的基础上的一个统计方法，极大似然原理的直观想法是，一个随机试验如有若干个可能的结果A，B，C，…，若在一次试验中，结果A出现了，那么可以认为实验条件对A的出现有利，也即出现的概率P(A)较大。极大似然原理的直观想法我们用下面例子说明。设甲箱中有99个白球，1个黑球；乙箱中有1个白球．99个黑球。现随机取出一箱，再从抽取的一箱中随机取出一球，结果是黑球，这一黑球从乙箱抽取的概率比从甲箱抽取的概率大得多，这时我们自然更多地相信这个黑球是取自乙箱的。一般说来，事件A发生的概率与某一未知参数$\theta$有关，$\theta$取值不同，则事件A发生的概率$P(A|\theta)$也不同，当我们在一次试验中事件A发生了，则认为此时的$\theta$值应是t的一切可能取值中使$P(A|\theta)$达到最大的那一个，极大似然估计法就是要选取这样的t值作为参数t的估计值，使所选取的样本在被选的总体中出现的可能性为最大 <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[《机器学习》周志华读书笔记]]></summary></entry><entry><title type="html">制作类RACE数据集</title><link href="http://localhost:4000/RACElike-datasets.html" rel="alternate" type="text/html" title="制作类RACE数据集" /><published>2021-12-21T00:00:00+08:00</published><updated>2021-12-21T00:00:00+08:00</updated><id>http://localhost:4000/RACElike-datasets</id><content type="html" xml:base="http://localhost:4000/RACElike-datasets.html"><![CDATA[<h1 id="目录">目录</h1>
<!-- TOC -->

<ul>
  <li><a href="#目录">目录</a></li>
  <li><a href="#race">RACE</a>
    <ul>
      <li><a href="#简介">简介</a></li>
      <li><a href="#race数据集格式">RACE数据集格式</a></li>
      <li><a href="#race数据集分布">RACE数据集分布</a></li>
      <li><a href="#race数据集中的长度">RACE数据集中的长度</a></li>
      <li><a href="#race数据集中的问题的统计信息">RACE数据集中的问题的统计信息</a></li>
    </ul>
  </li>
  <li><a href="#gaorace">GaoRACE</a>
    <ul>
      <li><a href="#gao他们对于race数据集的处理">Gao他们对于RACE数据集的处理</a></li>
      <li><a href="#gao处理后的race数据集统计信息">Gao处理后的RACE数据集统计信息</a></li>
      <li><a href="#gao处理后的数据集格式">Gao处理后的数据集格式</a>
        <ul>
          <li><a href="#预处理">预处理</a></li>
          <li><a href="#updated">updated</a></li>
          <li><a href="#预处理代码">预处理代码</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#mrc-阅读理解数据集">MRC 阅读理解数据集</a>
    <ul>
      <li><a href="#简介-1">简介</a></li>
      <li><a href="#title">Title</a></li>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#table-一张十分完整的表格">Table 一张十分完整的表格</a></li>
      <li><a href="#值得关注的地方">值得关注的地方</a></li>
    </ul>
  </li>
  <li><a href="#自制数据集">自制数据集</a>
    <ul>
      <li><a href="#大型题库">大型题库</a></li>
      <li><a href="#方法">方法</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->
<h1 id="race">RACE</h1>
<h2 id="简介">简介</h2>
<p>RACE数据集包含了中国初高中阅读理解题目，最初发布在2017年，一共含有28k短文和100k个问题，最开始发布的目的是为了<strong>阅读理解</strong>任务。它的特点是包含了很多需要推理的问题。</p>

<ul>
  <li>原RACE数据集<a href="http://www.cs.cmu.edu/~glai1/data/race/">地址</a></li>
  <li>下载地址<a href="http://www.cs.cmu.edu/~glai1/data/race/RACE.tar.gz">url</a></li>
  <li>论文地址：<a href="https://arxiv.org/abs/1704.04683">RACE: Large-scale ReAding Comprehension Dataset From Examinations</a></li>
</ul>

<h2 id="race数据集格式">RACE数据集格式</h2>
<p>Each passage is a JSON file. The JSON file contains following fields:</p>

<ol>
  <li>article: A string, which is the passage. 文章</li>
  <li>questions: A string list. Each string is a query. We have two types of questions. First one is an interrogative sentence. Another one has a placeholder, which is represented by _. 四个问题题干</li>
  <li>options: A list of the options list. Each options list contains 4 strings, which are the candidate option. 四个题目的四个选项</li>
  <li>answers: A list contains the golden label of each query.四个题目的正确答案</li>
  <li>id: Each passage has a unique id in this dataset.</li>
</ol>

<h2 id="race数据集分布">RACE数据集分布</h2>

<p><img src="../assets/img/posts/20211221/3.jpg" /></p>

<p>RACE-M表示初中题目，RACE-H表示高中题目</p>

<h2 id="race数据集中的长度">RACE数据集中的长度</h2>

<p><img src="../assets/img/posts/20211221/4.jpg" /></p>

<h2 id="race数据集中的问题的统计信息">RACE数据集中的问题的统计信息</h2>

<p><img src="../assets/img/posts/20211221/5.jpg" /></p>

<h1 id="gaorace">GaoRACE</h1>
<h2 id="gao他们对于race数据集的处理">Gao他们对于RACE数据集的处理</h2>
<ul>
  <li>去掉了那些误导选项和文章语义不相关的数据</li>
  <li>去掉了那些需要<code class="language-plaintext highlighter-rouge">world knowledge</code>生成的选项</li>
  <li>github<a href="https://github.com/Yifan-Gao/Distractor-Generation-RACE">url</a>,上面有预处理RACE数据集的代码</li>
</ul>

<h2 id="gao处理后的race数据集统计信息">Gao处理后的RACE数据集统计信息</h2>

<p><img src="../assets/img/posts/20211221/7.jpg" /></p>

<h2 id="gao处理后的数据集格式">Gao处理后的数据集格式</h2>

<h3 id="预处理">预处理</h3>

<p>首先把数据集规整到一个json文件里，分为dev,test,train三个json文件。</p>

<p>每一行包含以下信息：</p>

<p>article, sent(sentence), question(问题有两种，一种是疑问句，一种是填空), answer_text, answer, id, word_overlap_score, word_overlap_count, article_id, question_id, distractor_id.</p>

<p>那么一个问题会有2-3个误导选项，一篇文章又会有3-4个问题。相比于原本的数据集多了word-overlap指标，word-overlap就是词重叠率，交集比上并集。</p>

<h3 id="updated">updated</h3>
<p>updated数据集和original数据集格式类似，少了overlap，内容上去掉了一些语义不相关的题目。</p>

<h3 id="预处理代码">预处理代码</h3>
<p>利用torchtext框架预处理文本，流程大概如下：</p>
<ul>
  <li>定义Field：声明如何处理数据 定义</li>
  <li>Dataset：得到数据集，此时数据集里每一个样本是一个 经过 Field声明的预处理 预处理后的 wordlist</li>
  <li>建立vocab：在这一步建立词汇表，词向量(word embeddings)</li>
  <li>构造迭代器：构造迭代器，用来分批次训练模型</li>
</ul>

<p>Gao说有去掉一些语义不相关的误导选项，但是在代码中并没有看见这步操作？？</p>

<p><img src="../assets/img/posts/20211221/8.jpg" /></p>

<h1 id="mrc-阅读理解数据集">MRC 阅读理解数据集</h1>

<h2 id="简介-1">简介</h2>
<p>发现了一篇很好的综述，里面涵盖了2021年之前用到的所有MRC数据集。现在对这篇综述简单介绍一下</p>

<h2 id="title">Title</h2>
<p>English Machine Reading Comprehension Datasets: A Survey</p>

<h2 id="abstract">Abstract</h2>
<p>文献收集了60个英语阅读理解数据集，分别从不同维度进行比较，包括size, vocabulary, data source, method of creation, human performance level, first question word。调研发现维基百科是最多的数据来源，同时也发现了缺少很多why,when,where问题。</p>

<h2 id="table-一张十分完整的表格">Table 一张十分完整的表格</h2>

<p><img src="../assets/img/posts/20211221/44.jpg" /></p>

<p>首先我简单解释以下这个表格，这个表格一个收录了18个Multiple Choice Datasets,也就是说这18个数据集都着眼于多选题。</p>
<ul>
  <li>第一列是数据集的名称。</li>
  <li>第二列表示数据集中问题的个数(size)。</li>
  <li>第三列表示数据集中文章的来源，其中ER表示education resource, AG表示automatically generated即自动生成,CRW表示crowdsourcing。</li>
  <li>第四列表示答案的来源(answer)，其中UG表示user generated。</li>
  <li>第五列LB表示leader board available，即是否有排行榜，带*表示排行榜在<a href="https://paperswithcode.com/">网站</a>上发布。</li>
  <li>第六列表示人在该数据集上的表现。</li>
  <li>第七列表示该数据集是否有被解决，也就是说是否有比较好的模型能在该数据集上表现良好。</li>
  <li>第八列表示问题第一个单词出现最频繁的是哪个？比如what,how,which这样的单词。</li>
  <li>第九列PAD表示是否开源。</li>
</ul>

<h2 id="值得关注的地方">值得关注的地方</h2>
<p>这么多数据集中，来源于考试题目的有RACE,RACE-C,DREAM,ReClor,这些数据集的收集方法可以借鉴。</p>

<h1 id="自制数据集">自制数据集</h1>
<h2 id="大型题库">大型题库</h2>
<p>泸江，星火英语…</p>
<h2 id="方法">方法</h2>
<p>Python爬取网页</p>]]></content><author><name>Quehry</name></author><category term="work" /><summary type="html"><![CDATA[帮助学长制作RACE数据集]]></summary></entry><entry><title type="html">计算机图形学</title><link href="http://localhost:4000/Computer_Graphics.html" rel="alternate" type="text/html" title="计算机图形学" /><published>2021-12-21T00:00:00+08:00</published><updated>2021-12-21T00:00:00+08:00</updated><id>http://localhost:4000/Computer_Graphics</id><content type="html" xml:base="http://localhost:4000/Computer_Graphics.html"><![CDATA[<!-- TOC -->

<ul>
  <li><a href="#1-lecture-01-overview-of-computer-graphics">1. Lecture 01 Overview of Computer Graphics</a>
    <ul>
      <li><a href="#11-课程情况">1.1. 课程情况</a></li>
      <li><a href="#12-什么是好的画面">1.2. 什么是好的画面</a></li>
      <li><a href="#13-应用场景">1.3. 应用场景</a></li>
      <li><a href="#14-rasterization-光栅化">1.4. Rasterization 光栅化</a></li>
      <li><a href="#15-计算机视觉">1.5. 计算机视觉</a></li>
      <li><a href="#16-推荐书籍">1.6. 推荐书籍</a></li>
    </ul>
  </li>
  <li><a href="#2-lecture-02-review-of-linear-algebra">2. Lecture 02 Review of Linear Algebra</a>
    <ul>
      <li><a href="#21-图形学依赖学科">2.1. 图形学依赖学科</a></li>
      <li><a href="#22-向量">2.2. 向量</a></li>
      <li><a href="#23-矩阵">2.3. 矩阵</a></li>
    </ul>
  </li>
  <li><a href="#3-lecture-03-transformation">3. Lecture 03 Transformation</a>
    <ul>
      <li><a href="#31-why-transformation-为什么要变换">3.1. why transformation 为什么要变换</a></li>
      <li><a href="#32-d变换">3.2. D变换</a></li>
      <li><a href="#33-齐次坐标-homogeneous-coordinate">3.3. 齐次坐标 homogeneous coordinate</a></li>
    </ul>
  </li>
  <li><a href="#4-lecture-04-transformation-cont">4. Lecture 04 Transformation Cont.</a>
    <ul>
      <li><a href="#41-d-transformations">4.1. D Transformations</a></li>
      <li><a href="#42-view-transformation-视图变换">4.2. view transformation 视图变换</a></li>
      <li><a href="#43-projection-transformation-投影变换">4.3. projection transformation 投影变换</a></li>
    </ul>
  </li>
  <li><a href="#5-lecture05-rasterization-1triangles">5. Lecture05 Rasterization 1(Triangles)</a>
    <ul>
      <li><a href="#51-perspective-projection-透视投影">5.1. Perspective Projection 透视投影</a></li>
      <li><a href="#52-canonical-cube-to-screen-光栅化">5.2. Canonical Cube to Screen 光栅化</a></li>
      <li><a href="#53-different-raster-displays-不同的成像设备">5.3. Different Raster Displays 不同的成像设备</a></li>
      <li><a href="#54-三角形光栅化">5.4. 三角形光栅化</a></li>
    </ul>
  </li>
  <li><a href="#6-lecture-06-rasterization-2antialiasing-and-z-buffering">6. Lecture 06 Rasterization 2(Antialiasing and Z-Buffering)</a>
    <ul>
      <li><a href="#61-sampling-采样原理">6.1. sampling 采样原理</a></li>
      <li><a href="#62-frequency-domaine-信号处理频率">6.2. Frequency domaine 信号处理频率</a></li>
      <li><a href="#63-antialiasing-反走样抗锯齿">6.3. antialiasing 反走样/抗锯齿</a></li>
      <li><a href="#64-antialiasing-today-目前反走样的方法">6.4. antialiasing today 目前反走样的方法</a></li>
    </ul>
  </li>
  <li><a href="#7-lecture-07-shadingillumination-shading-and-graphics-pipeline">7. Lecture 07 Shading(Illumination, Shading, and Graphics Pipeline)</a>
    <ul>
      <li><a href="#71-painters-algorithm-画家算法">7.1. Painter’s Algorithm 画家算法</a></li>
      <li><a href="#72-z-buffer-深度缓存">7.2. Z-buffer 深度缓存</a></li>
      <li><a href="#73-目前为止学到了什么">7.3. 目前为止学到了什么</a></li>
      <li><a href="#74-shading-着色">7.4. shading 着色</a></li>
    </ul>
  </li>
  <li><a href="#8-shading-2shading-pipeline-texture-mapping">8. Shading 2(Shading, Pipeline, Texture Mapping)</a>
    <ul>
      <li><a href="#81-specular-term-高光项">8.1. Specular Term 高光项</a></li>
      <li><a href="#82-ambient-term-环境项">8.2. Ambient Term 环境项</a></li>
      <li><a href="#83-shading-frequencies-着色频率">8.3. Shading Frequencies 着色频率</a></li>
      <li><a href="#84-graphics-pipeline-图像管线实时渲染管线">8.4. Graphics Pipeline 图像管线/实时渲染管线</a></li>
      <li><a href="#85-texture-mapping-纹理映射">8.5. Texture Mapping 纹理映射</a></li>
    </ul>
  </li>
  <li><a href="#9-lecture-09-shading-3-texture-mapping">9. Lecture 09 Shading 3 (Texture Mapping)</a>
    <ul>
      <li><a href="#91-barycentric-coordinates重心坐标系">9.1. Barycentric Coordinates重心坐标系</a></li>
      <li><a href="#92-interpolate-插值">9.2. Interpolate 插值</a></li>
      <li><a href="#93-simple-texture-mapping-简单的纹理映射模型">9.3. Simple Texture Mapping 简单的纹理映射模型</a></li>
      <li><a href="#94-texture-magnification-纹理放大">9.4. Texture Magnification 纹理放大</a></li>
      <li><a href="#95-point-sampling-textures">9.5. Point Sampling Textures</a></li>
      <li><a href="#96-mipmap-范围查询">9.6. Mipmap 范围查询</a></li>
    </ul>
  </li>
  <li><a href="#10-lecture-10-geomrtry-1introduction">10. Lecture 10 Geomrtry 1(introduction)</a>
    <ul>
      <li><a href="#101-纹理的应用">10.1. 纹理的应用</a>
        <ul>
          <li><a href="#1011-environment-map-环境光映射">10.1.1. Environment Map 环境光映射</a></li>
          <li><a href="#1012-spherical-environment-map-球形环境光映射">10.1.2. Spherical Environment Map 球形环境光映射</a></li>
          <li><a href="#1013-纹理凹凸贴图bump-mapping">10.1.3. 纹理凹凸贴图bump mapping</a></li>
          <li><a href="#1014-位移贴图-displacement-mapping">10.1.4. 位移贴图 displacement mapping</a></li>
          <li><a href="#1015-三维纹理">10.1.5. 三维纹理</a></li>
        </ul>
      </li>
      <li><a href="#102-几何">10.2. 几何</a>
        <ul>
          <li><a href="#1021-分类">10.2.1. 分类</a></li>
          <li><a href="#1022-隐式几何">10.2.2. 隐式几何</a></li>
          <li><a href="#1023-显式几何">10.2.3. 显式几何</a></li>
          <li><a href="#1024-隐式的表达方式">10.2.4. 隐式的表达方式</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#11-lecture-11-geometry-2curves-and-surfaces">11. Lecture 11 Geometry 2(Curves and Surfaces)</a>
    <ul>
      <li><a href="#111-显式几何的表示方法">11.1. 显式几何的表示方法</a>
        <ul>
          <li><a href="#1111-point-cloud-点云">11.1.1. Point Cloud 点云</a></li>
          <li><a href="#1112-polygone-mesh">11.1.2. Polygone Mesh</a></li>
          <li><a href="#1113-一个例子">11.1.3. 一个例子</a></li>
        </ul>
      </li>
      <li><a href="#112-curves-曲线">11.2. Curves 曲线</a>
        <ul>
          <li><a href="#1121-贝塞尔曲线">11.2.1. 贝塞尔曲线</a></li>
          <li><a href="#1122-如何画一条贝塞尔曲线">11.2.2. 如何画一条贝塞尔曲线</a></li>
          <li><a href="#1123-piecewise-bézier-curves-逐段的贝塞尔曲线">11.2.3. Piecewise Bézier Curves 逐段的贝塞尔曲线</a></li>
          <li><a href="#1124-spline-样条">11.2.4. Spline 样条</a></li>
        </ul>
      </li>
      <li><a href="#113-曲面">11.3. 曲面</a>
        <ul>
          <li><a href="#1131-贝塞尔曲面">11.3.1. 贝塞尔曲面</a></li>
          <li><a href="#1132-曲面细分">11.3.2. 曲面细分</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#12-lecture-12-geometry-3">12. Lecture 12 Geometry 3</a>
    <ul>
      <li><a href="#121-mesh-subdivisionupsampling-网格细分">12.1. Mesh Subdivision(upsampling) 网格细分</a></li>
      <li><a href="#122-mesh-simplification-网格简化">12.2. Mesh Simplification 网格简化</a></li>
      <li><a href="#123-阴影-shadow-mapping">12.3. 阴影 Shadow mapping</a></li>
    </ul>
  </li>
  <li><a href="#13-lecture-13-ray-tracing-1">13. Lecture 13 Ray Tracing 1</a>
    <ul>
      <li><a href="#131-why-ray-tracing">13.1. Why ray tracing</a></li>
      <li><a href="#132-light-rays">13.2. Light Rays</a></li>
      <li><a href="#133-ray-casting-光线投射">13.3. Ray Casting 光线投射</a></li>
      <li><a href="#134-recursive-ray-tracing-递归光线追踪">13.4. Recursive Ray Tracing 递归光线追踪</a></li>
      <li><a href="#135-ray-surface-interaction-光线和表面相交">13.5. Ray-Surface interaction 光线和表面相交</a>
        <ul>
          <li><a href="#1351-ray-equation">13.5.1. Ray Equation</a></li>
          <li><a href="#1352-与圆相交的交点">13.5.2. 与圆相交的交点</a></li>
          <li><a href="#1353-intersection-with-implicit-surface">13.5.3. intersection with implicit surface</a></li>
          <li><a href="#1354-intersection-with-triangle-mesh">13.5.4. intersection with triangle mesh</a></li>
          <li><a href="#1355-accelerating-ray-surface-intersection">13.5.5. accelerating ray-surface intersection</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#14-lecture-14-ray-tracing-2">14. Lecture 14 Ray Tracing 2</a>
    <ul>
      <li><a href="#141-uniform-spatial-partitions-grids">14.1. Uniform Spatial Partitions (Grids)</a></li>
      <li><a href="#142-spatial-partitions-空间划分">14.2. Spatial Partitions 空间划分</a>
        <ul>
          <li><a href="#1421-一些划分示例">14.2.1. 一些划分示例</a></li>
          <li><a href="#1422-kd-tree">14.2.2. KD-Tree</a></li>
        </ul>
      </li>
      <li><a href="#143-object-partitions-物体划分">14.3. Object Partitions 物体划分</a>
        <ul>
          <li><a href="#1431-bounding-volume-hierarchybvh">14.3.1. Bounding Volume Hierarchy(BVH)</a></li>
          <li><a href="#1432-building-bvh">14.3.2. Building BVH</a></li>
          <li><a href="#1433-与空间划分的对比">14.3.3. 与空间划分的对比</a></li>
        </ul>
      </li>
      <li><a href="#144-whitted-style">14.4. Whitted style</a></li>
      <li><a href="#145-radiometry-辐射度量学">14.5. Radiometry 辐射度量学</a>
        <ul>
          <li><a href="#1451-一些物理量">14.5.1. 一些物理量</a></li>
          <li><a href="#1452-radiant-energy-and-flux">14.5.2. Radiant Energy and Flux</a></li>
          <li><a href="#1453-radiant-intensity">14.5.3. Radiant Intensity</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#15-lecture-15-ray-tracing">15. Lecture 15 Ray Tracing</a>
    <ul>
      <li><a href="#151-radiometry-cont-辐射度量学">15.1. Radiometry cont. 辐射度量学</a>
        <ul>
          <li><a href="#1511-继续上节课的内容">15.1.1. 继续上节课的内容</a></li>
          <li><a href="#1512-irradiance">15.1.2. Irradiance</a></li>
          <li><a href="#1513-radiance">15.1.3. Radiance</a></li>
        </ul>
      </li>
      <li><a href="#152-bidirectional-reflectance-distribution-function-brdf">15.2. Bidirectional Reflectance Distribution Function (BRDF)</a></li>
      <li><a href="#153-rendering-equation-渲染方程">15.3. Rendering Equation 渲染方程</a>
        <ul>
          <li><a href="#1531-如何理解渲染方程">15.3.1. 如何理解渲染方程</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#16-lecture-16-ray-tracing-4">16. Lecture 16 Ray Tracing 4</a>
    <ul>
      <li><a href="#161-monte-carlo-integration-蒙特卡洛积分">16.1. Monte Carlo Integration 蒙特卡洛积分</a></li>
      <li><a href="#162-path-tracing-路径追踪">16.2. Path Tracing 路径追踪</a>
        <ul>
          <li><a href="#1621-解渲染方程">16.2.1. 解渲染方程</a></li>
          <li><a href="#1622-最终的代码">16.2.2. 最终的代码</a></li>
        </ul>
      </li>
      <li><a href="#163-路径追踪">16.3. 路径追踪</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h1 id="1-lecture-01-overview-of-computer-graphics">1. Lecture 01 Overview of Computer Graphics</h1>
<h2 id="11-课程情况">1.1. 课程情况</h2>
<ul>
  <li>授课老师：闫令琪</li>
  <li>授课形式：网课（B站）</li>
</ul>

<h2 id="12-什么是好的画面">1.2. 什么是好的画面</h2>
<p>画面<strong>亮</strong></p>
<h2 id="13-应用场景">1.3. 应用场景</h2>
<p>电影，游戏，动画，设计，可视化，虚拟现实，增强现实，模拟，GUI图形用户接口。</p>

<p>电影中里程碑：阿凡达，大量应用面部捕捉技术。</p>
<h2 id="14-rasterization-光栅化">1.4. Rasterization 光栅化</h2>
<p>实时，FPS&gt;30</p>

<p>离线, FPS&lt;30</p>
<h2 id="15-计算机视觉">1.5. 计算机视觉</h2>
<p>计算机图形学离不开计算机视觉，但是视觉一般是对图像的处理。</p>

<h2 id="16-推荐书籍">1.6. 推荐书籍</h2>
<p>Tiger虎书</p>

<h1 id="2-lecture-02-review-of-linear-algebra">2. Lecture 02 Review of Linear Algebra</h1>
<h2 id="21-图形学依赖学科">2.1. 图形学依赖学科</h2>
<p>Optics, Mechanics, Linear algebra, statics, Singal processing, numerical analysis数值分析</p>

<h2 id="22-向量">2.2. 向量</h2>

<p>向量的定义</p>

<p><img src="../assets/img/posts/20211221/9.jpg" /></p>

<p>单位向量</p>

<p><img src="../assets/img/posts/20211221/10.jpg" /></p>

<p>向量计算，向量加法</p>

<p><img src="../assets/img/posts/20211221/11.jpg" /></p>

<p>用笛卡尔坐标系表示向量</p>

<p><img src="../assets/img/posts/20211221/12.jpg" /></p>

<p>向量乘法，点乘和叉乘，点乘在笛卡尔坐标系中就是对应元素相乘。</p>

<p>在图形学中，点乘是为了寻找两个向量的夹角(夹角可以判断两个向量方向的接近程度)，或者获得一个向量在另一个向量的投影，还可以获得向量的分解。</p>

<p><img src="../assets/img/posts/20211221/13.jpg" /></p>

<p>叉乘，叉积结果垂直于这两个向量所在的平面，满足右手定则。向量的叉乘可以写成矩阵形式。</p>

<p>在图形学中的应用：判断左右关系，比如a^b&gt;0，说明b在a的左边。还可以判断内外，比如判断一个点是否在一个三角形内。</p>

<p><img src="../assets/img/posts/20211221/14.jpg" /></p>

<p>坐标系的定义，右手坐标系</p>

<p><img src="../assets/img/posts/20211221/15.jpg" /></p>

<h2 id="23-矩阵">2.3. 矩阵</h2>

<p>矩阵定义</p>

<p><img src="../assets/img/posts/20211221/16.jpg" /></p>

<p>矩阵乘法</p>

<p><img src="../assets/img/posts/20211221/17.jpg" /></p>

<p>矩阵乘法没有交换律，但是有结合律</p>

<p>矩阵转置，矩阵的逆</p>

<p>向量的点乘和叉乘都可以写成矩阵乘法形式</p>

<p><img src="../assets/img/posts/20211221/18.jpg" /></p>

<h1 id="3-lecture-03-transformation">3. Lecture 03 Transformation</h1>

<h2 id="31-why-transformation-为什么要变换">3.1. why transformation 为什么要变换</h2>
<p>viewing: 3D to 2D projection</p>

<h2 id="32-d变换">3.2. D变换</h2>
<ul>
  <li>缩放 scale transform</li>
</ul>

<p><img src="../assets/img/posts/20211221/19.jpg" /></p>

<ul>
  <li>非均匀缩放 scale(non-uniform)</li>
</ul>

<p><img src="../assets/img/posts/20211221/20.jpg" /></p>

<ul>
  <li>翻转 reflection matrix</li>
</ul>

<p><img src="../assets/img/posts/20211221/21.jpg" /></p>

<ul>
  <li>切变 shear matrix</li>
</ul>

<p>竖直方向上没有变化，水平方向上发生了变化</p>

<p><img src="../assets/img/posts/20211221/22.jpg" /></p>

<ul>
  <li>旋转 Rotate</li>
</ul>

<p>旋转默认绕零点逆时针旋转</p>

<p><img src="../assets/img/posts/20211221/23.jpg" /></p>

<p>二维旋转矩阵R</p>

<p>上述所有的变化都可以写成x$\prime$=Mx，也就是线性变换</p>

<h2 id="33-齐次坐标-homogeneous-coordinate">3.3. 齐次坐标 homogeneous coordinate</h2>

<ul>
  <li>
    <p>为什么要引入齐次坐标，因为对于简单的平移操作并不能写成线性变换的形式，但是人们也不想认为平移是一种特殊的变换，所以引入齐次坐标</p>
  </li>
  <li>
    <p>齐次坐标</p>
  </li>
</ul>

<p>注意点和向量的表示方法不同</p>

<p><img src="../assets/img/posts/20211221/24.jpg" /></p>

<ul>
  <li>仿射变换 affine transformations</li>
</ul>

<p><img src="../assets/img/posts/20211221/25.jpg" /></p>

<ul>
  <li>2D Transformations</li>
</ul>

<p><img src="../assets/img/posts/20211221/26.jpg" /></p>

<ul>
  <li>
    <p>逆变换就是乘以逆矩阵</p>
  </li>
  <li>
    <p>复杂的变换都是简单的变换的组合，变换的组合顺序很重要</p>
  </li>
  <li>
    <p>绕着某一个点（非原点）旋转的分解</p>
  </li>
</ul>

<p><img src="../assets/img/posts/20211221/27.jpg" /></p>

<h1 id="4-lecture-04-transformation-cont">4. Lecture 04 Transformation Cont.</h1>

<h2 id="41-d-transformations">4.1. D Transformations</h2>

<ul>
  <li>齐次坐标</li>
</ul>

<p>对于w不等于1，每一个坐标除以w</p>

<p><img src="../assets/img/posts/20211221/28.jpg" /></p>

<ul>
  <li>正交矩阵</li>
</ul>

<p>一个矩阵的逆等于矩阵的转置，旋转矩阵就是一个正交矩阵</p>

<ul>
  <li>仿射变换（旋转+平移）</li>
</ul>

<p>仿射变换是先进行旋转再进行平移</p>

<p><img src="../assets/img/posts/20211221/29.jpg" /></p>

<ul>
  <li>矩阵表示（缩放，平移）</li>
</ul>

<p><img src="../assets/img/posts/20211221/30.jpg" /></p>

<ul>
  <li>旋转</li>
</ul>

<p>绕着某一个轴旋转</p>

<p><img src="../assets/img/posts/20211221/31.jpg" /></p>

<p>一般的旋转（分解成三个坐标轴的旋转）</p>

<p><img src="../assets/img/posts/20211221/32.jpg" /></p>

<p>Rodrigues’ Rotation Formula, 用向量n表示旋转轴，最终推出这个公式</p>

<p><img src="../assets/img/posts/20211221/33.jpg" /></p>

<h2 id="42-view-transformation-视图变换">4.2. view transformation 视图变换</h2>

<ul>
  <li>
    <p>观测变换viewing，包括了视图变化和投影变化</p>
  </li>
  <li>
    <p>MVP变换(model-&gt;view-&gt;projection)</p>
  </li>
</ul>

<p><img src="../assets/img/posts/20211221/34.jpg" /></p>

<ul>
  <li>view transformation(不等于viewing) 视图变换</li>
</ul>

<p>视图变换是把相机放到标准位置上，located at origin, look at -Z</p>

<p><img src="../assets/img/posts/20211221/35.jpg" /></p>

<p>利用逆变换，先平移再旋转</p>

<p><img src="../assets/img/posts/20211221/36.jpg" /></p>

<p>一般把model和view变换统称为view transformation</p>

<h2 id="43-projection-transformation-投影变换">4.3. projection transformation 投影变换</h2>
<ul>
  <li>orthographic vs perspectiive projection</li>
</ul>

<p><img src="../assets/img/posts/20211221/37.jpg" /></p>

<ul>
  <li>orthographic projection 正交投影</li>
</ul>

<p><img src="../assets/img/posts/20211221/38.jpg" /></p>

<p>平移，缩放（不考虑旋转）</p>

<p><img src="../assets/img/posts/20211221/39.jpg" /></p>

<ul>
  <li>perspective projection 透视投影</li>
</ul>

<p>满足近大远小</p>

<p>透视投影就是先把物体挤压成立方体，然后对立方体进行正交投影</p>

<p><img src="../assets/img/posts/20211221/41.jpg" /></p>

<p><img src="../assets/img/posts/20211221/40.jpg" /></p>

<p><img src="../assets/img/posts/20211221/42.jpg" /></p>

<p><img src="../assets/img/posts/20211221/43.jpg" /></p>

<h1 id="5-lecture05-rasterization-1triangles">5. Lecture05 Rasterization 1(Triangles)</h1>

<h2 id="51-perspective-projection-透视投影">5.1. Perspective Projection 透视投影</h2>
<ul>
  <li>首先是对上节课的透视投影的一些补充, 其中l=left, r=right, b=bottom, t=top, n=near, f=far，这些量可以描述视锥Frustum</li>
</ul>

<center><img src="../assets/img/posts/20211221/45.jpg" /></center>

<ul>
  <li>视锥Frustum的描述还可以用fovY(field of view)垂直视角和aspect ratio宽高比</li>
</ul>

<center><img src="../assets/img/posts/20211221/46.jpg" /></center>

<h2 id="52-canonical-cube-to-screen-光栅化">5.2. Canonical Cube to Screen 光栅化</h2>
<ul>
  <li>
    <p>把物体的数学描述以及与物体相关的颜色信息转换为屏幕上用于对应位置的像素及用于填充像素的颜色，这个过程称为光栅化。</p>
  </li>
  <li>
    <p>屏幕是最常见的光栅设备，每一个像素都是一个小方块，像素是最小的单位，一个像素的颜色可以用rgb三种颜色表示</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/47.jpg" /></center>

<ul>
  <li>屏幕空间screen space</li>
</ul>

<center><img src="../assets/img/posts/20211221/48.jpg" /></center>

<ul>
  <li>把之前投影后的小方块变成屏幕空间</li>
</ul>

<center><img src="../assets/img/posts/20211221/49.jpg" /></center>

<center><img src="../assets/img/posts/20211221/50.jpg" /></center>

<h2 id="53-different-raster-displays-不同的成像设备">5.3. Different Raster Displays 不同的成像设备</h2>
<ul>
  <li>
    <p>Oscilloscope 示波器</p>
  </li>
  <li>
    <p>Cathode Ray Tube 阴极射线管成像原理。早期电视屏幕就是这样实现成像，扫描成像。</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/51.jpg" /></center>

<ul>
  <li>Frame Buffer: Memory for a Raster Display 内存中的一块区域存储图像信息。</li>
</ul>

<center><img src="../assets/img/posts/20211221/52.jpg" /></center>

<ul>
  <li>LCD(liquid crystal display)液晶显示器，光的波动性原理。</li>
</ul>

<center><img src="../assets/img/posts/20211221/53.jpg" /></center>

<ul>
  <li>LED发光二极管</li>
</ul>

<center><img src="../assets/img/posts/20211221/54.jpg" /></center>

<h2 id="54-三角形光栅化">5.4. 三角形光栅化</h2>
<ul>
  <li>三角形是最基本的多边形，有很多好的性质。</li>
</ul>

<center><img src="../assets/img/posts/20211221/55.jpg" /></center>

<ul>
  <li>sampling 采样。三角形离散化。</li>
</ul>

<center><img src="../assets/img/posts/20211221/56.jpg" /></center>

<center><img src="../assets/img/posts/20211221/57.jpg" /></center>

<p>在不同的像素中心，确定是0还是1,表示在三角形里还是外</p>

<center><img src="../assets/img/posts/20211221/58.jpg" /></center>

<ul>
  <li>如何判断点和三角形关系，利用叉积，边界上的点自己定义。</li>
</ul>

<center><img src="../assets/img/posts/20211221/59.jpg" /></center>

<center><img src="../assets/img/posts/20211221/60.jpg" /></center>

<ul>
  <li>jaggies锯齿，走样aliasing</li>
</ul>

<center><img src="../assets/img/posts/20211221/61.jpg" /></center>

<center><img src="../assets/img/posts/20211221/62.jpg" /></center>

<h1 id="6-lecture-06-rasterization-2antialiasing-and-z-buffering">6. Lecture 06 Rasterization 2(Antialiasing and Z-Buffering)</h1>

<h2 id="61-sampling-采样原理">6.1. sampling 采样原理</h2>
<ul>
  <li>视频就是对时间进行采样</li>
  <li>采样的artifact(瑕疵)：锯齿，摩尔纹，轮胎效应(在时间上采样)</li>
</ul>

<center><img src="../assets/img/posts/20211221/63.jpg" /></center>

<ul>
  <li>反走样采样：可以对原始的图像进行滤波(模糊处理)然后再采样。</li>
</ul>

<center><img src="../assets/img/posts/20211221/64.jpg" /></center>

<ul>
  <li>采样速度跟不上信号变化的速度就会走样(aliasing)</li>
</ul>

<h2 id="62-frequency-domaine-信号处理频率">6.2. Frequency domaine 信号处理频率</h2>
<ul>
  <li>傅里叶变换：所有的周期函数都可以写成不同平吕的正弦函数的组合。傅里叶变换就是频域和时域/空间域的变换</li>
</ul>

<center><img src="../assets/img/posts/20211221/66.jpg" /></center>

<ul>
  <li>走样的原因(时域)：高频信号欠采样，高频信号和低频信号在某一采样速度下没有差别，就会产生走样</li>
</ul>

<center><img src="../assets/img/posts/20211221/65.jpg" /></center>

<center><img src="../assets/img/posts/20211221/67.jpg" /></center>

<ul>
  <li>
    <p>滤波：抹掉特定的频率。比如高通滤波(过滤到低频信号)</p>
  </li>
  <li>
    <p>卷积：图形学上的简化定义，见下图</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/68.jpg" /></center>

<ul>
  <li>卷积定律：时域上的卷积等于频域上的乘积</li>
</ul>

<center><img src="../assets/img/posts/20211221/69.jpg" /></center>

<ul>
  <li>采样：重复频域上的内容</li>
</ul>

<center><img src="../assets/img/posts/20211221/70.jpg" /></center>

<ul>
  <li>走样在频率上的解释：采样频率小会让频域上发生重叠</li>
</ul>

<center><img src="../assets/img/posts/20211221/71.jpg" /></center>

<h2 id="63-antialiasing-反走样抗锯齿">6.3. antialiasing 反走样/抗锯齿</h2>

<ul>
  <li>
    <p>第一种解决方法：增加采样率，相当于增加了频域上的两个信号的距离</p>
  </li>
  <li>
    <p>第二种解决方法：反走样。即先对信号进行滤波再采样</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/72.jpg" /></center>

<ul>
  <li>比如对于之前三角形的问题</li>
</ul>

<center><img src="../assets/img/posts/20211221/73.jpg" /></center>

<ul>
  <li>但是这种反走样的方法比较复杂，有一种更简单的近似方法(对滤波这一步的近似)：supersampling，就是在对每个像素点变成更多的小点</li>
</ul>

<center><img src="../assets/img/posts/20211221/74.jpg" /></center>

<h2 id="64-antialiasing-today-目前反走样的方法">6.4. antialiasing today 目前反走样的方法</h2>
<p>介绍了两种新的抗锯齿的操作：FXAA和TAA。FXAA的做法是把边界找到然后对边界进行处理。</p>

<center><img src="../assets/img/posts/20211221/75.jpg" /></center>

<h1 id="7-lecture-07-shadingillumination-shading-and-graphics-pipeline">7. Lecture 07 Shading(Illumination, Shading, and Graphics Pipeline)</h1>

<h2 id="71-painters-algorithm-画家算法">7.1. Painter’s Algorithm 画家算法</h2>
<ul>
  <li>首先画出远处的物体，然后再画近处的物体。画近处的物体再覆盖远处的物体。</li>
  <li>需要定义深度信息，根据深度信息排序</li>
</ul>

<h2 id="72-z-buffer-深度缓存">7.2. Z-buffer 深度缓存</h2>
<ul>
  <li>对每个像素都有最小的z值，除了一个frame buffer储存颜色信息外，还需要z-buffer储存深度信息。</li>
</ul>

<center><img src="../assets/img/posts/20211221/76.jpg" /></center>

<center><img src="../assets/img/posts/20211221/77.jpg" /></center>

<ul>
  <li>
    <p>假设每个像素最开始的时候深度为无限远</p>
  </li>
  <li>
    <p>特点是在像素维度进行操作</p>
  </li>
</ul>

<h2 id="73-目前为止学到了什么">7.3. 目前为止学到了什么</h2>

<center><img src="../assets/img/posts/20211221/78.jpg" /></center>

<h2 id="74-shading-着色">7.4. shading 着色</h2>
<ul>
  <li>
    <p>着色：对不同物体应用不同的材质</p>
  </li>
  <li>
    <p>一个简单的着色模型(Blinn-Phong Reflection model)</p>
  </li>
  <li>
    <p>局部着色，不考虑阴影</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/79.jpg" /></center>

<ul>
  <li>diffuse reflection 漫反射，一个物体有多亮与接收到多少光的能量有关。点光源的能量随距离缩减。在点光源的光线到达物体表面时被物体接受多少能量又与光线和法线的夹角的cos值有关，也就是说直射时接受的能量最大(相同距离)。漫反射表示不论观测角度在哪，你观测到的亮度应该是一样的。</li>
</ul>

<center><img src="../assets/img/posts/20211221/80.jpg" /></center>

<h1 id="8-shading-2shading-pipeline-texture-mapping">8. Shading 2(Shading, Pipeline, Texture Mapping)</h1>
<h2 id="81-specular-term-高光项">8.1. Specular Term 高光项</h2>
<ul>
  <li>着色包括三部分：漫反射，高光，环境光</li>
  <li>高光就是观测方向和镜面反射方向相同，即半程向量是否和法向量接近</li>
</ul>

<center><img src="../assets/img/posts/20211221/81.jpg" /></center>

<ul>
  <li>通常高光都是白色的</li>
</ul>

<h2 id="82-ambient-term-环境项">8.2. Ambient Term 环境项</h2>
<ul>
  <li>
    <p>环境光就是一些其他物体反射的光照亮背光物体</p>
  </li>
  <li>
    <p>这里介绍非常简化的模型</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/82.jpg" /></center>

<ul>
  <li>最终结果</li>
</ul>

<center><img src="../assets/img/posts/20211221/83.jpg" /></center>

<h2 id="83-shading-frequencies-着色频率">8.3. Shading Frequencies 着色频率</h2>
<ul>
  <li>
    <p>之前介绍的着色是应用在着色点，对应在屏幕空间是如何的呢？</p>
  </li>
  <li>
    <p>第一种：Shading ecah triangle 对每个三角形着色</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/84.jpg" /></center>

<ul>
  <li>第二种：shading each vertex 对顶点着色，然后插值</li>
</ul>

<center><img src="../assets/img/posts/20211221/85.jpg" /></center>

<ul>
  <li>第三种：shading each pixel 对每个像素点着色</li>
</ul>

<center><img src="../assets/img/posts/20211221/86.jpg" /></center>

<ul>
  <li>如何定义顶点的法向量呢？对周围的面的法向量求平均</li>
</ul>

<center><img src="../assets/img/posts/20211221/87.jpg" /></center>

<ul>
  <li>如何定义像素的法向量？</li>
</ul>

<center><img src="../assets/img/posts/20211221/88.jpg" /></center>

<h2 id="84-graphics-pipeline-图像管线实时渲染管线">8.4. Graphics Pipeline 图像管线/实时渲染管线</h2>
<ul>
  <li>一个实时渲染的流程/流水线</li>
</ul>

<center><img src="../assets/img/posts/20211221/89.jpg" /></center>

<ul>
  <li>现代的GPU允许写入顶点着色部分与片段着色部分的代码</li>
</ul>

<h2 id="85-texture-mapping-纹理映射">8.5. Texture Mapping 纹理映射</h2>
<ul>
  <li>
    <p>希望在物体的不同位置定义不同的属性，比如漫反射系数等等</p>
  </li>
  <li>
    <p>3维物体的表现都是一个平面</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/90.jpg" /></center>

<ul>
  <li>纹理映射就是对于一个平面定义不同的属性，有一个映射关系</li>
</ul>

<center><img src="../assets/img/posts/20211221/91.jpg" /></center>

<ul>
  <li>纹理也有坐标系</li>
</ul>

<center><img src="../assets/img/posts/20211221/92.jpg" /></center>

<h1 id="9-lecture-09-shading-3-texture-mapping">9. Lecture 09 Shading 3 (Texture Mapping)</h1>

<h2 id="91-barycentric-coordinates重心坐标系">9.1. Barycentric Coordinates重心坐标系</h2>

<center><img src="../assets/img/posts/20211221/93.jpg" /></center>

<h2 id="92-interpolate-插值">9.2. Interpolate 插值</h2>
<ul>
  <li>重心坐标系插值</li>
</ul>

<center><img src="../assets/img/posts/20211221/94.jpg" /></center>

<h2 id="93-simple-texture-mapping-简单的纹理映射模型">9.3. Simple Texture Mapping 简单的纹理映射模型</h2>

<center><img src="../assets/img/posts/20211221/95.jpg" /></center>

<h2 id="94-texture-magnification-纹理放大">9.4. Texture Magnification 纹理放大</h2>

<center><img src="../assets/img/posts/20211221/96.jpg" /></center>

<h2 id="95-point-sampling-textures">9.5. Point Sampling Textures</h2>
<ul>
  <li>就是走样问题</li>
</ul>

<center><img src="../assets/img/posts/20211221/97.jpg" /></center>

<h2 id="96-mipmap-范围查询">9.6. Mipmap 范围查询</h2>
<ul>
  <li>生成不同分辨率的图片</li>
</ul>

<center><img src="../assets/img/posts/20211221/98.jpg" /></center>

<ul>
  <li>
    <p>任何一个像素可以映射到纹理区域的一个点，mipmap可以让像素点快速查阅，因为他又很多层，不同的纹理区域的面积对应不同的层</p>
  </li>
  <li>
    <p>mipmap也不是最好的方法，只是一种折中的办法</p>
  </li>
  <li>
    <p>anisotropic filtering 各向异性过滤</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/99.jpg" /></center>

<h1 id="10-lecture-10-geomrtry-1introduction">10. Lecture 10 Geomrtry 1(introduction)</h1>
<h2 id="101-纹理的应用">10.1. 纹理的应用</h2>
<h3 id="1011-environment-map-环境光映射">10.1.1. Environment Map 环境光映射</h3>
<ul>
  <li>纹理可以用来映射环境光</li>
</ul>

<center><img src="../assets/img/posts/20211221/100.jpg" /></center>

<ul>
  <li>假设环境光来自无限远</li>
</ul>

<h3 id="1012-spherical-environment-map-球形环境光映射">10.1.2. Spherical Environment Map 球形环境光映射</h3>
<ul>
  <li>将环境光信息存在球上</li>
</ul>

<center><img src="../assets/img/posts/20211221/101.jpg" /></center>

<ul>
  <li>但是在边缘部分会有扭曲，解决方法有环境光存在正方体上</li>
</ul>

<h3 id="1013-纹理凹凸贴图bump-mapping">10.1.3. 纹理凹凸贴图bump mapping</h3>
<ul>
  <li>
    <p>纹理不仅可以表示颜色，还可以应用一个复杂的纹理来定义高度，也就改变了法线的方向</p>
  </li>
  <li>
    <p>凹凸贴图只增加表面细节，不添加新的三角形</p>
  </li>
</ul>

<h3 id="1014-位移贴图-displacement-mapping">10.1.4. 位移贴图 displacement mapping</h3>
<ul>
  <li>和凹凸贴图很像，但是移动了顶点</li>
</ul>

<center><img src="../assets/img/posts/20211221/102.jpg" /></center>

<h3 id="1015-三维纹理">10.1.5. 三维纹理</h3>
<ul>
  <li>
    <p>定义了空间中任意一个点的纹理坐标</p>
  </li>
  <li>
    <p>广泛应用于体积渲染</p>
  </li>
</ul>

<h2 id="102-几何">10.2. 几何</h2>
<h3 id="1021-分类">10.2.1. 分类</h3>
<ul>
  <li>隐式几何</li>
  <li>显式几何</li>
</ul>

<h3 id="1022-隐式几何">10.2.2. 隐式几何</h3>
<ul>
  <li>不给出点的具体坐标，而是给出点的坐标关系，比如$x^2+y^2+z^2=1$</li>
  <li>推广到一般形式, $f(x,y,z)=0$</li>
  <li>缺点：不直观，不好采样</li>
  <li>优点：可以很容易的判断点在不在几何体内</li>
</ul>

<h3 id="1023-显式几何">10.2.3. 显式几何</h3>
<ul>
  <li>直接给出或者参数映射的方式给出</li>
</ul>

<center><img src="../assets/img/posts/20211221/103.jpg" /></center>

<ul>
  <li>优点：采样方便，直观</li>
  <li>缺点：不好判断点是否在几何体内还是外</li>
</ul>

<h3 id="1024-隐式的表达方式">10.2.4. 隐式的表达方式</h3>
<ul>
  <li>公式定义</li>
</ul>

<center><img src="../assets/img/posts/20211221/104.jpg" /></center>

<ul>
  <li>通过几何体的布尔组合，目前有很多建模软件就是这么表示的</li>
</ul>

<center><img src="../assets/img/posts/20211221/105.jpg" /></center>

<ul>
  <li>距离函数定义，SDF有向距离场</li>
</ul>

<center><img src="../assets/img/posts/20211221/106.jpg" /></center>

<h1 id="11-lecture-11-geometry-2curves-and-surfaces">11. Lecture 11 Geometry 2(Curves and Surfaces)</h1>
<h2 id="111-显式几何的表示方法">11.1. 显式几何的表示方法</h2>

<h3 id="1111-point-cloud-点云">11.1.1. Point Cloud 点云</h3>
<ul>
  <li>点的集合</li>
  <li>优点：可以表示任何几何体</li>
</ul>

<h3 id="1112-polygone-mesh">11.1.2. Polygone Mesh</h3>
<ul>
  <li>使用顶点和图形表示(三角形，正方形)</li>
</ul>

<h3 id="1113-一个例子">11.1.3. 一个例子</h3>

<center><img src="../assets/img/posts/20211221/107.jpg" /></center>

<p>里面定义了顶点坐标，法线，纹理坐标和哪几个点组成一个三角形</p>

<h2 id="112-curves-曲线">11.2. Curves 曲线</h2>
<h3 id="1121-贝塞尔曲线">11.2.1. 贝塞尔曲线</h3>
<ul>
  <li>用一系列控制点定义曲线</li>
</ul>

<center><img src="../assets/img/posts/20211221/108.jpg" /></center>

<ul>
  <li>曲线不一定要经过控制点</li>
</ul>

<h3 id="1122-如何画一条贝塞尔曲线">11.2.2. 如何画一条贝塞尔曲线</h3>
<ul>
  <li>Casteljau Algorithm：这个算法的核心是画出每个时间t的点的位置(递归)</li>
</ul>

<center><img src="../assets/img/posts/20211221/109.jpg" /></center>

<p>其中$b_0^2$就是时间t的点的位置</p>

<ul>
  <li>大致流程</li>
</ul>

<center><img src="../assets/img/posts/20211221/110.jpg" /></center>

<ul>
  <li>代数形式</li>
</ul>

<center><img src="../assets/img/posts/20211221/111.jpg" /></center>

<ul>
  <li>生成的曲线只能在控制点的凸包内</li>
</ul>

<h3 id="1123-piecewise-bézier-curves-逐段的贝塞尔曲线">11.2.3. Piecewise Bézier Curves 逐段的贝塞尔曲线</h3>

<ul>
  <li>每四个控制点定义一条贝塞尔曲线</li>
</ul>

<center><img src="../assets/img/posts/20211221/112.jpg" /></center>

<ul>
  <li>C0连续(点连续)，C1连续(切线连续)</li>
</ul>

<h3 id="1124-spline-样条">11.2.4. Spline 样条</h3>
<ul>
  <li>样条是用一系列的点画出线条</li>
</ul>

<center><img src="../assets/img/posts/20211221/113.jpg" /></center>

<h2 id="113-曲面">11.3. 曲面</h2>
<h3 id="1131-贝塞尔曲面">11.3.1. 贝塞尔曲面</h3>
<ul>
  <li>使用贝塞尔曲线生成贝塞尔曲面</li>
</ul>

<center><img src="../assets/img/posts/20211221/114.jpg" /></center>

<ul>
  <li>竖直方向生成四条曲线，然后对于t来说四个点再作为控制前生成曲线</li>
</ul>

<center><img src="../assets/img/posts/20211221/115.jpg" /></center>

<h3 id="1132-曲面细分">11.3.2. 曲面细分</h3>
<ul>
  <li>使用很多三角形网格来表示曲面</li>
</ul>

<center><img src="../assets/img/posts/20211221/116.jpg" /></center>

<h1 id="12-lecture-12-geometry-3">12. Lecture 12 Geometry 3</h1>
<h2 id="121-mesh-subdivisionupsampling-网格细分">12.1. Mesh Subdivision(upsampling) 网格细分</h2>
<ul>
  <li>引入更多三角形，微调它们的位置</li>
  <li>Loop Subdivision：第一步增加三角形的数量，第二部调整三角形的位置</li>
</ul>

<center><img src="../assets/img/posts/20211221/117.jpg" /></center>

<ul>
  <li>Loop细分规则：</li>
</ul>

<center><img src="../assets/img/posts/20211221/118.jpg" /></center>

<ul>
  <li>另一种细分规则：Catmull-Clark Subdivision</li>
</ul>

<p>奇异点是这个点的度不是4的点(就是连接的边数不等于4)</p>

<center><img src="../assets/img/posts/20211221/119.jpg" /></center>

<center><img src="../assets/img/posts/20211221/120.jpg" /></center>

<ul>
  <li>这种细分方法可以用于任何面</li>
</ul>

<h2 id="122-mesh-simplification-网格简化">12.2. Mesh Simplification 网格简化</h2>
<ul>
  <li>
    <p>基本思路是为了减少网格数目但是保持它的基本形状</p>
  </li>
  <li>
    <p>一种方法：Collapsing an edge 边坍缩。删除一些点</p>
  </li>
  <li>
    <p>判断标准：quadric error metrics 二次误差度量</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/121.jpg" /></center>

<ul>
  <li>实际效果</li>
</ul>

<center><img src="../assets/img/posts/20211221/122.jpg" /></center>

<h2 id="123-阴影-shadow-mapping">12.3. 阴影 Shadow mapping</h2>
<ul>
  <li>
    <p>光栅化着色的时候是局部的，但是有时候会有问题，比如有东西挡在shading point和光源之间时，所以需要在这种情况下生成阴影</p>
  </li>
  <li>
    <p>光栅化生成阴影的方法叫做shadow mapping</p>
  </li>
  <li>
    <p>shadow mapping 的两步</p>
  </li>
  <li>
    <p>第一步：从光源出发，看向shading point，记录能看见的点的深度</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/123.jpg" /></center>

<ul>
  <li>第二步：从摄像机出发，看向shading point，如果看见的点的深度和光源所看见的深度相同，那么这个点不在阴影内，否则，它在阴影内。</li>
</ul>

<center><img src="../assets/img/posts/20211221/124.jpg" /></center>

<ul>
  <li>具体的例子：</li>
</ul>

<center><img src="../assets/img/posts/20211221/125.jpg" /></center>

<ul>
  <li>问题：走样，阴影分辨率，只能做硬阴影(hard shadow)…</li>
</ul>

<center><img src="../assets/img/posts/20211221/126.jpg" /></center>

<h1 id="13-lecture-13-ray-tracing-1">13. Lecture 13 Ray Tracing 1</h1>
<h2 id="131-why-ray-tracing">13.1. Why ray tracing</h2>
<ul>
  <li>光栅化的缺点：无法表示全局的光照、毛玻璃效果无法很好表示、阴影处理不算好</li>
  <li>光纤追踪很精准但是比较慢，经常做离线(电影制作)</li>
</ul>

<h2 id="132-light-rays">13.2. Light Rays</h2>
<ul>
  <li>光线沿直线传播</li>
  <li>光线不会交叉</li>
  <li>光线是不断折回然后打到人眼</li>
  <li>光路可逆性</li>
</ul>

<h2 id="133-ray-casting-光线投射">13.3. Ray Casting 光线投射</h2>
<ul>
  <li>从眼睛到像素点出发，到虚拟世界，再到光源(Local)</li>
</ul>

<center><img src="../assets/img/posts/20211221/127.jpg" /></center>

<ul>
  <li>从眼睛到像素点到虚拟世界的线叫做eye ray</li>
</ul>

<h2 id="134-recursive-ray-tracing-递归光线追踪">13.4. Recursive Ray Tracing 递归光线追踪</h2>
<ul>
  <li>如果在shading point 处可以折射，能量损失，则继续折射然后对每个点都算着色值</li>
</ul>

<center><img src="../assets/img/posts/20211221/128.jpg" /></center>

<ul>
  <li>对每个点都要计算是否处在阴影中</li>
</ul>

<h2 id="135-ray-surface-interaction-光线和表面相交">13.5. Ray-Surface interaction 光线和表面相交</h2>
<h3 id="1351-ray-equation">13.5.1. Ray Equation</h3>

<center><img src="../assets/img/posts/20211221/129.jpg" /></center>

<h3 id="1352-与圆相交的交点">13.5.2. 与圆相交的交点</h3>

<center><img src="../assets/img/posts/20211221/130.jpg" /></center>

<ul>
  <li>一个交点就是相切，两个交点就是相交</li>
</ul>

<h3 id="1353-intersection-with-implicit-surface">13.5.3. intersection with implicit surface</h3>
<ul>
  <li>与隐式表面相交</li>
</ul>

<center><img src="../assets/img/posts/20211221/131.jpg" /></center>

<h3 id="1354-intersection-with-triangle-mesh">13.5.4. intersection with triangle mesh</h3>
<ul>
  <li>
    <p>也就是与显式表面(三角形网格)相交</p>
  </li>
  <li>
    <p>第一种想法就是光线与每个三角形进行计算，但这样计算量太大</p>
  </li>
  <li>
    <p>第二种想法是光线与三角形所在的平面相交，然后判断交点是不是在三角形内</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/132.jpg" /></center>

<ul>
  <li>如何定义平面？一个点+法线</li>
</ul>

<center><img src="../assets/img/posts/20211221/133.jpg" /></center>

<ul>
  <li>
    <p>然后将光线方程带入平面方程中，就可以得出光线与平面的交点</p>
  </li>
  <li>
    <p>如何简化判断交点与三角形的位置关系？MT算法：</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/134.jpg" /></center>

<p>这个算法的核心就是利用重心坐标系：解出重心坐标后，如果它们都为正，那么点在三角形内</p>

<h3 id="1355-accelerating-ray-surface-intersection">13.5.5. accelerating ray-surface intersection</h3>
<ul>
  <li>
    <p>加速交点(一般指与三角形网格的交点)计算过程</p>
  </li>
  <li>
    <p>bounding volume 包围盒</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/135.jpg" /></center>

<p>引入包围盒的思路是：如果光线与包围盒都不相交，那么肯定不会与里面的几何体有交点</p>

<ul>
  <li>包围盒由三个对面的交集</li>
</ul>

<center><img src="../assets/img/posts/20211221/136.jpg" /></center>

<p>轴对齐包围盒(就是对面与坐标轴平行)axis-aligned bounding box</p>

<ul>
  <li>先考虑二维的情况Ray intersection with aabb</li>
</ul>

<center><img src="../assets/img/posts/20211221/137.jpg" /></center>

<p>找到最大的时间和最小的时间</p>

<ul>
  <li>三维：对于三组对面，计算$t_{min}$和$t_{max}$，然后找到$t_{enter}$和$t_{exit}$。那么我们就知道了进入的时间和出去的时间，如果进去的时间小于出去的时间，那么光线进入了aabb，表示光线在盒子里呆过一段时间</li>
</ul>

<center><img src="../assets/img/posts/20211221/138.jpg" /></center>

<ul>
  <li>还要要保证进入的时间和出去的时间都要大于0</li>
</ul>

<h1 id="14-lecture-14-ray-tracing-2">14. Lecture 14 Ray Tracing 2</h1>
<h2 id="141-uniform-spatial-partitions-grids">14.1. Uniform Spatial Partitions (Grids)</h2>
<ul>
  <li>继续上节课的加速计算话题</li>
  <li>一种加速方法：生成grid</li>
</ul>

<center><img src="../assets/img/posts/20211221/139.jpg" /></center>

<p>找到aabb后，创建网格，存储aabb内几何体</p>

<ul>
  <li>然后光线沿着这些小格子相交</li>
</ul>

<center><img src="../assets/img/posts/20211221/140.jpg" /></center>

<h2 id="142-spatial-partitions-空间划分">14.2. Spatial Partitions 空间划分</h2>
<h3 id="1421-一些划分示例">14.2.1. 一些划分示例</h3>

<center><img src="../assets/img/posts/20211221/141.jpg" /></center>

<p>八叉树Oct-Tree，KD-Tree，BSP-Tree</p>

<h3 id="1422-kd-tree">14.2.2. KD-Tree</h3>

<center><img src="../assets/img/posts/20211221/142.jpg" /></center>

<ul>
  <li>
    <p>每次划分都沿着坐标轴移动，对于中间的结点都有子节点，只存储叶子结点的数据</p>
  </li>
  <li>
    <p>缺点：一个物体可能存在在多个叶子节点里</p>
  </li>
</ul>

<h2 id="143-object-partitions-物体划分">14.3. Object Partitions 物体划分</h2>
<h3 id="1431-bounding-volume-hierarchybvh">14.3.1. Bounding Volume Hierarchy(BVH)</h3>
<ul>
  <li>这种方法是目前图形学中使用较多的方法</li>
</ul>

<center><img src="../assets/img/posts/20211221/143.jpg" /></center>

<ul>
  <li>
    <p>沿着物体不断细分出bbox</p>
  </li>
  <li>
    <p>bvh的缺点：两部分bbox可能相交</p>
  </li>
</ul>

<h3 id="1432-building-bvh">14.3.2. Building BVH</h3>
<ul>
  <li>如何划分结点？选择一个维度进行划分，每次找最长的结点进行细分，细分的结点在中位数，当结点处图形较少，则停止</li>
</ul>

<center><img src="../assets/img/posts/20211221/144.jpg" /></center>

<h3 id="1433-与空间划分的对比">14.3.3. 与空间划分的对比</h3>

<center><img src="../assets/img/posts/20211221/145.jpg" /></center>

<h2 id="144-whitted-style">14.4. Whitted style</h2>
<ul>
  <li>到目前为止，已经讲了国内光线追踪会讲的内容。也就是讲完了Whitted style光线追踪</li>
</ul>

<h2 id="145-radiometry-辐射度量学">14.5. Radiometry 辐射度量学</h2>
<h3 id="1451-一些物理量">14.5.1. 一些物理量</h3>
<ul>
  <li>new terms: radiant flux, intensity, irradiance, radiance</li>
</ul>

<h3 id="1452-radiant-energy-and-flux">14.5.2. Radiant Energy and Flux</h3>
<ul>
  <li>randiant flux就是单位时间能量/功率</li>
</ul>

<center><img src="../assets/img/posts/20211221/146.jpg" /></center>

<h3 id="1453-radiant-intensity">14.5.3. Radiant Intensity</h3>
<ul>
  <li>辐射强度就是单位立体角(solid angle)的功率</li>
</ul>

<center><img src="../assets/img/posts/20211221/147.jpg" /></center>

<ul>
  <li>那么立体角是什么呢？立体角就是二维空间的角在三维空间的沿伸，就是球面面积除以半径的平方</li>
</ul>

<center><img src="../assets/img/posts/20211221/148.jpg" /></center>

<h1 id="15-lecture-15-ray-tracing">15. Lecture 15 Ray Tracing</h1>
<h2 id="151-radiometry-cont-辐射度量学">15.1. Radiometry cont. 辐射度量学</h2>
<h3 id="1511-继续上节课的内容">15.1.1. 继续上节课的内容</h3>

<ul>
  <li>微分立体角，就是球坐标系上对$\theta$和$\phi$的微分</li>
</ul>

<center><img src="../assets/img/posts/20211221/149.jpg" /></center>

<h3 id="1512-irradiance">15.1.2. Irradiance</h3>
<ul>
  <li>单位面积的功率</li>
</ul>

<center><img src="../assets/img/posts/20211221/150.jpg" /></center>

<ul>
  <li>面积是投影的面积</li>
</ul>

<h3 id="1513-radiance">15.1.3. Radiance</h3>
<ul>
  <li>randiance就是单位投影面积单位立体角的功率</li>
</ul>

<center><img src="../assets/img/posts/20211221/151.jpg" /></center>

<ul>
  <li>irradiance和radiance的区别：irradiance是某一个面积上接受的能量，而radiance是某一个面积某一个角度上接受的能量</li>
</ul>

<center><img src="../assets/img/posts/20211221/152.jpg" /></center>

<h2 id="152-bidirectional-reflectance-distribution-function-brdf">15.2. Bidirectional Reflectance Distribution Function (BRDF)</h2>
<ul>
  <li>
    <p>双向反射分布方程BRDF是描述光线传播的方程</p>
  </li>
  <li>
    <p>某一个方向$\omega_i$的光线打到某一个表面然后被吸收同时从另一个方向$\omega_r$反射出去</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/153.jpg" /></center>

<ul>
  <li>反射方程</li>
</ul>

<center><img src="../assets/img/posts/20211221/154.jpg" /></center>

<ul>
  <li>
    <p>观察某一个物体的反射光线不止从光源有光线，还有其他物体反射的光</p>
  </li>
  <li>
    <p>渲染方程Rendering Equation</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/155.jpg" /></center>

<ul>
  <li>渲染方程两部分组成，一部分是自身发光，另一部分是接受的光线的反射光线(半球上每个方向)</li>
</ul>

<h2 id="153-rendering-equation-渲染方程">15.3. Rendering Equation 渲染方程</h2>

<h3 id="1531-如何理解渲染方程">15.3.1. 如何理解渲染方程</h3>
<ul>
  <li>
    <p>反射的光线由两个个部分组成：自身的emission和从各个方向的反射光</p>
  </li>
  <li>
    <p>如何考虑物体反射的光？把物体看作一个光源，也就是看作一个递归的过程</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/156.jpg" /></center>

<ul>
  <li>通过数学式子简化渲染方程：</li>
</ul>

<center><img src="../assets/img/posts/20211221/157.jpg" /></center>

<ul>
  <li>然后通过逆矩阵可以解出L</li>
</ul>

<center><img src="../assets/img/posts/20211221/158.jpg" /></center>

<ul>
  <li>
    <p>光线弹射一次叫做直接光照、弹射两次及以上叫做间接光照</p>
  </li>
  <li>
    <p>那么就可以发现与光栅化的区别</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/159.jpg" /></center>

<ul>
  <li>在多次弹射后场景会趋于一个固定的亮度</li>
</ul>

<h1 id="16-lecture-16-ray-tracing-4">16. Lecture 16 Ray Tracing 4</h1>
<h2 id="161-monte-carlo-integration-蒙特卡洛积分">16.1. Monte Carlo Integration 蒙特卡洛积分</h2>
<ul>
  <li>有些函数不太好用解析式写出来</li>
  <li>蒙特卡洛积分就是数值积分的方法</li>
</ul>

<center><img src="../assets/img/posts/20211221/160.jpg" /></center>

<ul>
  <li>就是采样值除以采样密度</li>
</ul>

<center><img src="../assets/img/posts/20211221/161.jpg" /></center>

<h2 id="162-path-tracing-路径追踪">16.2. Path Tracing 路径追踪</h2>
<ul>
  <li>与whitted sytle的区别：whitted sytle没有考虑全局光照</li>
</ul>

<h3 id="1621-解渲染方程">16.2.1. 解渲染方程</h3>
<ul>
  <li>考虑一个简单的模型，只有直接光照</li>
</ul>

<center><img src="../assets/img/posts/20211221/162.jpg" /></center>

<ul>
  <li>每一个$\omega_i$都看作采样，那么可以应用蒙特卡洛积分</li>
</ul>

<center><img src="../assets/img/posts/20211221/163.jpg" /></center>

<ul>
  <li>应用全局光照，将物体反射面也看做光源，做一个递归</li>
</ul>

<center><img src="../assets/img/posts/20211221/164.jpg" /></center>

<ul>
  <li>
    <p>但是这样会出现一个问题，那就是爆炸，如果我取多个X，那么弹射很多次后就会爆炸</p>
  </li>
  <li>
    <p>解决方法，对每个点只取一个方向，也就是N=1，所以它叫做路径追踪</p>
  </li>
  <li>
    <p>这样噪声会比较大，但是从每个像素点有多个路径，所以还是可以接受</p>
  </li>
  <li>
    <p>第二个问题是递归不会停止？解决方法：俄罗斯轮盘赌，即在某一个程度停止递归</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/165.jpg" /></center>

<ul>
  <li>那么我们可以设定一个概率P来决定每个点是否打出一条光线，同时保证期望不变</li>
</ul>

<center><img src="../assets/img/posts/20211221/166.jpg" /></center>

<ul>
  <li>
    <p>到目前为止已经是一个正确的path tracing的渲染方法，但是这样效率比较低</p>
  </li>
  <li>
    <p>效率低的原因：每个点打到或者打不到光源是随机的，也就是说浪费了很多光线</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211221/167.jpg" /></center>

<ul>
  <li>可以在光源上采样，这样没有光线会浪费，渲染方程就需要写成在光源上采样</li>
</ul>

<center><img src="../assets/img/posts/20211221/168.jpg" /></center>

<ul>
  <li>那么我们就可以将渲染方程分为两部分，一部分是光源直接光照，方法使用上面提到的在光源上采样，另一部分是间接光照，保持不变</li>
</ul>

<h3 id="1622-最终的代码">16.2.2. 最终的代码</h3>

<center><img src="../assets/img/posts/20211221/169.jpg" /></center>

<ul>
  <li>但还有一个小问题，就是中间有物体遮挡，需要添加一个判断</li>
</ul>

<center><img src="../assets/img/posts/20211221/170.jpg" /></center>

<h2 id="163-路径追踪">16.3. 路径追踪</h2>
<ul>
  <li>在之前，ray tracing主要指whitted-style ray tracing</li>
  <li>但现在，只要设计了光线传播方法，就是ray tracing，路径追踪只是其中的一个方法</li>
</ul>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[Games 101 introduction to computer graphics 课程笔记]]></summary></entry><entry><title type="html">组会记录</title><link href="http://localhost:4000/1221%E7%BB%84%E4%BC%9A.html" rel="alternate" type="text/html" title="组会记录" /><published>2021-12-21T00:00:00+08:00</published><updated>2021-12-21T00:00:00+08:00</updated><id>http://localhost:4000/1221%E7%BB%84%E4%BC%9A</id><content type="html" xml:base="http://localhost:4000/1221%E7%BB%84%E4%BC%9A.html"><![CDATA[<h1 id="vae">VAE</h1>
<h2 id="ae">AE</h2>
<p>Auto-Encoder自动编码器，比如Seq2seq模型。</p>

<h2 id="vaevariational-auto-encoder">VAE(Variational Auto-Encoder)</h2>
<p>在实际情况中，我们需要在模型的准确率上与隐含向量服从标准正态分布之间做一个权衡，所谓模型的准确率就是指解码器生成的图片与原图片的相似程度。我们可以让网络自己来做这个决定，非常简单，我们只需要将这两者都做一个loss，然后在将他们求和作为总的loss，这样网络就能够自己选择如何才能够使得这个总的loss下降。另外我们要衡量两种分布的相似程度，如何看过之前一片GAN的数学推导，你就知道会有一个东西叫KL-divergence来衡量两种分布的相似程度，这里我们就是用KL-divergence来表示隐含向量与标准正态分布之间差异的loss，另外一个loss仍然使用生成图片与原图片的均方误差来表示。</p>

<h2 id="kl消失">KL消失</h2>
<p>KL消失后，VAE就变成了AE
原因：</p>
<ul>
  <li>KL项本身太容易被优化</li>
  <li>一旦崩塌，Decoder会忽视Z<sub>x</sub></li>
  <li>Z<sub>x</sub>的表示学习依赖于Decoder</li>
</ul>

<h2 id="解决kl消失的思路">解决KL消失的思路</h2>
<p>…</p>

<h1 id="analyze-pretraining-language-model">Analyze Pretraining Language Model</h1>
<h2 id="perspective-of-knowledge">Perspective of knowledge</h2>
<ul>
  <li>Syntacitic/Semantic/lexical 句法，语义，词汇</li>
  <li>重构语法树</li>
  <li>Attention中很多头可能没有用，学到了很多冗余的信息</li>
  <li>Analyze Feed Forward Neural Network</li>
  <li>浅层词汇信息，深层语义信息</li>
  <li>Prompt</li>
</ul>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[组会笔记]]></summary></entry><entry><title type="html">推荐系统</title><link href="http://localhost:4000/Recommender_system.html" rel="alternate" type="text/html" title="推荐系统" /><published>2021-12-16T00:00:00+08:00</published><updated>2021-12-16T00:00:00+08:00</updated><id>http://localhost:4000/Recommender_system</id><content type="html" xml:base="http://localhost:4000/Recommender_system.html"><![CDATA[<!-- TOC -->

<ul>
  <li><a href="#1-推荐系统总览">1. 推荐系统总览</a>
    <ul>
      <li><a href="#11-协同过滤-collaborative-filtering">1.1. 协同过滤 Collaborative Filtering</a></li>
      <li><a href="#12-显式反馈和隐式反馈">1.2. 显式反馈和隐式反馈</a></li>
      <li><a href="#13-推荐任务">1.3. 推荐任务</a></li>
    </ul>
  </li>
  <li><a href="#2-矩阵分解-matrix-factorization">2. 矩阵分解 Matrix Factorization</a></li>
  <li><a href="#3-autorec">3. AutoRec</a>
    <ul>
      <li><a href="#31-overview">3.1. overview</a></li>
      <li><a href="#32-formula">3.2. formula</a></li>
    </ul>
  </li>
  <li><a href="#4-personalized-ranking-for-recommender-system">4. Personalized Ranking for Recommender System</a>
    <ul>
      <li><a href="#41-overview">4.1. overview</a></li>
      <li><a href="#42-bayesian-personalized-ranking-loss-贝叶斯损失">4.2. Bayesian Personalized Ranking loss 贝叶斯损失</a></li>
      <li><a href="#43-hinge-loss">4.3. Hinge Loss</a></li>
    </ul>
  </li>
  <li><a href="#5-neural-collaborative-filtering-for-personalized-ranking-使用协同过滤网络个性化排序">5. Neural Collaborative Filtering for Personalized Ranking 使用协同过滤网络个性化排序</a>
    <ul>
      <li><a href="#51-the-neumf-model">5.1. The NeuMF model</a></li>
      <li><a href="#52-evaluator">5.2. Evaluator</a></li>
      <li><a href="#53-代码">5.3. 代码</a></li>
    </ul>
  </li>
  <li><a href="#6-sequence-aware-recommender-systems">6. Sequence-Aware Recommender Systems</a>
    <ul>
      <li><a href="#61-model-architectures">6.1. Model Architectures</a></li>
      <li><a href="#62-negative-sampling-负采样">6.2. Negative Sampling 负采样</a></li>
    </ul>
  </li>
  <li><a href="#7-feature-rich-recommender-systems">7. Feature-Rich Recommender Systems</a></li>
  <li><a href="#8-factorization-machines-因子分解机">8. Factorization Machines 因子分解机</a>
    <ul>
      <li><a href="#81-2-way-factorization-machines">8.1. 2-Way Factorization Machines</a></li>
      <li><a href="#82-an-efficient-optimization-citerion">8.2. An Efficient Optimization Citerion</a></li>
    </ul>
  </li>
  <li><a href="#9-deep-factorization-machines-深度因子分解机deeofm">9. Deep Factorization Machines 深度因子分解机DeeoFM</a>
    <ul>
      <li><a href="#91-model-architectures-模型架构">9.1. Model Architectures 模型架构</a></li>
    </ul>
  </li>
</ul>

<!-- /TOC -->

<h2 id="1-推荐系统总览">1. 推荐系统总览</h2>
<h3 id="11-协同过滤-collaborative-filtering">1.1. 协同过滤 Collaborative Filtering</h3>
<p>协同过滤最早出现在1992年Tapestry system，“人们相互协作，相互帮助，执行过滤程序，以处理大量的电子邮件和张贴到新闻组的信息。”现在协同过滤的概念更加广泛，从广义上讲，它是利用涉及多个用户、代理和数据源之间协作的技术来过滤<strong>信息或模式</strong>的过程。</p>

<p>协同过滤模型可以分为:1.memory-based CF; 2.model-based CF. 其中Memory-based CF又可以分为item-based和user-based CF。model-based CF有矩阵分解模型。</p>

<p>总的来说，协同过滤就是利用用户-物品的数据来预测和推荐。</p>

<h3 id="12-显式反馈和隐式反馈">1.2. 显式反馈和隐式反馈</h3>
<p>为了学习用户的偏好，系统需要收集用户的反馈feedback。反馈可以分为显式和隐式。</p>

<p>显式反馈就是需要用户主动提供兴趣偏好。比如点赞、点踩。</p>

<p>隐式反馈则是间接反映用户的喜好，比如购物历史记录，浏览记录，观看记录甚至是鼠标移动。</p>

<h3 id="13-推荐任务">1.3. 推荐任务</h3>
<p>电影推荐、新闻推荐、评分预测rating prediction task、top-n reommendation。如果使用了时间戳信息，那么我们构建了sequence-aware recommendation。针对新用户推荐新物品称为cold-start recommendation冷启动推荐。</p>

<h2 id="2-矩阵分解-matrix-factorization">2. 矩阵分解 Matrix Factorization</h2>
<p>The Matrix Factorization Model矩阵分解模型</p>

<p>R是user-item矩阵，行数是用户数量，列数是物品数量,那么R∈R<sup>mxn</sup>。P是user latent matrix，P∈R<sup>mxk</sup>，Q是item latent matrix，Q∈R<sup>nxk</sup></p>

<p>矩阵分解就是把R分解成P和Q，那么预测的评分就是：</p>

<p><img src="../assets/img/posts/20211216/2.jpg" /></p>

<p>但是上面这个式子没有考虑偏置，我们会有下面这个完整的式子：</p>

<p><img src="../assets/img/posts/20211216/3.jpg" /></p>

<p>那么<strong>目标函数</strong>可以定义为：</p>

<p><img src="../assets/img/posts/20211216/4.jpg" /></p>

<p>右边那一串是正则项，为了避免过拟合</p>

<p>下面这张图值观的展示了矩阵分解过程：</p>

<p><img src="../assets/img/posts/20211216/5.jpg" /></p>

<h2 id="3-autorec">3. AutoRec</h2>
<h3 id="31-overview">3.1. overview</h3>
<p>使用autoencoder预测评分，上小节介绍的矩阵分解模型是线性模型，它不能捕捉复杂的非线性关系，比如用户的偏好。这一小节介绍一个非线性协同过滤神经网络模型AutoRec。</p>

<p>AutoRec是基于自编码器的结构，自编码器是一种特殊的神经网络架构，他的输入和输出的架构是相同的，自编码器通过无监督学习来训练获取输入数据在较低维度的表达，在神经网络的后段，这些低纬度的信息再次被重构回高维的数据表达。</p>

<p>所以AutoRec的架构也是输入层，隐藏层和重构输出层。它的目的是输入一个只有部分兴趣矩阵，输出一个完整的兴趣矩阵。</p>

<p>AutoRec可以分为user-based 和 item-based</p>

<h3 id="32-formula">3.2. formula</h3>
<p>针对item-based：</p>

<p>$R_{*i}$表示兴趣矩阵的第i列，不知道的项填为0。那么神经网络的构架可以定义为：</p>

<center><img src="../assets/img/posts/20211216/6.jpg" /></center>

<p>h()表示最终的输出，输出一个完整的兴趣矩阵，那么误差定义为：</p>

<center><img src="../assets/img/posts/20211216/7.jpg" /></center>

<h2 id="4-personalized-ranking-for-recommender-system">4. Personalized Ranking for Recommender System</h2>
<h3 id="41-overview">4.1. overview</h3>
<p>在上一节中，我们用到了显式反馈，同时模型只在能观察到的评分上训练。那么这种模型有两个缺点：第一个是很多的反馈并不是显式的。第二个是没有观察到的评分被完全忽略了。</p>

<p>个性化推荐可以分为:1.pointwise;2.pairwise;3.listwise。Pointwise表示每次预测单个偏好，pairwise则是预测出一系列的偏好然后进行排序，listwise则是预测所有的item并进行排序。</p>

<h3 id="42-bayesian-personalized-ranking-loss-贝叶斯损失">4.2. Bayesian Personalized Ranking loss 贝叶斯损失</h3>
<ul>
  <li>
    <p>贝叶斯损失是一种pairwise个性化推荐损失。它被广泛应用于多种推荐系统中。它假设用户相对于无观察项，更加喜欢positive item</p>
  </li>
  <li>
    <p>训练集格式是(u, i, j)表示用户u喜欢i超过j，BPR希望最大化下面这个后验概率：</p>
  </li>
</ul>

<center><img src="../assets/img/posts/20211216/8.jpg" /></center>

<p>其中$\Theta$表示推荐系统的参数，$&gt;_u$表示用户u对所有item的排序。</p>

<center><img src="../assets/img/posts/20211216/9.jpg" /></center>

<h3 id="43-hinge-loss">4.3. Hinge Loss</h3>
<ul>
  <li>数学表达式</li>
</ul>

<center><img src="../assets/img/posts/20211216/10.jpg" /></center>

<p>其中m表示安全系数，它的目的是让不喜欢的项离喜欢的项更远。它和贝叶斯都是为了优化positive sample和negative sample之间的距离。</p>

<h2 id="5-neural-collaborative-filtering-for-personalized-ranking-使用协同过滤网络个性化排序">5. Neural Collaborative Filtering for Personalized Ranking 使用协同过滤网络个性化排序</h2>
<p>本小节重新将目光聚集到隐式反馈中，介绍协同过滤推荐系统NeuMF。NeuMF利用隐式反馈，它由两个子结构组成，分别是generalized matrix factorization(GMF)和MLP。不同于评分的预测如AutoRec，它将生成一系列的推荐，它根据用户是否看过这场电影来区分为正例和反例</p>

<h3 id="51-the-neumf-model">5.1. The NeuMF model</h3>
<p>NeuMF的网络结构由两部分组成。</p>

<ul>
  <li>一部分是GMF，也就是matrix factorization的类似形式，输入用户向量$p_u$和物品向量$q_i$，返回x</li>
</ul>

<center><img src="../assets/img/posts/20211216/12.jpg" /></center>

<ul>
  <li>另一部分是MLP，输入和GMF一样，但是用不同的字母表示，具体公式如下：</li>
</ul>

<center><img src="../assets/img/posts/20211216/13.jpg" /></center>

<ul>
  <li>最后对这两个子结构concatenate一下，就是最终的输出</li>
</ul>

<center><img src="../assets/img/posts/20211216/14.jpg" /></center>

<ul>
  <li>大体的网络结构如下</li>
</ul>

<center><img src="../assets/img/posts/20211216/11.jpg" /></center>

<h3 id="52-evaluator">5.2. Evaluator</h3>
<p>有两个性能度量指标</p>

<ul>
  <li>hit rate at given cutting off l，记作Hit@l</li>
</ul>

<center><img src="../assets/img/posts/20211216/15.jpg" /></center>

<p>这个式子的主题思路是判断推荐的物品是否在top l中，m表示用户的数量，$rank_{u,g_u}$表示对于用户u和物品$g_u$的排名，1表示指标函数</p>

<ul>
  <li>AUC，即ROC曲线下的面积，也是模型泛化能力的一个指标</li>
</ul>

<center><img src="../assets/img/posts/20211216/16.jpg" /></center>

<p>其中$S_u$表示模型对于u的推荐物品集，I表示item set，AUC越大越好</p>

<h3 id="53-代码">5.3. 代码</h3>
<p>网络结构就是上面介绍的那样，net的输出是用户和物品匹配出的一个推荐值(我的想法)。在进行训练的时候，会给出正例物品(即用户有过评分的物品)和反例物品(用户没有评分，也就是没有看过)分别与用户得到一个推荐值，然后利用上一小节介绍的贝叶斯损失来优化(让评分过的物品有更高的推荐值)，然后最终我们希望返回一系列的推荐物品，这些推荐物品都是没有负例物品，然后根据推荐值进行排序。性能指标是hit或者auc。hit的思想是让真实评分的物品在推荐列表中。</p>

<h2 id="6-sequence-aware-recommender-systems">6. Sequence-Aware Recommender Systems</h2>
<p>之前的模型都没有考虑时序信息，这小节的Caser模型将会考虑用户的时序信息。</p>
<h3 id="61-model-architectures">6.1. Model Architectures</h3>
<p>模型的输入$E^{(u,t)}$表示用户u的近期L个评价的物品，Caser模型有横向和纵向的卷积层，输入矩阵分别与卷积层作用后，结果concatenate变成$z$，$z$再和用户的一般信息结合，也就是$z$和$p_u$concatenate最终输出$\hat{y}_{uit}$，其中$p_u$表示用户u的item信息</p>

<center><img src="../assets/img/posts/20211216/17.jpg" /></center>

<h3 id="62-negative-sampling-负采样">6.2. Negative Sampling 负采样</h3>
<p>我们需要对数据集进行重新处理，比如一个人喜欢9部电影，同时我们的L=5，那么我们将最近的一部电影留出来作为test，其余的都作为训练集，可以划分出3个训练集。同时我们也需要进行负采样(采样没有评分的item)</p>

<h2 id="7-feature-rich-recommender-systems">7. Feature-Rich Recommender Systems</h2>
<p>之前的模型大都用到了用户物品的交互矩阵，但是很少有用到一些额外的信息，比如物品的特征，用户的简介，发生交互的背景等等…利用这些信息可以获得用户的兴趣特征。本节提出了一个新的任务CTR(click-through rate)，也就是点击率任务，对象可以是广告、电影等等。</p>

<h2 id="8-factorization-machines-因子分解机">8. Factorization Machines 因子分解机</h2>
<p>Factorization machines(FM)是一个监督算法，可用于分类，回归和排名任务。它有两个优点：1.它能处理稀疏的数据；2.它能减少时间复杂度和线性复杂度</p>

<h3 id="81-2-way-factorization-machines">8.1. 2-Way Factorization Machines</h3>
<p>$x$表示样本的特征值，而$y$表示它的标签值，即click/non-click。第二项表示线性项，第三项表示矩阵分解项</p>

<center><img src="../assets/img/posts/20211216/18.jpg" /></center>

<h3 id="82-an-efficient-optimization-citerion">8.2. An Efficient Optimization Citerion</h3>
<p>上面式子的第三项时间复杂度太高，我们可以简化一下</p>

<center><img src="../assets/img/posts/20211216/19.jpg" /></center>

<h2 id="9-deep-factorization-machines-深度因子分解机deeofm">9. Deep Factorization Machines 深度因子分解机DeeoFM</h2>
<p>上小节提到的因子分解机用到的都是线性模型(单线性和双线性)，这种模型在真实数据表现并不好。这里我们就可以结合因子分解机和深度神经网络，比如我们这小节即将介绍的DeepFM。</p>

<h3 id="91-model-architectures-模型架构">9.1. Model Architectures 模型架构</h3>
<p>DeepFM由两部分组成，FM component和deep component，FM部分和上小节提到的2-way FM做法一样，主要是处理低纬度特征，而deep部分用到的MLP来处理高维度和非线性。这两部分使用相同的输入/嵌入层然后它们的结果整合成最终的预测。模型结构如下图：</p>

<center><img src="../assets/img/posts/20211216/20.jpg" /></center>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[d2l推荐系统笔记]]></summary></entry><entry><title type="html">Robotics</title><link href="http://localhost:4000/Robotics.html" rel="alternate" type="text/html" title="Robotics" /><published>2021-12-13T00:00:00+08:00</published><updated>2021-12-13T00:00:00+08:00</updated><id>http://localhost:4000/Robotics</id><content type="html" xml:base="http://localhost:4000/Robotics.html"><![CDATA[<h1 id="报告">报告</h1>
<h2 id="报告内容">报告内容</h2>
<p>用solidworks设计一个至少三个自由度的机械臂，并且描述它的动能，移动能力等等。提交的是截图，源文件等等。</p>
<h2 id="报告格式">报告格式</h2>
<ol>
  <li>标题，下面有姓名学号电话等等</li>
  <li>摘要</li>
  <li>正文</li>
</ol>]]></content><author><name>Quehry</name></author><category term="note" /><summary type="html"><![CDATA[《机器人》课程随堂笔记]]></summary></entry></feed>