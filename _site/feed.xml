<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-12-26T13:44:38+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Quehry</title><subtitle>Artificial Intelligence trends and concepts made easy.</subtitle><author><name>Quehry</name></author><entry><title type="html">机器学习</title><link href="http://localhost:4000/Machine-Learning.html" rel="alternate" type="text/html" title="机器学习" /><published>2021-12-22T00:00:00+08:00</published><updated>2021-12-22T00:00:00+08:00</updated><id>http://localhost:4000/Machine-Learning</id><content type="html" xml:base="http://localhost:4000/Machine-Learning.html">&lt;h1 id=&quot;目录&quot;&gt;目录&lt;/h1&gt;

&lt;!-- TOC --&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#目录&quot;&gt;目录&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#第2章-模型评估与选择&quot;&gt;第2章 模型评估与选择&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#20-思维导图&quot;&gt;2.0 思维导图&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#21-经验误差与过拟合&quot;&gt;2.1 经验误差与过拟合&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#22-评估方法&quot;&gt;2.2 评估方法&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#221-留出法&quot;&gt;2.2.1 留出法&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#222-交叉验证法&quot;&gt;2.2.2 交叉验证法&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#223-自助法&quot;&gt;2.2.3 自助法&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#23-性能度量&quot;&gt;2.3 性能度量&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#231-错误率与精度&quot;&gt;2.3.1 错误率与精度&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#232-查准率查全率与f1&quot;&gt;2.3.2 查准率、查全率与F1&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#233-roc与auc&quot;&gt;2.3.3 ROC与AUC&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#234-代价敏感错误率与代价曲线&quot;&gt;2.3.4 代价敏感错误率与代价曲线&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#24-比较检验&quot;&gt;2.4 比较检验&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#241-假设检验&quot;&gt;2.4.1 假设检验&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#242-交叉验证t检验&quot;&gt;2.4.2 交叉验证t检验&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#243-mcnemar检验&quot;&gt;2.4.3 McNemar检验&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#244-friedman检验与nemenyi后续检验&quot;&gt;2.4.4 Friedman检验与Nemenyi后续检验&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#25-偏差与方差&quot;&gt;2.5 偏差与方差&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#第3章-线性模型&quot;&gt;第3章 线性模型&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#31-基本形式&quot;&gt;3.1 基本形式&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#32-线性回归&quot;&gt;3.2 线性回归&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#33-对数几率回归&quot;&gt;3.3 对数几率回归&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#34-线性判别分析&quot;&gt;3.4 线性判别分析&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- /TOC --&gt;

&lt;h1 id=&quot;第2章-模型评估与选择&quot;&gt;第2章 模型评估与选择&lt;/h1&gt;

&lt;h2 id=&quot;20-思维导图&quot;&gt;2.0 思维导图&lt;/h2&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/16.jpg&quot; /&gt;&lt;/center&gt;

&lt;h2 id=&quot;21-经验误差与过拟合&quot;&gt;2.1 经验误差与过拟合&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;定义：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;错误率(error rate)&lt;/strong&gt;: 如果m个样本中有a个样本分类错误，则错误率E=a/m。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;精度(accuracy)&lt;/strong&gt;：精度=1-错误率。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;训练误差&lt;/strong&gt;：学习器在训练集上的误差称为训练误差或者经验误差。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;泛化误差(generalization error)&lt;/strong&gt;：学习器在新样本上的误差称为泛化误差。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;过拟合(overfitting)&lt;/strong&gt;：当学习器把训练样本学得太好了的时候，很可能把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这就会导致泛化性能下降。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;欠拟合(underfitting)&lt;/strong&gt;：对训练样本的一般性质尚未学好。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;22-评估方法&quot;&gt;2.2 评估方法&lt;/h2&gt;
&lt;p&gt;通常我们可通过实验测试来对学习器的泛化误差进行评估而做出选择，于是我们需要一个&lt;strong&gt;测试集(testing set)&lt;/strong&gt;，然后以测试集上的测试误差作为泛化误差的近似。下面介绍一些常见的处理数据集D的方法(数据集D-&amp;gt;训练集S+测试集T)&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h3 id=&quot;221-留出法&quot;&gt;2.2.1 留出法&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;留出发(hold-out)直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T。在S上训练出模型后，用T来评估其测试误差。&lt;/li&gt;
  &lt;li&gt;以采样的角度来看待数据集划分的过程，则保留类别比例的采样方式通常称为&lt;strong&gt;分层采样&lt;/strong&gt;。比如1000个样本，50%的正例，50%负例，以7：3划分数据集，那么训练集包含350个正例和350个负例就是分层采样。&lt;/li&gt;
  &lt;li&gt;在使用留出法评估结果时，一般要采用若干次随即划分、重复进行实验评估后取平均值作为留出法的评估结果。&lt;/li&gt;
  &lt;li&gt;常用做法时将大约2/3~4/5的样本用于训练。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;222-交叉验证法&quot;&gt;2.2.2 交叉验证法&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;交叉验证法(cross validation)&lt;/strong&gt;先将数据集D划分为k个大小相似的互斥子集，每个子集D&lt;sub&gt;i&lt;/sub&gt;都尽可能保持数据分布的一致性。然后每次用k-1个自己的并集作为训练集，余下的那个子集作为测试集，这样就可以获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。&lt;/li&gt;
  &lt;li&gt;交叉验证也称为k折交叉验证。k的常见取值有10、5、20。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;留一法&lt;/strong&gt;就是k=m，其中数据集D有m个样本。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;223-自助法&quot;&gt;2.2.3 自助法&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;自助法(bootstrapping)&lt;/strong&gt;：给定包含m个样本的数据集D，每次随机从D中挑选一个样本，将其拷贝放入D’, 然后再将该样本放回最初的数据集D中，这个过程重复执行m次后，我们获得了包含m个样本的数据集D’。我们将D’作为训练集，D\D’作为测试集。&lt;/li&gt;
  &lt;li&gt;不难发现大概有36.8%(m趋于无限大时)的样本再m次采样中始终不被采到。&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;$\lim\limits_{m\rightarrow\infty}(1-\frac{1}{m})^m = \frac{1}{e} ≈ 0.368$&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;缺点：自助法产生的数据集改变了初始数据集的分布，这样会引入估计偏差。因此，在初始数据量足够时，留出法和交叉验证法更常用一些。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;23-性能度量&quot;&gt;2.3 性能度量&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是&lt;strong&gt;性能度量(performance measure)&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;在预测任务中，给定D = {(x&lt;sub&gt;1&lt;/sub&gt;,y&lt;sub&gt;1&lt;/sub&gt;), (x&lt;sub&gt;2&lt;/sub&gt;,y&lt;sub&gt;2&lt;/sub&gt;)…(x&lt;sub&gt;m&lt;/sub&gt;,y&lt;sub&gt;m&lt;/sub&gt;)}, 其中y&lt;sub&gt;i&lt;/sub&gt;是x&lt;sub&gt;i&lt;/sub&gt;的真实标记。学习器f。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;均方误差(mean squared error)&lt;/strong&gt;：回归任务最常用的性能度量是均方误差：&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;$E(f;D)=\frac{1}{m}\sum_1^m(f(x_i)-y_i)^2$&lt;/center&gt;

&lt;p&gt;接下来我将介绍&lt;strong&gt;分类任务&lt;/strong&gt;中常用的性能度量&lt;/p&gt;

&lt;h3 id=&quot;231-错误率与精度&quot;&gt;2.3.1 错误率与精度&lt;/h3&gt;
&lt;p&gt;本章开头提到了错误率和精度，这是分类任务中最常用的两种性能度量，既适用于二分类任务，也适用于多分类任务。&lt;/p&gt;

&lt;h3 id=&quot;232-查准率查全率与f1&quot;&gt;2.3.2 查准率、查全率与F1&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;针对二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例(true positive)、假正例(false positive)、真反例(true negative)、假反例(false negative)，分别记为TP、FP、TN、FN。&lt;/li&gt;
  &lt;li&gt;混淆矩阵(confusion matrix)&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;真\预&lt;/td&gt;
      &lt;td&gt;正例&lt;/td&gt;
      &lt;td&gt;反例&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;正例&lt;/td&gt;
      &lt;td&gt;TP&lt;/td&gt;
      &lt;td&gt;FN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;反例&lt;/td&gt;
      &lt;td&gt;FP&lt;/td&gt;
      &lt;td&gt;TN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;查准率(precision)，记为P，它表示选择的好瓜中有多少是真正的好瓜&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;$P=\frac{TP}{TP+FP}$&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;查全率(recall)，记为R，它表示好瓜中有多少被选出来了&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;$R=\frac{TP}{TP+FN}$&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;P-R曲线：在很多情形下，我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器认为最可能是正例的样本，排在最后的则是学习器认为最不可能是正例的样本，按此顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率、查准率，也得到了P-R图。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/2.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;如果一个学习器的P-R曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者。如果两者曲线相交，则可以通过比较平衡点(break-even-point)来比较，越大越好。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;F1度量：F1综合考虑了查准率和查全率，是他们的调和平均&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;$F1=\frac{2*P*R}{P+R}$&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;F1的一般形式：有时候我们希望赋予查准率和查重率不同的权重：&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;$F_\beta=\frac{(1+\beta^2)*P*R}{\beta^2*P+R}$&lt;/center&gt;

&lt;p&gt;其中β大于1表示查全率有更大影响&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;有时候我们有多个二分类混淆矩阵，我们希望在这n个混淆矩阵上综合考虑查准率和查全率，那么我们有以下的度量&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;宏查准率macro-P，宏查全率macro-R，宏F1：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;$macro-P=\frac{1}{n}\sum_1^nP_i$&lt;/center&gt;

&lt;center&gt;$macro-R=\frac{1}{n}\sum_1^nR_i$&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;微查准率micro-P，微查全率micro-P，微F1：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对TP、FP、TN、FN进行平均&lt;/p&gt;

&lt;center&gt;$micro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}$&lt;/center&gt;

&lt;center&gt;$micro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}$&lt;/center&gt;

&lt;h3 id=&quot;233-roc与auc&quot;&gt;2.3.3 ROC与AUC&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为反类。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ROC曲线是从排序角度来出发研究学习器的泛化性能的工具。ROC的全称是Receiver Operating Characteristic。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;与P-R曲线类似，我们根据学习器的预测结果进行排序，按此顺序逐个把样本作为正例进行预测，计算出真正例率TPR(true positive rate)与假正例率FPR(false positive rate)并以它们分别为横纵坐标画出ROC曲线&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;$TPR=\frac{TP}{TP+FN}$&lt;/center&gt;

&lt;center&gt;$FPR=\frac{FP}{FP+TN}$&lt;/center&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/3.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;同样的，如果一个学习器的ROC曲线被另一个学习器完全包住，则认为后者的性能优于前者。若两个曲线相交，则比较曲线下的面积AUC(area under curve)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;形式化的看，AUC考虑的是样本预测的排序质量，那么我们可以定义一个排序损失l&lt;sub&gt;rank&lt;/sub&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/4.jpg&quot; /&gt;&lt;/center&gt;

&lt;center&gt;AUC=1-l&lt;sub&gt;rank&lt;/sub&gt;&lt;/center&gt;

&lt;h3 id=&quot;234-代价敏感错误率与代价曲线&quot;&gt;2.3.4 代价敏感错误率与代价曲线&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;有时候将正例误判断为负例与将负例误判断为正例所带来的后果不一样，我们赋予它们非均等代价&lt;/li&gt;
  &lt;li&gt;代价矩阵(cost matrix)&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;真\预&lt;/td&gt;
      &lt;td&gt;第0类&lt;/td&gt;
      &lt;td&gt;第1类&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;第0类&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;cost&lt;sub&gt;01&lt;/sub&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;第1类&lt;/td&gt;
      &lt;td&gt;cost&lt;sub&gt;10&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;其中cost&lt;sub&gt;ij&lt;/sub&gt;表示将第i类样本预测为第j类样本的代价&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;代价敏感错误率&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/5.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;在非均等代价下，ROC曲线不能直接反映出学习器的期望总代价，而代价曲线则可达到此目的，横轴是正例概率代价，纵轴是归一化代价&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/6.jpg&quot; /&gt;&lt;/center&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/7.jpg&quot; /&gt;&lt;/center&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/8.jpg&quot; /&gt;&lt;/center&gt;

&lt;h2 id=&quot;24-比较检验&quot;&gt;2.4 比较检验&lt;/h2&gt;
&lt;p&gt;我们有了实验评估方法与性能度量，是否就可以对学习器的性能进行评估比较了呢？实际上，机器学习中性能比较这件事比大家想象的要复杂很多，我们需要考虑多种因素的影响，下面介绍一些对学习器性能进行比较的方法，本小节默认以错误率作为性能度量。(但我觉得似乎同一个数据集下就可以比较)&lt;/p&gt;

&lt;h3 id=&quot;241-假设检验&quot;&gt;2.4.1 假设检验&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;假设检验中的假设是对学习器泛化错误率分布的某种猜想或者判断，例如“ε=ε&lt;sub&gt;0&lt;/sub&gt;”这样的假设&lt;/li&gt;
  &lt;li&gt;现实任务中我们并不知道学习器的泛化错误率，我们只能获知其测试错误率$\hat{\epsilon}$，但直观上，两者接近的可能性比较大，因此可根据测试错误率估推出泛化错误率的分布。&lt;/li&gt;
  &lt;li&gt;泛化错误率为$\epsilon$的学习器被测得测试错误率为$\hat{\epsilon}$的概率：&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/9.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;我们发现$\epsilon$符合二项分布&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/10.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;二项检验：我们可以使用&lt;strong&gt;二项检验(binomial test)&lt;/strong&gt;来对“$\epsilon$&amp;lt;0.3”这样的假设进行检验，即在$\alpha$显著度下，$1-\alpha$置信度下判断假设是否成立。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;t检验：我们也可以用t检验(t-test)来检验。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;上面介绍的都是针对单个学习器泛化性能的假设进行检验&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;242-交叉验证t检验&quot;&gt;2.4.2 交叉验证t检验&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;对于两个学习器A和B，若我们使用k折交叉验证法得到的测试错误率分别是$\epsilon_1^A$, $\epsilon_2^A$…$\epsilon_k^A$和$\epsilon_1^B$, $\epsilon_2^B$…$\epsilon_k^B$。其中$\epsilon_i^A$和$\epsilon_i^B$是在相同第i折训练集上得到的结果。则可以用k折交叉验证“成对t检验”来进行比较检验。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;我们这里的基本思想是若两个学习器的性能相同，则它们使用相同的训练测试集得到的错误率应该相同，即$\epsilon_i^A=\epsilon_1^B$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\Delta_i$ = $\epsilon_i^A$ - $\epsilon_i^B$，然后对$\Delta$进行分析&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;243-mcnemar检验&quot;&gt;2.4.3 McNemar检验&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;对于二分类问题，使用留出法不仅可以估计出学习器A和B的测试错误率，还可获得两学习器分类结果的差别，即两者都正确、都错误…的样本数&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/11.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;若我们假设两学习器性能相同，则应有e&lt;sub&gt;01&lt;/sub&gt;=e&lt;sub&gt;10&lt;/sub&gt;，那么变量|e&lt;sub&gt;01&lt;/sub&gt;-e&lt;sub&gt;10&lt;/sub&gt;|应该服从正态分布/卡方分布，然后用McNemar检验&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;244-friedman检验与nemenyi后续检验&quot;&gt;2.4.4 Friedman检验与Nemenyi后续检验&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;交叉验证t检验与McNemar检验都是在一个数据集上比较两个算法的性能，但是很多时候，我们会在一组数据集上比较多个算法。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;当有多个算法参与比较时，一种做法是在每个数据集上本别列出两两比较的结果。另一种方法则是基于算法排列的&lt;strong&gt;Friedman检验&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;假定我们用D&lt;sub&gt;1&lt;/sub&gt;, D&lt;sub&gt;2&lt;/sub&gt;, D&lt;sub&gt;3&lt;/sub&gt;, D&lt;sub&gt;4&lt;/sub&gt;四个数据集对算法A、B、C进行比较。首先，使用留出法或交叉验证法得到每个算法在每个数据集上的测试结果。然后在每个数据集上根据测试性能由好到坏排序，并赋予序值1,2,…若算法的测试性能相同，则平分序值。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/12.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;然后使用Fredman检验来判断这些算法是否性能都相同，若相同，那么它们的平均序值应当相同。r&lt;sub&gt;i&lt;/sub&gt;表示第i个算法的平均序值，那么它的均值和方差应该满足…&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;若“所有算法的性能相同”这个假设被拒绝，则说明算法的性能显著不同，这时需要后续检验(post-hoc test)来进一步区分各算法，常用的有Nemenyi后续检验&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Nemenyi检验计算出平均序值差别的临界值域&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/13.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;在表中找到k=3时q&lt;sub&gt;0.05&lt;/sub&gt;=2.344，根据公式计算出临界值域CD=1.657，由表中的平均序值可知A与B算法的差距，以及算法B与C的差距均未超过临界值域，而算法A与C的差距超过临界值域，因此检验结果认为算法A与C的性能显著不同，而算法A与B以及算法B与C的性能没有显著差别。&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/14.jpg&quot; /&gt;&lt;/center&gt;

&lt;h2 id=&quot;25-偏差与方差&quot;&gt;2.5 偏差与方差&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;对学习算法除了通过实验估计其泛化性能，人们往往还希望了解它“为什么”具有这样的性能。偏差-方差分解是解释学习算法泛化性能的一种重要工具&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;偏差&lt;/strong&gt;度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;方差&lt;/strong&gt;度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;噪声&lt;/strong&gt;则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;一般来说，偏置与方差是有冲突的，也就是偏置大的方差小，偏置小的方差大&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/15.jpg&quot; /&gt;&lt;/center&gt;

&lt;h1 id=&quot;第3章-线性模型&quot;&gt;第3章 线性模型&lt;/h1&gt;
&lt;h2 id=&quot;31-基本形式&quot;&gt;3.1 基本形式&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;给定由d个属性描述的示例$x=(x_1;x_2;…x_d)$，这是一个列向量，其中$x_i$是$x$在第i个属性上的取值。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;线性模型(linear model)&lt;/strong&gt;试图学得一个通过属性线性组合来进行预测的函数：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/17.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;向量形式：&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;$f(x)=\omega^Tx+b$&lt;/center&gt;

&lt;p&gt;其中$\omega=(\omega_1;\omega_2…\omega_d)$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;当$\omega$和b学得后，模型就得以确定&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;线性模型的优点：形式简单，易于建模，良好的可解释性(comprehensibility)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;32-线性回归&quot;&gt;3.2 线性回归&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;对离散属性的处理：1.若属性值存在“序”的关系，可通过连续化将其转化为连续值，比如高、矮变成1、0；2.若不存在序的关系，可转化为k维向量。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;均方误差是回归任务中最常用的性能度量，试图让均方误差最小化：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/18.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;均方误差有非常好的几何意义，它对应了欧氏距离。基于均方误差最小化来进行模型求解的方法称为&lt;strong&gt;最小二乘法(least square method)&lt;/strong&gt;。在线性回归中，最小二乘法就是试图找到一条直线，使得样本到直线上的欧氏距离之和最小&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;首先观察一个属性值的情况。求解$\omega$和$b$使得均方误差最小化的过程，称为线性回归的最小二乘“参数估计”，我们令均方误差分别对$\omega$和$b$求导令其为零，可以得到最优解的&lt;strong&gt;闭式解(closed-form)&lt;/strong&gt;,即解析解&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/19.jpg&quot; /&gt;&lt;/center&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/20.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;更一般的情况是d个属性，称其为“多元线性回归”，同样的步骤，只是$\omega$变成向量形式，自变量写成m*(d+1)的矩阵形式，m对应了m个样本，d+1对应了d个属性和偏置。同样的求导为0然后得出$\hat{\omega}$最优解的闭式解，其中$\hat{\omega}=(\omega;b)$。当$X^TX$为满秩矩阵&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;或正定矩阵时，有唯一的解：&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/21.jpg&quot; /&gt;&lt;/center&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/22.jpg&quot; /&gt;&lt;/center&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/23.jpg&quot; /&gt;&lt;/center&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/24.jpg&quot; /&gt;&lt;/center&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/25.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;然而现实任务中往往不是满秩矩阵，例如在许多任务中我们会遇到大量的变量其数目甚至超过样例数。那么我们会求出$\hat{\omega}=(\omega;b)$的多个解，它们都能使均方误差最小化。选择哪一个解作为输出与学习算法的归纳偏好决定，常见的做法是&lt;strong&gt;引入正则化(regularization)&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;广义线性模型(generalized linear model)&lt;/strong&gt;:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/26.jpg&quot; /&gt;&lt;/center&gt;

&lt;center&gt;$g(y)=\omega^Tx+b$&lt;/center&gt;

&lt;p&gt;其中g()单调可微，被称为联系函数。比如当g()=ln()时称为对数线性回归&lt;/p&gt;

&lt;h2 id=&quot;33-对数几率回归&quot;&gt;3.3 对数几率回归&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;上一节讨论的是线性回归进行回归学习。那么如果我们要做的是分类任务应该怎么改变？我们只需要找到一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;考虑二分类问题，那么我们可以用单位阶跃函数/Heaviside函数联系y与z，其中y是真是标记，z是预测值，$z=\omega^Tx+b$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/27.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;但是它有个问题就是不连续，所以我们希望找到能在一定程度上近似它的替代函数。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;对数几率函数(logistic function)&lt;/strong&gt;就是一个替代函数：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;$y=\frac{1}{1+e^{-z}}$&lt;/center&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/28.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;那么这个式子可以转化为：可以把y视为样本x作为正例的可能性，则1-y是其反例的可能性，两者的比值称为几率(odds)：&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/29.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;若将y视为类后验概率估计。则式子可以重写为：&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/30.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;接下来我们可以通过&lt;strong&gt;极大似然法(maximum likelihood method)&lt;/strong&gt;来估计$\omega$和$b$。给定数据集，对数似然函数&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;为：&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/31.jpg&quot; /&gt;&lt;/center&gt;

&lt;p&gt;即每个样本属于其真实标记的概率越大越好。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;推导过程：&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211222/32.jpg&quot; /&gt;&lt;/center&gt;

&lt;p&gt;上面有个式子应该有问题，(3.26)应该是&lt;/p&gt;

&lt;center&gt;$p(y_i|x_i;\omega,b) = p_1(\hat{x_i};\beta)^{y_i} + p_0(\hat{x_i};\beta)^{1-y_i}$&lt;/center&gt;

&lt;p&gt;因为$\beta$是高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法如&lt;strong&gt;梯度下降法(gradient descent method)&lt;/strong&gt;和牛顿法都可以求得最优解&lt;/p&gt;

&lt;h2 id=&quot;34-线性判别分析&quot;&gt;3.4 线性判别分析&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;线性判别分析(Linear Discriminant Analysis，简称LDA)是一种经典的线性学习方法。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;注意这里的测试集其实是验证集，我们通常把实际情况中遇到的数据集称为测试集。 &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;满秩矩阵指方阵的秩等于矩阵的行数/列数，满秩矩阵有逆矩阵且对于y=Xb有唯一的解 &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;统计学中，似然函数是一种关于统计模型参数的函数。给定输出x时，关于参数θ的似然函数L(θ|x)（在数值上）等于给定参数θ后变量X的概率：L(θ|x)=P(X=x|θ)。 &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Quehry</name></author><category term="note" /><summary type="html">目录</summary></entry><entry><title type="html">组会记录</title><link href="http://localhost:4000/1221%E7%BB%84%E4%BC%9A.html" rel="alternate" type="text/html" title="组会记录" /><published>2021-12-21T00:00:00+08:00</published><updated>2021-12-21T00:00:00+08:00</updated><id>http://localhost:4000/1221%E7%BB%84%E4%BC%9A</id><content type="html" xml:base="http://localhost:4000/1221%E7%BB%84%E4%BC%9A.html">&lt;h1 id=&quot;vae&quot;&gt;VAE&lt;/h1&gt;
&lt;h2 id=&quot;ae&quot;&gt;AE&lt;/h2&gt;
&lt;p&gt;Auto-Encoder自动编码器，比如Seq2seq模型。&lt;/p&gt;

&lt;h2 id=&quot;vaevariational-auto-encoder&quot;&gt;VAE(Variational Auto-Encoder)&lt;/h2&gt;
&lt;p&gt;在实际情况中，我们需要在模型的准确率上与隐含向量服从标准正态分布之间做一个权衡，所谓模型的准确率就是指解码器生成的图片与原图片的相似程度。我们可以让网络自己来做这个决定，非常简单，我们只需要将这两者都做一个loss，然后在将他们求和作为总的loss，这样网络就能够自己选择如何才能够使得这个总的loss下降。另外我们要衡量两种分布的相似程度，如何看过之前一片GAN的数学推导，你就知道会有一个东西叫KL-divergence来衡量两种分布的相似程度，这里我们就是用KL-divergence来表示隐含向量与标准正态分布之间差异的loss，另外一个loss仍然使用生成图片与原图片的均方误差来表示。&lt;/p&gt;

&lt;h2 id=&quot;kl消失&quot;&gt;KL消失&lt;/h2&gt;
&lt;p&gt;KL消失后，VAE就变成了AE
原因：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;KL项本身太容易被优化&lt;/li&gt;
  &lt;li&gt;一旦崩塌，Decoder会忽视Z&lt;sub&gt;x&lt;/sub&gt;&lt;/li&gt;
  &lt;li&gt;Z&lt;sub&gt;x&lt;/sub&gt;的表示学习依赖于Decoder&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;解决kl消失的思路&quot;&gt;解决KL消失的思路&lt;/h2&gt;
&lt;p&gt;…&lt;/p&gt;

&lt;h1 id=&quot;analyze-pretraining-language-model&quot;&gt;Analyze Pretraining Language Model&lt;/h1&gt;
&lt;h2 id=&quot;perspective-of-knowledge&quot;&gt;Perspective of knowledge&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Syntacitic/Semantic/lexical 句法，语义，词汇&lt;/li&gt;
  &lt;li&gt;重构语法树&lt;/li&gt;
  &lt;li&gt;Attention中很多头可能没有用，学到了很多冗余的信息&lt;/li&gt;
  &lt;li&gt;Analyze Feed Forward Neural Network&lt;/li&gt;
  &lt;li&gt;浅层词汇信息，深层语义信息&lt;/li&gt;
  &lt;li&gt;Prompt&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Quehry</name></author><category term="note" /><summary type="html">VAE AE Auto-Encoder自动编码器，比如Seq2seq模型。</summary></entry><entry><title type="html">制作类RACE数据集</title><link href="http://localhost:4000/RACElike-datasets.html" rel="alternate" type="text/html" title="制作类RACE数据集" /><published>2021-12-21T00:00:00+08:00</published><updated>2021-12-21T00:00:00+08:00</updated><id>http://localhost:4000/RACElike-datasets</id><content type="html" xml:base="http://localhost:4000/RACElike-datasets.html">&lt;h1 id=&quot;目录&quot;&gt;目录&lt;/h1&gt;
&lt;!-- TOC --&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#目录&quot;&gt;目录&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#race&quot;&gt;RACE&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#简介&quot;&gt;简介&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#race数据集格式&quot;&gt;RACE数据集格式&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#race数据集分布&quot;&gt;RACE数据集分布&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#race数据集中的长度&quot;&gt;RACE数据集中的长度&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#race数据集中的问题的统计信息&quot;&gt;RACE数据集中的问题的统计信息&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gaorace&quot;&gt;GaoRACE&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#gao他们对于race数据集的处理&quot;&gt;Gao他们对于RACE数据集的处理&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#gao处理后的race数据集统计信息&quot;&gt;Gao处理后的RACE数据集统计信息&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#gao处理后的数据集格式&quot;&gt;Gao处理后的数据集格式&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#预处理&quot;&gt;预处理&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#updated&quot;&gt;updated&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#预处理代码&quot;&gt;预处理代码&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#mrc-阅读理解数据集&quot;&gt;MRC 阅读理解数据集&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#简介-1&quot;&gt;简介&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#title&quot;&gt;Title&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#abstract&quot;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#table-一张十分完整的表格&quot;&gt;Table 一张十分完整的表格&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#值得关注的地方&quot;&gt;值得关注的地方&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#自制数据集&quot;&gt;自制数据集&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#大型题库&quot;&gt;大型题库&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#方法&quot;&gt;方法&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- /TOC --&gt;
&lt;h1 id=&quot;race&quot;&gt;RACE&lt;/h1&gt;
&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;
&lt;p&gt;RACE数据集包含了中国初高中阅读理解题目，最初发布在2017年，一共含有28k短文和100k个问题，最开始发布的目的是为了&lt;strong&gt;阅读理解&lt;/strong&gt;任务。它的特点是包含了很多需要推理的问题。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;原RACE数据集&lt;a href=&quot;http://www.cs.cmu.edu/~glai1/data/race/&quot;&gt;地址&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;下载地址&lt;a href=&quot;http://www.cs.cmu.edu/~glai1/data/race/RACE.tar.gz&quot;&gt;url&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1704.04683&quot;&gt;RACE: Large-scale ReAding Comprehension Dataset From Examinations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;race数据集格式&quot;&gt;RACE数据集格式&lt;/h2&gt;
&lt;p&gt;Each passage is a JSON file. The JSON file contains following fields:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;article: A string, which is the passage. 文章&lt;/li&gt;
  &lt;li&gt;questions: A string list. Each string is a query. We have two types of questions. First one is an interrogative sentence. Another one has a placeholder, which is represented by _. 四个问题题干&lt;/li&gt;
  &lt;li&gt;options: A list of the options list. Each options list contains 4 strings, which are the candidate option. 四个题目的四个选项&lt;/li&gt;
  &lt;li&gt;answers: A list contains the golden label of each query.四个题目的正确答案&lt;/li&gt;
  &lt;li&gt;id: Each passage has a unique id in this dataset.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;race数据集分布&quot;&gt;RACE数据集分布&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/3.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RACE-M表示初中题目，RACE-H表示高中题目&lt;/p&gt;

&lt;h2 id=&quot;race数据集中的长度&quot;&gt;RACE数据集中的长度&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/4.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;race数据集中的问题的统计信息&quot;&gt;RACE数据集中的问题的统计信息&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/5.jpg&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;gaorace&quot;&gt;GaoRACE&lt;/h1&gt;
&lt;h2 id=&quot;gao他们对于race数据集的处理&quot;&gt;Gao他们对于RACE数据集的处理&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;去掉了那些误导选项和文章语义不相关的数据&lt;/li&gt;
  &lt;li&gt;去掉了那些需要&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;world knowledge&lt;/code&gt;生成的选项&lt;/li&gt;
  &lt;li&gt;github&lt;a href=&quot;https://github.com/Yifan-Gao/Distractor-Generation-RACE&quot;&gt;url&lt;/a&gt;,上面有预处理RACE数据集的代码&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;gao处理后的race数据集统计信息&quot;&gt;Gao处理后的RACE数据集统计信息&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/7.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;gao处理后的数据集格式&quot;&gt;Gao处理后的数据集格式&lt;/h2&gt;

&lt;h3 id=&quot;预处理&quot;&gt;预处理&lt;/h3&gt;

&lt;p&gt;首先把数据集规整到一个json文件里，分为dev,test,train三个json文件。&lt;/p&gt;

&lt;p&gt;每一行包含以下信息：&lt;/p&gt;

&lt;p&gt;article, sent(sentence), question(问题有两种，一种是疑问句，一种是填空), answer_text, answer, id, word_overlap_score, word_overlap_count, article_id, question_id, distractor_id.&lt;/p&gt;

&lt;p&gt;那么一个问题会有2-3个误导选项，一篇文章又会有3-4个问题。相比于原本的数据集多了word-overlap指标，word-overlap就是词重叠率，交集比上并集。&lt;/p&gt;

&lt;h3 id=&quot;updated&quot;&gt;updated&lt;/h3&gt;
&lt;p&gt;updated数据集和original数据集格式类似，少了overlap，内容上去掉了一些语义不相关的题目。&lt;/p&gt;

&lt;h3 id=&quot;预处理代码&quot;&gt;预处理代码&lt;/h3&gt;
&lt;p&gt;利用torchtext框架预处理文本，流程大概如下：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;定义Field：声明如何处理数据 定义&lt;/li&gt;
  &lt;li&gt;Dataset：得到数据集，此时数据集里每一个样本是一个 经过 Field声明的预处理 预处理后的 wordlist&lt;/li&gt;
  &lt;li&gt;建立vocab：在这一步建立词汇表，词向量(word embeddings)&lt;/li&gt;
  &lt;li&gt;构造迭代器：构造迭代器，用来分批次训练模型&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Gao说有去掉一些语义不相关的误导选项，但是在代码中并没有看见这步操作？？&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/8.jpg&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;mrc-阅读理解数据集&quot;&gt;MRC 阅读理解数据集&lt;/h1&gt;

&lt;h2 id=&quot;简介-1&quot;&gt;简介&lt;/h2&gt;
&lt;p&gt;发现了一篇很好的综述，里面涵盖了2021年之前用到的所有MRC数据集。现在对这篇综述简单介绍一下&lt;/p&gt;

&lt;h2 id=&quot;title&quot;&gt;Title&lt;/h2&gt;
&lt;p&gt;English Machine Reading Comprehension Datasets: A Survey&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;文献收集了60个英语阅读理解数据集，分别从不同维度进行比较，包括size, vocabulary, data source, method of creation, human performance level, first question word。调研发现维基百科是最多的数据来源，同时也发现了缺少很多why,when,where问题。&lt;/p&gt;

&lt;h2 id=&quot;table-一张十分完整的表格&quot;&gt;Table 一张十分完整的表格&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/44.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;首先我简单解释以下这个表格，这个表格一个收录了18个Multiple Choice Datasets,也就是说这18个数据集都着眼于多选题。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;第一列是数据集的名称。&lt;/li&gt;
  &lt;li&gt;第二列表示数据集中问题的个数(size)。&lt;/li&gt;
  &lt;li&gt;第三列表示数据集中文章的来源，其中ER表示education resource, AG表示automatically generated即自动生成,CRW表示crowdsourcing。&lt;/li&gt;
  &lt;li&gt;第四列表示答案的来源(answer)，其中UG表示user generated。&lt;/li&gt;
  &lt;li&gt;第五列LB表示leader board available，即是否有排行榜，带*表示排行榜在&lt;a href=&quot;https://paperswithcode.com/&quot;&gt;网站&lt;/a&gt;上发布。&lt;/li&gt;
  &lt;li&gt;第六列表示人在该数据集上的表现。&lt;/li&gt;
  &lt;li&gt;第七列表示该数据集是否有被解决，也就是说是否有比较好的模型能在该数据集上表现良好。&lt;/li&gt;
  &lt;li&gt;第八列表示问题第一个单词出现最频繁的是哪个？比如what,how,which这样的单词。&lt;/li&gt;
  &lt;li&gt;第九列PAD表示是否开源。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;值得关注的地方&quot;&gt;值得关注的地方&lt;/h2&gt;
&lt;p&gt;这么多数据集中，来源于考试题目的有RACE,RACE-C,DREAM,ReClor,这些数据集的收集方法可以借鉴。&lt;/p&gt;

&lt;h1 id=&quot;自制数据集&quot;&gt;自制数据集&lt;/h1&gt;
&lt;h2 id=&quot;大型题库&quot;&gt;大型题库&lt;/h2&gt;
&lt;p&gt;泸江，星火英语…&lt;/p&gt;
&lt;h2 id=&quot;方法&quot;&gt;方法&lt;/h2&gt;
&lt;p&gt;Python爬取网页&lt;/p&gt;</content><author><name>Quehry</name></author><category term="work" /><summary type="html">目录</summary></entry><entry><title type="html">计算机图形学</title><link href="http://localhost:4000/Computer_Graphics.html" rel="alternate" type="text/html" title="计算机图形学" /><published>2021-12-21T00:00:00+08:00</published><updated>2021-12-21T00:00:00+08:00</updated><id>http://localhost:4000/Computer_Graphics</id><content type="html" xml:base="http://localhost:4000/Computer_Graphics.html">&lt;!-- TOC --&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#1-计算机图形学&quot;&gt;1. 计算机图形学&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#11-lecture-01-overview-of-computer-graphics&quot;&gt;1.1. Lecture 01 Overview of Computer Graphics&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#111-课程情况&quot;&gt;1.1.1. 课程情况&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#112-什么是好的画面&quot;&gt;1.1.2. 什么是好的画面&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#113-应用场景&quot;&gt;1.1.3. 应用场景&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#114-rasterization-光栅化&quot;&gt;1.1.4. Rasterization 光栅化&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#115-计算机视觉&quot;&gt;1.1.5. 计算机视觉&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#116-推荐书籍&quot;&gt;1.1.6. 推荐书籍&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#12-lecture-02-review-of-linear-algebra&quot;&gt;1.2. Lecture 02 Review of Linear Algebra&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#121-图形学依赖学科&quot;&gt;1.2.1. 图形学依赖学科&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#122-向量&quot;&gt;1.2.2. 向量&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#123-矩阵&quot;&gt;1.2.3. 矩阵&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#13-lecture-03-transformation&quot;&gt;1.3. Lecture 03 Transformation&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#131-why-transformation-为什么要变换&quot;&gt;1.3.1. why transformation 为什么要变换&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#132-d变换&quot;&gt;1.3.2. D变换&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#133-齐次坐标-homogeneous-coordinate&quot;&gt;1.3.3. 齐次坐标 homogeneous coordinate&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#14-lecture-04-transformation-cont&quot;&gt;1.4. Lecture 04 Transformation Cont.&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#141-d-transformations&quot;&gt;1.4.1. D Transformations&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#142-view-transformation-视图变换&quot;&gt;1.4.2. view transformation 视图变换&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#143-projection-transformation-投影变换&quot;&gt;1.4.3. projection transformation 投影变换&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#15-lecture05-rasterization-1triangles&quot;&gt;1.5. Lecture05 Rasterization 1(Triangles)&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#151-perspective-projection-透视投影&quot;&gt;1.5.1. Perspective Projection 透视投影&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#152-canonical-cube-to-screen-光栅化&quot;&gt;1.5.2. Canonical Cube to Screen 光栅化&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#153-different-raster-displays-不同的成像设备&quot;&gt;1.5.3. Different Raster Displays 不同的成像设备&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#154-三角形光栅化&quot;&gt;1.5.4. 三角形光栅化&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#16-lecture-06-rasterization-2antialiasing-and-z-buffering&quot;&gt;1.6. Lecture 06 Rasterization 2(Antialiasing and Z-Buffering)&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#161-sampling-采样原理&quot;&gt;1.6.1 sampling 采样原理&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#162-frequency-domaine-信号处理频率&quot;&gt;1.6.2. Frequency domaine 信号处理频率&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#163-antialiasing-反走样抗锯齿&quot;&gt;1.6.3. antialiasing 反走样/抗锯齿&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#164-antialiasing-today-目前反走样的方法&quot;&gt;1.6.4. antialiasing today 目前反走样的方法&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#17-lecture-07-shadingillumination-shading-and-graphics-pipeline&quot;&gt;1.7. Lecture 07 Shading(Illumination, Shading, and Graphics Pipeline)&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#171-painters-algorithm-画家算法&quot;&gt;1.7.1. Painter’s Algorithm 画家算法&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#172-z-buffer-深度缓存&quot;&gt;1.7.2. Z-buffer 深度缓存&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#173-目前为止学到了什么&quot;&gt;1.7.3. 目前为止学到了什么&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#174-shading-着色&quot;&gt;1.7.4. shading 着色&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#18-shading-2shading-pipeline-texture-mapping&quot;&gt;1.8. Shading 2(Shading, Pipeline, Texture Mapping)&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#181-specular-term-高光项&quot;&gt;1.8.1. Specular Term 高光项&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#182-ambient-term-环境项&quot;&gt;1.8.2. Ambient Term 环境项&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#183-shading-frequencies-着色频率&quot;&gt;1.8.3. Shading Frequencies 着色频率&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#184-graphics-pipeline-图像管线实时渲染管线&quot;&gt;1.8.4. Graphics Pipeline 图像管线/实时渲染管线&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#185-texture-mapping-纹理映射&quot;&gt;1.8.5. Texture Mapping 纹理映射&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#19-lecture-09-shading-3-texture-mapping&quot;&gt;1.9. Lecture 09 Shading 3 (Texture Mapping)&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#191-barycentric-coordinates重心坐标系&quot;&gt;1.9.1 Barycentric Coordinates重心坐标系&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#192-interpolate-插值&quot;&gt;1.9.2. Interpolate 插值&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#193-simple-texture-mapping-简单的纹理映射模型&quot;&gt;1.9.3. Simple Texture Mapping 简单的纹理映射模型&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#194-texture-magnification-纹理放大&quot;&gt;1.9.4. Texture Magnification 纹理放大&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#195-point-sampling-textures&quot;&gt;1.9.5. Point Sampling Textures&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#196-mipmap-范围查询&quot;&gt;1.9.6. Mipmap 范围查询&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- /TOC --&gt;

&lt;h1 id=&quot;1-计算机图形学&quot;&gt;1. 计算机图形学&lt;/h1&gt;
&lt;h2 id=&quot;11-lecture-01-overview-of-computer-graphics&quot;&gt;1.1. Lecture 01 Overview of Computer Graphics&lt;/h2&gt;
&lt;h3 id=&quot;111-课程情况&quot;&gt;1.1.1. 课程情况&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;授课老师：闫令琪&lt;/li&gt;
  &lt;li&gt;授课形式：网课（B站）&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;112-什么是好的画面&quot;&gt;1.1.2. 什么是好的画面&lt;/h3&gt;
&lt;p&gt;画面&lt;strong&gt;亮&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;113-应用场景&quot;&gt;1.1.3. 应用场景&lt;/h3&gt;
&lt;p&gt;电影，游戏，动画，设计，可视化，虚拟现实，增强现实，模拟，GUI图形用户接口。&lt;/p&gt;

&lt;p&gt;电影中里程碑：阿凡达，大量应用面部捕捉技术。&lt;/p&gt;
&lt;h3 id=&quot;114-rasterization-光栅化&quot;&gt;1.1.4. Rasterization 光栅化&lt;/h3&gt;
&lt;p&gt;实时，FPS&amp;gt;30&lt;/p&gt;

&lt;p&gt;离线, FPS&amp;lt;30&lt;/p&gt;
&lt;h3 id=&quot;115-计算机视觉&quot;&gt;1.1.5. 计算机视觉&lt;/h3&gt;
&lt;p&gt;计算机图形学离不开计算机视觉，但是视觉一般是对图像的处理。&lt;/p&gt;

&lt;h3 id=&quot;116-推荐书籍&quot;&gt;1.1.6. 推荐书籍&lt;/h3&gt;
&lt;p&gt;Tiger虎书&lt;/p&gt;

&lt;h2 id=&quot;12-lecture-02-review-of-linear-algebra&quot;&gt;1.2. Lecture 02 Review of Linear Algebra&lt;/h2&gt;
&lt;h3 id=&quot;121-图形学依赖学科&quot;&gt;1.2.1. 图形学依赖学科&lt;/h3&gt;
&lt;p&gt;Optics, Mechanics, Linear algebra, statics, Singal processing, numerical analysis数值分析&lt;/p&gt;

&lt;h3 id=&quot;122-向量&quot;&gt;1.2.2. 向量&lt;/h3&gt;

&lt;p&gt;向量的定义&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/9.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;单位向量&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/10.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;向量计算，向量加法&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/11.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;用笛卡尔坐标系表示向量&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/12.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;向量乘法，点乘和叉乘，点乘在笛卡尔坐标系中就是对应元素相乘。&lt;/p&gt;

&lt;p&gt;在图形学中，点乘是为了寻找两个向量的夹角(夹角可以判断两个向量方向的接近程度)，或者获得一个向量在另一个向量的投影，还可以获得向量的分解。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/13.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;叉乘，叉积结果垂直于这两个向量所在的平面，满足右手定则。向量的叉乘可以写成矩阵形式。&lt;/p&gt;

&lt;p&gt;在图形学中的应用：判断左右关系，比如a^b&amp;gt;0，说明b在a的左边。还可以判断内外，比如判断一个点是否在一个三角形内。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/14.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;坐标系的定义，右手坐标系&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/15.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;123-矩阵&quot;&gt;1.2.3. 矩阵&lt;/h3&gt;

&lt;p&gt;矩阵定义&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/16.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;矩阵乘法&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/17.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;矩阵乘法没有交换律，但是有结合律&lt;/p&gt;

&lt;p&gt;矩阵转置，矩阵的逆&lt;/p&gt;

&lt;p&gt;向量的点乘和叉乘都可以写成矩阵乘法形式&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/18.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;13-lecture-03-transformation&quot;&gt;1.3. Lecture 03 Transformation&lt;/h2&gt;

&lt;h3 id=&quot;131-why-transformation-为什么要变换&quot;&gt;1.3.1. why transformation 为什么要变换&lt;/h3&gt;
&lt;p&gt;viewing: 3D to 2D projection&lt;/p&gt;

&lt;h3 id=&quot;132-d变换&quot;&gt;1.3.2. D变换&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;缩放 scale transform&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/19.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;非均匀缩放 scale(non-uniform)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/20.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;翻转 reflection matrix&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/21.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;切变 shear matrix&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;竖直方向上没有变化，水平方向上发生了变化&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/22.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;旋转 Rotate&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;旋转默认绕零点逆时针旋转&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/23.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;二维旋转矩阵R&lt;/p&gt;

&lt;p&gt;上述所有的变化都可以写成x$\prime$=Mx，也就是线性变换&lt;/p&gt;

&lt;h3 id=&quot;133-齐次坐标-homogeneous-coordinate&quot;&gt;1.3.3. 齐次坐标 homogeneous coordinate&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;为什么要引入齐次坐标，因为对于简单的平移操作并不能写成线性变换的形式，但是人们也不想认为平移是一种特殊的变换，所以引入齐次坐标&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;齐次坐标&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注意点和向量的表示方法不同&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/24.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;仿射变换 affine transformations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/25.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;2D Transformations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/26.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;逆变换就是乘以逆矩阵&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;复杂的变换都是简单的变换的组合，变换的组合顺序很重要&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;绕着某一个点（非原点）旋转的分解&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/27.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;14-lecture-04-transformation-cont&quot;&gt;1.4. Lecture 04 Transformation Cont.&lt;/h2&gt;

&lt;h3 id=&quot;141-d-transformations&quot;&gt;1.4.1. D Transformations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;齐次坐标&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于w不等于1，每一个坐标除以w&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/28.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;正交矩阵&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一个矩阵的逆等于矩阵的转置，旋转矩阵就是一个正交矩阵&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;仿射变换（旋转+平移）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;仿射变换是先进行旋转再进行平移&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/29.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;矩阵表示（缩放，平移）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/30.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;旋转&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;绕着某一个轴旋转&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/31.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一般的旋转（分解成三个坐标轴的旋转）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/32.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Rodrigues’ Rotation Formula, 用向量n表示旋转轴，最终推出这个公式&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/33.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;142-view-transformation-视图变换&quot;&gt;1.4.2. view transformation 视图变换&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;观测变换viewing，包括了视图变化和投影变化&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MVP变换(model-&amp;gt;view-&amp;gt;projection)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/34.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;view transformation(不等于viewing) 视图变换&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;视图变换是把相机放到标准位置上，located at origin, look at -Z&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/35.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;利用逆变换，先平移再旋转&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/36.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一般把model和view变换统称为view transformation&lt;/p&gt;

&lt;h3 id=&quot;143-projection-transformation-投影变换&quot;&gt;1.4.3. projection transformation 投影变换&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;orthographic vs perspectiive projection&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/37.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;orthographic projection 正交投影&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/38.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;平移，缩放（不考虑旋转）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/39.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;perspective projection 透视投影&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;满足近大远小&lt;/p&gt;

&lt;p&gt;透视投影就是先把物体挤压成立方体，然后对立方体进行正交投影&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/41.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/40.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/42.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211221/43.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;15-lecture05-rasterization-1triangles&quot;&gt;1.5. Lecture05 Rasterization 1(Triangles)&lt;/h2&gt;

&lt;h3 id=&quot;151-perspective-projection-透视投影&quot;&gt;1.5.1. Perspective Projection 透视投影&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;首先是对上节课的透视投影的一些补充, 其中l=left, r=right, b=bottom, t=top, n=near, f=far，这些量可以描述视锥Frustum&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/45.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;视锥Frustum的描述还可以用fovY(field of view)垂直视角和aspect ratio宽高比&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/46.jpg&quot; /&gt;&lt;/center&gt;

&lt;h3 id=&quot;152-canonical-cube-to-screen-光栅化&quot;&gt;1.5.2. Canonical Cube to Screen 光栅化&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;把物体的数学描述以及与物体相关的颜色信息转换为屏幕上用于对应位置的像素及用于填充像素的颜色，这个过程称为光栅化。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;屏幕是最常见的光栅设备，每一个像素都是一个小方块，像素是最小的单位，一个像素的颜色可以用rgb三种颜色表示&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/47.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;屏幕空间screen space&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/48.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;把之前投影后的小方块变成屏幕空间&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/49.jpg&quot; /&gt;&lt;/center&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/50.jpg&quot; /&gt;&lt;/center&gt;

&lt;h3 id=&quot;153-different-raster-displays-不同的成像设备&quot;&gt;1.5.3. Different Raster Displays 不同的成像设备&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Oscilloscope 示波器&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cathode Ray Tube 阴极射线管成像原理。早期电视屏幕就是这样实现成像，扫描成像。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/51.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;Frame Buffer: Memory for a Raster Display 内存中的一块区域存储图像信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/52.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;LCD(liquid crystal display)液晶显示器，光的波动性原理。&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/53.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;LED发光二极管&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/54.jpg&quot; /&gt;&lt;/center&gt;

&lt;h3 id=&quot;154-三角形光栅化&quot;&gt;1.5.4. 三角形光栅化&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;三角形是最基本的多边形，有很多好的性质。&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/55.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;sampling 采样。三角形离散化。&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/56.jpg&quot; /&gt;&lt;/center&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/57.jpg&quot; /&gt;&lt;/center&gt;

&lt;p&gt;在不同的像素中心，确定是0还是1,表示在三角形里还是外&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/58.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;如何判断点和三角形关系，利用叉积，边界上的点自己定义。&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/59.jpg&quot; /&gt;&lt;/center&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/60.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;jaggies锯齿，走样aliasing&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/61.jpg&quot; /&gt;&lt;/center&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/62.jpg&quot; /&gt;&lt;/center&gt;

&lt;h2 id=&quot;16-lecture-06-rasterization-2antialiasing-and-z-buffering&quot;&gt;1.6. Lecture 06 Rasterization 2(Antialiasing and Z-Buffering)&lt;/h2&gt;

&lt;h3 id=&quot;161-sampling-采样原理&quot;&gt;1.6.1 sampling 采样原理&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;视频就是对时间进行采样&lt;/li&gt;
  &lt;li&gt;采样的artifact(瑕疵)：锯齿，摩尔纹，轮胎效应(在时间上采样)&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/63.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;反走样采样：可以对原始的图像进行滤波(模糊处理)然后再采样。&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/64.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;采样速度跟不上信号变化的速度就会走样(aliasing)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;162-frequency-domaine-信号处理频率&quot;&gt;1.6.2. Frequency domaine 信号处理频率&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;傅里叶变换：所有的周期函数都可以写成不同平吕的正弦函数的组合。傅里叶变换就是频域和时域/空间域的变换&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/66.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;走样的原因(时域)：高频信号欠采样，高频信号和低频信号在某一采样速度下没有差别，就会产生走样&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/65.jpg&quot; /&gt;&lt;/center&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/67.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;滤波：抹掉特定的频率。比如高通滤波(过滤到低频信号)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;卷积：图形学上的简化定义，见下图&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/68.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;卷积定律：时域上的卷积等于频域上的乘积&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/69.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;采样：重复频域上的内容&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/70.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;走样在频率上的解释：采样频率小会让频域上发生重叠&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/71.jpg&quot; /&gt;&lt;/center&gt;

&lt;h3 id=&quot;163-antialiasing-反走样抗锯齿&quot;&gt;1.6.3. antialiasing 反走样/抗锯齿&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;第一种解决方法：增加采样率，相当于增加了频域上的两个信号的距离&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;第二种解决方法：反走样。即先对信号进行滤波再采样&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/72.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;比如对于之前三角形的问题&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/73.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;但是这种反走样的方法比较复杂，有一种更简单的近似方法(对滤波这一步的近似)：supersampling，就是在对每个像素点变成更多的小点&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/74.jpg&quot; /&gt;&lt;/center&gt;

&lt;h3 id=&quot;164-antialiasing-today-目前反走样的方法&quot;&gt;1.6.4. antialiasing today 目前反走样的方法&lt;/h3&gt;
&lt;p&gt;介绍了两种新的抗锯齿的操作：FXAA和TAA。FXAA的做法是把边界找到然后对边界进行处理。&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/75.jpg&quot; /&gt;&lt;/center&gt;

&lt;h2 id=&quot;17-lecture-07-shadingillumination-shading-and-graphics-pipeline&quot;&gt;1.7. Lecture 07 Shading(Illumination, Shading, and Graphics Pipeline)&lt;/h2&gt;

&lt;h3 id=&quot;171-painters-algorithm-画家算法&quot;&gt;1.7.1. Painter’s Algorithm 画家算法&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;首先画出远处的物体，然后再画近处的物体。画近处的物体再覆盖远处的物体。&lt;/li&gt;
  &lt;li&gt;需要定义深度信息，根据深度信息排序&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;172-z-buffer-深度缓存&quot;&gt;1.7.2. Z-buffer 深度缓存&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;对每个像素都有最小的z值，除了一个frame buffer储存颜色信息外，还需要z-buffer储存深度信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/76.jpg&quot; /&gt;&lt;/center&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/77.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;假设每个像素最开始的时候深度为无限远&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;特点是在像素维度进行操作&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;173-目前为止学到了什么&quot;&gt;1.7.3. 目前为止学到了什么&lt;/h3&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/78.jpg&quot; /&gt;&lt;/center&gt;

&lt;h3 id=&quot;174-shading-着色&quot;&gt;1.7.4. shading 着色&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;着色：对不同物体应用不同的材质&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;一个简单的着色模型(Blinn-Phong Reflection model)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;局部着色，不考虑阴影&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/79.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;diffuse reflection 漫反射，一个物体有多亮与接收到多少光的能量有关。点光源的能量随距离缩减。在点光源的光线到达物体表面时被物体接受多少能量又与光线和法线的夹角的cos值有关，也就是说直射时接受的能量最大(相同距离)。漫反射表示不论观测角度在哪，你观测到的亮度应该是一样的。&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/80.jpg&quot; /&gt;&lt;/center&gt;

&lt;h2 id=&quot;18-shading-2shading-pipeline-texture-mapping&quot;&gt;1.8. Shading 2(Shading, Pipeline, Texture Mapping)&lt;/h2&gt;
&lt;h3 id=&quot;181-specular-term-高光项&quot;&gt;1.8.1. Specular Term 高光项&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;着色包括三部分：漫反射，高光，环境光&lt;/li&gt;
  &lt;li&gt;高光就是观测方向和镜面反射方向相同，即半程向量是否和法向量接近&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/81.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;通常高光都是白色的&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;182-ambient-term-环境项&quot;&gt;1.8.2. Ambient Term 环境项&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;环境光就是一些其他物体反射的光照亮背光物体&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;这里介绍非常简化的模型&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/82.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;最终结果&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/83.jpg&quot; /&gt;&lt;/center&gt;

&lt;h3 id=&quot;183-shading-frequencies-着色频率&quot;&gt;1.8.3. Shading Frequencies 着色频率&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;之前介绍的着色是应用在着色点，对应在屏幕空间是如何的呢？&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;第一种：Shading ecah triangle 对每个三角形着色&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/84.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;第二种：shading each vertex 对顶点着色，然后插值&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/85.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;第三种：shading each pixel 对每个像素点着色&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/86.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;如何定义顶点的法向量呢？对周围的面的法向量求平均&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/87.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;如何定义像素的法向量？&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/88.jpg&quot; /&gt;&lt;/center&gt;

&lt;h3 id=&quot;184-graphics-pipeline-图像管线实时渲染管线&quot;&gt;1.8.4. Graphics Pipeline 图像管线/实时渲染管线&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;一个实时渲染的流程/流水线&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/89.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;现代的GPU允许写入顶点着色部分与片段着色部分的代码&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;185-texture-mapping-纹理映射&quot;&gt;1.8.5. Texture Mapping 纹理映射&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;希望在物体的不同位置定义不同的属性，比如漫反射系数等等&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;3维物体的表现都是一个平面&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/90.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;纹理映射就是对于一个平面定义不同的属性，有一个映射关系&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/91.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;纹理也有坐标系&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/92.jpg&quot; /&gt;&lt;/center&gt;

&lt;h2 id=&quot;19-lecture-09-shading-3-texture-mapping&quot;&gt;1.9. Lecture 09 Shading 3 (Texture Mapping)&lt;/h2&gt;

&lt;h3 id=&quot;191-barycentric-coordinates重心坐标系&quot;&gt;1.9.1 Barycentric Coordinates重心坐标系&lt;/h3&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/93.jpg&quot; /&gt;&lt;/center&gt;

&lt;h3 id=&quot;192-interpolate-插值&quot;&gt;1.9.2. Interpolate 插值&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;重心坐标系插值&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/94.jpg&quot; /&gt;&lt;/center&gt;

&lt;h3 id=&quot;193-simple-texture-mapping-简单的纹理映射模型&quot;&gt;1.9.3. Simple Texture Mapping 简单的纹理映射模型&lt;/h3&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/95.jpg&quot; /&gt;&lt;/center&gt;

&lt;h3 id=&quot;194-texture-magnification-纹理放大&quot;&gt;1.9.4. Texture Magnification 纹理放大&lt;/h3&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/96.jpg&quot; /&gt;&lt;/center&gt;

&lt;h3 id=&quot;195-point-sampling-textures&quot;&gt;1.9.5. Point Sampling Textures&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;就是走样问题&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/97.jpg&quot; /&gt;&lt;/center&gt;

&lt;h3 id=&quot;196-mipmap-范围查询&quot;&gt;1.9.6. Mipmap 范围查询&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;生成不同分辨率的图片&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/98.jpg&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;任何一个像素可以映射到纹理区域的一个点，mipmap可以让像素点快速查阅，因为他又很多层，不同的纹理区域的面积对应不同的层&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;mipmap也不是最好的方法，只是一种折中的办法&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;anisotropic filtering 各向异性过滤&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;../assets/img/posts/20211221/99.jpg&quot; /&gt;&lt;/center&gt;</content><author><name>Quehry</name></author><category term="note" /><summary type="html"></summary></entry><entry><title type="html">推荐系统</title><link href="http://localhost:4000/Recommender_system.html" rel="alternate" type="text/html" title="推荐系统" /><published>2021-12-16T00:00:00+08:00</published><updated>2021-12-16T00:00:00+08:00</updated><id>http://localhost:4000/Recommender_system</id><content type="html" xml:base="http://localhost:4000/Recommender_system.html">&lt;h1 id=&quot;推荐系统&quot;&gt;推荐系统&lt;/h1&gt;
&lt;h2 id=&quot;1矩阵分解-matrix-factorization&quot;&gt;1.矩阵分解 Matrix Factorization&lt;/h2&gt;
&lt;h3 id=&quot;11-the-matrix-factorization-model&quot;&gt;1.1 The Matrix Factorization Model&lt;/h3&gt;
&lt;p&gt;R是user-item矩阵，行数是用户数量，列数是物品数量,那么R∈R&lt;sup&gt;mxn&lt;/sup&gt;。P是user latent matrix，P∈R&lt;sup&gt;mxk&lt;/sup&gt;，Q是item latent matrix，Q∈R&lt;sup&gt;nxk&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;矩阵分解就是把R分解成P和Q，那么预测的评分就是：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211216/2.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;但是上面这个式子没有考虑偏置，我们会有下面这个完整的式子：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211216/3.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;那么&lt;strong&gt;目标函数&lt;/strong&gt;可以定义为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211216/4.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;右边那一串是正则项，为了避免过拟合&lt;/p&gt;

&lt;p&gt;下面这张图值观的展示了矩阵分解过程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211216/5.jpg&quot; /&gt;&lt;/p&gt;</content><author><name>Quehry</name></author><category term="note" /><summary type="html">推荐系统 1.矩阵分解 Matrix Factorization 1.1 The Matrix Factorization Model R是user-item矩阵，行数是用户数量，列数是物品数量,那么R∈Rmxn。P是user latent matrix，P∈Rmxk，Q是item latent matrix，Q∈Rnxk</summary></entry><entry><title type="html">Robotics</title><link href="http://localhost:4000/Robotics.html" rel="alternate" type="text/html" title="Robotics" /><published>2021-12-13T00:00:00+08:00</published><updated>2021-12-13T00:00:00+08:00</updated><id>http://localhost:4000/Robotics</id><content type="html" xml:base="http://localhost:4000/Robotics.html">&lt;h1 id=&quot;报告&quot;&gt;报告&lt;/h1&gt;
&lt;h2 id=&quot;报告内容&quot;&gt;报告内容&lt;/h2&gt;
&lt;p&gt;用solidworks设计一个至少三个自由度的机械臂，并且描述它的动能，移动能力等等。提交的是截图，源文件等等。&lt;/p&gt;
&lt;h2 id=&quot;报告格式&quot;&gt;报告格式&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;标题，下面有姓名学号电话等等&lt;/li&gt;
  &lt;li&gt;摘要&lt;/li&gt;
  &lt;li&gt;正文&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Quehry</name></author><category term="note" /><summary type="html">报告 报告内容 用solidworks设计一个至少三个自由度的机械臂，并且描述它的动能，移动能力等等。提交的是截图，源文件等等。 报告格式 标题，下面有姓名学号电话等等 摘要 正文</summary></entry><entry><title type="html">数据挖掘</title><link href="http://localhost:4000/datamining.html" rel="alternate" type="text/html" title="数据挖掘" /><published>2021-12-10T00:00:00+08:00</published><updated>2021-12-10T00:00:00+08:00</updated><id>http://localhost:4000/datamining</id><content type="html" xml:base="http://localhost:4000/datamining.html">&lt;!-- TOC --&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#总体情况&quot;&gt;总体情况&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#第一章-开始数据挖掘之旅&quot;&gt;第一章 开始数据挖掘之旅&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#11-亲和性分析&quot;&gt;1.1 亲和性分析&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#12-分类&quot;&gt;1.2 分类&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#第二章-用scikit-learn估计器分类&quot;&gt;第二章 用scikit-learn估计器分类&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#21-scikit-learn&quot;&gt;2.1 scikit-learn&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#22-邻近算法knn&quot;&gt;2.2 邻近算法KNN&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#第三章-用决策树预测获胜球队&quot;&gt;第三章 用决策树预测获胜球队&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#31-决策树&quot;&gt;3.1 决策树&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#32-随机森林&quot;&gt;3.2 随机森林&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#第四章-用亲和性分析方法推荐电影&quot;&gt;第四章 用亲和性分析方法推荐电影&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#41-亲和性分析&quot;&gt;4.1 亲和性分析&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#42-apriori算法&quot;&gt;4.2 Apriori算法&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#第五章-用转换器抽取特征&quot;&gt;第五章 用转换器抽取特征&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#51-抽取特征&quot;&gt;5.1 抽取特征&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#52-特征选择&quot;&gt;5.2 特征选择&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#53-创建特征&quot;&gt;5.3 创建特征&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#第六章-使用朴素贝叶斯进行社会媒体挖掘&quot;&gt;第六章 使用朴素贝叶斯进行社会媒体挖掘&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#61-消歧&quot;&gt;6.1 消歧&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#62-文本转换器&quot;&gt;6.2 文本转换器&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#63-朴素贝叶斯&quot;&gt;6.3 朴素贝叶斯&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#64-f1值&quot;&gt;6.4 F1值&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#第九章-作者归属问题&quot;&gt;第九章 作者归属问题&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#91-作者归属&quot;&gt;9.1 作者归属&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#92-支持向量机&quot;&gt;9.2 支持向量机&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#93-基础svm的局限性&quot;&gt;9.3 基础SVM的局限性&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#第十章-新闻语料分类&quot;&gt;第十章 新闻语料分类&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#101-新闻语料聚类&quot;&gt;10.1 新闻语料聚类&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#102-k-means算法&quot;&gt;10.2 K-means算法&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- /TOC --&gt;

&lt;h1 id=&quot;总体情况&quot;&gt;总体情况&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;书籍:Python数据挖掘入门与实践&lt;/li&gt;
  &lt;li&gt;github_url:https://github.com/LinXueyuanStdio/PythonDataMining&lt;/li&gt;
  &lt;li&gt;配套代码和笔记，很适合迅速上手&lt;/li&gt;
  &lt;li&gt;这篇博客主要记录一些比较重要的算法&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;第一章-开始数据挖掘之旅&quot;&gt;第一章 开始数据挖掘之旅&lt;/h2&gt;
&lt;h3 id=&quot;11-亲和性分析&quot;&gt;1.1 亲和性分析&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;亲和性分析根据样本个体（物体）之间的&lt;strong&gt;相似度&lt;/strong&gt;，确定它们关系的亲疏。&lt;/li&gt;
  &lt;li&gt;例子：商品推荐。&lt;/li&gt;
  &lt;li&gt;我们要找出“如果顾客购买了商品X，那么他们可能愿意购买商品Y”这样的规则。简单粗暴的做法是，找出数据集中所有同时购买的两件商品。找出规则后，还需要判断其优劣，我们挑好的规则用。&lt;/li&gt;
  &lt;li&gt;规则的优劣有多种判断标准，常用的有支持度(support)和置信度(confidence)&lt;/li&gt;
  &lt;li&gt;支持度：数据集中规则应验的次数，统计起来很简单。有时候，还需要对支持度进行规范化，即再除以规则有效前提下的总数量。&lt;/li&gt;
  &lt;li&gt;置信度是衡量规则的准确性如何。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;12-分类&quot;&gt;1.2 分类&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;根据特征分出类别&lt;/li&gt;
  &lt;li&gt;例子：Iris植物分类数据集，通过四个特征分出三个类别&lt;/li&gt;
  &lt;li&gt;特征连续值变成离散值&lt;/li&gt;
  &lt;li&gt;OneR算法：它根据已有数据中，具有相同特征值的个体最可能属于哪个类别进行分类。比如对于某一个特征值来说，属于A的类别有80个，属于B的类别有20个，那么对于这个特征值来说，取值为1代表为A类别，错误率有20％。给出所有特征值，找出错误率最小的特征值作为判断标准。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;第二章-用scikit-learn估计器分类&quot;&gt;第二章 用scikit-learn估计器分类&lt;/h2&gt;
&lt;h3 id=&quot;21-scikit-learn&quot;&gt;2.1 scikit-learn&lt;/h3&gt;
&lt;p&gt;scikit-learn里面已经封装好很多数据挖掘的算法&lt;/p&gt;

&lt;p&gt;现介绍数据挖掘框架的搭建方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;转换器（Transformer）用于数据预处理，数据转换&lt;/li&gt;
  &lt;li&gt;流水线（Pipeline）组合数据挖掘流程，方便再次使用（封装）&lt;/li&gt;
  &lt;li&gt;估计器（Estimator）用于分类，聚类，回归分析（各种算法对象）
    &lt;ul&gt;
      &lt;li&gt;所有的估计器都有下面2个函数
        &lt;ul&gt;
          &lt;li&gt;fit() 训练
            &lt;ul&gt;
              &lt;li&gt;用法：estimator.fit(X_train, y_train)，&lt;/li&gt;
              &lt;li&gt;estimator = KNeighborsClassifier() 是scikit-learn算法对象&lt;/li&gt;
              &lt;li&gt;X_train = dataset.data 是numpy数组&lt;/li&gt;
              &lt;li&gt;y_train = dataset.target 是numpy数组&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;predict() 预测
            &lt;ul&gt;
              &lt;li&gt;用法：estimator.predict(X_test)&lt;/li&gt;
              &lt;li&gt;estimator = KNeighborsClassifier() 是scikit-learn算法对象&lt;/li&gt;
              &lt;li&gt;X_test = dataset.data 是numpy数组&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;22-邻近算法knn&quot;&gt;2.2 邻近算法KNN&lt;/h3&gt;
&lt;p&gt;邻近算法，或者说K最邻近（KNN，K-NearestNeighbor）分类算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是K个最近的邻居的意思，说的是每个样本都可以用它最接近的K个邻近值来代表。近邻算法就是将数据集合中每一个记录进行分类的方法。&lt;/p&gt;

&lt;p&gt;例子：分类，Ionosphere数据集&lt;/p&gt;

&lt;h2 id=&quot;第三章-用决策树预测获胜球队&quot;&gt;第三章 用决策树预测获胜球队&lt;/h2&gt;

&lt;h3 id=&quot;31-决策树&quot;&gt;3.1 决策树&lt;/h3&gt;
&lt;p&gt;例子：预测NBA球队获胜情况&lt;/p&gt;

&lt;p&gt;决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。&lt;/p&gt;

&lt;p&gt;分类树（决策树）是一种十分常用的分类方法。它是一种监督学习，所谓监督学习就是给定一堆样本，每个样本都有一组属性和一个类别，这些类别是事先确定的，那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类。这样的机器学习就被称之为监督学习。&lt;/p&gt;

&lt;p&gt;scikit-learn库实现了分类回归树（Classification and Regression Trees，CART）算法并将其作为生成决策树的默认算法，它支持连续型特征和类别型特征。&lt;/p&gt;

&lt;h3 id=&quot;32-随机森林&quot;&gt;3.2 随机森林&lt;/h3&gt;
&lt;p&gt;随机森林指的是利用多棵树对样本进行训练并预测的一种分类器。&lt;/p&gt;

&lt;p&gt;在机器学习中，随机森林是一个包含多个决策树的分类器， 并且其输出的类别是由个别树输出的类别的众数而定。&lt;/p&gt;

&lt;h2 id=&quot;第四章-用亲和性分析方法推荐电影&quot;&gt;第四章 用亲和性分析方法推荐电影&lt;/h2&gt;
&lt;h3 id=&quot;41-亲和性分析&quot;&gt;4.1 亲和性分析&lt;/h3&gt;
&lt;p&gt;亲和性分析就是分析两个样本之间的疏密关系，常用的算法有Apriori，Apriori算法的一大特点是根据最小支持度生成&lt;strong&gt;频繁项集&lt;/strong&gt;（frequent itemest），它只从数据集中频繁出现的商品中选取共同出现的商品组成频繁项集。其他亲和性分析算法有Eclat和频繁项集挖掘算法（FP-growth）。&lt;/p&gt;

&lt;h3 id=&quot;42-apriori算法&quot;&gt;4.2 Apriori算法&lt;/h3&gt;
&lt;p&gt;Apriori算法主要有两个阶段，第一个阶段是根据最小支持度生成频繁项集，第二个阶段是根据最小置信度选择规则，返回规则。&lt;/p&gt;

&lt;p&gt;本章的例子是电影推荐。&lt;/p&gt;

&lt;p&gt;第一个阶段，算法会先生成长度较小的项集，再将这个项集作为超集寻找长度较大的项集。&lt;/p&gt;

&lt;p&gt;第二个阶段是从频繁项集中抽取关联规则。把其中几部电影作为前提，另一部电影作为结论。组成如下形式的规则：如果用户喜欢前提中的所有电影，那么他们也会喜欢结论中的电影。&lt;/p&gt;

&lt;h2 id=&quot;第五章-用转换器抽取特征&quot;&gt;第五章 用转换器抽取特征&lt;/h2&gt;
&lt;h3 id=&quot;51-抽取特征&quot;&gt;5.1 抽取特征&lt;/h3&gt;
&lt;p&gt;抽取数据集的特征是重要的一步，在之前的学习中我们都获得了数据集的特征，但很多没有处理的文本特征并不是很明显，比如一段文本等等。特征值可以分为连续特征，序数特征，类别型特征。&lt;/p&gt;

&lt;h3 id=&quot;52-特征选择&quot;&gt;5.2 特征选择&lt;/h3&gt;
&lt;p&gt;通常特征有很多，但我们只想选择其中一部分。&lt;strong&gt;选用干净的数据，选取更具描述性的特征。&lt;/strong&gt;判断特征相关性：书中列举的例子是判断一个人的收入能不能超过五万，利用单变量卡方检验(或者皮尔逊相关系数)判断各个特征的相关性，然后给出了三个最好的特征，分别是年龄，资本收入和资本损失。&lt;/p&gt;

&lt;h3 id=&quot;53-创建特征&quot;&gt;5.3 创建特征&lt;/h3&gt;
&lt;p&gt;主成分分析算法（Principal Component Analysis，PCA）的目的是找到能用较少信息描述数据集的特征组合。&lt;/p&gt;

&lt;h2 id=&quot;第六章-使用朴素贝叶斯进行社会媒体挖掘&quot;&gt;第六章 使用朴素贝叶斯进行社会媒体挖掘&lt;/h2&gt;
&lt;h3 id=&quot;61-消歧&quot;&gt;6.1 消歧&lt;/h3&gt;
&lt;p&gt;本章我们将处理文本，文本通常被称为无结构格式。文本挖掘的一个难点来自于歧义，比如bank一词多义。本章将探讨区别Twitter消息中Python的意思。&lt;/p&gt;

&lt;h3 id=&quot;62-文本转换器&quot;&gt;6.2 文本转换器&lt;/h3&gt;
&lt;p&gt;Python中处理文本的库NLTK(Natural Language Toolkit)。据作者说很好用，可以作自然语言处理。N元语法是指由连续的词组成的子序列。&lt;/p&gt;

&lt;h3 id=&quot;63-朴素贝叶斯&quot;&gt;6.3 朴素贝叶斯&lt;/h3&gt;
&lt;p&gt;朴素贝叶斯概率模型是以对贝叶斯统计方法的朴素解释为基础。&lt;/p&gt;

&lt;p&gt;贝叶斯定理公式如下：&lt;/p&gt;

&lt;p&gt;$ P(A|B) = \frac {P(B|A)P(A)}{P(B)} $&lt;/p&gt;

&lt;p&gt;贝叶斯公式可以用它来计算个体属于给定类别的概率。朴素贝叶斯算法假定了各个特征之间相互独立，那么我们计算文档D属于类别C的概率为P(D|C)=P(D1|C)*P(D2|C)…P(Dn|C)。贝叶斯分类器是输入数据来更新贝叶斯的先验概率和后验概率，输入贝叶斯模型后，返回不同类别中概率的最大值。&lt;/p&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;举例说明下计算过程，假如数据集中有以下一条用二值特征表示的数据：[1, 0, 0, 1]。&lt;br /&gt;
训练集中有75%的数据属于类别0，25%属于类别1，且每个特征属于每个类别的似然度如下。&lt;br /&gt;
类别0：[0.3, 0.4, 0.4, 0.7] &lt;br /&gt;
类别1：[0.7, 0.3, 0.4, 0.9] &lt;br /&gt;
拿类别0中特征1的似然度举例子，上面这两行数据可以这样理解：类别0中有30%的数据，特征1的值为1。&lt;br /&gt;
我们来计算一下这条数据属于类别0的概率。类别为0时，P(C=0) = 0.75。&lt;br /&gt;
朴素贝叶斯算法用不到P(D)，因此我们不用计算它。我们来看下计算过程。&lt;br /&gt;
P(D|C=0) = P(D1|C=0) x P(D2|C=0) x P(D3|C=0) x P(D4|C=0)&lt;br /&gt;
= 0.3 x 0.6 x 0.6 x 0.7 &lt;br /&gt;
= 0.0756 &lt;br /&gt;
现在，我们就可以计算该条数据从属于每个类别的概率。需要提醒的是，我们没有计算P(D)，因此，计算结果不是实际的概率。由于两次都不计算P(D)，结果具有可比较性，能够区分出大小就足够了。来看下计算结果。&lt;br /&gt;
P(C=0|D) = P(C=0) P(D|C=0) &lt;br /&gt;
= 0.75 * 0.0756 &lt;br /&gt;
= 0.0567&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;64-f1值&quot;&gt;6.4 F1值&lt;/h3&gt;
&lt;p&gt;F1值是一种评价指标。F1值是以每个类别为基础进行定义的，包括两大概念：准确率（precision）和召回率（recall）。准确率是指预测结果属于某一类的个体，实际属于该类的比例。召回率是指被正确预测为某个类别的个体数量与数据集中该类别个体总量的比例。F1值是准确率和召回率的调和平均数。&lt;/p&gt;

&lt;h2 id=&quot;第九章-作者归属问题&quot;&gt;第九章 作者归属问题&lt;/h2&gt;
&lt;h3 id=&quot;91-作者归属&quot;&gt;9.1 作者归属&lt;/h3&gt;
&lt;p&gt;作者归属（authorship attribution）是作者分析的一个细分领域，研究目标是从一组可能的作者中找到文档真正的主人。利用功能词进行分类，功能词是指本身含义很少，但是是组成句子必不可少的部分。&lt;/p&gt;

&lt;h3 id=&quot;92-支持向量机&quot;&gt;9.2 支持向量机&lt;/h3&gt;
&lt;p&gt;支持向量机（SVM）分类算法背后的思想很简单，它是一种二类分类器（扩展后可用来对多个类别进行分类）。假如我们有两个类别的数据，而这两个类别恰好能被一条线分开，线上所有点为一类，线下所有点属于另一类。SVM要做的就是找到这条线，用它来做预测，跟线性回归原理很像。&lt;/p&gt;

&lt;p&gt;下图中有三条线，那么哪一条线的分类效果最好呢？直觉告诉我们从左下到右上的这一条线效果最好，因为每一个点到这条线的距离最远，那么寻找这条线就变成了最优化问题。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211210/2.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于多种类别的分类问题，我们创建多个SVM分类器，其中每个SVM分类器还是二分类。连接多个分类器的方法有很多，比如说我们可以将每个类别创建一对多分类器。把训练数据分为两个类别——属于特定类别的数据和其他所有类别数据。对新数据进行分类时，从这些类别中找出最匹配的。&lt;/p&gt;

&lt;h3 id=&quot;93-基础svm的局限性&quot;&gt;9.3 基础SVM的局限性&lt;/h3&gt;
&lt;p&gt;最基础的SVM只能区分线性可分的两种类别，如果数据线性不可分，就需要将其置入更高维的空间中，加入更多伪特征直到数据线性可分。寻找最佳分隔线时往往需要计算个体之间的内积。我们把内核函数定义为数据集中两个个体函数的点积。&lt;/p&gt;

&lt;p&gt;常用的内核函数有几种。线性内核最简单，它无外乎两个个体的特征向量的点积、带权重的特征和偏置项。多项式内核提高点积的阶数（比如2）。此外，还有高斯内核（rbf）、Sigmoind内核。&lt;/p&gt;

&lt;h2 id=&quot;第十章-新闻语料分类&quot;&gt;第十章 新闻语料分类&lt;/h2&gt;
&lt;h3 id=&quot;101-新闻语料聚类&quot;&gt;10.1 新闻语料聚类&lt;/h3&gt;
&lt;p&gt;之前我们研究的都是监督学习，在已经知道类别的情况下进行分类。本章着眼于无监督学习，聚类。&lt;/p&gt;

&lt;h3 id=&quot;102-k-means算法&quot;&gt;10.2 K-means算法&lt;/h3&gt;
&lt;p&gt;k-means聚类算法迭代寻找最能够代表数据的聚类质心点。算法开始时使用从训练数据中随机选取的几个数据点作为质心点。k-means中的k表示寻找多少个质心点，同时也是算法将会找到的簇的数量。例如，把k设置为3，数据集所有数据将会被分成3个簇。&lt;/p&gt;

&lt;p&gt;k-means算法分为两个步骤：为每一个数据点分配簇标签，更新各簇的质心点。k-means算法会重复上述两个步骤；每次更新质心点时，所有质心点将会小范围移动。这会
轻微改变每个数据点在簇内的位置，从而引发下一次迭代时质心点的变动。这个过程会重复执行直到条件不再满足时为止。通常是在迭代一定次数后，或者当质心点的整体移动量很小时，就可以终止算法的运行。有时可以等算法自行终止运行，这表明簇已经相当稳定——数据点所属的簇不再变动，质心点也不再改变时。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211210/3.jpg&quot; /&gt;&lt;/p&gt;</content><author><name>Quehry</name></author><category term="note" /><summary type="html"></summary></entry><entry><title type="html">RACE数据集相关文献</title><link href="http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html" rel="alternate" type="text/html" title="RACE数据集相关文献" /><published>2021-11-30T00:00:00+08:00</published><updated>2021-11-30T00:00:00+08:00</updated><id>http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE</id><content type="html" xml:base="http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html">&lt;h1 id=&quot;目录&quot;&gt;&lt;strong&gt;目录&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#目录&quot;&gt;&lt;strong&gt;目录&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#文献整理&quot;&gt;文献整理&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#要求&quot;&gt;要求&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#搜集到相关文献标题和地址&quot;&gt;搜集到相关文献标题和地址&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#第一篇&quot;&gt;第一篇&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#title&quot;&gt;Title&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#author&quot;&gt;Author&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#abstract&quot;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bert-distractor-generation&quot;&gt;BERT distractor generation&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#1bert-based-distractor-generationbdg&quot;&gt;1)BERT-based distractor generation(BDG)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#2multi-task-with-parallel-mlm&quot;&gt;2)Multi-task with Parallel MLM&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#3answer-negative-regularization&quot;&gt;3)Answer Negative Regularization&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#multiple-distractor-generation&quot;&gt;Multiple Distractor Generation&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#1selecting-distractors-by-entropy-maximization&quot;&gt;1)Selecting Distractors by Entropy Maximization&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#2bdg-em&quot;&gt;2)BDG-EM&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#performance-evaluation&quot;&gt;Performance Evaluation&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#1datasets&quot;&gt;1)datasets&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#2implementation-details&quot;&gt;2)implementation details&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#3compared-methods&quot;&gt;3)compared methods&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#4token-score-comparison&quot;&gt;4)token score comparison&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#5mcq-model-accuracy-comparison&quot;&gt;5)MCQ Model Accuracy Comparison&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#6parameter-study-on-γ&quot;&gt;6）Parameter Study on γ&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#我的看法&quot;&gt;我的看法&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#第二篇&quot;&gt;第二篇&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#title-1&quot;&gt;Title&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#author-1&quot;&gt;Author&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#abstract-1&quot;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#method&quot;&gt;Method&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#1question-generation&quot;&gt;1)question generation&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#2distractor-generation&quot;&gt;2)distractor generation&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#3qa-filtering&quot;&gt;3)QA filtering&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#results&quot;&gt;Results&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#1quantitative-evaluation&quot;&gt;1)quantitative evaluation&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#2question-answering-ability&quot;&gt;2)question answering ability&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#3human-evaluation&quot;&gt;3)human evaluation&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#conclusion&quot;&gt;conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#第三篇&quot;&gt;第三篇&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#title-2&quot;&gt;Title&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#author-2&quot;&gt;Author&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#abstract-2&quot;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#framework-description-网络结构&quot;&gt;Framework Description 网络结构&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#1task-definition&quot;&gt;1)Task Definition&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#2framework-overview&quot;&gt;2)Framework overview&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#3hierarchical-encoder&quot;&gt;3)Hierarchical encoder&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#4static-attention-mechanism&quot;&gt;4)static attention mechanism&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#5encoding-layer&quot;&gt;5)encoding layer&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#6matching-layer&quot;&gt;6)matching layer&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#7nomalization-layer&quot;&gt;7)nomalization layer&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#8distractor-decoder&quot;&gt;8)distractor decoder&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#9question-based-initializer&quot;&gt;9)question-based initializer&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#10dynamic-hierarchical-attention-mechanism&quot;&gt;10)dynamic hierarchical attention mechanism&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#11training-and-inference&quot;&gt;11)training and inference&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#experimental-setting-实验设置&quot;&gt;experimental setting 实验设置&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#1dataset&quot;&gt;1)dataset&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#2implementation-details-1&quot;&gt;2)implementation details&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#3baselines-and-ablations&quot;&gt;3)baselines and ablations&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#results-and-analysis-结果与分析&quot;&gt;results and analysis 结果与分析&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#我的看法-1&quot;&gt;我的看法&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#第四篇&quot;&gt;第四篇&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#title-3&quot;&gt;Title&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#author-3&quot;&gt;Author&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#abstract-3&quot;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#proposed-framework-网络结构&quot;&gt;Proposed Framework 网络结构&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#1notations-and-task-definition&quot;&gt;1)notations and task definition&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#2model-overview&quot;&gt;2)model overview&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#3encoding-article-and-question&quot;&gt;3)encoding article and question&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#4co-attention-between-article-and-question&quot;&gt;4)Co-attention between article and question&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#5merging-sentence-representation&quot;&gt;5)Merging sentence representation&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#6question-initialization&quot;&gt;6)question initialization&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#7hierarchical-attention&quot;&gt;7)hierarchical attention&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#8semantic-similarity-loss&quot;&gt;8)semantic similarity loss&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#experimental-settings&quot;&gt;Experimental Settings&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#1dataset-1&quot;&gt;1)dataset&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#2baselines-and-evaluation-metrics&quot;&gt;2)baselines and evaluation metrics&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#3implementation-details&quot;&gt;3)implementation details&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#results-and-analysis-结果与分析&quot;&gt;Results and Analysis 结果与分析&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#我的看法-2&quot;&gt;我的看法&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#补充&quot;&gt;补充&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#race数据集简介&quot;&gt;RACE数据集简介&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bleu&quot;&gt;BLEU&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#rouge&quot;&gt;ROUGE&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;文献整理&quot;&gt;文献整理&lt;/h1&gt;

&lt;h2 id=&quot;要求&quot;&gt;要求&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/requirements.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;搜集到相关文献标题和地址&quot;&gt;搜集到相关文献标题和地址&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2010.05384.pdf&quot;&gt;A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2010.09598.pdf&quot;&gt;Better Distractions: Transformer-based Distractor Generation and Multiple Choice Question Filtering&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ojs.aaai.org//index.php/AAAI/article/view/4606&quot;&gt;Generating Distractors for Reading Comprehension Questions from Real Examinations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ojs.aaai.org/index.php/AAAI/article/view/6522&quot;&gt;Co-attention hierarchical network: Generating coherent long distractors for reading comprehension&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aclanthology.org/2020.coling-main.189.pdf&quot;&gt;Automatic Distractor Generation for Multiple Choice Questions in Standard Tests&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aclanthology.org/W18-0533.pdf&quot;&gt;Distractor Generation for Multiple Choice Questions Using Learning to Rank&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ojs.aaai.org/index.php/AAAI/article/view/16559&quot;&gt;Knowledge-Driven Distractor Generation for Cloze-style Multiple Choice Questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;第一篇&quot;&gt;第一篇&lt;/h1&gt;
&lt;h2 id=&quot;title&quot;&gt;Title&lt;/h2&gt;
&lt;p&gt;A BERT-based Distractor Generation Scheme with Multi-tasking and
Negative Answer Training Strategies&lt;/p&gt;
&lt;h2 id=&quot;author&quot;&gt;Author&lt;/h2&gt;
&lt;p&gt;Ho-Lam Chung, Ying-Hong Chan, Yao-Chung Fan&lt;/p&gt;
&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;现有的DG&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;局限在只能生成一个误导选项，我们需要生成多个误导选项，文章中提到他们团队用multi-tasking和negative answer training技巧来生成多个误导选项，模型结果达到了学界顶尖。&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;DG效果不好，文章提出了两个提升的空间：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;DG质量提升：&lt;br /&gt;
 BERT模型来提升误导选项质量&lt;/li&gt;
  &lt;li&gt;多个误导选项生成：
 运用了覆盖的方法来选择distractor，而不是选择概率最高但是语义很相近的distractor&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;bert-distractor-generation&quot;&gt;BERT distractor generation&lt;/h2&gt;
&lt;h3 id=&quot;1bert-based-distractor-generationbdg&quot;&gt;1)BERT-based distractor generation(BDG)&lt;/h3&gt;
&lt;p&gt;输入：段落P，答案A，问题Q，用C表示这三者concatenate后的结果。&lt;br /&gt;
BDG模型是一个自回归模型，在预测阶段，每次输入C和上一次预测的词元，BDG迭代预测词元，直到预测出特殊词元[S]停止。下面这张图简单介绍了这个过程。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/2.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;网络结构简单介绍：h&lt;sub&gt;[M]&lt;/sub&gt;表示bert输出的隐藏状态，隐藏状态再输入到一个全连接层中用来预测词元。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/3.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2multi-task-with-parallel-mlm&quot;&gt;2)Multi-task with Parallel MLM&lt;/h3&gt;
&lt;p&gt;MLM全称masked language model，遮蔽语言模型,通过并行BDG和P-MLM来训练模型让模型有更好的效果。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/4.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图中左边的sequential MLM就是之前提到的BDG，BDG模型是一个词接一个词的预测，P-MLM是对所有的masked token进行预测，最后的损失函数是这两者相加&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;，公式如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/5.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/6.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/7.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;作者如此设计的思路是：BDG可能会忽略整体语义语义信息，但是会过拟合单个词预测。那么并行一个P-MLM可以防止过拟合。&lt;/p&gt;

&lt;h3 id=&quot;3answer-negative-regularization&quot;&gt;3)Answer Negative Regularization&lt;/h3&gt;
&lt;p&gt;目前机器预测的distractor和answer有很高的相似度，下面一张表可以展示相似度。其中PM表示机器，Gold表示人工，作者将这类问题称为answer copying problem。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/8.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为了解决这个问题，作者提出了answer negative loss来让机器更多的选择与answer不同的词来表示新的distractor，公式如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/9.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看出BDG的loss替换成了AN的loss，每一项都减去了Answer negative loss。&lt;/p&gt;

&lt;h2 id=&quot;multiple-distractor-generation&quot;&gt;Multiple Distractor Generation&lt;/h2&gt;
&lt;h3 id=&quot;1selecting-distractors-by-entropy-maximization&quot;&gt;1)Selecting Distractors by Entropy Maximization&lt;/h3&gt;
&lt;p&gt;选择语义不同的distractor set。文章借鉴了MRC&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;的方法，让BDGmodel生成很多distractor组成 $\hat{D}$ = {$\hat{d}$&lt;sub&gt;1&lt;/sub&gt;, $\hat{d}$&lt;sub&gt;2&lt;/sub&gt;, $\hat{d}$&lt;sub&gt;3&lt;/sub&gt;…}，然后找出最好的一组选项，一般情况下由三个误导选项和一个答案组成。选择的一句是最大化下面这个公式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/10.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2bdg-em&quot;&gt;2)BDG-EM&lt;/h3&gt;
&lt;p&gt;我们可以通过不同的BDG模型来生成不同的误导选项最后组合，不同的模型区别是有没有answer negative/multi-task training，比如我们有这几个模型:$\hat{D}$,$\hat{D}$&lt;sub&gt;PM&lt;/sub&gt;,$\hat{D}$&lt;sub&gt;PM+AN&lt;/sub&gt;，它们分别代表含PM&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;和含AN&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/11.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;performance-evaluation&quot;&gt;Performance Evaluation&lt;/h2&gt;
&lt;h3 id=&quot;1datasets&quot;&gt;1)datasets&lt;/h3&gt;
&lt;p&gt;RACE,沿用了&lt;a href=&quot;https://ojs.aaai.org//index.php/AAAI/article/view/4606&quot;&gt;Gao&lt;/a&gt;那篇论文的处理,后面也会梳理那篇论文&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/12.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2implementation-details&quot;&gt;2)implementation details&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;tokenizer: wordpiece tokenizer&lt;/li&gt;
  &lt;li&gt;framewordk:huggingface trainsformers&lt;/li&gt;
  &lt;li&gt;optimizer:adamW(lr:5e-5)&lt;/li&gt;
  &lt;li&gt;github_url: &lt;a href=&quot;https://github.com/voidful/BDG&quot;&gt;BDG&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3compared-methods&quot;&gt;3)compared methods&lt;/h3&gt;
&lt;p&gt;比较了不同的distractor generation&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CO-Att：出自&lt;a href=&quot;https://ojs.aaai.org/index.php/AAAI/article/view/6522&quot;&gt;Zhou&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;DS-Att: 出自&lt;a href=&quot;https://ojs.aaai.org//index.php/AAAI/article/view/4606&quot;&gt;Gao&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;GPT:baseline&lt;/li&gt;
  &lt;li&gt;BDG: 没有应用P-MLM和Answer negative&lt;/li&gt;
  &lt;li&gt;BDG&lt;sub&gt;PM&lt;/sub&gt;&lt;/li&gt;
  &lt;li&gt;BDG&lt;sub&gt;AN+PM&lt;/sub&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;4token-score-comparison&quot;&gt;4)token score comparison&lt;/h3&gt;
&lt;p&gt;BLEU和ROUGE(L)两种判断指标&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/13.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;copying problem的效果&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/14.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;5mcq-model-accuracy-comparison&quot;&gt;5)MCQ Model Accuracy Comparison&lt;/h3&gt;
&lt;p&gt;与回答系统相结合，将生成好的选项（一个正确答案三个误导选项）放入MCQ answering model，下面是回答正确率的表格&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/15.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看出作者的模型选项的误导性还是很高的。&lt;/p&gt;

&lt;h3 id=&quot;6parameter-study-on-γ&quot;&gt;6）Parameter Study on γ&lt;/h3&gt;
&lt;p&gt;之前使用P-MLM并行训练时候有个权重参数γ，下表显示了不同γ值的影响，对于只有PM的模型来说，γ=6，对于既有AN和PM来说，γ=7&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/16.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;现存的DG可以分为cloze-style distractor generation和 reading comprehension distractor generation，前者主要是word filling，后者主要看重语义信息，基于两者的设计出了很多模型，目前来看还是考虑语义信息生成的误导选项更好。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/17.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;我的看法&quot;&gt;我的看法&lt;/h2&gt;
&lt;p&gt;文章中的模型提到了三种技术，第一是bert预训练模型使用。第二是P-MLM的并行使用， 它的使用让模型可以考虑段落的语义信息，那么生成的误导选项是sentence-level而不是之前模型所使用的类似word-filling这种word-level。第三是Answer negative loss的使用，它的使用相当于让模型不要考虑与正确答案语义很接近的误导选项，因为目前大多数DG生成多个选项时语义与正确答案都非常接近，这与实际情况不符，同时也起不到误导的作用。  &lt;br /&gt;
同时文章提出了生成多个误导选项时使用不同模型生成的误导选项拼在一起作为选项是一种比较好的解决方法，让一次性生成多个误导选型有了一定的可用性。&lt;br /&gt;
文章的代码开源，可以去&lt;a href=&quot;https://github.com/voidful/BDG&quot;&gt;github&lt;/a&gt;上看训练细节和网络结构细节。&lt;/p&gt;

&lt;h1 id=&quot;第二篇&quot;&gt;第二篇&lt;/h1&gt;
&lt;h2 id=&quot;title-1&quot;&gt;Title&lt;/h2&gt;
&lt;p&gt;Better Distractions: Transformer-based Distractor Generation and Multiple Choice Question Filtering&lt;/p&gt;
&lt;h2 id=&quot;author-1&quot;&gt;Author&lt;/h2&gt;
&lt;p&gt;Jeroen Offerijns, Suzan Verberne, Tessa Verhoef&lt;/p&gt;
&lt;h2 id=&quot;abstract-1&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;运用GPT2模型生成三个误导选项，同时用BERT模型去回答这个问题，只挑选出回答正确的问题。相当于使用了QA作为一个过滤器(QA filtering)。&lt;/p&gt;
&lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt;
&lt;p&gt;作者使用了Question generation model, distractor generation model和question answer filter，作者将从这三方面介绍，下图是大概的流程图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/18.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;1question-generation&quot;&gt;1)question generation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;预训练模型：GPT-2&lt;/li&gt;
  &lt;li&gt;数据集：English SQuAD&lt;/li&gt;
  &lt;li&gt;tokenizer：Byte-Pair-Encoding(BPE) tokenizer&lt;/li&gt;
  &lt;li&gt;optimizer:Adam&lt;/li&gt;
  &lt;li&gt;下图展示了QG的输入，黑框内被tokenizer标记为特殊词元&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/19.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2distractor-generation&quot;&gt;2)distractor generation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;预训练模型：GPT-2&lt;/li&gt;
  &lt;li&gt;数据集：RACE&lt;/li&gt;
  &lt;li&gt;tokenizer:BPE&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;使用了repetition penalty技术，保证了尽量不会生成相似的text，并且过滤到那些不好的生成（比如生成了空字符串）&lt;/li&gt;
  &lt;li&gt;输入：经典的C(context)，A(answer),Q(question)，下图展示了输入格式&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/20.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3qa-filtering&quot;&gt;3)QA filtering&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;预训练模型：DistilBERT&lt;/li&gt;
  &lt;li&gt;网络结构：CQA&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;输入到distilbert，再连接一个dropout，全连接层和softmax，最后输出一个答案，具体结构如下图&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/21.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;h3 id=&quot;1quantitative-evaluation&quot;&gt;1)quantitative evaluation&lt;/h3&gt;
&lt;p&gt;下表中展示了和上一篇论文类似的指标,与现有的模型进行了比较：SEQ2SEQ,HSA&lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;和CHN&lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;。可以看出BLEU明显要比之前模型要好，但是ROUGE没有之前的高。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/22.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2question-answering-ability&quot;&gt;2)question answering ability&lt;/h3&gt;
&lt;p&gt;用GPT-2模型生成误导选项再输入到QAmodel中，具体结果见下图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/23.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3human-evaluation&quot;&gt;3)human evaluation&lt;/h3&gt;
&lt;p&gt;人工评估，从两方面评估distractor生成的好坏：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Is the question well-formed and can you understand the meaning?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;If the question is at least understandable, does the answer make sense in relation to the question?&lt;/strong&gt;
评估过程中，使用了155个没有经过QA筛选和155经过QA筛选的，了解一下QA过滤模型的效果。整体来说QA过滤器还是有一点效果，具体结果如下：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/24.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion-1&quot;&gt;conclusion&lt;/h2&gt;
&lt;p&gt;我认为作者使用的DG模型主要有两大特色，一个是使用了GPT2预训练模型，目前使用基于transformer的模型已经成为主流。第二个是使用了QA过滤器来筛选掉回答错误的，有一定提升但不显著。&lt;/p&gt;

&lt;h1 id=&quot;第三篇&quot;&gt;第三篇&lt;/h1&gt;
&lt;h2 id=&quot;title-2&quot;&gt;Title&lt;/h2&gt;
&lt;p&gt;Generating Distractors for Reading Comprehension Questions from Real Examinations&lt;/p&gt;
&lt;h2 id=&quot;author-2&quot;&gt;Author&lt;/h2&gt;
&lt;p&gt;Yifan Gao, Lidong Bing, Piji Li,
Irwin King, Michael R. Lyu&lt;/p&gt;
&lt;h2 id=&quot;abstract-2&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;上面两篇文献都有提到这篇文章。作者使用了&lt;strong&gt;Hierarchical encoder-decoder framework&lt;/strong&gt; with &lt;strong&gt;static&lt;/strong&gt; and &lt;strong&gt;dynamic&lt;/strong&gt; attention mechanisms来生成有语义信息的误导选项。使用了编码器-解码器结构网络和静态和动态注意力机制。&lt;/p&gt;
&lt;h2 id=&quot;framework-description-网络结构&quot;&gt;Framework Description 网络结构&lt;/h2&gt;
&lt;h3 id=&quot;1task-definition&quot;&gt;1)Task Definition&lt;/h3&gt;
&lt;p&gt;输入：文章，问题和答案。P代表文章，s&lt;sub&gt;1&lt;/sub&gt;,s&lt;sub&gt;2&lt;/sub&gt;,s&lt;sub&gt;3&lt;/sub&gt;…表示不同的句子，q和a分别表示问题和答案，那么我们的任务是生成误导选项$\overline{d}$。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/25.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2framework-overview&quot;&gt;2)Framework overview&lt;/h3&gt;
&lt;p&gt;网络结构如下图所示，下面将从各个组成部分分别介绍：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/26.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3hierarchical-encoder&quot;&gt;3)Hierarchical encoder&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;word embedding&lt;/strong&gt;:词嵌入，将每个句子s&lt;sub&gt;i&lt;/sub&gt;中的每个词元变成词向量(w&lt;sub&gt;i,1&lt;/sub&gt;,w&lt;sub&gt;i,2&lt;/sub&gt;,w&lt;sub&gt;i,3&lt;/sub&gt;…)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;word encoder&lt;/strong&gt;:将句子s&lt;sub&gt;i&lt;/sub&gt;的词向量(w&lt;sub&gt;i,1&lt;/sub&gt;,w&lt;sub&gt;i,2&lt;/sub&gt;,w&lt;sub&gt;i,3&lt;/sub&gt;…)作为输入，用&lt;strong&gt;双向LSTM&lt;/strong&gt;作为编码器，获得word-level representation h&lt;sub&gt;i,j&lt;/sub&gt;&lt;sup&gt;e&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/27.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;sentence encoder&lt;/strong&gt;:将word encoder中每个句子正向LSTM的最后一个隐藏状态和反向LSTM的最开始的隐藏状态作为输入到另一个双向LSTM中获得&lt;strong&gt;sentence-level representation&lt;/strong&gt;(u&lt;sub&gt;1&lt;/sub&gt;,u&lt;sub&gt;2&lt;/sub&gt;,u&lt;sub&gt;3&lt;/sub&gt;…)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;4static-attention-mechanism&quot;&gt;4)static attention mechanism&lt;/h3&gt;
&lt;p&gt;目的：生成的误导选项必须和问题Q语义相关，但是和答案A必须语义不相关。我们从(s&lt;sub&gt;1&lt;/sub&gt;,s&lt;sub&gt;2&lt;/sub&gt;,s&lt;sub&gt;3&lt;/sub&gt;…)学习到句子的权重分布(γ&lt;sub&gt;1&lt;/sub&gt;,γ&lt;sub&gt;2&lt;/sub&gt;,γ&lt;sub&gt;3&lt;/sub&gt;…)，然后将问题q和答案a作为query。&lt;/p&gt;

&lt;h3 id=&quot;5encoding-layer&quot;&gt;5)encoding layer&lt;/h3&gt;
&lt;p&gt;我们希望把问题q，答案a和句子s都变成一样的长度的向量表示，也就是上图中紫色虚线部分。对于q和a，我们用两个独立的双向LSTM来获得(&lt;strong&gt;a&lt;/strong&gt;&lt;sub&gt;1&lt;/sub&gt;,&lt;strong&gt;a&lt;/strong&gt;&lt;sub&gt;2&lt;/sub&gt;…&lt;strong&gt;a&lt;/strong&gt;&lt;sub&gt;k&lt;/sub&gt;)和(&lt;strong&gt;q&lt;/strong&gt;&lt;sub&gt;1&lt;/sub&gt;,&lt;strong&gt;q&lt;/strong&gt;&lt;sub&gt;2&lt;/sub&gt;…&lt;strong&gt;q&lt;/strong&gt;&lt;sub&gt;l&lt;/sub&gt;)，然后用平均池化层平均一下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/28.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于句子s，我们不用u而用h：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/29.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;6matching-layer&quot;&gt;6)matching layer&lt;/h3&gt;
&lt;p&gt;目的：加重与问题q有关的句子，减轻与答案a有关的句子。o&lt;sub&gt;i&lt;/sub&gt;表示不同句子的importance score&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/30.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;7nomalization-layer&quot;&gt;7)nomalization layer&lt;/h3&gt;
&lt;p&gt;目的：有些问题q和一两个句子有关，而有些问题q和很多句子有关，比如summarizing，下面的τ(temperature)就是这个作用&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/31.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/32.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;作者介绍static attention mechanism用了很大篇幅&lt;/p&gt;

&lt;h3 id=&quot;8distractor-decoder&quot;&gt;8)distractor decoder&lt;/h3&gt;
&lt;p&gt;解码器使用的也是LSTM，但是并没有使用编码器的最后一个隐藏状态作为初始状态，而是定义了一个
&lt;strong&gt;question-based initializer&lt;/strong&gt;来让生成的误导选项语法和问题q一致&lt;/p&gt;

&lt;h3 id=&quot;9question-based-initializer&quot;&gt;9)question-based initializer&lt;/h3&gt;
&lt;p&gt;定义了一个question LSTM来编码问题q，使用最后一层的cell state和hidden state作为decoder初始状态，同时输入q&lt;sub&gt;last&lt;/sub&gt;，表示问题q的最后一个词元。&lt;/p&gt;

&lt;h3 id=&quot;10dynamic-hierarchical-attention-mechanism&quot;&gt;10)dynamic hierarchical attention mechanism&lt;/h3&gt;
&lt;p&gt;常规的注意力机制将一篇文章作为长句子，然后decoder的每一个时间步都与encoder中所有的hidden state进行比较，但是这种方法并不适合目前的模型。原因：首先LSTM不能处理这么长的输入，其次，一些问题只与部分句子有关。&lt;br /&gt;
目的：每个decoder时间步只关注&lt;strong&gt;重要句子&lt;/strong&gt;，作者将这种注意力机制称为动态注意力机制，因为不同的时间步，word-level和sentence-level 注意力分布都不同。&lt;br /&gt;
每一个时间步的输入是词元d&lt;sub&gt;t-1&lt;/sub&gt;和上一个隐藏状态h&lt;sub&gt;t-1&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/33.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/34.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;α和β分别表示word-level,sentence-level权重，最后使用之前静态注意力机制获得的γ来调节α和β&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/35.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/36.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;获得上下文变量&lt;strong&gt;c&lt;/strong&gt;&lt;sub&gt;t&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/37.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;获得attention vector $\tilde{h}$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/38.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;11training-and-inference&quot;&gt;11)training and inference&lt;/h3&gt;
&lt;p&gt;损失函数：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/39.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;生成多个误导选项的方法是束搜索，但是生成的误导选项很相似，作者做了相应的处理方法，但我觉得效果还是很差&lt;/p&gt;

&lt;h2 id=&quot;experimental-setting-实验设置&quot;&gt;experimental setting 实验设置&lt;/h2&gt;
&lt;h3 id=&quot;1dataset&quot;&gt;1)dataset&lt;/h3&gt;
&lt;p&gt;RACE数据集，作者做了相应的处理，去掉了很多不合理的和语义不相关的，作者的处理标准是：对于误导选项中的词元，如果它们在文章中出现的次数小于5次，那么将被保留，同时去掉了那些需要在句子中间和句子开始填空的问题。下表展示了处理后的数据集的一些信息：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/40.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2implementation-details-1&quot;&gt;2)implementation details&lt;/h3&gt;
&lt;p&gt;词表：保留了频率最高的50k个词元，同时使用GloVe作为词嵌入预训练模型。其他的细节都可以在文章中看见，这里不一一列出了，主要是超参数的设置。&lt;/p&gt;

&lt;h3 id=&quot;3baselines-and-ablations&quot;&gt;3)baselines and ablations&lt;/h3&gt;
&lt;p&gt;与HRED&lt;sup id=&quot;fnref:10&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;和seq2seq比较&lt;/p&gt;

&lt;h2 id=&quot;results-and-analysis-结果与分析&quot;&gt;results and analysis 结果与分析&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/41.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;人工评估：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/42.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;大致过程是这样：四个误导选项，分别来自seq2seq，HRED，作者的模型和原本的误导选项，让英语能力很好的人来选择最适合的选项，得出的结果可以发现，作者的模型生成的误导选项拥有最好的误导效果。&lt;/p&gt;

&lt;p&gt;下图直观展示了static attention distribution：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/43.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;我的看法-1&quot;&gt;我的看法&lt;/h2&gt;
&lt;p&gt;这篇文章应该是第一个提出用处理后的RACE数据集来处理MCQ问题，处理后的RACE数据集在后面也有很多文献用到，这篇文章使用了seq2seq网络结构同时使用了静态和动态注意力机制，对于网络结构和注意力机制的解释非常完全和详细，虽然这篇文章的效果放到现在来看可能不是最好了，但是它提出来的评估标准可能会成为一个通用的标准。它的数据集和训练代码在&lt;a href=&quot;https://github.com/Yifan-Gao/Distractor-Generation-RACE&quot;&gt;github&lt;/a&gt;上也完全开源。&lt;/p&gt;

&lt;h1 id=&quot;第四篇&quot;&gt;第四篇&lt;/h1&gt;
&lt;h2 id=&quot;title-3&quot;&gt;Title&lt;/h2&gt;
&lt;p&gt;Co-attention hierarchical network: Generating coherent long distractors for reading comprehension&lt;/p&gt;
&lt;h2 id=&quot;author-3&quot;&gt;Author&lt;/h2&gt;
&lt;p&gt;Xiaorui Zhou, Senlin Luo, Yunfang Wu&lt;/p&gt;
&lt;h2 id=&quot;abstract-3&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;这篇文献是针对上一篇Gao的文章(seq2seq)所作的改进。本篇文章提出了Gao的模型的两个问题：1.没有建立文章和问题的关系，他的解决方法是使用&lt;strong&gt;co-attention enhanced hierarchical architecture&lt;/strong&gt;来捕获文章和问题之间的关系，让解码器生成更有关联的误导选项。2.没有加重整篇文章和误导选项的关系。作者的解决思路是添加一个额外的语义相关性损失函数，让生成的误导选项与整篇文章更有关联。&lt;/p&gt;
&lt;h2 id=&quot;proposed-framework-网络结构&quot;&gt;Proposed Framework 网络结构&lt;/h2&gt;
&lt;h3 id=&quot;1notations-and-task-definition&quot;&gt;1)notations and task definition&lt;/h3&gt;
&lt;p&gt;article T=(s&lt;sub&gt;1&lt;/sub&gt;,s&lt;sub&gt;2&lt;/sub&gt;…s&lt;sub&gt;k&lt;/sub&gt;)，一篇文章有k个句子s，同时每个句子都有不同的长度l，s&lt;sub&gt;i&lt;/sub&gt;=(w&lt;sub&gt;i,1&lt;/sub&gt;,w&lt;sub&gt;i,2&lt;/sub&gt;…w&lt;sub&gt;i,l&lt;/sub&gt;)，每个文章有m个问题和z个误导选项，Q=(q&lt;sub&gt;1&lt;/sub&gt;,q&lt;sub&gt;2&lt;/sub&gt;…q&lt;sub&gt;m&lt;/sub&gt;),D=(d&lt;sub&gt;1&lt;/sub&gt;,d&lt;sub&gt;2&lt;/sub&gt;…d&lt;sub&gt;z&lt;/sub&gt;),我们的任务是根据输入的T和Q生成D&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/44.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2model-overview&quot;&gt;2)model overview&lt;/h3&gt;
&lt;p&gt;整体结构如下图所示，下面将从各个部分分别介绍：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/45.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3encoding-article-and-question&quot;&gt;3)encoding article and question&lt;/h3&gt;
&lt;p&gt;文章和问题的编码器结构&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;hierarchical article encoder&lt;/strong&gt;
双向LSTM，和上一篇结构很像，很多部分我就简单列个式子。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/46.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;每一句最后的词元来表示整个句子&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/47.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;sentence-level encoder：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/48.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;同样，用最后一个句子来表示整篇文章&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/49.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;用&lt;strong&gt;H&lt;/strong&gt;&lt;sup&gt;*&lt;/sup&gt;来作为sentence-level representation of article,我们有&lt;strong&gt;H&lt;/strong&gt;&lt;sub&gt;:t&lt;/sub&gt;&lt;sup&gt;*&lt;/sup&gt;=h&lt;sub&gt;t&lt;/sub&gt;&lt;sup&gt;s&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;这样，通过使用两个双向LSTM获得word-level encoding和sentence-level encoding&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;question encoder&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/50.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;用&lt;strong&gt;U&lt;/strong&gt;&lt;sup&gt;*&lt;/sup&gt;来作为word-level representations of question, 我们有&lt;strong&gt;U&lt;/strong&gt;&lt;sub&gt;:t&lt;/sub&gt;&lt;sup&gt;*&lt;/sup&gt;=h&lt;sub&gt;t&lt;/sub&gt;&lt;sup&gt;q&lt;/sup&gt;&lt;/p&gt;

&lt;h3 id=&quot;4co-attention-between-article-and-question&quot;&gt;4)Co-attention between article and question&lt;/h3&gt;
&lt;p&gt;Co-attention mechanism就是使用了两个方向的注意力机制，有从article到question的，也有question到article的。&lt;br /&gt;
用一个“相似”矩阵S表示H和U的关系：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/51.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;S&lt;sub&gt;i,j&lt;/sub&gt;就表示第i个句子和第j个问题词元的相似性&lt;/p&gt;

&lt;p&gt;我们可以获得两个特殊的矩阵&lt;strong&gt;S&lt;/strong&gt;&lt;sup&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/sup&gt;和&lt;strong&gt;S&lt;/strong&gt;&lt;sup&gt;&lt;strong&gt;T&lt;/strong&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/52.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;article-to-question attention&lt;br /&gt;
$\tilde{U}$&lt;sub&gt;:j&lt;/sub&gt; = $\sum$ S&lt;sub&gt;i,j&lt;/sub&gt;&lt;sup&gt;Q&lt;/sup&gt;U&lt;sub&gt;:,i&lt;/sub&gt;&lt;/li&gt;
  &lt;li&gt;question-to-article attention&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/53.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;最后，将问题的词级表示H，两个方向的注意力结果$\tilde{U}$和$\tilde{H}$结合一下获得G&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/54.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;5merging-sentence-representation&quot;&gt;5)Merging sentence representation&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/55.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Z表示final representation of sentence-level hidden states&lt;/p&gt;

&lt;h3 id=&quot;6question-initialization&quot;&gt;6)question initialization&lt;/h3&gt;
&lt;p&gt;接下来就进入decoder环节，这里的question initialization和上篇文献处理方法相同&lt;/p&gt;

&lt;h3 id=&quot;7hierarchical-attention&quot;&gt;7)hierarchical attention&lt;/h3&gt;
&lt;p&gt;不同时间步有不同的句子相关，和上篇文献的处理方法动态注意力机制相同。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/56.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/57.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/58.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/59.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;8semantic-similarity-loss&quot;&gt;8)semantic similarity loss&lt;/h3&gt;
&lt;p&gt;目的：获得文章和误导选项的关系。还记得之前定义的e&lt;sub&gt;T&lt;/sub&gt;吗，它表示整篇文章，那么我们通过下面的公式可以获得distractor representation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/60.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中S&lt;sub&gt;M&lt;/sub&gt;是decoder最后一个隐藏状态，那么我们通过cos计算相似关系，那么最终的损失函数&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/61.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experimental-settings&quot;&gt;Experimental Settings&lt;/h2&gt;
&lt;h3 id=&quot;1dataset-1&quot;&gt;1)dataset&lt;/h3&gt;
&lt;p&gt;使用了上篇文献处理过的RACE数据集。&lt;/p&gt;

&lt;h3 id=&quot;2baselines-and-evaluation-metrics&quot;&gt;2)baselines and evaluation metrics&lt;/h3&gt;
&lt;p&gt;与seq2seq，HRED，HCP&lt;sup id=&quot;fnref:11&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;，HSA&lt;sup id=&quot;fnref:12&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;比较。&lt;/p&gt;

&lt;h3 id=&quot;3implementation-details&quot;&gt;3)implementation details&lt;/h3&gt;
&lt;p&gt;网络超参数设置技巧，不展开了&lt;/p&gt;

&lt;h2 id=&quot;results-and-analysis-结果与分析-1&quot;&gt;Results and Analysis 结果与分析&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/62.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/63.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/64.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;介绍一下上面这张表，这张表是人工评估的结果，从三个维度分析，分别是fluency,coherence,distracting ability。可以看出作者的模型并不是在所有维度都是最好的。&lt;/p&gt;

&lt;p&gt;下图是案例分析：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/65.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;我的看法-2&quot;&gt;我的看法&lt;/h2&gt;
&lt;p&gt;这篇文献是基于上一篇文献的方法进行了两个改进：1.关联了整篇文章和问题，解决方法是使用了Co-attention mechanism。2.让distractor和article语义相关，方法是定义了相关性loss。&lt;/p&gt;

&lt;h1 id=&quot;补充&quot;&gt;补充&lt;/h1&gt;
&lt;h2 id=&quot;race数据集简介&quot;&gt;RACE数据集简介&lt;/h2&gt;
&lt;p&gt;RACE数据集是一个来源于中学考试题目的大规模阅读理解数据集，包含了大约 28000 个文章以及近 100000 个问题。它的形式类似于英语考试中的阅读理解（选择题），给定一篇文章，通过阅读并理解文章（Passage），针对提出的问题（Question）从四个选项中选择正确的答案（Answers）。&lt;/p&gt;
&lt;h2 id=&quot;bleu&quot;&gt;BLEU&lt;/h2&gt;
&lt;p&gt;BLEU是一个评价指标，最开始用于机器翻译任务，定义如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/66.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;它的总体思想就是准确率，假如给定标准译文reference，神经网络生成的句子是candidate，句子长度为n，candidate中有m个单词出现在reference，m/n就是bleu的1-gram的计算公式。BLEU还有许多变种。根据n-gram可以划分成多种评价指标，常见的指标有BLEU-1、BLEU-2、BLEU-3、BLEU-4四种，其中n-gram指的是连续的单词个数为n。&lt;/p&gt;

&lt;h2 id=&quot;rouge&quot;&gt;ROUGE&lt;/h2&gt;
&lt;p&gt;Rouge(Recall-Oriented Understudy for Gisting Evaluation)，是评估自动文摘以及机器翻译的一组指标。它通过将自动生成的摘要或翻译与一组参考摘要（通常是人工生成的）进行比较计算，得出相应的分值，以衡量自动生成的摘要或翻译与参考摘要之间的“相似度”。它的定义如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/67.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;文献中使用的ROUGE-L是一种变种，L即是LCS(longest common subsequence，最长公共子序列)的首字母，因为Rouge-L使用了最长公共子序列。Rouge-L计算方式如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/68.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中LCS(X, Y)是X和Y的最长公共子序列的长度,m、n分别表示参考摘要和自动摘要的长度（一般就是所含词的个数），R&lt;sub&gt;lcs&lt;/sub&gt;,P&lt;sub&gt;lcs&lt;/sub&gt;分别表示召回率和准确率。最后的F&lt;sub&gt;lcs&lt;/sub&gt;即是我们所说的Rouge-L。&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;distractor generation 误导选项生成，简称DG &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;当我们test时，只需要Sequential MLM decoder来预测。 &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;multi-choice reading comprehension (MRC) model &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;P-MLM &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Answer negative &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Byte-Pair-Encoding &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;context，question，answer &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;hierarchical encoder-decoder model with static attention &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;hierarchical model enhanced with co-attention &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:10&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;hierarchical encoder-decoder &lt;a href=&quot;#fnref:10&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:11&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;相当于HRED+copy,是基于HRED的网络结构 &lt;a href=&quot;#fnref:11&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:12&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;就是上篇文献的网络 &lt;a href=&quot;#fnref:12&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Quehry</name></author><category term="paper" /><summary type="html">目录</summary></entry><entry><title type="html">软件方法</title><link href="http://localhost:4000/%E8%BD%AF%E4%BB%B6%E6%96%B9%E6%B3%95.html" rel="alternate" type="text/html" title="软件方法" /><published>2021-11-30T00:00:00+08:00</published><updated>2021-11-30T00:00:00+08:00</updated><id>http://localhost:4000/%E8%BD%AF%E4%BB%B6%E6%96%B9%E6%B3%95</id><content type="html" xml:base="http://localhost:4000/%E8%BD%AF%E4%BB%B6%E6%96%B9%E6%B3%95.html">&lt;h1 id=&quot;目录&quot;&gt;目录&lt;/h1&gt;
&lt;!-- TOC --&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#目录&quot;&gt;目录&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#软件方法&quot;&gt;软件方法&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#课程要求&quot;&gt;课程要求&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#随记&quot;&gt;随记&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#ppt整理&quot;&gt;PPT整理&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#1-对象类&quot;&gt;1. 对象，类&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#2面向对象&quot;&gt;2.面向对象&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#3java&quot;&gt;3.JAVA&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#4数据结构&quot;&gt;4.数据结构&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- /TOC --&gt;
&lt;h1 id=&quot;软件方法&quot;&gt;软件方法&lt;/h1&gt;
&lt;h2 id=&quot;课程要求&quot;&gt;课程要求&lt;/h2&gt;
&lt;p&gt;学习面向对象这种软件开发方法（目前概念越来越广），通过java来了解面向对象具体怎么实现。&lt;/p&gt;

&lt;h3 id=&quot;随记&quot;&gt;随记&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;类，对象：
    &lt;ul&gt;
      &lt;li&gt;给类赋值变成实例/对象&lt;/li&gt;
      &lt;li&gt;c语言可以构建面向对象所有的结构&lt;/li&gt;
      &lt;li&gt;对象就是给类声明的一个变量&lt;/li&gt;
      &lt;li&gt;类集合了属性和方法&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;面向对象的三大特征：
    &lt;ul&gt;
      &lt;li&gt;封装（encapsulation）:
        &lt;ul&gt;
          &lt;li&gt;private, protected, public&lt;/li&gt;
          &lt;li&gt;可作用于属性和方法&lt;/li&gt;
          &lt;li&gt;一般是隐藏对象的属性和实现细节，但是提供方法的接口&lt;/li&gt;
          &lt;li&gt;提供公开的方法&lt;/li&gt;
          &lt;li&gt;提高了软件开发的效率&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/1.jpg&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;继承（inheritance）：
        &lt;ul&gt;
          &lt;li&gt;子类与父类&lt;/li&gt;
          &lt;li&gt;子类自动具有父类属性和方法，添加自己特有的属性和方法，并且子类使用父类的方法也可以覆盖/重写父类方法&lt;/li&gt;
          &lt;li&gt;可以实现代码的复用（当然功能不止于此）&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;多态（polymorphism）：
        &lt;ul&gt;
          &lt;li&gt;父类有多个子类&lt;/li&gt;
          &lt;li&gt;子类覆盖/重写父类方法&lt;/li&gt;
          &lt;li&gt;相当于是根据实际创建的对象类型动态决定使用哪个方法&lt;/li&gt;
          &lt;li&gt;所有的子类都可以看成父类的类型，运行时，系统会自动调用各种子类的方法&lt;/li&gt;
          &lt;li&gt;UML可以画出类之间的关系&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;java程序设计
    &lt;ul&gt;
      &lt;li&gt;百分百面向对象
        &lt;ul&gt;
          &lt;li&gt;不存在类以外代码&lt;/li&gt;
          &lt;li&gt;只能采用面向对象方法编程&lt;/li&gt;
          &lt;li&gt;java文件命名规范
            &lt;ul&gt;
              &lt;li&gt;必须以.java结尾&lt;/li&gt;
              &lt;li&gt;源文件中如果只有一个类，文件类必须与该类名相同&lt;/li&gt;
              &lt;li&gt;如果有多个类，且没有public类，文件名可与任一类名相同&lt;/li&gt;
              &lt;li&gt;有多个类，且有public类，文件名必须与该类名相同&lt;/li&gt;
              &lt;li&gt;一个JAVA源文件只能有一个public类，一个文件中只能有一个main主函数&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;静态方法/static，可以直接用类和函数名直接调用，和普通方法的区别是不用new一个示例&lt;/li&gt;
      &lt;li&gt;多态的实现，先定义抽象的（abstract）父类，然后子类继承父类然后定义父类的抽象方法
        &lt;ul&gt;
          &lt;li&gt;通过抽象方法固定通用接口&lt;/li&gt;
          &lt;li&gt;子类通过强制实现抽象方法实现多态&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;ppt整理&quot;&gt;PPT整理&lt;/h2&gt;

&lt;h3 id=&quot;1-对象类&quot;&gt;1. 对象，类&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;使用对象之前要先声明和创造&lt;/li&gt;
  &lt;li&gt;类定义了对象的类型，所有对象都是类的实例，所有的类描述了属性和定义了方法&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2面向对象&quot;&gt;2.面向对象&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;封装：保护类的属性和方法,private,default,protected,public&lt;/li&gt;
  &lt;li&gt;继承：B继承A，重用，修改，添加，A所有的属性都存在于B中，A的方法可以在B中重新定义，B的改动不会影响A&lt;/li&gt;
  &lt;li&gt;多态：一个对象属于多个类，通过使用不同类中的方法属于不同的类，父类是抽象类，各个子类继承父类并定义方法，调用的时候根据不同子类调用方法。判断类型是否相同instanceof，声明的时候可以这么声明: A a = new B(),其中B是A的子类。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3java&quot;&gt;3.JAVA&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;x = bool ? a : b，表示如果bool为true，执行a，如果为false执行b&lt;/li&gt;
  &lt;li&gt;for(Point p : this.getVect())表示遍历&lt;/li&gt;
  &lt;li&gt;exception:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/69.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;还有异常的抛出throws&lt;/p&gt;

&lt;p&gt;try-catch-finally&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;  
	    &lt;span class=&quot;c1&quot;&gt;// 可能会发生异常的程序代码  &lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; 
&lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Type1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;id1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;  
	    &lt;span class=&quot;c1&quot;&gt;// 捕获并处置try抛出的异常类型Type1  &lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; 
&lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Type2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;id2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;  
	    &lt;span class=&quot;c1&quot;&gt;//捕获并处置try抛出的异常类型Type2  &lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;finally&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;  
	    &lt;span class=&quot;c1&quot;&gt;// 无论是否发生异常，都将执行的语句块  &lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;自定义异常：&lt;/p&gt;
&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NombreNegatifException&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Exception&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;NombreNegatifException&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Vous avez un nombre négatif !&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;文件读写：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;类FileReader,FileWriter,使用里面的方法read()和write(x)和close()&lt;/p&gt;

&lt;p&gt;比如：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211130/70.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;枚举enum，举例说明&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;enum&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Jour&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;no&quot;&gt;LUNDI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;MARDI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;MERCREDI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;JEUDI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;VENDREDI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;SAMEDI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;DIMANCHE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;EssaiJour&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nc&quot;&gt;Jour&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jour&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Jour&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;valueOf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]);&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jour&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Jour&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;SAMEDI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;fin de semaine : &quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;switch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jour&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
        &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;SAMEDI&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;DIMANCHE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;se reposer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;travailler&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;接口interface,迭代器iterator&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;举例：&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Main&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nd&quot;&gt;@FunctionalInterface&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;interface&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;maFonction&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nc&quot;&gt;Integer&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;appliquer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Integer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Integer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;transforme&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Integer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maFonction&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nc&quot;&gt;Vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Integer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nouveauVect&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Integer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;();&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++)&lt;/span&gt; 
        &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;nouveauVect&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;appliquer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)));&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;nouveauVect&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nouveauVect&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nc&quot;&gt;Vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Integer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Integer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;vi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;83&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;18&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Les valeurs du vecteur initial : &quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; 
        &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;vi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforme&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
        &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Les valeurs du vecteur modifié : &quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;nc&quot;&gt;Iterator&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;iterator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;hasNext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;4数据结构&quot;&gt;4.数据结构&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;数据结构一般含有以下功能：创建，插入，寻找，删除，排序&lt;/li&gt;
  &lt;li&gt;二维数组,举例说明：&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[][]={&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;e&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;i&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;o&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;u&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;1&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;2&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;3&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;4&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;};&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sousTab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;str&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sousTab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; 
        &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Valeur du tableau à l&apos;indice [&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;][&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;]: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;或者&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tabEntiers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tabEntiers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; 
&lt;span class=&quot;c1&quot;&gt;// création effective du tableau précédent&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;列表，包含ArrayList, LinkedList&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ArrayList是实现了基于动态数组的数据结构，LinkedList基于链表的数据结构。对于随机访问get和set，ArrayList优于LinkedList，因为ArrayList可以随机定位，而LinkedList要移动指针一步一步的移动到节点处。（参考数组与链表来思考）。对于新增和删除操作add和remove，LinedList比较占优势，只需要对指针进行修改即可，而ArrayList要移动数据来填补被删除的对象的空间。&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Liste&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;protected&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valeur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; 
    &lt;span class=&quot;kd&quot;&gt;protected&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Liste&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;succ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;protected&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Liste&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
    
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;valeur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; 
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valeur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;changerValeur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; 
        &lt;span class=&quot;n&quot;&gt;valeur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Liste&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;succ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; 
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;succ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;changerSucc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Liste&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; 
        &lt;span class=&quot;n&quot;&gt;succ&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;changerPred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Liste&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; 
        &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这是一个链表的简写，每一层包含了上一个元素，这一个元素，下一个元素&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;哈希表，通过简历KV关系查找，相比于之前的顺序访问或者其他指数访问要快。&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;java.util.HashMap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TestHash&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nc&quot;&gt;HashMap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;annuaire&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;HashMap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;();&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// ajout des valeurs&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;annuaire&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Alfred&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;2399020806&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;annuaire&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Daniel&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;2186000000&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// obtention d&apos;un numéro&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;annuaire&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;containsKey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Danielle&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt; 
        &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;annuaire&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Danielle&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;«&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Danielle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;+num&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; 
        &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pas trouve&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;树状结构tree&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;一般包含结点&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;，&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;结点的度&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;该结点下有多少子树的数目&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;，&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;树的度&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;不同的遍历方法&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;：&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;前序遍历&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;，&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;首先结点&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;，&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;然后左子树&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;，&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;右子树&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;中序遍历&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;，&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;左子树&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;，&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;结点&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;，&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;右子树&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;后序遍历&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;，&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;左子树&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;，&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;右子树&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;，&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;结点&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;层序遍历&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;，&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;从上到下&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;，&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;从左到右&lt;/span&gt;

&lt;span class=&quot;err&quot;&gt;```&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;java&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Arbre&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;protected&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valeur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;protected&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Arbre&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; 
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;valeur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; 
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valeur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;boolean&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;existeFilsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filsGauche&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; 
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;boolean&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;existeFilsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filsDroit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Arbre&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;filsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Arbre&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;filsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;affecterValeur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&amp;lt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valeur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;affecterFilsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Arbre&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filsGauche&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;affecterFilsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Arbre&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filsDroit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;}&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;boolean&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;feuille&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;filsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;hauteur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;existeFilsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;hauteur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;existeFilsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;hauteur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// Constructeurs&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Arbre&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;valeur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;filsGauche&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filsDroit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Arbre&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Arbre&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Arbre&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;valeur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;filsGauche&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filsDroit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Affichage&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;afficherPrefixe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valeur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;\t&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;existeFilsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;afficherPrefixe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;existeFilsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;afficherPrefixe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;afficherInfixe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;existeFilsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;afficherInfixe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valeur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;\t&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;existeFilsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;afficherInfixe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;afficherPostfixe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;existeFilsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;afficherPostfixe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;existeFilsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;afficherPostfixe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valeur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;\t&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;二叉排序树是指左子树小于结点小于右子树，而且结点值不重复。判断是否为二叉排序树：&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;boolean&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;superieur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// vrai si x est supérieur à tous les éléments de l’arbre&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feuille&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valeur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(((&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;existeFilsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())?&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;filsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;superieur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;):&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;existeFilsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())?&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;filsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;superieur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;):&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; 
&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;boolean&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;inferieur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;//similaire a superieur ... }&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;boolean&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;binrech&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
 &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feuille&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
 &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;existeFilsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()?&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;superieur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valeur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filsGauche&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;binrech&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()):&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
 &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;existeFilsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()?&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;inferieur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valeur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filsDroit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;binrech&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()):&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Quehry</name></author><category term="school" /><summary type="html">目录</summary></entry><entry><title type="html">课程总结</title><link href="http://localhost:4000/%E5%A4%A7%E5%9B%9B%E4%B8%8A%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93.html" rel="alternate" type="text/html" title="课程总结" /><published>2021-11-28T00:00:00+08:00</published><updated>2021-11-28T00:00:00+08:00</updated><id>http://localhost:4000/%E5%A4%A7%E5%9B%9B%E4%B8%8A%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93</id><content type="html" xml:base="http://localhost:4000/%E5%A4%A7%E5%9B%9B%E4%B8%8A%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93.html">&lt;h1 id=&quot;概率统计&quot;&gt;概率统计&lt;/h1&gt;

&lt;h3 id=&quot;简介&quot;&gt;简介&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;授课老师：牛薇&lt;/li&gt;
  &lt;li&gt;授课材料：一份法语讲义，一份习题集（10个EX），上课用的PPT&lt;/li&gt;
  &lt;li&gt;B站有录播，up主：却道成归&lt;/li&gt;
  &lt;li&gt;笔记记在侧边栏为大四上A的笔记本最前面&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;内容总览&quot;&gt;内容总览&lt;/h3&gt;

&lt;p&gt;一半时间概率一半时间统计&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;概率
    &lt;ul&gt;
      &lt;li&gt;先从之前学的概率空间讲起，介绍了概率分布（离散or连续），密度函数，期望方差，收敛性。&lt;/li&gt;
      &lt;li&gt;估计，比如说用平均值估计期望，用频率估计概率等等&lt;/li&gt;
      &lt;li&gt;估计又分为点估计和区间估计，点估计中介绍了似然函数以及最大似然法来找估计量&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;统计
    &lt;ul&gt;
      &lt;li&gt;主要介绍了几种检验方法来检验分布、估计量选择的好坏&lt;/li&gt;
      &lt;li&gt;包括了参数检验，分布检验，比较检验等等&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A4纸&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211128/1.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211128/2.jpg&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;流体力学&quot;&gt;流体力学&lt;/h1&gt;

&lt;h3 id=&quot;简介-1&quot;&gt;简介&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;授课老师：方乐&lt;/li&gt;
  &lt;li&gt;授课材料：PPT，TD都是6个，分别对应六大章&lt;/li&gt;
  &lt;li&gt;B站有录播&lt;/li&gt;
  &lt;li&gt;笔记在侧边栏为大四上A的中后部分和大四上B前面&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;内容总览-1&quot;&gt;内容总览&lt;/h3&gt;

&lt;p&gt;第一章主要讲了流体的概念和动力学的公式。第二章从能量角度出发，介绍了NS方程（斯托克斯方程），和伯努利原理（压强和流速的关系）。第三章介绍了雷诺数，无量纲分析，雷诺数大的是湍流，雷诺数小的是层流。第四章介绍了边界层，第五章介绍了湍流，系统平均。第六章介绍了涡量。&lt;/p&gt;

&lt;h3 id=&quot;a4纸&quot;&gt;A4纸&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211128/3.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211128/4.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;报告&quot;&gt;报告&lt;/h3&gt;
&lt;p&gt;结课之前需要我们写一个报告，什么形式的都可以，我觉得这种方式挺好的，自由发挥，我做的实验，用牛奶和墨水还原了卡门涡街。&lt;/p&gt;

&lt;h1 id=&quot;电磁辐射波&quot;&gt;电磁辐射波&lt;/h1&gt;

&lt;h3 id=&quot;简介-2&quot;&gt;简介&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;授课老师: José Penuelas(负责前几章教学), Bertrand Vilquin(负责后几章教学), 孙鸣捷老师(负责TD)&lt;/li&gt;
  &lt;li&gt;授课形式：线上讲解原理，线下TD&lt;/li&gt;
  &lt;li&gt;授课材料：PPT，讲义，TD&lt;/li&gt;
  &lt;li&gt;B站有录播&lt;/li&gt;
  &lt;li&gt;有笔记，侧边栏叫做电磁学(大四上)&lt;/li&gt;
  &lt;li&gt;考试闭卷，所以没有A4纸&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;内容总览-2&quot;&gt;内容总览&lt;/h3&gt;
&lt;p&gt;首先回顾了之前学的波动物理和电磁学，电磁辐射，顾名思义是要将辐射，讲了波导，腔和光电效应，能级跃迁等等&lt;/p&gt;

&lt;h1 id=&quot;传感器&quot;&gt;传感器&lt;/h1&gt;

&lt;h3 id=&quot;简介-3&quot;&gt;简介&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;授课老师：徐平&lt;/li&gt;
  &lt;li&gt;授课形式：线下授课，做实验&lt;/li&gt;
  &lt;li&gt;授课材料：大学生MOOC&lt;/li&gt;
  &lt;li&gt;没有考试，没有笔记&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;内容总览-3&quot;&gt;内容总览&lt;/h3&gt;
&lt;p&gt;讲解了传感器的基本原理，构造和常见传感器，每节课都需要在MOOC上做题，也有安排答辩，我和蔡卓江、宋正浩、刘亚林、马卫一一组讲解了机器狗。做实验是指去214玩小车，上面有不少传感器，也有大疆的线上模拟器，还是挺不错的一次动手实验。&lt;/p&gt;

&lt;h1 id=&quot;结构力学&quot;&gt;结构力学&lt;/h1&gt;

&lt;h3 id=&quot;简介-4&quot;&gt;简介&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;授课老师：黄行蓉，Jean-Piere Lainé&lt;/li&gt;
  &lt;li&gt;授课形式：J-P录制ppt，黄老师线下授课&lt;/li&gt;
  &lt;li&gt;授课材料：讲义，PPT，TD&lt;/li&gt;
  &lt;li&gt;B站有录播&lt;/li&gt;
  &lt;li&gt;笔记：侧边栏结构力学，还有最后第八章记在大四上C前面&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;内容总览-4&quot;&gt;内容总览&lt;/h3&gt;
&lt;p&gt;结构力学分为了两大部分，弹性力学和材料力学。在弹性力学部分，首先介绍了应力和应力张量的概念，张量可以写成3*3矩阵形式，其中对角线上的元素被称作正应力。第二章介绍了应变，首先介绍了很多种张量，F、H、C、E，然后介绍了形变张量ε。第三章介绍了本构方程（应力应变关系方程）。第四章介绍了能量，包括最小势能和最大余能等等。&lt;/p&gt;

&lt;p&gt;第二部分是材料力学，主题内容和弹性力学类似，但是引进了力螺旋的概念，这个概念在中国授课好像是没有的，它描述了合力和力矩。第一张介绍了内力，在材料力学部分我们主要研究梁这个结构，它包括了中轴线和截面，这部分内容和之前学的理论力学很相似。第二章介绍了应力，可以用内力表示应力，用一些惯性矩、艾力函数连接。第三章介绍了应变和本构方程，第四章介绍了能量部分，主要是三大定理：théorème de ménabréa;théorème de maxwell-betti;théorème de castigliano。&lt;/p&gt;

&lt;h3 id=&quot;a4纸-1&quot;&gt;A4纸&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211128/5.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211128/6.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211128/7.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211128/8.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211128/9.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/img/posts/20211128/10.jpg&quot; /&gt;&lt;/p&gt;</content><author><name>Quehry</name></author><category term="school" /><summary type="html">概率统计</summary></entry></feed>