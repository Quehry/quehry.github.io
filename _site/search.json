[
  
    {
      "title"       : "机器学习",
      "category"    : "",
      "tags"        : "note",
      "url"         : "./Machine-Learning.html",
      "date"        : "2021-12-22 00:00:00 +0800",
      "description" : "《机器学习》周志华读书笔记",
      "content"     : "目录 目录 1. 第1章 绪论 2. 第2章 模型评估与选择 2.1. 思维导图 2.2. 经验误差与过拟合 2.3. 评估方法 2.3.1. 留出法 2.3.2. 交叉验证法 2.3.3. 自助法 2.4. 性能度量 2.4.1. 错误率与精度 2.4.2. 查准率、查全率与F1 2.4.3. ROC与AUC 2.4.4. 代价敏感错误率与代价曲线 2.5. 比较检验 2.5.1. 假设检验 2.5.2. 交叉验证t检验 2.5.3. McNemar检验 2.5.4. Friedman检验与Nemenyi后续检验 2.6. 偏差与方差 3. 第3章 线性模型 3.1. 思维导图 3.2. 基本形式 3.3. 线性回归 3.4. 对数几率回归 3.5. 线性判别分析 3.6. 多分类学习 3.7. 类别不平衡问题 4. 第4章 决策树 4.1. 思维导图 4.1.1. 章节导图 4.1.2. 如何生成一棵决策树 4.2. 基本流程 4.3. 划分选择 4.3.1. 信息增益 4.3.2. 增益率 4.3.3. 基尼指数 4.4. 剪枝处理 4.4.1. 预剪枝 4.4.2. 后剪枝 4.5. 连续与缺失值 4.5.1. 连续值处理 4.5.2. 缺失值处理 4.6. 多变量决策树 4.7. 阅读材料 1. 第1章 绪论2. 第2章 模型评估与选择2.1. 思维导图2.2. 经验误差与过拟合定义： 错误率(error rate): 如果m个样本中有a个样本分类错误，则错误率E=a/m。 精度(accuracy)：精度=1-错误率。 训练误差：学习器在训练集上的误差称为训练误差或者经验误差。 泛化误差(generalization error)：学习器在新样本上的误差称为泛化误差。 过拟合(overfitting)：当学习器把训练样本学得太好了的时候，很可能把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这就会导致泛化性能下降。 欠拟合(underfitting)：对训练样本的一般性质尚未学好。2.3. 评估方法通常我们可通过实验测试来对学习器的泛化误差进行评估而做出选择，于是我们需要一个测试集(testing set)，然后以测试集上的测试误差作为泛化误差的近似。下面介绍一些常见的处理数据集D的方法(数据集D-&gt;训练集S+测试集T)12.3.1. 留出法 留出发(hold-out)直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T。在S上训练出模型后，用T来评估其测试误差。 以采样的角度来看待数据集划分的过程，则保留类别比例的采样方式通常称为分层采样。比如1000个样本，50%的正例，50%负例，以7：3划分数据集，那么训练集包含350个正例和350个负例就是分层采样。 在使用留出法评估结果时，一般要采用若干次随即划分、重复进行实验评估后取平均值作为留出法的评估结果。 常用做法时将大约2/3~4/5的样本用于训练。2.3.2. 交叉验证法 交叉验证法(cross validation)先将数据集D划分为k个大小相似的互斥子集，每个子集Di都尽可能保持数据分布的一致性。然后每次用k-1个自己的并集作为训练集，余下的那个子集作为测试集，这样就可以获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。 交叉验证也称为k折交叉验证。k的常见取值有10、5、20。 留一法就是k=m，其中数据集D有m个样本。2.3.3. 自助法 自助法(bootstrapping)：给定包含m个样本的数据集D，每次随机从D中挑选一个样本，将其拷贝放入D’, 然后再将该样本放回最初的数据集D中，这个过程重复执行m次后，我们获得了包含m个样本的数据集D’。我们将D’作为训练集，D\\D’作为测试集。 不难发现大概有36.8%(m趋于无限大时)的样本再m次采样中始终不被采到。$\\lim\\limits_{m\\rightarrow\\infty}(1-\\frac{1}{m})^m = \\frac{1}{e} ≈ 0.368$ 缺点：自助法产生的数据集改变了初始数据集的分布，这样会引入估计偏差。因此，在初始数据量足够时，留出法和交叉验证法更常用一些。2.4. 性能度量 对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量(performance measure)。 在预测任务中，给定D = {(x1,y1), (x2,y2)…(xm,ym)}, 其中yi是xi的真实标记。学习器f。 均方误差(mean squared error)：回归任务最常用的性能度量是均方误差：$E(f;D)=\\frac{1}{m}\\sum_1^m(f(x_i)-y_i)^2$接下来我将介绍分类任务中常用的性能度量2.4.1. 错误率与精度本章开头提到了错误率和精度，这是分类任务中最常用的两种性能度量，既适用于二分类任务，也适用于多分类任务。2.4.2. 查准率、查全率与F1 针对二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例(true positive)、假正例(false positive)、真反例(true negative)、假反例(false negative)，分别记为TP、FP、TN、FN。 混淆矩阵(confusion matrix) 真\\预 正例 反例 正例 TP FN 反例 FP TN 查准率(precision)，记为P，它表示选择的好瓜中有多少是真正的好瓜$P=\\frac{TP}{TP+FP}$ 查全率(recall)，记为R，它表示好瓜中有多少被选出来了$R=\\frac{TP}{TP+FN}$ 一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。 P-R曲线：在很多情形下，我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器认为最可能是正例的样本，排在最后的则是学习器认为最不可能是正例的样本，按此顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率、查准率，也得到了P-R图。 如果一个学习器的P-R曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者。如果两者曲线相交，则可以通过比较平衡点(break-even-point)来比较，越大越好。 F1度量：F1综合考虑了查准率和查全率，是他们的调和平均 $F1=\\frac{2*P*R}{P+R}$ F1的一般形式：有时候我们希望赋予查准率和查重率不同的权重：$F_\\beta=\\frac{(1+\\beta^2)*P*R}{\\beta^2*P+R}$其中β大于1表示查全率有更大影响 有时候我们有多个二分类混淆矩阵，我们希望在这n个混淆矩阵上综合考虑查准率和查全率，那么我们有以下的度量 宏查准率macro-P，宏查全率macro-R，宏F1： $macro-P=\\frac{1}{n}\\sum_1^nP_i$$macro-R=\\frac{1}{n}\\sum_1^nR_i$ 微查准率micro-P，微查全率micro-P，微F1：对TP、FP、TN、FN进行平均$micro-P=\\frac{\\overline{TP}}{\\overline{TP}+\\overline{FP}}$$micro-P=\\frac{\\overline{TP}}{\\overline{TP}+\\overline{FN}}$2.4.3. ROC与AUC 很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为反类。 ROC曲线是从排序角度来出发研究学习器的泛化性能的工具。ROC的全称是Receiver Operating Characteristic。 与P-R曲线类似，我们根据学习器的预测结果进行排序，按此顺序逐个把样本作为正例进行预测，计算出真正例率TPR(true positive rate)与假正例率FPR(false positive rate)并以它们分别为横纵坐标画出ROC曲线 $TPR=\\frac{TP}{TP+FN}$$FPR=\\frac{FP}{FP+TN}$ 同样的，如果一个学习器的ROC曲线被另一个学习器完全包住，则认为后者的性能优于前者。若两个曲线相交，则比较曲线下的面积AUC(area under curve) 形式化的看，AUC考虑的是样本预测的排序质量，那么我们可以定义一个排序损失lrank AUC=1-lrank2.4.4. 代价敏感错误率与代价曲线 有时候将正例误判断为负例与将负例误判断为正例所带来的后果不一样，我们赋予它们非均等代价 代价矩阵(cost matrix) 真\\预 第0类 第1类 第0类 0 cost01 第1类 cost10 0 其中costij表示将第i类样本预测为第j类样本的代价 代价敏感错误率 在非均等代价下，ROC曲线不能直接反映出学习器的期望总代价，而代价曲线则可达到此目的，横轴是正例概率代价，纵轴是归一化代价  2.5. 比较检验我们有了实验评估方法与性能度量，是否就可以对学习器的性能进行评估比较了呢？实际上，机器学习中性能比较这件事比大家想象的要复杂很多，我们需要考虑多种因素的影响，下面介绍一些对学习器性能进行比较的方法，本小节默认以错误率作为性能度量。(但我觉得似乎同一个数据集下就可以比较)2.5.1. 假设检验 假设检验中的假设是对学习器泛化错误率分布的某种猜想或者判断，例如“ε=ε0”这样的假设 现实任务中我们并不知道学习器的泛化错误率，我们只能获知其测试错误率$\\hat{\\epsilon}$，但直观上，两者接近的可能性比较大，因此可根据测试错误率估推出泛化错误率的分布。 泛化错误率为$\\epsilon$的学习器被测得测试错误率为$\\hat{\\epsilon}$的概率： 我们发现$\\epsilon$符合二项分布 二项检验：我们可以使用二项检验(binomial test)来对“$\\epsilon$&lt;0.3”这样的假设进行检验，即在$\\alpha$显著度下，$1-\\alpha$置信度下判断假设是否成立。 t检验：我们也可以用t检验(t-test)来检验。 上面介绍的都是针对单个学习器泛化性能的假设进行检验 2.5.2. 交叉验证t检验 对于两个学习器A和B，若我们使用k折交叉验证法得到的测试错误率分别是$\\epsilon_1^A$, $\\epsilon_2^A$…$\\epsilon_k^A$和$\\epsilon_1^B$, $\\epsilon_2^B$…$\\epsilon_k^B$。其中$\\epsilon_i^A$和$\\epsilon_i^B$是在相同第i折训练集上得到的结果。则可以用k折交叉验证“成对t检验”来进行比较检验。 我们这里的基本思想是若两个学习器的性能相同，则它们使用相同的训练测试集得到的错误率应该相同，即$\\epsilon_i^A=\\epsilon_1^B$ $\\Delta_i$ = $\\epsilon_i^A$ - $\\epsilon_i^B$，然后对$\\Delta$进行分析 2.5.3. McNemar检验 对于二分类问题，使用留出法不仅可以估计出学习器A和B的测试错误率，还可获得两学习器分类结果的差别，即两者都正确、都错误…的样本数 若我们假设两学习器性能相同，则应有e01=e10，那么变量|e01-e10|应该服从正态分布/卡方分布，然后用McNemar检验2.5.4. Friedman检验与Nemenyi后续检验 交叉验证t检验与McNemar检验都是在一个数据集上比较两个算法的性能，但是很多时候，我们会在一组数据集上比较多个算法。 当有多个算法参与比较时，一种做法是在每个数据集上本别列出两两比较的结果。另一种方法则是基于算法排列的Friedman检验 假定我们用D1, D2, D3, D4四个数据集对算法A、B、C进行比较。首先，使用留出法或交叉验证法得到每个算法在每个数据集上的测试结果。然后在每个数据集上根据测试性能由好到坏排序，并赋予序值1,2,…若算法的测试性能相同，则平分序值。 然后使用Fredman检验来判断这些算法是否性能都相同，若相同，那么它们的平均序值应当相同。ri表示第i个算法的平均序值，那么它的均值和方差应该满足… 若“所有算法的性能相同”这个假设被拒绝，则说明算法的性能显著不同，这时需要后续检验(post-hoc test)来进一步区分各算法，常用的有Nemenyi后续检验 Nemenyi检验计算出平均序值差别的临界值域 在表中找到k=3时q0.05=2.344，根据公式计算出临界值域CD=1.657，由表中的平均序值可知A与B算法的差距，以及算法B与C的差距均未超过临界值域，而算法A与C的差距超过临界值域，因此检验结果认为算法A与C的性能显著不同，而算法A与B以及算法B与C的性能没有显著差别。2.6. 偏差与方差 对学习算法除了通过实验估计其泛化性能，人们往往还希望了解它“为什么”具有这样的性能。偏差-方差分解是解释学习算法泛化性能的一种重要工具 偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力 方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响 噪声则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度 一般来说，偏置与方差是有冲突的，也就是偏置大的方差小，偏置小的方差大3. 第3章 线性模型3.1. 思维导图3.2. 基本形式 给定由d个属性描述的示例$x=(x_1;x_2;…x_d)$，这是一个列向量，其中$x_i$是$x$在第i个属性上的取值。 线性模型(linear model)试图学得一个通过属性线性组合来进行预测的函数： 向量形式：$f(x)=\\omega^Tx+b$其中$\\omega=(\\omega_1;\\omega_2…\\omega_d)$ 当$\\omega$和b学得后，模型就得以确定 线性模型的优点：形式简单，易于建模，良好的可解释性(comprehensibility) 3.3. 线性回归 对离散属性的处理：1.若属性值存在“序”的关系，可通过连续化将其转化为连续值，比如高、矮变成1、0；2.若不存在序的关系，可转化为k维向量。 均方误差是回归任务中最常用的性能度量，试图让均方误差最小化： 均方误差有非常好的几何意义，它对应了欧氏距离。基于均方误差最小化来进行模型求解的方法称为最小二乘法(least square method)。在线性回归中，最小二乘法就是试图找到一条直线，使得样本到直线上的欧氏距离之和最小 首先观察一个属性值的情况。求解$\\omega$和$b$使得均方误差最小化的过程，称为线性回归的最小二乘“参数估计”，我们令均方误差分别对$\\omega$和$b$求导令其为零，可以得到最优解的闭式解(closed-form),即解析解   更一般的情况是d个属性，称其为“多元线性回归”，同样的步骤，只是$\\omega$变成向量形式，自变量写成m*(d+1)的矩阵形式，m对应了m个样本，d+1对应了d个属性和偏置。同样的求导为0然后得出$\\hat{\\omega}$最优解的闭式解，其中$\\hat{\\omega}=(\\omega;b)$。当$X^TX$为满秩矩阵2或正定矩阵时，有唯一的解：     然而现实任务中往往不是满秩矩阵，例如在许多任务中我们会遇到大量的变量其数目甚至超过样例数。那么我们会求出$\\hat{\\omega}=(\\omega;b)$的多个解，它们都能使均方误差最小化。选择哪一个解作为输出与学习算法的归纳偏好决定，常见的做法是引入正则化(regularization) 广义线性模型(generalized linear model): $g(y)=\\omega^Tx+b$其中g()单调可微，被称为联系函数。比如当g()=ln()时称为对数线性回归3.4. 对数几率回归 上一节讨论的是线性回归进行回归学习。那么如果我们要做的是分类任务应该怎么改变？我们只需要找到一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。 考虑二分类问题，那么我们可以用单位阶跃函数/Heaviside函数联系y与z，其中y是真是标记，z是预测值，$z=\\omega^Tx+b$ 但是它有个问题就是不连续，所以我们希望找到能在一定程度上近似它的替代函数。 对数几率函数(logistic function)就是一个替代函数： $y=\\frac{1}{1+e^{-z}}$ 那么这个式子可以转化为：可以把y视为样本x作为正例的可能性，则1-y是其反例的可能性，两者的比值称为几率(odds)： 若将y视为类后验概率估计。则式子可以重写为： 接下来我们可以通过极大似然法(maximum likelihood method)来估计$\\omega$和$b$。给定数据集，对数似然函数3为：即每个样本属于其真实标记的概率越大越好。 推导过程：上面有个式子应该有问题，(3.26)应该是$p(y_i|x_i;\\omega,b) = p_1(\\hat{x_i};\\beta)^{y_i}p_0(\\hat{x_i};\\beta)^{1-y_i}$因为$\\beta$是高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法如梯度下降法(gradient descent method)和牛顿法都可以求得最优解3.5. 线性判别分析 线性判别分析(Linear Discriminant Analysis，简称LDA)是一种经典的线性学习方法。 LDA的思想非常朴素: 给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离;在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。 令$X_i$、$\\mu_i$、$\\Sigma_i$分别表示第i类示例的集合、均值向量4、协方差矩阵5。 欲使同类样例的投影点尽可能接近，可以让同类样例投影点的协方差尽可能小，即$\\omega^T\\Sigma_0\\omega+\\omega^T\\Sigma_1\\omega$尽可能小;而欲使异类样例的投影点尽可能远离，可以让类中心之间的距离尽可能大，即$||\\omega^T\\mu_0-\\omega^T\\mu_1||$尽可能大: 剩余推导过程：  值得一提的是，LDA可从贝时斯决策理论的角度来阐释，并可证明，当两类数据同先验、满足高斯分布且协方差相等时，LDA可达到最优分类。3.6. 多分类学习 现实中常遇到多分类学习任务，有些二分类学习方法可直接推广到多分类，但在更多情形下，我们是基于一些基本策略，利用二分类学习器来解决多分类问题。 不失一般性，考虑N个类别$C_1$、$C_2$…$C_N$，多分类学习的基本思路是”拆解法”，即将多分类任务拆为若干个二分类任务求解。具体来说，先对问题进行拆分，然后为拆出的每个二分类任务训练一个分类器;在测试时，对这些分类器的预测结果进行集成以获得最终的多分类结果。 这里我们着重介绍如何拆分，最经典的拆分策略有三种：一对一(One vs One)、一对其余(One vs Rest)、多对多(Many vs Many) 一对一：将这N个类别两两配对，从而产生 N(N-1)/2个二分类任务。在测试阶段，新样本将同时提交给所有分类器，于是我们将得到N(N-1)/2个分类结果，最终结果可通过投票产生:即把被预测得最多的类别作为最终分类结果。 一对其余：OvR则是每次将一个类的样例作为正例，所有其他类的样例作为反例来训练N个分类器。在测试时若仅有一个分类器预测为正类，则对应的类别标记作为最终分类结果。若有多个分类器预测为正类，则通常考虑各分类器的预测置信度，选择置信度最大的类别标记作为分类结果。 多对多MvM是每次将若干个类作为正类，若干个其他类作为反类。这里我们介绍一种最常用的MvM技术：纠错输出码(ECOC) ECOC是将编码的思想引入类别拆分，主要分为两步：   1.编码： 对N个类别做M次划分，每次划分将一部分类别划为正类，一部分划为反类，从而形成一个二分类训练集;这样一共产生M个训练集，可训练出M个分类器  2.解码：:M个分类器分别对测试样本进行预测，这些预测标记组成一个编码.将这个预测编码与每个类别各自的编码进行比较，返回其中距离最小的类别作为最终预测结果。 类别划分通过编码矩阵指定。编码矩阵有多种形式，常见的有二元码和三元码，前者将每个类别分别指定为正类和反类，后者在正、反类之外，还可指定“停用类”3.7. 类别不平衡问题 前面介绍的分类学习方法都有一个共同的基本假设：即不同类别的训练样例数目相当。如果不同类别的样例数差别很大，会对学习过程造成困扰。 类别不平衡(class-imbalance)就是指分类任务中不同类别的训练样例数目差别很大的情况。 再缩放(rescaling)是类别不平衡中的一个基本策略：比如在最简单的二分类问题中，我们假设y大于0.5为正例，y小于0.5为负例，但是在类别不平衡时，我们可以改变阈值来达到再平衡：   将  变成 现有的解决类别不平衡的技术大体上有三类做法(这里我们均假设正例样本少):  1.第一类是直接对训练集里的反类样例进行”欠采样” (undersampling)，即去除一些反例使得正、反例数日接近，然后再进行学习;  2.第二类是对训练集里的正类样例进行”过采样” (oversampling)，即增加一些正例使得正、反例数目接近，然后再进行学习;  3.第三类则是直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将上面的公式(改变阈值)嵌入到其决策过程中，称为”阔值移动” (threshold-moving) 需注意的是，过采样法不能简单地对初始正例样本进行重复来样，否则会招致严重的过拟合；过采样法的代表性算法SMOTE是通过对训练集里的正例进行插值来产生额外的正例.另一方面，欠采样法若随机丢弃反例可能丢失一些重要信息;欠采样法的代表性算法EasyEnsemble则是利用集成学习机制，将反例划分为若干个集合供不同学习器使用，这样对每个学习器来看都进行了欠采样，但在全局来看却不会丢失重要信息。4. 第4章 决策树4.1. 思维导图4.1.1. 章节导图4.1.2. 如何生成一棵决策树4.2. 基本流程 决策树是基于树结构来进行决策，其中包含一个根结点，多个内部结点和多个叶结点 叶结点对应决策结果，其他每个结点都对应一个属性测试 每个结点包含的样本集合根据属性测试被划分到子结点中，那么根结点包含样本全集 决策树学习的目的是为了产生一棵泛化能力强的决策树 决策树的生成是一个递归过程，下面这张图展示了递归的过程：对于每个结点，首先判断该结点的样本集是否属于同一个类别C，如果是，则将该结点标记为C类叶结点。再判断该结点的样本集的属性值是否完全相同(或者是否为空集)，如果是，则将该结点标记为D类叶结点，其中D类是这些样本中最多的类别。如果该结点即不是同属于一个类别也不是属性值取值相同，那么则需要继续划分，选择一个最优的划分属性$a_*$,创建新的分支，对于每个分支结点首先判断是否为空，如果为空则判定为E类叶结点，其中E类是父结点中类别最多的类。如果子结点不为空则递归。4.3. 划分选择可以发现生成决策树最关键的步骤就是选择最优划分属性，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的纯度越来越高。我们有很多指标来确定选择哪一个属性作为最优划分选择，下面将分别介绍：4.3.1. 信息增益 信息熵(information entropy)是度量样本集合纯度最常用的一种指标。下面是信息熵的定义公式：  其中$p_k$表示第k类样本在样本集D中所占比例，信息熵越小表示D的纯度越高。 假设离散属性a有V个可能的取值${a^1,a^2…a^V}$,那么我们可以计算出在使用a作为划分属性前后的信息熵差别，也就是信息增益(information gain)：  对每一个子结点$D^v$都赋予权重同时相加。 著名的ID3决策树学习算法就是以信息增益作为准则来选择划分属性，我们希望找到信息增益最大的属性。 书上使用信息增益划分的例子： 4.3.2. 增益率 信息增益对可取值数目较多的属性有所偏好，比如我们使用编号这一属性来划分，每一个编号都只有一个样本，那么信息增益肯定增大了，但是决策树的泛化能力显然下降了。 增益率(gain ratio)，我们通过对信息增益除以IV来平衡属性数目带来的影响，增益率的定义如下：   IV(intrinsic value)的定义如下：  IV是属性a的固有值，属性a可取的数值数目越多，那么IV就越大 但是增益率也有问题，那就是对于可取值数目较少的属性有偏好，所以著名的C4.5决策树算法并不是直接使用增益率，而是先从候选划分属性中找出信息增益高于平均水平的属性，然后再从中选择增益率最高的属性4.3.3. 基尼指数 CART决策树使用基尼指数来选择划分属性 数据集D的纯度定义如下  直观来说，Gini反映了从数据集随便抽取两个样本，它们类别不一致概率 基尼指数定义如下：  很明显，我们希望基尼指数越小越好，所以我们选择基尼指数最小的属性最为最优划分属性。4.4. 剪枝处理 不难发现，上面对于属性的划分很容易过拟合，所以针对过拟合现象，决策树选择剪枝(pruning)来对付过拟合 剪枝就是去掉一些分支来降低过拟合的风险，剪枝可以分为预剪枝和后剪枝 预剪枝(prepruning):在决策树生成的过程中，对每个结点在划分前进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分 后剪枝(postpruning):从训练集生成了一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能的提升，则将该子树替换为叶结点。4.4.1. 预剪枝 如何判断决策树泛化性能？可以使用留出法预留一部分数据用作验证集进行性能评估，性能度量可以用之前介绍的那些，本小节使用精度作为性能度量 预剪枝生成的决策树： 可以发现预剪枝显著减少了分支的数量，这样可以减少决策树的训练时间开销。但是这样也有一个问题，就是有些分支的当前划分虽然不能提升泛化性能，但是后续划分却有可能导致性能显著提高，这样就带来了欠拟合的风险4.4.2. 后剪枝 首先生成决策树，然后对每个结点进行评估是否需要剪枝 虽然后剪枝决策树的欠拟合风险小，泛化性能也往往优于预剪枝决策树，但是后剪枝的时间开销大4.5. 连续与缺失值4.5.1. 连续值处理 到目前为止仅讨论了基于离散属性来生成决策树，但是现实学习任务中通常会遇到连续属性 很明显连续属性不能根据连续属性的可取值来对结点进行划分，我们需要用到连续属性离散化的技术，最简单的策略是二分法 给定样本集D和和连续属性a，假定a在D上出现了n个不同的取值，将这些值从小到大排序，然后每次选择每两个数的中位数t作为划分点，那么连续值就可以当作离散值来处理了，分出的子样本集分别记作$D_t^+$和$D_t^-$ 划分结果： 需要注意的是：连续属性在划分后并不会被丢失，后续划分仍然可以使用4.5.2. 缺失值处理 在实际数据中一般都有很多缺失值，所以我们需要考虑如何对含有缺失值的数据进行学习 给几个定义：$\\tilde{D}$表示D中属性a上没有缺失值的样本子集，假设属性值a可取值{$a^1$,$a^2$…$a^V$}, $\\tilde{D}^v$表示$\\tilde{D}$中属性值a取值为$a^v$的子集，$\\tilde{D}_k$表示样本子集，我们为每个样本赋予权重$\\omega_x$(决策树开始阶段，根结点中权重初始化为1)并定义： 直观地看，对属性a，$\\rho$表示无缺失值样本所占比例，$\\tilde{p}_k$表示无缺失样本中第k类样本所占的比例，$\\tilde{r}_v$表示无缺失值样本在属性上取值为$a^v$所占的比例 那么我们可以将信息增益的公式推广为   那么对于那些在该属性上缺失的值如何处理呢？分两种情况：，若样本$x$在划分属性$a$上的取值己知, 则将$x$划入与其取值对应的子结点，且样本权值在于结点中保持为$\\omega_x$, 若样本$x$在划分属性$a$上的取值未知，则将$x$同时划入所有子结点, 且样本权值在与属性值$a^v$对应的子结点中调整为$\\tilde{r}_v*\\omega_x$，直观地看，这就是让同一个样本以不同的概率划入到不同的子结点中去。 C4.5就是使用了上述的解决方法 4.6. 多变量决策树 若我们把每个属性视为坐标空间中的一个坐标轴，则d个属性描述的样本就对应了d维空间中的一个数据点，对样本分类则意味着在这个坐标空间中寻找不同类样本之间的分类边界，决策树生成的分类边界有个明显的特点：轴平行，即它的分类边界由若干个与坐标轴平行的分段组成 这样的决策树由于要进行大量的属性测试，预测时间开销会很大，所以我们希望使用如下图红线所示的斜划分。多变量决策树就是能实现这样斜划分甚至更复杂划分的决策树 以实现斜划分的决策树为例，非叶结点不再是仅对某一个属性，而是对属性的线性组合进行测试4.7. 阅读材料 多变量决策树算法主要有OC1，还有一些算法试图在决策树的叶结点上嵌入神经网络，比如感知机树在每个叶结点上训练一个感知机 有些决策树学习算法可进行“增量学习”(incrementallearning)，即在接收到新样本后可对己学得的模型进行调整，而不用完全重新学习。主要机制是通过调整分支路径上的划分属性次序来对树进行部分重构，代表性算法ID4、ID5R、ITI等。增量学习可有效地降低每次接收到新样本后的训练时间开销，但多步增量学习后的模型会与基于全部数据训练而得的模型有较大差别。 注意这里的测试集其实是验证集，我们通常把实际情况中遇到的数据集称为测试集。 &#8617; 满秩矩阵指方阵的秩等于矩阵的行数/列数，满秩矩阵有逆矩阵且对于y=Xb有唯一的解 &#8617; 统计学中，似然函数是一种关于统计模型参数的函数。给定输出x时，关于参数θ的似然函数L(θ|x)（在数值上）等于给定参数θ后变量X的概率：L(θ|x)=P(X=x|θ)。 &#8617; 随机变量的期望组成的向量称为期望向量或者均值向量 &#8617; 协方差矩阵的每个元素是各个向量元素之间的协方差。协方差就是Covariance &#8617;"
    } ,
  
    {
      "title"       : "制作类RACE数据集",
      "category"    : "",
      "tags"        : "work",
      "url"         : "./RACElike-datasets.html",
      "date"        : "2021-12-21 00:00:00 +0800",
      "description" : "帮助学长制作RACE数据集",
      "content"     : "目录 目录 RACE 简介 RACE数据集格式 RACE数据集分布 RACE数据集中的长度 RACE数据集中的问题的统计信息 GaoRACE Gao他们对于RACE数据集的处理 Gao处理后的RACE数据集统计信息 Gao处理后的数据集格式 预处理 updated 预处理代码 MRC 阅读理解数据集 简介 Title Abstract Table 一张十分完整的表格 值得关注的地方 自制数据集 大型题库 方法 RACE简介RACE数据集包含了中国初高中阅读理解题目，最初发布在2017年，一共含有28k短文和100k个问题，最开始发布的目的是为了阅读理解任务。它的特点是包含了很多需要推理的问题。 原RACE数据集地址 下载地址url 论文地址：RACE: Large-scale ReAding Comprehension Dataset From ExaminationsRACE数据集格式Each passage is a JSON file. The JSON file contains following fields: article: A string, which is the passage. 文章 questions: A string list. Each string is a query. We have two types of questions. First one is an interrogative sentence. Another one has a placeholder, which is represented by _. 四个问题题干 options: A list of the options list. Each options list contains 4 strings, which are the candidate option. 四个题目的四个选项 answers: A list contains the golden label of each query.四个题目的正确答案 id: Each passage has a unique id in this dataset.RACE数据集分布RACE-M表示初中题目，RACE-H表示高中题目RACE数据集中的长度RACE数据集中的问题的统计信息GaoRACEGao他们对于RACE数据集的处理 去掉了那些误导选项和文章语义不相关的数据 去掉了那些需要world knowledge生成的选项 githuburl,上面有预处理RACE数据集的代码Gao处理后的RACE数据集统计信息Gao处理后的数据集格式预处理首先把数据集规整到一个json文件里，分为dev,test,train三个json文件。每一行包含以下信息：article, sent(sentence), question(问题有两种，一种是疑问句，一种是填空), answer_text, answer, id, word_overlap_score, word_overlap_count, article_id, question_id, distractor_id.那么一个问题会有2-3个误导选项，一篇文章又会有3-4个问题。相比于原本的数据集多了word-overlap指标，word-overlap就是词重叠率，交集比上并集。updatedupdated数据集和original数据集格式类似，少了overlap，内容上去掉了一些语义不相关的题目。预处理代码利用torchtext框架预处理文本，流程大概如下： 定义Field：声明如何处理数据 定义 Dataset：得到数据集，此时数据集里每一个样本是一个 经过 Field声明的预处理 预处理后的 wordlist 建立vocab：在这一步建立词汇表，词向量(word embeddings) 构造迭代器：构造迭代器，用来分批次训练模型Gao说有去掉一些语义不相关的误导选项，但是在代码中并没有看见这步操作？？MRC 阅读理解数据集简介发现了一篇很好的综述，里面涵盖了2021年之前用到的所有MRC数据集。现在对这篇综述简单介绍一下TitleEnglish Machine Reading Comprehension Datasets: A SurveyAbstract文献收集了60个英语阅读理解数据集，分别从不同维度进行比较，包括size, vocabulary, data source, method of creation, human performance level, first question word。调研发现维基百科是最多的数据来源，同时也发现了缺少很多why,when,where问题。Table 一张十分完整的表格首先我简单解释以下这个表格，这个表格一个收录了18个Multiple Choice Datasets,也就是说这18个数据集都着眼于多选题。 第一列是数据集的名称。 第二列表示数据集中问题的个数(size)。 第三列表示数据集中文章的来源，其中ER表示education resource, AG表示automatically generated即自动生成,CRW表示crowdsourcing。 第四列表示答案的来源(answer)，其中UG表示user generated。 第五列LB表示leader board available，即是否有排行榜，带*表示排行榜在网站上发布。 第六列表示人在该数据集上的表现。 第七列表示该数据集是否有被解决，也就是说是否有比较好的模型能在该数据集上表现良好。 第八列表示问题第一个单词出现最频繁的是哪个？比如what,how,which这样的单词。 第九列PAD表示是否开源。值得关注的地方这么多数据集中，来源于考试题目的有RACE,RACE-C,DREAM,ReClor,这些数据集的收集方法可以借鉴。自制数据集大型题库泸江，星火英语…方法Python爬取网页"
    } ,
  
    {
      "title"       : "计算机图形学",
      "category"    : "",
      "tags"        : "note",
      "url"         : "./Computer_Graphics.html",
      "date"        : "2021-12-21 00:00:00 +0800",
      "description" : "Games 101 introduction to computer graphics 课程笔记",
      "content"     : "1. Lecture 01 Overview of Computer Graphics 1.1. 课程情况 1.2. 什么是好的画面 1.3. 应用场景 1.4. Rasterization 光栅化 1.5. 计算机视觉 1.6. 推荐书籍 2. Lecture 02 Review of Linear Algebra 2.1. 图形学依赖学科 2.2. 向量 2.3. 矩阵 3. Lecture 03 Transformation 3.1. why transformation 为什么要变换 3.2. D变换 3.3. 齐次坐标 homogeneous coordinate 4. Lecture 04 Transformation Cont. 4.1. D Transformations 4.2. view transformation 视图变换 4.3. projection transformation 投影变换 5. Lecture05 Rasterization 1(Triangles) 5.1. Perspective Projection 透视投影 5.2. Canonical Cube to Screen 光栅化 5.3. Different Raster Displays 不同的成像设备 5.4. 三角形光栅化 6. Lecture 06 Rasterization 2(Antialiasing and Z-Buffering) 6.1. sampling 采样原理 6.2. Frequency domaine 信号处理频率 6.3. antialiasing 反走样/抗锯齿 6.4. antialiasing today 目前反走样的方法 7. Lecture 07 Shading(Illumination, Shading, and Graphics Pipeline) 7.1. Painter’s Algorithm 画家算法 7.2. Z-buffer 深度缓存 7.3. 目前为止学到了什么 7.4. shading 着色 8. Shading 2(Shading, Pipeline, Texture Mapping) 8.1. Specular Term 高光项 8.2. Ambient Term 环境项 8.3. Shading Frequencies 着色频率 8.4. Graphics Pipeline 图像管线/实时渲染管线 8.5. Texture Mapping 纹理映射 9. Lecture 09 Shading 3 (Texture Mapping) 9.1. Barycentric Coordinates重心坐标系 9.2. Interpolate 插值 9.3. Simple Texture Mapping 简单的纹理映射模型 9.4. Texture Magnification 纹理放大 9.5. Point Sampling Textures 9.6. Mipmap 范围查询 10. Lecture 10 Geomrtry 1(introduction) 10.1. 纹理的应用 10.1.1. Environment Map 环境光映射 10.1.2. Spherical Environment Map 球形环境光映射 10.1.3. 纹理凹凸贴图bump mapping 10.1.4. 位移贴图 displacement mapping 10.1.5. 三维纹理 10.2. 几何 10.2.1. 分类 10.2.2. 隐式几何 10.2.3. 显式几何 10.2.4. 隐式的表达方式 11. Lecture 11 Geometry 2(Curves and Surfaces) 11.1. 显式几何的表示方法 11.1.1. Point Cloud 点云 11.1.2. Polygone Mesh 11.1.3. 一个例子 11.2. Curves 曲线 11.2.1. 贝塞尔曲线 11.2.2. 如何画一条贝塞尔曲线 11.2.3. Piecewise Bézier Curves 逐段的贝塞尔曲线 11.2.4. Spline 样条 11.3. 曲面 11.3.1. 贝塞尔曲面 11.3.2. 曲面细分 12. Lecture 12 Geometry 3 12.1. Mesh Subdivision(upsampling) 网格细分 12.2. Mesh Simplification 网格简化 12.3. 阴影 Shadow mapping 13. Lecture 13 Ray Tracing 1 13.1. Why ray tracing 13.2. Light Rays 13.3. Ray Casting 光线投射 13.4. Recursive Ray Tracing 递归光线追踪 13.5. Ray-Surface interaction 光线和表面相交 13.5.1. Ray Equation 13.5.2. 与圆相交的交点 13.5.3. intersection with implicit surface 13.5.4. intersection with triangle mesh 13.5.5. accelerating ray-surface intersection 14. Lecture 14 Ray Tracing 2 14.1. Uniform Spatial Partitions (Grids) 14.2. Spatial Partitions 空间划分 14.2.1. 一些划分示例 14.2.2. KD-Tree 14.3. Object Partitions 物体划分 14.3.1. Bounding Volume Hierarchy(BVH) 14.3.2. Building BVH 14.3.3. 与空间划分的对比 14.4. Whitted style 14.5. Radiometry 辐射度量学 14.5.1. 一些物理量 14.5.2. Radiant Energy and Flux 14.5.3. Radiant Intensity 15. Lecture 15 Ray Tracing 15.1. Radiometry cont. 辐射度量学 15.1.1. 继续上节课的内容 15.1.2. Irradiance 15.1.3. Radiance 15.2. Bidirectional Reflectance Distribution Function (BRDF) 15.3. Rendering Equation 渲染方程 15.3.1. 如何理解渲染方程 16. Lecture 16 Ray Tracing 4 16.1. Monte Carlo Integration 蒙特卡洛积分 16.2. Path Tracing 路径追踪 16.2.1. 解渲染方程 16.2.2. 最终的代码 16.3. 路径追踪 1. Lecture 01 Overview of Computer Graphics1.1. 课程情况 授课老师：闫令琪 授课形式：网课（B站）1.2. 什么是好的画面画面亮1.3. 应用场景电影，游戏，动画，设计，可视化，虚拟现实，增强现实，模拟，GUI图形用户接口。电影中里程碑：阿凡达，大量应用面部捕捉技术。1.4. Rasterization 光栅化实时，FPS&gt;30离线, FPS&lt;301.5. 计算机视觉计算机图形学离不开计算机视觉，但是视觉一般是对图像的处理。1.6. 推荐书籍Tiger虎书2. Lecture 02 Review of Linear Algebra2.1. 图形学依赖学科Optics, Mechanics, Linear algebra, statics, Singal processing, numerical analysis数值分析2.2. 向量向量的定义单位向量向量计算，向量加法用笛卡尔坐标系表示向量向量乘法，点乘和叉乘，点乘在笛卡尔坐标系中就是对应元素相乘。在图形学中，点乘是为了寻找两个向量的夹角(夹角可以判断两个向量方向的接近程度)，或者获得一个向量在另一个向量的投影，还可以获得向量的分解。叉乘，叉积结果垂直于这两个向量所在的平面，满足右手定则。向量的叉乘可以写成矩阵形式。在图形学中的应用：判断左右关系，比如a^b&gt;0，说明b在a的左边。还可以判断内外，比如判断一个点是否在一个三角形内。坐标系的定义，右手坐标系2.3. 矩阵矩阵定义矩阵乘法矩阵乘法没有交换律，但是有结合律矩阵转置，矩阵的逆向量的点乘和叉乘都可以写成矩阵乘法形式3. Lecture 03 Transformation3.1. why transformation 为什么要变换viewing: 3D to 2D projection3.2. D变换 缩放 scale transform 非均匀缩放 scale(non-uniform) 翻转 reflection matrix 切变 shear matrix竖直方向上没有变化，水平方向上发生了变化 旋转 Rotate旋转默认绕零点逆时针旋转二维旋转矩阵R上述所有的变化都可以写成x$\\prime$=Mx，也就是线性变换3.3. 齐次坐标 homogeneous coordinate 为什么要引入齐次坐标，因为对于简单的平移操作并不能写成线性变换的形式，但是人们也不想认为平移是一种特殊的变换，所以引入齐次坐标 齐次坐标 注意点和向量的表示方法不同 仿射变换 affine transformations 2D Transformations 逆变换就是乘以逆矩阵 复杂的变换都是简单的变换的组合，变换的组合顺序很重要 绕着某一个点（非原点）旋转的分解 4. Lecture 04 Transformation Cont.4.1. D Transformations 齐次坐标对于w不等于1，每一个坐标除以w 正交矩阵一个矩阵的逆等于矩阵的转置，旋转矩阵就是一个正交矩阵 仿射变换（旋转+平移）仿射变换是先进行旋转再进行平移 矩阵表示（缩放，平移） 旋转绕着某一个轴旋转一般的旋转（分解成三个坐标轴的旋转）Rodrigues’ Rotation Formula, 用向量n表示旋转轴，最终推出这个公式4.2. view transformation 视图变换 观测变换viewing，包括了视图变化和投影变化 MVP变换(model-&gt;view-&gt;projection) view transformation(不等于viewing) 视图变换视图变换是把相机放到标准位置上，located at origin, look at -Z利用逆变换，先平移再旋转一般把model和view变换统称为view transformation4.3. projection transformation 投影变换 orthographic vs perspectiive projection orthographic projection 正交投影平移，缩放（不考虑旋转） perspective projection 透视投影满足近大远小透视投影就是先把物体挤压成立方体，然后对立方体进行正交投影5. Lecture05 Rasterization 1(Triangles)5.1. Perspective Projection 透视投影 首先是对上节课的透视投影的一些补充, 其中l=left, r=right, b=bottom, t=top, n=near, f=far，这些量可以描述视锥Frustum 视锥Frustum的描述还可以用fovY(field of view)垂直视角和aspect ratio宽高比5.2. Canonical Cube to Screen 光栅化 把物体的数学描述以及与物体相关的颜色信息转换为屏幕上用于对应位置的像素及用于填充像素的颜色，这个过程称为光栅化。 屏幕是最常见的光栅设备，每一个像素都是一个小方块，像素是最小的单位，一个像素的颜色可以用rgb三种颜色表示 屏幕空间screen space 把之前投影后的小方块变成屏幕空间5.3. Different Raster Displays 不同的成像设备 Oscilloscope 示波器 Cathode Ray Tube 阴极射线管成像原理。早期电视屏幕就是这样实现成像，扫描成像。 Frame Buffer: Memory for a Raster Display 内存中的一块区域存储图像信息。 LCD(liquid crystal display)液晶显示器，光的波动性原理。 LED发光二极管5.4. 三角形光栅化 三角形是最基本的多边形，有很多好的性质。 sampling 采样。三角形离散化。在不同的像素中心，确定是0还是1,表示在三角形里还是外 如何判断点和三角形关系，利用叉积，边界上的点自己定义。 jaggies锯齿，走样aliasing6. Lecture 06 Rasterization 2(Antialiasing and Z-Buffering)6.1. sampling 采样原理 视频就是对时间进行采样 采样的artifact(瑕疵)：锯齿，摩尔纹，轮胎效应(在时间上采样) 反走样采样：可以对原始的图像进行滤波(模糊处理)然后再采样。 采样速度跟不上信号变化的速度就会走样(aliasing)6.2. Frequency domaine 信号处理频率 傅里叶变换：所有的周期函数都可以写成不同平吕的正弦函数的组合。傅里叶变换就是频域和时域/空间域的变换 走样的原因(时域)：高频信号欠采样，高频信号和低频信号在某一采样速度下没有差别，就会产生走样 滤波：抹掉特定的频率。比如高通滤波(过滤到低频信号) 卷积：图形学上的简化定义，见下图 卷积定律：时域上的卷积等于频域上的乘积 采样：重复频域上的内容 走样在频率上的解释：采样频率小会让频域上发生重叠6.3. antialiasing 反走样/抗锯齿 第一种解决方法：增加采样率，相当于增加了频域上的两个信号的距离 第二种解决方法：反走样。即先对信号进行滤波再采样 比如对于之前三角形的问题 但是这种反走样的方法比较复杂，有一种更简单的近似方法(对滤波这一步的近似)：supersampling，就是在对每个像素点变成更多的小点6.4. antialiasing today 目前反走样的方法介绍了两种新的抗锯齿的操作：FXAA和TAA。FXAA的做法是把边界找到然后对边界进行处理。7. Lecture 07 Shading(Illumination, Shading, and Graphics Pipeline)7.1. Painter’s Algorithm 画家算法 首先画出远处的物体，然后再画近处的物体。画近处的物体再覆盖远处的物体。 需要定义深度信息，根据深度信息排序7.2. Z-buffer 深度缓存 对每个像素都有最小的z值，除了一个frame buffer储存颜色信息外，还需要z-buffer储存深度信息。 假设每个像素最开始的时候深度为无限远 特点是在像素维度进行操作 7.3. 目前为止学到了什么7.4. shading 着色 着色：对不同物体应用不同的材质 一个简单的着色模型(Blinn-Phong Reflection model) 局部着色，不考虑阴影 diffuse reflection 漫反射，一个物体有多亮与接收到多少光的能量有关。点光源的能量随距离缩减。在点光源的光线到达物体表面时被物体接受多少能量又与光线和法线的夹角的cos值有关，也就是说直射时接受的能量最大(相同距离)。漫反射表示不论观测角度在哪，你观测到的亮度应该是一样的。8. Shading 2(Shading, Pipeline, Texture Mapping)8.1. Specular Term 高光项 着色包括三部分：漫反射，高光，环境光 高光就是观测方向和镜面反射方向相同，即半程向量是否和法向量接近 通常高光都是白色的8.2. Ambient Term 环境项 环境光就是一些其他物体反射的光照亮背光物体 这里介绍非常简化的模型 最终结果8.3. Shading Frequencies 着色频率 之前介绍的着色是应用在着色点，对应在屏幕空间是如何的呢？ 第一种：Shading ecah triangle 对每个三角形着色 第二种：shading each vertex 对顶点着色，然后插值 第三种：shading each pixel 对每个像素点着色 如何定义顶点的法向量呢？对周围的面的法向量求平均 如何定义像素的法向量？8.4. Graphics Pipeline 图像管线/实时渲染管线 一个实时渲染的流程/流水线 现代的GPU允许写入顶点着色部分与片段着色部分的代码8.5. Texture Mapping 纹理映射 希望在物体的不同位置定义不同的属性，比如漫反射系数等等 3维物体的表现都是一个平面 纹理映射就是对于一个平面定义不同的属性，有一个映射关系 纹理也有坐标系9. Lecture 09 Shading 3 (Texture Mapping)9.1. Barycentric Coordinates重心坐标系9.2. Interpolate 插值 重心坐标系插值9.3. Simple Texture Mapping 简单的纹理映射模型9.4. Texture Magnification 纹理放大9.5. Point Sampling Textures 就是走样问题9.6. Mipmap 范围查询 生成不同分辨率的图片 任何一个像素可以映射到纹理区域的一个点，mipmap可以让像素点快速查阅，因为他又很多层，不同的纹理区域的面积对应不同的层 mipmap也不是最好的方法，只是一种折中的办法 anisotropic filtering 各向异性过滤 10. Lecture 10 Geomrtry 1(introduction)10.1. 纹理的应用10.1.1. Environment Map 环境光映射 纹理可以用来映射环境光 假设环境光来自无限远10.1.2. Spherical Environment Map 球形环境光映射 将环境光信息存在球上 但是在边缘部分会有扭曲，解决方法有环境光存在正方体上10.1.3. 纹理凹凸贴图bump mapping 纹理不仅可以表示颜色，还可以应用一个复杂的纹理来定义高度，也就改变了法线的方向 凹凸贴图只增加表面细节，不添加新的三角形 10.1.4. 位移贴图 displacement mapping 和凹凸贴图很像，但是移动了顶点10.1.5. 三维纹理 定义了空间中任意一个点的纹理坐标 广泛应用于体积渲染 10.2. 几何10.2.1. 分类 隐式几何 显式几何10.2.2. 隐式几何 不给出点的具体坐标，而是给出点的坐标关系，比如$x^2+y^2+z^2=1$ 推广到一般形式, $f(x,y,z)=0$ 缺点：不直观，不好采样 优点：可以很容易的判断点在不在几何体内10.2.3. 显式几何 直接给出或者参数映射的方式给出 优点：采样方便，直观 缺点：不好判断点是否在几何体内还是外10.2.4. 隐式的表达方式 公式定义 通过几何体的布尔组合，目前有很多建模软件就是这么表示的 距离函数定义，SDF有向距离场11. Lecture 11 Geometry 2(Curves and Surfaces)11.1. 显式几何的表示方法11.1.1. Point Cloud 点云 点的集合 优点：可以表示任何几何体11.1.2. Polygone Mesh 使用顶点和图形表示(三角形，正方形)11.1.3. 一个例子里面定义了顶点坐标，法线，纹理坐标和哪几个点组成一个三角形11.2. Curves 曲线11.2.1. 贝塞尔曲线 用一系列控制点定义曲线 曲线不一定要经过控制点11.2.2. 如何画一条贝塞尔曲线 Casteljau Algorithm：这个算法的核心是画出每个时间t的点的位置(递归)其中$b_0^2$就是时间t的点的位置 大致流程 代数形式 生成的曲线只能在控制点的凸包内11.2.3. Piecewise Bézier Curves 逐段的贝塞尔曲线 每四个控制点定义一条贝塞尔曲线 C0连续(点连续)，C1连续(切线连续)11.2.4. Spline 样条 样条是用一系列的点画出线条11.3. 曲面11.3.1. 贝塞尔曲面 使用贝塞尔曲线生成贝塞尔曲面 竖直方向生成四条曲线，然后对于t来说四个点再作为控制前生成曲线11.3.2. 曲面细分 使用很多三角形网格来表示曲面12. Lecture 12 Geometry 312.1. Mesh Subdivision(upsampling) 网格细分 引入更多三角形，微调它们的位置 Loop Subdivision：第一步增加三角形的数量，第二部调整三角形的位置 Loop细分规则： 另一种细分规则：Catmull-Clark Subdivision奇异点是这个点的度不是4的点(就是连接的边数不等于4) 这种细分方法可以用于任何面12.2. Mesh Simplification 网格简化 基本思路是为了减少网格数目但是保持它的基本形状 一种方法：Collapsing an edge 边坍缩。删除一些点 判断标准：quadric error metrics 二次误差度量 实际效果12.3. 阴影 Shadow mapping 光栅化着色的时候是局部的，但是有时候会有问题，比如有东西挡在shading point和光源之间时，所以需要在这种情况下生成阴影 光栅化生成阴影的方法叫做shadow mapping shadow mapping 的两步 第一步：从光源出发，看向shading point，记录能看见的点的深度 第二步：从摄像机出发，看向shading point，如果看见的点的深度和光源所看见的深度相同，那么这个点不在阴影内，否则，它在阴影内。 具体的例子： 问题：走样，阴影分辨率，只能做硬阴影(hard shadow)…13. Lecture 13 Ray Tracing 113.1. Why ray tracing 光栅化的缺点：无法表示全局的光照、毛玻璃效果无法很好表示、阴影处理不算好 光纤追踪很精准但是比较慢，经常做离线(电影制作)13.2. Light Rays 光线沿直线传播 光线不会交叉 光线是不断折回然后打到人眼 光路可逆性13.3. Ray Casting 光线投射 从眼睛到像素点出发，到虚拟世界，再到光源(Local) 从眼睛到像素点到虚拟世界的线叫做eye ray13.4. Recursive Ray Tracing 递归光线追踪 如果在shading point 处可以折射，能量损失，则继续折射然后对每个点都算着色值 对每个点都要计算是否处在阴影中13.5. Ray-Surface interaction 光线和表面相交13.5.1. Ray Equation13.5.2. 与圆相交的交点 一个交点就是相切，两个交点就是相交13.5.3. intersection with implicit surface 与隐式表面相交13.5.4. intersection with triangle mesh 也就是与显式表面(三角形网格)相交 第一种想法就是光线与每个三角形进行计算，但这样计算量太大 第二种想法是光线与三角形所在的平面相交，然后判断交点是不是在三角形内 如何定义平面？一个点+法线 然后将光线方程带入平面方程中，就可以得出光线与平面的交点 如何简化判断交点与三角形的位置关系？MT算法： 这个算法的核心就是利用重心坐标系：解出重心坐标后，如果它们都为正，那么点在三角形内13.5.5. accelerating ray-surface intersection 加速交点(一般指与三角形网格的交点)计算过程 bounding volume 包围盒 引入包围盒的思路是：如果光线与包围盒都不相交，那么肯定不会与里面的几何体有交点 包围盒由三个对面的交集轴对齐包围盒(就是对面与坐标轴平行)axis-aligned bounding box 先考虑二维的情况Ray intersection with aabb找到最大的时间和最小的时间 三维：对于三组对面，计算$t_{min}$和$t_{max}$，然后找到$t_{enter}$和$t_{exit}$。那么我们就知道了进入的时间和出去的时间，如果进去的时间小于出去的时间，那么光线进入了aabb，表示光线在盒子里呆过一段时间 还要要保证进入的时间和出去的时间都要大于014. Lecture 14 Ray Tracing 214.1. Uniform Spatial Partitions (Grids) 继续上节课的加速计算话题 一种加速方法：生成grid找到aabb后，创建网格，存储aabb内几何体 然后光线沿着这些小格子相交14.2. Spatial Partitions 空间划分14.2.1. 一些划分示例八叉树Oct-Tree，KD-Tree，BSP-Tree14.2.2. KD-Tree 每次划分都沿着坐标轴移动，对于中间的结点都有子节点，只存储叶子结点的数据 缺点：一个物体可能存在在多个叶子节点里 14.3. Object Partitions 物体划分14.3.1. Bounding Volume Hierarchy(BVH) 这种方法是目前图形学中使用较多的方法 沿着物体不断细分出bbox bvh的缺点：两部分bbox可能相交 14.3.2. Building BVH 如何划分结点？选择一个维度进行划分，每次找最长的结点进行细分，细分的结点在中位数，当结点处图形较少，则停止14.3.3. 与空间划分的对比14.4. Whitted style 到目前为止，已经讲了国内光线追踪会讲的内容。也就是讲完了Whitted style光线追踪14.5. Radiometry 辐射度量学14.5.1. 一些物理量 new terms: radiant flux, intensity, irradiance, radiance14.5.2. Radiant Energy and Flux randiant flux就是单位时间能量/功率14.5.3. Radiant Intensity 辐射强度就是单位立体角(solid angle)的功率 那么立体角是什么呢？立体角就是二维空间的角在三维空间的沿伸，就是球面面积除以半径的平方15. Lecture 15 Ray Tracing15.1. Radiometry cont. 辐射度量学15.1.1. 继续上节课的内容 微分立体角，就是球坐标系上对$\\theta$和$\\phi$的微分15.1.2. Irradiance 单位面积的功率 面积是投影的面积15.1.3. Radiance randiance就是单位投影面积单位立体角的功率 irradiance和radiance的区别：irradiance是某一个面积上接受的能量，而radiance是某一个面积某一个角度上接受的能量15.2. Bidirectional Reflectance Distribution Function (BRDF) 双向反射分布方程BRDF是描述光线传播的方程 某一个方向$\\omega_i$的光线打到某一个表面然后被吸收同时从另一个方向$\\omega_r$反射出去 反射方程 观察某一个物体的反射光线不止从光源有光线，还有其他物体反射的光 渲染方程Rendering Equation 渲染方程两部分组成，一部分是自身发光，另一部分是接受的光线的反射光线(半球上每个方向)15.3. Rendering Equation 渲染方程15.3.1. 如何理解渲染方程 反射的光线由两个个部分组成：自身的emission和从各个方向的反射光 如何考虑物体反射的光？把物体看作一个光源，也就是看作一个递归的过程 通过数学式子简化渲染方程： 然后通过逆矩阵可以解出L 光线弹射一次叫做直接光照、弹射两次及以上叫做间接光照 那么就可以发现与光栅化的区别 在多次弹射后场景会趋于一个固定的亮度16. Lecture 16 Ray Tracing 416.1. Monte Carlo Integration 蒙特卡洛积分 有些函数不太好用解析式写出来 蒙特卡洛积分就是数值积分的方法 就是采样值除以采样密度16.2. Path Tracing 路径追踪 与whitted sytle的区别：whitted sytle没有考虑全局光照16.2.1. 解渲染方程 考虑一个简单的模型，只有直接光照 每一个$\\omega_i$都看作采样，那么可以应用蒙特卡洛积分 应用全局光照，将物体反射面也看做光源，做一个递归 但是这样会出现一个问题，那就是爆炸，如果我取多个X，那么弹射很多次后就会爆炸 解决方法，对每个点只取一个方向，也就是N=1，所以它叫做路径追踪 这样噪声会比较大，但是从每个像素点有多个路径，所以还是可以接受 第二个问题是递归不会停止？解决方法：俄罗斯轮盘赌，即在某一个程度停止递归 那么我们可以设定一个概率P来决定每个点是否打出一条光线，同时保证期望不变 到目前为止已经是一个正确的path tracing的渲染方法，但是这样效率比较低 效率低的原因：每个点打到或者打不到光源是随机的，也就是说浪费了很多光线 可以在光源上采样，这样没有光线会浪费，渲染方程就需要写成在光源上采样 那么我们就可以将渲染方程分为两部分，一部分是光源直接光照，方法使用上面提到的在光源上采样，另一部分是间接光照，保持不变16.2.2. 最终的代码 但还有一个小问题，就是中间有物体遮挡，需要添加一个判断16.3. 路径追踪 在之前，ray tracing主要指whitted-style ray tracing 但现在，只要设计了光线传播方法，就是ray tracing，路径追踪只是其中的一个方法"
    } ,
  
    {
      "title"       : "组会记录",
      "category"    : "",
      "tags"        : "note",
      "url"         : "./1221%E7%BB%84%E4%BC%9A.html",
      "date"        : "2021-12-21 00:00:00 +0800",
      "description" : "组会笔记",
      "content"     : "VAEAEAuto-Encoder自动编码器，比如Seq2seq模型。VAE(Variational Auto-Encoder)在实际情况中，我们需要在模型的准确率上与隐含向量服从标准正态分布之间做一个权衡，所谓模型的准确率就是指解码器生成的图片与原图片的相似程度。我们可以让网络自己来做这个决定，非常简单，我们只需要将这两者都做一个loss，然后在将他们求和作为总的loss，这样网络就能够自己选择如何才能够使得这个总的loss下降。另外我们要衡量两种分布的相似程度，如何看过之前一片GAN的数学推导，你就知道会有一个东西叫KL-divergence来衡量两种分布的相似程度，这里我们就是用KL-divergence来表示隐含向量与标准正态分布之间差异的loss，另外一个loss仍然使用生成图片与原图片的均方误差来表示。KL消失KL消失后，VAE就变成了AE原因： KL项本身太容易被优化 一旦崩塌，Decoder会忽视Zx Zx的表示学习依赖于Decoder解决KL消失的思路…Analyze Pretraining Language ModelPerspective of knowledge Syntacitic/Semantic/lexical 句法，语义，词汇 重构语法树 Attention中很多头可能没有用，学到了很多冗余的信息 Analyze Feed Forward Neural Network 浅层词汇信息，深层语义信息 Prompt"
    } ,
  
    {
      "title"       : "推荐系统",
      "category"    : "",
      "tags"        : "note",
      "url"         : "./Recommender_system.html",
      "date"        : "2021-12-16 00:00:00 +0800",
      "description" : "d2l推荐系统笔记",
      "content"     : "1. 推荐系统总览 1.1. 协同过滤 Collaborative Filtering 1.2. 显式反馈和隐式反馈 1.3. 推荐任务 2. 矩阵分解 Matrix Factorization 3. AutoRec 3.1. overview 3.2. formula 4. Personalized Ranking for Recommender System 4.1. overview 4.2. Bayesian Personalized Ranking loss 贝叶斯损失 4.3. Hinge Loss 5. Neural Collaborative Filtering for Personalized Ranking 使用协同过滤网络个性化排序 5.1. The NeuMF model 5.2. Evaluator 5.3. 代码 6. Sequence-Aware Recommender Systems 6.1. Model Architectures 6.2. Negative Sampling 负采样 7. Feature-Rich Recommender Systems 8. Factorization Machines 8.1. 2-Way Factorization Machines 1. 推荐系统总览1.1. 协同过滤 Collaborative Filtering协同过滤最早出现在1992年Tapestry system，“人们相互协作，相互帮助，执行过滤程序，以处理大量的电子邮件和张贴到新闻组的信息。”现在协同过滤的概念更加广泛，从广义上讲，它是利用涉及多个用户、代理和数据源之间协作的技术来过滤信息或模式的过程。协同过滤模型可以分为:1.memory-based CF; 2.model-based CF. 其中Memory-based CF又可以分为item-based和user-based CF。model-based CF有矩阵分解模型。总的来说，协同过滤就是利用用户-物品的数据来预测和推荐。1.2. 显式反馈和隐式反馈为了学习用户的偏好，系统需要收集用户的反馈feedback。反馈可以分为显式和隐式。显式反馈就是需要用户主动提供兴趣偏好。比如点赞、点踩。隐式反馈则是间接反映用户的喜好，比如购物历史记录，浏览记录，观看记录甚至是鼠标移动。1.3. 推荐任务电影推荐、新闻推荐、评分预测rating prediction task、top-n reommendation。如果使用了时间戳信息，那么我们构建了sequence-aware recommendation。针对新用户推荐新物品称为cold-start recommendation冷启动推荐。2. 矩阵分解 Matrix FactorizationThe Matrix Factorization Model矩阵分解模型R是user-item矩阵，行数是用户数量，列数是物品数量,那么R∈Rmxn。P是user latent matrix，P∈Rmxk，Q是item latent matrix，Q∈Rnxk矩阵分解就是把R分解成P和Q，那么预测的评分就是：但是上面这个式子没有考虑偏置，我们会有下面这个完整的式子：那么目标函数可以定义为：右边那一串是正则项，为了避免过拟合下面这张图值观的展示了矩阵分解过程：3. AutoRec3.1. overview使用autoencoder预测评分，上小节介绍的矩阵分解模型是线性模型，它不能捕捉复杂的非线性关系，比如用户的偏好。这一小节介绍一个非线性协同过滤神经网络模型AutoRec。AutoRec是基于自编码器的结构，自编码器是一种特殊的神经网络架构，他的输入和输出的架构是相同的，自编码器通过无监督学习来训练获取输入数据在较低维度的表达，在神经网络的后段，这些低纬度的信息再次被重构回高维的数据表达。所以AutoRec的架构也是输入层，隐藏层和重构输出层。它的目的是输入一个只有部分兴趣矩阵，输出一个完整的兴趣矩阵。AutoRec可以分为user-based 和 item-based3.2. formula针对item-based：$R_{*i}$表示兴趣矩阵的第i列，不知道的项填为0。那么神经网络的构架可以定义为：h()表示最终的输出，输出一个完整的兴趣矩阵，那么误差定义为：4. Personalized Ranking for Recommender System4.1. overview在上一节中，我们用到了显式反馈，同时模型只在能观察到的评分上训练。那么这种模型有两个缺点：第一个是很多的反馈并不是显式的。第二个是没有观察到的评分被完全忽略了。个性化推荐可以分为:1.pointwise;2.pairwise;3.listwise。Pointwise表示每次预测单个偏好，pairwise则是预测出一系列的偏好然后进行排序，listwise则是预测所有的item并进行排序。4.2. Bayesian Personalized Ranking loss 贝叶斯损失 贝叶斯损失是一种pairwise个性化推荐损失。它被广泛应用于多种推荐系统中。它假设用户相对于无观察项，更加喜欢positive item 训练集格式是(u, i, j)表示用户u喜欢i超过j，BPR希望最大化下面这个后验概率： 其中$\\Theta$表示推荐系统的参数，$&gt;_u$表示用户u对所有item的排序。4.3. Hinge Loss 数学表达式其中m表示安全系数，它的目的是让不喜欢的项离喜欢的项更远。它和贝叶斯都是为了优化positive sample和negative sample之间的距离。5. Neural Collaborative Filtering for Personalized Ranking 使用协同过滤网络个性化排序本小节重新将目光聚集到隐式反馈中，介绍协同过滤推荐系统NeuMF。NeuMF利用隐式反馈，它由两个子结构组成，分别是generalized matrix factorization(GMF)和MLP。不同于评分的预测如AutoRec，它将生成一系列的推荐，它根据用户是否看过这场电影来区分为正例和反例5.1. The NeuMF modelNeuMF的网络结构由两部分组成。 一部分是GMF，也就是matrix factorization的类似形式，输入用户向量$p_u$和物品向量$q_i$，返回x 另一部分是MLP，输入和GMF一样，但是用不同的字母表示，具体公式如下： 最后对这两个子结构concatenate一下，就是最终的输出 大体的网络结构如下5.2. Evaluator有两个性能度量指标 hit rate at given cutting off l，记作Hit@l这个式子的主题思路是判断推荐的物品是否在top l中，m表示用户的数量，$rank_{u,g_u}$表示对于用户u和物品$g_u$的排名，1表示指标函数 AUC，即ROC曲线下的面积，也是模型泛化能力的一个指标其中$S_u$表示模型对于u的推荐物品集，I表示item set，AUC越大越好5.3. 代码网络结构就是上面介绍的那样，net的输出是用户和物品匹配出的一个推荐值(我的想法)。在进行训练的时候，会给出正例物品(即用户有过评分的物品)和反例物品(用户没有评分，也就是没有看过)分别与用户得到一个推荐值，然后利用上一小节介绍的贝叶斯损失来优化(让评分过的物品有更高的推荐值)，然后最终我们希望返回一系列的推荐物品，这些推荐物品都是没有负例物品，然后根据推荐值进行排序。性能指标是hit或者auc。hit的思想是让真实评分的物品在推荐列表中。6. Sequence-Aware Recommender Systems之前的模型都没有考虑时序信息，这小节的Caser模型将会考虑用户的时序信息。6.1. Model Architectures模型的输入$E^{(u,t)}$表示用户u的近期L个评价的物品，Caser模型有横向和纵向的卷积层，输入矩阵分别与卷积层作用后，结果concatenate变成$z$，$z$再和用户的一般信息结合，也就是$z$和$p_u$concatenate最终输出$\\hat{y}_{uit}$，其中$p_u$表示用户u的item信息6.2. Negative Sampling 负采样我们需要对数据集进行重新处理，比如一个人喜欢9部电影，同时我们的L=5，那么我们将最近的一部电影留出来作为test，其余的都作为训练集，可以划分出3个训练集。同时我们也需要进行负采样(采样没有评分的item)7. Feature-Rich Recommender Systems之前的模型大都用到了用户物品的交互矩阵，但是很少有用到一些额外的信息，比如物品的特征，用户的简介，发生交互的背景等等…利用这些信息可以获得用户的兴趣特征。本节提出了一个新的任务CTR(click-through rate)，也就是点击率任务，对象可以是广告、电影等等。8. Factorization MachinesFactorization machines(FM)是一个监督算法，可用于分类，回归和排名任务。它有两个优点：1.它能处理稀疏的数据；2.它能减少时间复杂度和线性复杂度8.1. 2-Way Factorization Machines"
    } ,
  
    {
      "title"       : "Robotics",
      "category"    : "",
      "tags"        : "note",
      "url"         : "./Robotics.html",
      "date"        : "2021-12-13 00:00:00 +0800",
      "description" : "《机器人》课程随堂笔记",
      "content"     : "报告报告内容用solidworks设计一个至少三个自由度的机械臂，并且描述它的动能，移动能力等等。提交的是截图，源文件等等。报告格式 标题，下面有姓名学号电话等等 摘要 正文"
    } ,
  
    {
      "title"       : "数据挖掘",
      "category"    : "",
      "tags"        : "note",
      "url"         : "./datamining.html",
      "date"        : "2021-12-10 00:00:00 +0800",
      "description" : "《Python数据挖掘入门与实践》笔记",
      "content"     : "总体情况 第一章 开始数据挖掘之旅 1.1 亲和性分析 1.2 分类 第二章 用scikit-learn估计器分类 2.1 scikit-learn 2.2 邻近算法KNN 第三章 用决策树预测获胜球队 3.1 决策树 3.2 随机森林 第四章 用亲和性分析方法推荐电影 4.1 亲和性分析 4.2 Apriori算法 第五章 用转换器抽取特征 5.1 抽取特征 5.2 特征选择 5.3 创建特征 第六章 使用朴素贝叶斯进行社会媒体挖掘 6.1 消歧 6.2 文本转换器 6.3 朴素贝叶斯 6.4 F1值 第九章 作者归属问题 9.1 作者归属 9.2 支持向量机 9.3 基础SVM的局限性 第十章 新闻语料分类 10.1 新闻语料聚类 10.2 K-means算法 总体情况 书籍:Python数据挖掘入门与实践 github_url:https://github.com/LinXueyuanStdio/PythonDataMining 配套代码和笔记，很适合迅速上手 这篇博客主要记录一些比较重要的算法第一章 开始数据挖掘之旅1.1 亲和性分析 亲和性分析根据样本个体（物体）之间的相似度，确定它们关系的亲疏。 例子：商品推荐。 我们要找出“如果顾客购买了商品X，那么他们可能愿意购买商品Y”这样的规则。简单粗暴的做法是，找出数据集中所有同时购买的两件商品。找出规则后，还需要判断其优劣，我们挑好的规则用。 规则的优劣有多种判断标准，常用的有支持度(support)和置信度(confidence) 支持度：数据集中规则应验的次数，统计起来很简单。有时候，还需要对支持度进行规范化，即再除以规则有效前提下的总数量。 置信度是衡量规则的准确性如何。1.2 分类 根据特征分出类别 例子：Iris植物分类数据集，通过四个特征分出三个类别 特征连续值变成离散值 OneR算法：它根据已有数据中，具有相同特征值的个体最可能属于哪个类别进行分类。比如对于某一个特征值来说，属于A的类别有80个，属于B的类别有20个，那么对于这个特征值来说，取值为1代表为A类别，错误率有20％。给出所有特征值，找出错误率最小的特征值作为判断标准。第二章 用scikit-learn估计器分类2.1 scikit-learnscikit-learn里面已经封装好很多数据挖掘的算法现介绍数据挖掘框架的搭建方法： 转换器（Transformer）用于数据预处理，数据转换 流水线（Pipeline）组合数据挖掘流程，方便再次使用（封装） 估计器（Estimator）用于分类，聚类，回归分析（各种算法对象） 所有的估计器都有下面2个函数 fit() 训练 用法：estimator.fit(X_train, y_train)， estimator = KNeighborsClassifier() 是scikit-learn算法对象 X_train = dataset.data 是numpy数组 y_train = dataset.target 是numpy数组 predict() 预测 用法：estimator.predict(X_test) estimator = KNeighborsClassifier() 是scikit-learn算法对象 X_test = dataset.data 是numpy数组 2.2 邻近算法KNN邻近算法，或者说K最邻近（KNN，K-NearestNeighbor）分类算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是K个最近的邻居的意思，说的是每个样本都可以用它最接近的K个邻近值来代表。近邻算法就是将数据集合中每一个记录进行分类的方法。例子：分类，Ionosphere数据集第三章 用决策树预测获胜球队3.1 决策树例子：预测NBA球队获胜情况决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。分类树（决策树）是一种十分常用的分类方法。它是一种监督学习，所谓监督学习就是给定一堆样本，每个样本都有一组属性和一个类别，这些类别是事先确定的，那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类。这样的机器学习就被称之为监督学习。scikit-learn库实现了分类回归树（Classification and Regression Trees，CART）算法并将其作为生成决策树的默认算法，它支持连续型特征和类别型特征。3.2 随机森林随机森林指的是利用多棵树对样本进行训练并预测的一种分类器。在机器学习中，随机森林是一个包含多个决策树的分类器， 并且其输出的类别是由个别树输出的类别的众数而定。第四章 用亲和性分析方法推荐电影4.1 亲和性分析亲和性分析就是分析两个样本之间的疏密关系，常用的算法有Apriori，Apriori算法的一大特点是根据最小支持度生成频繁项集（frequent itemest），它只从数据集中频繁出现的商品中选取共同出现的商品组成频繁项集。其他亲和性分析算法有Eclat和频繁项集挖掘算法（FP-growth）。4.2 Apriori算法Apriori算法主要有两个阶段，第一个阶段是根据最小支持度生成频繁项集，第二个阶段是根据最小置信度选择规则，返回规则。本章的例子是电影推荐。第一个阶段，算法会先生成长度较小的项集，再将这个项集作为超集寻找长度较大的项集。第二个阶段是从频繁项集中抽取关联规则。把其中几部电影作为前提，另一部电影作为结论。组成如下形式的规则：如果用户喜欢前提中的所有电影，那么他们也会喜欢结论中的电影。第五章 用转换器抽取特征5.1 抽取特征抽取数据集的特征是重要的一步，在之前的学习中我们都获得了数据集的特征，但很多没有处理的文本特征并不是很明显，比如一段文本等等。特征值可以分为连续特征，序数特征，类别型特征。5.2 特征选择通常特征有很多，但我们只想选择其中一部分。选用干净的数据，选取更具描述性的特征。判断特征相关性：书中列举的例子是判断一个人的收入能不能超过五万，利用单变量卡方检验(或者皮尔逊相关系数)判断各个特征的相关性，然后给出了三个最好的特征，分别是年龄，资本收入和资本损失。5.3 创建特征主成分分析算法（Principal Component Analysis，PCA）的目的是找到能用较少信息描述数据集的特征组合。第六章 使用朴素贝叶斯进行社会媒体挖掘6.1 消歧本章我们将处理文本，文本通常被称为无结构格式。文本挖掘的一个难点来自于歧义，比如bank一词多义。本章将探讨区别Twitter消息中Python的意思。6.2 文本转换器Python中处理文本的库NLTK(Natural Language Toolkit)。据作者说很好用，可以作自然语言处理。N元语法是指由连续的词组成的子序列。6.3 朴素贝叶斯朴素贝叶斯概率模型是以对贝叶斯统计方法的朴素解释为基础。贝叶斯定理公式如下：$ P(A|B) = \\frac {P(B|A)P(A)}{P(B)} $贝叶斯公式可以用它来计算个体属于给定类别的概率。朴素贝叶斯算法假定了各个特征之间相互独立，那么我们计算文档D属于类别C的概率为P(D|C)=P(D1|C)*P(D2|C)…P(Dn|C)。贝叶斯分类器是输入数据来更新贝叶斯的先验概率和后验概率，输入贝叶斯模型后，返回不同类别中概率的最大值。示例： 举例说明下计算过程，假如数据集中有以下一条用二值特征表示的数据：[1, 0, 0, 1]。训练集中有75%的数据属于类别0，25%属于类别1，且每个特征属于每个类别的似然度如下。类别0：[0.3, 0.4, 0.4, 0.7] 类别1：[0.7, 0.3, 0.4, 0.9] 拿类别0中特征1的似然度举例子，上面这两行数据可以这样理解：类别0中有30%的数据，特征1的值为1。我们来计算一下这条数据属于类别0的概率。类别为0时，P(C=0) = 0.75。朴素贝叶斯算法用不到P(D)，因此我们不用计算它。我们来看下计算过程。P(D|C=0) = P(D1|C=0) x P(D2|C=0) x P(D3|C=0) x P(D4|C=0)= 0.3 x 0.6 x 0.6 x 0.7 = 0.0756 现在，我们就可以计算该条数据从属于每个类别的概率。需要提醒的是，我们没有计算P(D)，因此，计算结果不是实际的概率。由于两次都不计算P(D)，结果具有可比较性，能够区分出大小就足够了。来看下计算结果。P(C=0|D) = P(C=0) P(D|C=0) = 0.75 * 0.0756 = 0.05676.4 F1值F1值是一种评价指标。F1值是以每个类别为基础进行定义的，包括两大概念：准确率（precision）和召回率（recall）。准确率是指预测结果属于某一类的个体，实际属于该类的比例。召回率是指被正确预测为某个类别的个体数量与数据集中该类别个体总量的比例。F1值是准确率和召回率的调和平均数。第九章 作者归属问题9.1 作者归属作者归属（authorship attribution）是作者分析的一个细分领域，研究目标是从一组可能的作者中找到文档真正的主人。利用功能词进行分类，功能词是指本身含义很少，但是是组成句子必不可少的部分。9.2 支持向量机支持向量机（SVM）分类算法背后的思想很简单，它是一种二类分类器（扩展后可用来对多个类别进行分类）。假如我们有两个类别的数据，而这两个类别恰好能被一条线分开，线上所有点为一类，线下所有点属于另一类。SVM要做的就是找到这条线，用它来做预测，跟线性回归原理很像。下图中有三条线，那么哪一条线的分类效果最好呢？直觉告诉我们从左下到右上的这一条线效果最好，因为每一个点到这条线的距离最远，那么寻找这条线就变成了最优化问题。对于多种类别的分类问题，我们创建多个SVM分类器，其中每个SVM分类器还是二分类。连接多个分类器的方法有很多，比如说我们可以将每个类别创建一对多分类器。把训练数据分为两个类别——属于特定类别的数据和其他所有类别数据。对新数据进行分类时，从这些类别中找出最匹配的。9.3 基础SVM的局限性最基础的SVM只能区分线性可分的两种类别，如果数据线性不可分，就需要将其置入更高维的空间中，加入更多伪特征直到数据线性可分。寻找最佳分隔线时往往需要计算个体之间的内积。我们把内核函数定义为数据集中两个个体函数的点积。常用的内核函数有几种。线性内核最简单，它无外乎两个个体的特征向量的点积、带权重的特征和偏置项。多项式内核提高点积的阶数（比如2）。此外，还有高斯内核（rbf）、Sigmoind内核。第十章 新闻语料分类10.1 新闻语料聚类之前我们研究的都是监督学习，在已经知道类别的情况下进行分类。本章着眼于无监督学习，聚类。10.2 K-means算法k-means聚类算法迭代寻找最能够代表数据的聚类质心点。算法开始时使用从训练数据中随机选取的几个数据点作为质心点。k-means中的k表示寻找多少个质心点，同时也是算法将会找到的簇的数量。例如，把k设置为3，数据集所有数据将会被分成3个簇。k-means算法分为两个步骤：为每一个数据点分配簇标签，更新各簇的质心点。k-means算法会重复上述两个步骤；每次更新质心点时，所有质心点将会小范围移动。这会轻微改变每个数据点在簇内的位置，从而引发下一次迭代时质心点的变动。这个过程会重复执行直到条件不再满足时为止。通常是在迭代一定次数后，或者当质心点的整体移动量很小时，就可以终止算法的运行。有时可以等算法自行终止运行，这表明簇已经相当稳定——数据点所属的簇不再变动，质心点也不再改变时。"
    } ,
  
    {
      "title"       : "软件方法",
      "category"    : "",
      "tags"        : "school",
      "url"         : "./%E8%BD%AF%E4%BB%B6%E6%96%B9%E6%B3%95.html",
      "date"        : "2021-11-30 00:00:00 +0800",
      "description" : "课堂记录",
      "content"     : "目录 目录 软件方法 课程要求 随记 PPT整理 1. 对象，类 2.面向对象 3.JAVA 4.数据结构 5. 常用数据结构方法 软件方法课程要求学习面向对象这种软件开发方法（目前概念越来越广），通过java来了解面向对象具体怎么实现。随记 类，对象： 给类赋值变成实例/对象 c语言可以构建面向对象所有的结构 对象就是给类声明的一个变量 类集合了属性和方法 面向对象的三大特征： 封装（encapsulation）: private, protected, public 可作用于属性和方法 一般是隐藏对象的属性和实现细节，但是提供方法的接口 提供公开的方法 提高了软件开发的效率 继承（inheritance）： 子类与父类 子类自动具有父类属性和方法，添加自己特有的属性和方法，并且子类使用父类的方法也可以覆盖/重写父类方法 可以实现代码的复用（当然功能不止于此） 多态（polymorphism）： 父类有多个子类 子类覆盖/重写父类方法 相当于是根据实际创建的对象类型动态决定使用哪个方法 所有的子类都可以看成父类的类型，运行时，系统会自动调用各种子类的方法 UML可以画出类之间的关系 java程序设计 百分百面向对象 不存在类以外代码 只能采用面向对象方法编程 java文件命名规范 必须以.java结尾 源文件中如果只有一个类，文件类必须与该类名相同 如果有多个类，且没有public类，文件名可与任一类名相同 有多个类，且有public类，文件名必须与该类名相同 一个JAVA源文件只能有一个public类，一个文件中只能有一个main主函数 静态方法/static，可以直接用类和函数名直接调用，和普通方法的区别是不用new一个示例 static 方法可以直接调用，abstract方法存在的类肯定是抽象类 抽象方法不定义具体内容 多态的实现，先定义抽象的（abstract）父类，然后子类继承父类然后定义父类的抽象方法 通过抽象方法固定通用接口 子类通过强制实现抽象方法实现多态 抽象父类可以定义属性和构造函数 抽象父类不能实例化，只能通过向上转型的方法定义 抽象父类可以向下转型成子类 父类的方法一般是抽象方法，不定义具体内容，留给子类定义，父类出现的抽象方法子类必须全部定义 多态的主要特点就是父类的方法全部是抽象的 PPT整理1. 对象，类 使用对象之前要先声明和创造 类定义了对象的类型，所有对象都是类的实例，所有的类描述了属性和定义了方法2.面向对象 封装：保护类的属性和方法,private,default,protected,public 继承：B继承A，重用，修改，添加，A所有的属性都存在于B中，A的方法可以在B中重新定义，B的改动不会影响A 多态：一个对象属于多个类，通过使用不同类中的方法属于不同的类，父类是抽象类，各个子类继承父类并定义方法，调用的时候根据不同子类调用方法。判断类型是否相同instanceof，声明的时候可以这么声明: A a = new B(),其中B是A的子类。3.JAVA x = bool ? a : b，表示如果bool为true，执行a，如果为false执行b for(Point p : this.getVect())表示遍历 exception:还有异常的抛出throwstry-catch-finallytry { // 可能会发生异常的程序代码 } catch (Type1 id1){ // 捕获并处置try抛出的异常类型Type1 } catch (Type2 id2){ //捕获并处置try抛出的异常类型Type2 }finally { // 无论是否发生异常，都将执行的语句块 }自定义异常：class NombreNegatifException extends Exception{ public NombreNegatifException() { System.out.println(\"Vous avez un nombre négatif !\"); } } 文件读写：类FileReader,FileWriter,使用里面的方法read()和write(x)和close()比如： 枚举enum，举例说明public enum Jour { LUNDI, MARDI, MERCREDI, JEUDI, VENDREDI, SAMEDI, DIMANCHE;}class EssaiJour { public static void main(String[] args) { Jour jour = Jour.valueOf(args[0]); if (jour == Jour.SAMEDI) System.out.print(\"fin de semaine : \"); switch(jour) { case SAMEDI : case DIMANCHE : System.out.println(\"se reposer\"); break; default : System.out.println(\"travailler\"); break; } }} 接口interface,迭代器iterator举例：class Main { @FunctionalInterface public interface maFonction { Integer appliquer(Integer p); } public static Vector&lt;Integer&gt; transforme(Vector&lt;Integer&gt; v, maFonction function) { Vector&lt;Integer&gt; nouveauVect = new Vector&lt;Integer&gt;(); for (int i = 0; i &lt; 3; i++) { nouveauVect.add(function.appliquer(v.get(i))); } nouveauVect.add(v.get(3)); return nouveauVect; } public static void main(String[] args) { Vector&lt;Integer&gt; vi = new Vector&lt;Integer&gt;(4); vi.add(1); vi.add(4); vi.add(83); vi.add(18); System.out.println(\"Les valeurs du vecteur initial : \"); System.out.print(vi.get(0)+\" \"); System.out.print(vi.get(1)+\" \"); System.out.print(vi.get(2)+\" \"); System.out.println(vi.get(3)); vi = transforme(vi,s -&gt; (s * 2)); System.out.println(\"Les valeurs du vecteur modifié : \"); Iterator iter = vi.iterator(); while (iter.hasNext()) { System.out.print(iter.next() + \"\"); } System.out.println(); }}4.数据结构 数据结构一般含有以下功能：创建，插入，寻找，删除，排序 二维数组,举例说明：String tab[][]={ {\"a\", \"e\", \"i\", \"o\", \"u\"}, {\"1\", \"2\", \"3\", \"4\"} };int i = 0, j = 0;for(String sousTab[] : tab) { j = 0; for(String str : sousTab) { System.out.println(\"Valeur du tableau à l'indice [\"+i+\"][\"+j+\"]: \" + tab[i][j]); j++; } i++;}或者int[] tabEntiers ;tabEntiers = new int[40] ; // création effective du tableau précédent 列表，包含ArrayList, LinkedListArrayList是实现了基于动态数组的数据结构，LinkedList基于链表的数据结构。对于随机访问get和set，ArrayList优于LinkedList，因为ArrayList可以随机定位，而LinkedList要移动指针一步一步的移动到节点处。（参考数组与链表来思考）。对于新增和删除操作add和remove，LinedList比较占优势，只需要对指针进行修改即可，而ArrayList要移动数据来填补被删除的对象的空间。public class Liste&lt;T&gt; { protected T valeur; protected Liste&lt;T&gt; succ; protected Liste&lt;T&gt; pred; public T valeur() { return valeur; } public void changerValeur(T x) { valeur = x; } public Liste&lt;T&gt; succ() { return succ; } public void changerSucc(Liste&lt;T&gt; y) { succ = y; } public void changerPred(Liste&lt;T&gt; y) { pred = y; }} 这是一个链表的简写，每一层包含了上一个元素，这一个元素，下一个元素 哈希表，通过简历KV关系查找，相比于之前的顺序访问或者其他指数访问要快。import java.util.HashMap;public class TestHash { public static void main(String[] args) { HashMap&lt;String,String&gt; annuaire = new HashMap&lt;String,String&gt;(); // ajout des valeurs annuaire.put(\"Alfred\",\"2399020806\"); annuaire.put(\"Daniel\", \"2186000000\"); // obtention d'un numéro if (annuaire.containsKey(\"Danielle\")) { String num = annuaire.get(\"Danielle\"); System.out.println(Danielle : \"+num\"); } else { System.out.println(\"pas trouve\"); } }} 树状结构tree一般包含结点，结点的度(该结点下有多少子树的数目)，树的度不同的遍历方法： 前序遍历，首先结点，然后左子树，右子树中序遍历，左子树，结点，右子树后序遍历，左子树，右子树，结点层序遍历，从上到下，从左到右class Arbre { protected &lt;T&gt; valeur; protected Arbre filsGauche, filsDroit; public &lt;T&gt; valeur() { return valeur; } public boolean existeFilsGauche() { return filsGauche != null; } public boolean existeFilsDroit() { return filsDroit != null; } public Arbre filsGauche() { return filsGauche; } public Arbre filsDroit() { return filsDroit; } public void affecterValeur(&lt;T&gt; c) { valeur = c; } public void affecterFilsGauche(Arbre g) { filsGauche = g; } public void affecterFilsDroit(Arbre d) { filsDroit = d;} public boolean feuille() {return (filsDroit==null &amp;&amp; filsGauche==null); }public int hauteur() { int g = existeFilsGauche() ? filsGauche.hauteur() : 0; int d = existeFilsDroit() ? filsDroit.hauteur() : 0; return Math.max(g,d) + 1 ;}// Constructeurspublic Arbre(T val) { valeur = val; filsGauche = filsDroit = null;}public Arbre(T val, Arbre&lt;T&gt; g, Arbre&lt;T&gt; d) { valeur = val; filsGauche = g; filsDroit = d;}// Affichagepublic void afficherPrefixe() { System.out.print(valeur+\"\\t\"); if (existeFilsGauche()) filsGauche.afficherPrefixe(); if (existeFilsDroit()) filsDroit.afficherPrefixe();}public void afficherInfixe() { if (existeFilsGauche()) filsGauche.afficherInfixe(); System.out.print(valeur+\"\\t\"); if (existeFilsDroit())filsDroit.afficherInfixe();}public void afficherPostfixe() { if (existeFilsGauche()) filsGauche.afficherPostfixe(); if (existeFilsDroit())filsDroit.afficherPostfixe(); System.out.print(valeur+\"\\t\");}二叉排序树是指左子树小于结点小于右子树，而且结点值不重复。判断是否为二叉排序树：public boolean superieur(char x) {// vrai si x est supérieur à tous les éléments de l’arbre if (feuille()) return (x&gt;=valeur); else return (((this.existeFilsGauche())? (this.filsGauche).superieur(x):true) &amp; ((this.existeFilsDroit())? (this.filsDroit).superieur(x):true));} public boolean inferieur(char x) {//similaire a superieur ... }public boolean binrech() { if (feuille()) return true; else return( (existeFilsGauche()? (filsGauche.superieur(valeur) &amp;&amp; filsGauche.binrech()):true) &amp; (existeFilsDroit()? (filsDroit.inferieur(valeur) &amp;&amp; filsDroit.binrech()):true));} 5. 常用数据结构方法二维数组array[][]的定义和访问 数据结构 vector&lt;String&gt; ArrayList&lt;String&gt; LinkedList&lt;String&gt; HashMap&lt;String,int&gt; 添加 add(i, str) add(i,str) add(i,str) put(str,i) 查找 get(i) get(i) get(i) get(str) 索引 indexOf(str) indexOf(str) indexOf(str)   删除 remove(i)/remove(str) remove(i/str) remove(i/str) remove(i) 清除 clear() clear() clear()   查看大小 size() size() size() size() 迭代器 iterator() iterator() iterator()   变成数组   toArray() toArray()   hashmap还可以返回键值对entryset()，也可以判断是否含有key和value，containsKey(),containsValue()"
    } ,
  
    {
      "title"       : "RACE数据集相关文献",
      "category"    : "",
      "tags"        : "paper",
      "url"         : "./RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html",
      "date"        : "2021-11-30 00:00:00 +0800",
      "description" : "文献整理",
      "content"     : "目录 目录 文献整理 要求 搜集到相关文献标题和地址 第一篇 Title Author Abstract Introduction BERT distractor generation 1)BERT-based distractor generation(BDG) 2)Multi-task with Parallel MLM 3)Answer Negative Regularization Multiple Distractor Generation 1)Selecting Distractors by Entropy Maximization 2)BDG-EM Performance Evaluation 1)datasets 2)implementation details 3)compared methods 4)token score comparison 5)MCQ Model Accuracy Comparison 6）Parameter Study on γ Conclusion 我的看法 第二篇 Title Author Abstract Method 1)question generation 2)distractor generation 3)QA filtering Results 1)quantitative evaluation 2)question answering ability 3)human evaluation conclusion 第三篇 Title Author Abstract Framework Description 网络结构 1)Task Definition 2)Framework overview 3)Hierarchical encoder 4)static attention mechanism 5)encoding layer 6)matching layer 7)nomalization layer 8)distractor decoder 9)question-based initializer 10)dynamic hierarchical attention mechanism 11)training and inference experimental setting 实验设置 1)dataset 2)implementation details 3)baselines and ablations results and analysis 结果与分析 我的看法 第四篇 Title Author Abstract Proposed Framework 网络结构 1)notations and task definition 2)model overview 3)encoding article and question 4)Co-attention between article and question 5)Merging sentence representation 6)question initialization 7)hierarchical attention 8)semantic similarity loss Experimental Settings 1)dataset 2)baselines and evaluation metrics 3)implementation details Results and Analysis 结果与分析 我的看法 补充 RACE数据集简介 BLEU ROUGE 文献整理要求搜集到相关文献标题和地址 A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies Better Distractions: Transformer-based Distractor Generation and Multiple Choice Question Filtering Generating Distractors for Reading Comprehension Questions from Real Examinations Co-attention hierarchical network: Generating coherent long distractors for reading comprehension Automatic Distractor Generation for Multiple Choice Questions in Standard Tests Distractor Generation for Multiple Choice Questions Using Learning to Rank Knowledge-Driven Distractor Generation for Cloze-style Multiple Choice Questions第一篇TitleA BERT-based Distractor Generation Scheme with Multi-tasking andNegative Answer Training StrategiesAuthorHo-Lam Chung, Ying-Hong Chan, Yao-Chung FanAbstract现有的DG1局限在只能生成一个误导选项，我们需要生成多个误导选项，文章中提到他们团队用multi-tasking和negative answer training技巧来生成多个误导选项，模型结果达到了学界顶尖。IntroductionDG效果不好，文章提出了两个提升的空间： DG质量提升： BERT模型来提升误导选项质量 多个误导选项生成： 运用了覆盖的方法来选择distractor，而不是选择概率最高但是语义很相近的distractorBERT distractor generation1)BERT-based distractor generation(BDG)输入：段落P，答案A，问题Q，用C表示这三者concatenate后的结果。BDG模型是一个自回归模型，在预测阶段，每次输入C和上一次预测的词元，BDG迭代预测词元，直到预测出特殊词元[S]停止。下面这张图简单介绍了这个过程。网络结构简单介绍：h[M]表示bert输出的隐藏状态，隐藏状态再输入到一个全连接层中用来预测词元。2)Multi-task with Parallel MLMMLM全称masked language model，遮蔽语言模型,通过并行BDG和P-MLM来训练模型让模型有更好的效果。上图中左边的sequential MLM就是之前提到的BDG，BDG模型是一个词接一个词的预测，P-MLM是对所有的masked token进行预测，最后的损失函数是这两者相加2，公式如下：作者如此设计的思路是：BDG可能会忽略整体语义语义信息，但是会过拟合单个词预测。那么并行一个P-MLM可以防止过拟合。3)Answer Negative Regularization目前机器预测的distractor和answer有很高的相似度，下面一张表可以展示相似度。其中PM表示机器，Gold表示人工，作者将这类问题称为answer copying problem。为了解决这个问题，作者提出了answer negative loss来让机器更多的选择与answer不同的词来表示新的distractor，公式如下：可以看出BDG的loss替换成了AN的loss，每一项都减去了Answer negative loss。Multiple Distractor Generation1)Selecting Distractors by Entropy Maximization选择语义不同的distractor set。文章借鉴了MRC3的方法，让BDGmodel生成很多distractor组成 $\\hat{D}$ = {$\\hat{d}$1, $\\hat{d}$2, $\\hat{d}$3…}，然后找出最好的一组选项，一般情况下由三个误导选项和一个答案组成。选择的一句是最大化下面这个公式：2)BDG-EM我们可以通过不同的BDG模型来生成不同的误导选项最后组合，不同的模型区别是有没有answer negative/multi-task training，比如我们有这几个模型:$\\hat{D}$,$\\hat{D}$PM,$\\hat{D}$PM+AN，它们分别代表含PM4和含AN5Performance Evaluation1)datasetsRACE,沿用了Gao那篇论文的处理,后面也会梳理那篇论文2)implementation details tokenizer: wordpiece tokenizer framewordk:huggingface trainsformers optimizer:adamW(lr:5e-5) github_url: BDG3)compared methods比较了不同的distractor generation CO-Att：出自Zhou DS-Att: 出自Gao GPT:baseline BDG: 没有应用P-MLM和Answer negative BDGPM BDGAN+PM4)token score comparisonBLEU和ROUGE(L)两种判断指标copying problem的效果5)MCQ Model Accuracy Comparison与回答系统相结合，将生成好的选项（一个正确答案三个误导选项）放入MCQ answering model，下面是回答正确率的表格可以看出作者的模型选项的误导性还是很高的。6）Parameter Study on γ之前使用P-MLM并行训练时候有个权重参数γ，下表显示了不同γ值的影响，对于只有PM的模型来说，γ=6，对于既有AN和PM来说，γ=7Conclusion现存的DG可以分为cloze-style distractor generation和 reading comprehension distractor generation，前者主要是word filling，后者主要看重语义信息，基于两者的设计出了很多模型，目前来看还是考虑语义信息生成的误导选项更好。我的看法文章中的模型提到了三种技术，第一是bert预训练模型使用。第二是P-MLM的并行使用， 它的使用让模型可以考虑段落的语义信息，那么生成的误导选项是sentence-level而不是之前模型所使用的类似word-filling这种word-level。第三是Answer negative loss的使用，它的使用相当于让模型不要考虑与正确答案语义很接近的误导选项，因为目前大多数DG生成多个选项时语义与正确答案都非常接近，这与实际情况不符，同时也起不到误导的作用。 同时文章提出了生成多个误导选项时使用不同模型生成的误导选项拼在一起作为选项是一种比较好的解决方法，让一次性生成多个误导选型有了一定的可用性。文章的代码开源，可以去github上看训练细节和网络结构细节。第二篇TitleBetter Distractions: Transformer-based Distractor Generation and Multiple Choice Question FilteringAuthorJeroen Offerijns, Suzan Verberne, Tessa VerhoefAbstract运用GPT2模型生成三个误导选项，同时用BERT模型去回答这个问题，只挑选出回答正确的问题。相当于使用了QA作为一个过滤器(QA filtering)。Method作者使用了Question generation model, distractor generation model和question answer filter，作者将从这三方面介绍，下图是大概的流程图。1)question generation 预训练模型：GPT-2 数据集：English SQuAD tokenizer：Byte-Pair-Encoding(BPE) tokenizer optimizer:Adam 下图展示了QG的输入，黑框内被tokenizer标记为特殊词元2)distractor generation 预训练模型：GPT-2 数据集：RACE tokenizer:BPE6 使用了repetition penalty技术，保证了尽量不会生成相似的text，并且过滤到那些不好的生成（比如生成了空字符串） 输入：经典的C(context)，A(answer),Q(question)，下图展示了输入格式3)QA filtering 预训练模型：DistilBERT 网络结构：CQA7输入到distilbert，再连接一个dropout，全连接层和softmax，最后输出一个答案，具体结构如下图Results1)quantitative evaluation下表中展示了和上一篇论文类似的指标,与现有的模型进行了比较：SEQ2SEQ,HSA8和CHN9。可以看出BLEU明显要比之前模型要好，但是ROUGE没有之前的高。2)question answering ability用GPT-2模型生成误导选项再输入到QAmodel中，具体结果见下图。3)human evaluation人工评估，从两方面评估distractor生成的好坏： Is the question well-formed and can you understand the meaning? If the question is at least understandable, does the answer make sense in relation to the question?评估过程中，使用了155个没有经过QA筛选和155经过QA筛选的，了解一下QA过滤模型的效果。整体来说QA过滤器还是有一点效果，具体结果如下：conclusion我认为作者使用的DG模型主要有两大特色，一个是使用了GPT2预训练模型，目前使用基于transformer的模型已经成为主流。第二个是使用了QA过滤器来筛选掉回答错误的，有一定提升但不显著。第三篇TitleGenerating Distractors for Reading Comprehension Questions from Real ExaminationsAuthorYifan Gao, Lidong Bing, Piji Li,Irwin King, Michael R. LyuAbstract上面两篇文献都有提到这篇文章。作者使用了Hierarchical encoder-decoder framework with static and dynamic attention mechanisms来生成有语义信息的误导选项。使用了编码器-解码器结构网络和静态和动态注意力机制。Framework Description 网络结构1)Task Definition输入：文章，问题和答案。P代表文章，s1,s2,s3…表示不同的句子，q和a分别表示问题和答案，那么我们的任务是生成误导选项$\\overline{d}$。2)Framework overview网络结构如下图所示，下面将从各个组成部分分别介绍：3)Hierarchical encoder word embedding:词嵌入，将每个句子si中的每个词元变成词向量(wi,1,wi,2,wi,3…) word encoder:将句子si的词向量(wi,1,wi,2,wi,3…)作为输入，用双向LSTM作为编码器，获得word-level representation hi,je sentence encoder:将word encoder中每个句子正向LSTM的最后一个隐藏状态和反向LSTM的最开始的隐藏状态作为输入到另一个双向LSTM中获得sentence-level representation(u1,u2,u3…)4)static attention mechanism目的：生成的误导选项必须和问题Q语义相关，但是和答案A必须语义不相关。我们从(s1,s2,s3…)学习到句子的权重分布(γ1,γ2,γ3…)，然后将问题q和答案a作为query。5)encoding layer我们希望把问题q，答案a和句子s都变成一样的长度的向量表示，也就是上图中紫色虚线部分。对于q和a，我们用两个独立的双向LSTM来获得(a1,a2…ak)和(q1,q2…ql)，然后用平均池化层平均一下：对于句子s，我们不用u而用h：6)matching layer目的：加重与问题q有关的句子，减轻与答案a有关的句子。oi表示不同句子的importance score7)nomalization layer目的：有些问题q和一两个句子有关，而有些问题q和很多句子有关，比如summarizing，下面的τ(temperature)就是这个作用作者介绍static attention mechanism用了很大篇幅8)distractor decoder解码器使用的也是LSTM，但是并没有使用编码器的最后一个隐藏状态作为初始状态，而是定义了一个question-based initializer来让生成的误导选项语法和问题q一致9)question-based initializer定义了一个question LSTM来编码问题q，使用最后一层的cell state和hidden state作为decoder初始状态，同时输入qlast，表示问题q的最后一个词元。10)dynamic hierarchical attention mechanism常规的注意力机制将一篇文章作为长句子，然后decoder的每一个时间步都与encoder中所有的hidden state进行比较，但是这种方法并不适合目前的模型。原因：首先LSTM不能处理这么长的输入，其次，一些问题只与部分句子有关。目的：每个decoder时间步只关注重要句子，作者将这种注意力机制称为动态注意力机制，因为不同的时间步，word-level和sentence-level 注意力分布都不同。每一个时间步的输入是词元dt-1和上一个隐藏状态ht-1α和β分别表示word-level,sentence-level权重，最后使用之前静态注意力机制获得的γ来调节α和β获得上下文变量ct获得attention vector $\\tilde{h}$11)training and inference损失函数：生成多个误导选项的方法是束搜索，但是生成的误导选项很相似，作者做了相应的处理方法，但我觉得效果还是很差experimental setting 实验设置1)datasetRACE数据集，作者做了相应的处理，去掉了很多不合理的和语义不相关的，作者的处理标准是：对于误导选项中的词元，如果它们在文章中出现的次数小于5次，那么将被保留，同时去掉了那些需要在句子中间和句子开始填空的问题。下表展示了处理后的数据集的一些信息：2)implementation details词表：保留了频率最高的50k个词元，同时使用GloVe作为词嵌入预训练模型。其他的细节都可以在文章中看见，这里不一一列出了，主要是超参数的设置。3)baselines and ablations与HRED10和seq2seq比较results and analysis 结果与分析人工评估：大致过程是这样：四个误导选项，分别来自seq2seq，HRED，作者的模型和原本的误导选项，让英语能力很好的人来选择最适合的选项，得出的结果可以发现，作者的模型生成的误导选项拥有最好的误导效果。下图直观展示了static attention distribution：我的看法这篇文章应该是第一个提出用处理后的RACE数据集来处理MCQ问题，处理后的RACE数据集在后面也有很多文献用到，这篇文章使用了seq2seq网络结构同时使用了静态和动态注意力机制，对于网络结构和注意力机制的解释非常完全和详细，虽然这篇文章的效果放到现在来看可能不是最好了，但是它提出来的评估标准可能会成为一个通用的标准。它的数据集和训练代码在github上也完全开源。第四篇TitleCo-attention hierarchical network: Generating coherent long distractors for reading comprehensionAuthorXiaorui Zhou, Senlin Luo, Yunfang WuAbstract这篇文献是针对上一篇Gao的文章(seq2seq)所作的改进。本篇文章提出了Gao的模型的两个问题：1.没有建立文章和问题的关系，他的解决方法是使用co-attention enhanced hierarchical architecture来捕获文章和问题之间的关系，让解码器生成更有关联的误导选项。2.没有加重整篇文章和误导选项的关系。作者的解决思路是添加一个额外的语义相关性损失函数，让生成的误导选项与整篇文章更有关联。Proposed Framework 网络结构1)notations and task definitionarticle T=(s1,s2…sk)，一篇文章有k个句子s，同时每个句子都有不同的长度l，si=(wi,1,wi,2…wi,l)，每个文章有m个问题和z个误导选项，Q=(q1,q2…qm),D=(d1,d2…dz),我们的任务是根据输入的T和Q生成D2)model overview整体结构如下图所示，下面将从各个部分分别介绍：3)encoding article and question文章和问题的编码器结构 hierarchical article encoder双向LSTM，和上一篇结构很像，很多部分我就简单列个式子。每一句最后的词元来表示整个句子sentence-level encoder：同样，用最后一个句子来表示整篇文章用H*来作为sentence-level representation of article,我们有H:t*=hts这样，通过使用两个双向LSTM获得word-level encoding和sentence-level encoding question encoder用U*来作为word-level representations of question, 我们有U:t*=htq4)Co-attention between article and questionCo-attention mechanism就是使用了两个方向的注意力机制，有从article到question的，也有question到article的。用一个“相似”矩阵S表示H和U的关系：Si,j就表示第i个句子和第j个问题词元的相似性我们可以获得两个特殊的矩阵SQ和ST article-to-question attention$\\tilde{U}$:j = $\\sum$ Si,jQU:,i question-to-article attention最后，将问题的词级表示H，两个方向的注意力结果$\\tilde{U}$和$\\tilde{H}$结合一下获得G5)Merging sentence representationZ表示final representation of sentence-level hidden states6)question initialization接下来就进入decoder环节，这里的question initialization和上篇文献处理方法相同7)hierarchical attention不同时间步有不同的句子相关，和上篇文献的处理方法动态注意力机制相同。8)semantic similarity loss目的：获得文章和误导选项的关系。还记得之前定义的eT吗，它表示整篇文章，那么我们通过下面的公式可以获得distractor representation:其中SM是decoder最后一个隐藏状态，那么我们通过cos计算相似关系，那么最终的损失函数Experimental Settings1)dataset使用了上篇文献处理过的RACE数据集。2)baselines and evaluation metrics与seq2seq，HRED，HCP11，HSA12比较。3)implementation details网络超参数设置技巧，不展开了Results and Analysis 结果与分析介绍一下上面这张表，这张表是人工评估的结果，从三个维度分析，分别是fluency,coherence,distracting ability。可以看出作者的模型并不是在所有维度都是最好的。下图是案例分析：我的看法这篇文献是基于上一篇文献的方法进行了两个改进：1.关联了整篇文章和问题，解决方法是使用了Co-attention mechanism。2.让distractor和article语义相关，方法是定义了相关性loss。补充RACE数据集简介RACE数据集是一个来源于中学考试题目的大规模阅读理解数据集，包含了大约 28000 个文章以及近 100000 个问题。它的形式类似于英语考试中的阅读理解（选择题），给定一篇文章，通过阅读并理解文章（Passage），针对提出的问题（Question）从四个选项中选择正确的答案（Answers）。BLEUBLEU是一个评价指标，最开始用于机器翻译任务，定义如下它的总体思想就是准确率，假如给定标准译文reference，神经网络生成的句子是candidate，句子长度为n，candidate中有m个单词出现在reference，m/n就是bleu的1-gram的计算公式。BLEU还有许多变种。根据n-gram可以划分成多种评价指标，常见的指标有BLEU-1、BLEU-2、BLEU-3、BLEU-4四种，其中n-gram指的是连续的单词个数为n。ROUGERouge(Recall-Oriented Understudy for Gisting Evaluation)，是评估自动文摘以及机器翻译的一组指标。它通过将自动生成的摘要或翻译与一组参考摘要（通常是人工生成的）进行比较计算，得出相应的分值，以衡量自动生成的摘要或翻译与参考摘要之间的“相似度”。它的定义如下：文献中使用的ROUGE-L是一种变种，L即是LCS(longest common subsequence，最长公共子序列)的首字母，因为Rouge-L使用了最长公共子序列。Rouge-L计算方式如下图：其中LCS(X, Y)是X和Y的最长公共子序列的长度,m、n分别表示参考摘要和自动摘要的长度（一般就是所含词的个数），Rlcs,Plcs分别表示召回率和准确率。最后的Flcs即是我们所说的Rouge-L。 distractor generation 误导选项生成，简称DG &#8617; 当我们test时，只需要Sequential MLM decoder来预测。 &#8617; multi-choice reading comprehension (MRC) model &#8617; P-MLM &#8617; Answer negative &#8617; Byte-Pair-Encoding &#8617; context，question，answer &#8617; hierarchical encoder-decoder model with static attention &#8617; hierarchical model enhanced with co-attention &#8617; hierarchical encoder-decoder &#8617; 相当于HRED+copy,是基于HRED的网络结构 &#8617; 就是上篇文献的网络 &#8617;"
    } ,
  
    {
      "title"       : "课程总结",
      "category"    : "",
      "tags"        : "school",
      "url"         : "./%E5%A4%A7%E5%9B%9B%E4%B8%8A%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93.html",
      "date"        : "2021-11-28 00:00:00 +0800",
      "description" : "记录课程和课程笔记",
      "content"     : "概率统计 概率统计 - 简介 - 内容总览 流体力学 - 简介 - 内容总览 - A4纸 - 报告 电磁辐射波 - 简介 - 内容总览 传感器 - 简介 - 内容总览 结构力学 - 简介 - 内容总览 - A4纸 项目管理 - 简介 - 内容总览 工程热力学 - 简介 - 报告Pre - 内容总览 Presse - 简介 - 内容总览 Audiovisual - 简介 - 内容总览 软件方法 - 简介 - 内容总览 机器人 - 简介 - 内容总览 - 报告简介 授课老师：牛薇 授课材料：一份法语讲义，一份习题集（10个EX），上课用的PPT B站有录播，up主：却道成归 笔记记在侧边栏为大四上A的笔记本最前面内容总览一半时间概率一半时间统计 概率 先从之前学的概率空间讲起，介绍了概率分布（离散or连续），密度函数，期望方差，收敛性。 估计，比如说用平均值估计期望，用频率估计概率等等 估计又分为点估计和区间估计，点估计中介绍了似然函数以及最大似然法来找估计量 统计 主要介绍了几种检验方法来检验分布、估计量选择的好坏 包括了参数检验，分布检验，比较检验等等 A4纸流体力学简介 授课老师：方乐 授课材料：PPT，TD都是6个，分别对应六大章 B站有录播 笔记在侧边栏为大四上A的中后部分和大四上B前面内容总览第一章主要讲了流体的概念和动力学的公式。第二章从能量角度出发，介绍了NS方程（斯托克斯方程），和伯努利原理（压强和流速的关系）。第三章介绍了雷诺数，无量纲分析，雷诺数大的是湍流，雷诺数小的是层流。第四章介绍了边界层，第五章介绍了湍流，系统平均。第六章介绍了涡量。A4纸报告结课之前需要我们写一个报告，什么形式的都可以，我觉得这种方式挺好的，自由发挥，我做的实验，用牛奶和墨水还原了卡门涡街。电磁辐射波简介 授课老师: José Penuelas(负责前几章教学), Bertrand Vilquin(负责后几章教学), 孙鸣捷老师(负责TD) 授课形式：线上讲解原理，线下TD 授课材料：PPT，讲义，TD B站有录播 有笔记，侧边栏叫做电磁学(大四上) 考试闭卷，所以没有A4纸内容总览首先回顾了之前学的波动物理和电磁学，电磁辐射，顾名思义是要将辐射，讲了波导，腔和光电效应，能级跃迁等等传感器简介 授课老师：徐平 授课形式：线下授课，做实验 授课材料：大学生MOOC 没有考试，没有笔记内容总览讲解了传感器的基本原理，构造和常见传感器，每节课都需要在MOOC上做题，也有安排答辩，我和蔡卓江、宋正浩、刘亚林、马卫一一组讲解了机器狗。做实验是指去214玩小车，上面有不少传感器，也有大疆的线上模拟器，还是挺不错的一次动手实验。结构力学简介 授课老师：黄行蓉，Jean-Piere Lainé 授课形式：J-P录制ppt，黄老师线下授课 授课材料：讲义，PPT，TD B站有录播 笔记：侧边栏结构力学，还有最后第八章记在大四上C前面内容总览结构力学分为了两大部分，弹性力学和材料力学。在弹性力学部分，首先介绍了应力和应力张量的概念，张量可以写成3*3矩阵形式，其中对角线上的元素被称作正应力。第二章介绍了应变，首先介绍了很多种张量，F、H、C、E，然后介绍了形变张量ε。第三章介绍了本构方程（应力应变关系方程）。第四章介绍了能量，包括最小势能和最大余能等等。第二部分是材料力学，主题内容和弹性力学类似，但是引进了力螺旋的概念，这个概念在中国授课好像是没有的，它描述了合力和力矩。第一张介绍了内力，在材料力学部分我们主要研究梁这个结构，它包括了中轴线和截面，这部分内容和之前学的理论力学很相似。第二章介绍了应力，可以用内力表示应力，用一些惯性矩、艾力函数连接。第三章介绍了应变和本构方程，第四章介绍了能量部分，主要是三大定理：théorème de ménabréa;théorème de maxwell-betti;théorème de castigliano。A4纸项目管理简介 授课老师：张敏 授课形式：线下授课 授课材料：书(没买)，PPT B站有录播 开卷考试，没有笔记内容总览是上学期经济管理的一部分展开讲，讲了什么是项目，项目管理系统，基于关键路径的项目管理，项目管理的决策工程热力学简介 授课老师：Guillaume Merle 授课形式：看课本，付小尧定期答疑 授课材料：课本、PPT 闭卷考试，不能带A4纸，有三次小测 纯英文授课报告Pre 报告题目：SABRE发动机效率分析内容总览一共学习了课本上第1、2、3、4、5、6、7、9、10、11、12章内容，其中第一章介绍热力学基本单位和概念，第二章介绍能量转移的形式是功和热，顺便引出热力学第一定律，第三章介绍纯物质的相和相变，第四章介绍封闭系统的能量变化，即质量不变体积可变的系统，第五章介绍开放系统的质量变化，即体积可变质量不变，第六章介绍热机的效率基本概念，第七章介绍熵的概念以及热力学第二定律，到这为止都是之前热力学学过的东西，第九章介绍了内燃机的工作原理，包括了很多个不同的循环，比如otto、diesel、brayton循环。第十章介绍了热电机的原理和ranking循环。第十一章介绍了冰箱和热泵的基本原理。第十二章是用到的数学表达式的推理。Presse简介 法语课程 授课老师:Vanessa 授课形式：线下授课 授课材料：讲义 闭卷考试 期间写过几次PE内容总览 Règle de la classe Vingt ans qui ont déjà tout changé: 11 septembre(effodrement des Tours Jumelles), l’incendie de la cathédrale Notre-Dame de Paris, les confinements en France et dans le monde, la mort de Michael Jackson, l’accident nucléaire de Fukushima Le 11 Septembre apparaît comme la matrice du XXIe siècle les personnalités les plus marquantes des 20 dernières années: Barack Obama, Donald Trump, Oussama Ben Laden, Emmanuel Macron, Angela Merkel… François Hollande les chanteurs: Johnny Hallydat, Céline Dion, Stromae le confinement en France: haltères, machine à pain, graines de tomates… des solutions pour prospérer et progresser: Cité flottante, Télétravail, Agriculture spatiale, le train intelligent la voiture autonome ne tient pas encore la route: sensible, risque le tourisme spatial, bonne ou mauvaise idée: huit, Elon Musk l’agence spatiale européenne recrute une nouvelle promotion, Feriez-vous un bon astronaute les Français et Internet la 5G vous inquiète-t-elle? la 5G, amie ou ennemie? Sept innovations contre le handicap: le gant qui traduit la langue des signes, les lunettes qui parlent aux malvoyants Jeux vidéo Bientôt de l’e-sport aux Jeux OlympiquesAudiovisual简介 法语课程 授课老师：Fabien 授课形式：瞩目线上 授课材料：讲义 闭卷考试内容总览 Séance 1 - les étudiants français sous la Covid-19 Séance 2 - Afghanistan: partir ou rester Séance 3 - Réalité Virtuelle: une méthode d’emphaie Séance 4 - Utiliser des produits ménagers bio Séance 5 - Pandémies et environnement Séance 6 - Les transports du futur Séance 7 - Le tourisme spatial Séance 8 - Robots de guerre Séance 9 - Le bilan de la COP 26 Séance 10 - Révolution du vin Séance 11 – Rouler au whisky Séance 12 — Un champion de la pâtisserie软件方法简介 授课老师：于雷， Olivier Roux 授课形式：OR线上授课，于雷助教，有报告和TP 授课材料：PPT，TP 闭卷考试，手写代码内容总览在另一份博客里有具体整理，主要是介绍面向对象的开发方法，同时用java来展示如何进行面向对象的编程机器人简介 授课老师：严亮 授课形式：线下授课 授课材料：PPT 无考试，有大作业(报告) 英文授课内容总览只有四节课，介绍了自由度，动力学，逆动力学，速度和力报告用solidworks设计一个三自由度的机械臂，并附上运动学分析"
    } ,
  
    {
      "title"       : "MCQ文献阅读",
      "category"    : "",
      "tags"        : "paper",
      "url"         : "./%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html",
      "date"        : "2021-11-25 00:00:00 +0800",
      "description" : "整理最新MCQ研究进展和问题",
      "content"     : "整理集合多选题自动生成： 定义输入一篇文章，从这篇文章生成一系列多选题 模型结构待补充 工作流程六大步： 输入文章预处理 选择句子 从句子中选择关键字 生成疑问句 生成误导选项 后期处理 主流的研究关注点与现存的问题 目前大多数处理文本都不考虑公式、图片和图标等等信息，文本预处理只提取文本信息，今后MCQG的研究需要关注处理嵌入在文本中的信息的能力。 现有的MCQ生成方法侧重于从单个句子生成问题，然而，文本可能通过多个句子来生成句子，所以今后的研究应该侧重于从多个句子中生成问题。 关键字的选择取决于下游任务或者应用领域，早期的关键字选择依赖于基本的统计和句法信息。最新的研究趋势是使用机器学习或者语义信息作为选择的标准。 误导选项的选择同样与应用领域有关，目前的MCQ生成系统中使用的都是简单的误导选项生成，但在实际情况中，误导选项可以是非常多种类的，可以是不同的命名体，数字大小，多个单词的误导选项等等。作者认为文本的深层语义分析或使用基于神经嵌入的方法可能是复杂误导答案生成的一个可能的方向。 一篇关于MCQG的综述TitleAutomatic Multiple Choice Question Generation From Text: A SurveyAuthorDhawaleswar Rao CH and Sujan Kumar SahaAbstractMCQ1工作20年前已经开始研究，综述将概括目前常见的多选题自动生成的工作流和评估系统。1.introduction介绍了MCQ的重要性，是评估知识学习的工具之一，优点是耗时短但是人工出题需要很多时间，所以通过一段文章自动生成问题是人们关注的重点。2.multiple choice question介绍了MCQ和MCQ的基本结构，由题干，正确答案和误导答案组成，同时具体介绍了MCQ的优缺点。 优点 缺点 快速评估，耗时短 涵盖的知识面很小 可以实现机器阅卷 答案有猜测出来的可能 3.research motivation and objectivesMCQ的研究动机主要来源于人工出题繁琐且耗时间。4.review methodology作者从大量paper中挑选了86篇文章做来做MCQ的综述。介绍了一下检索文章的步骤同时介绍了不同的QG的方法分布5.discussion on the appproaches for MCQ generation自动生成MCQ和手动生成MCQ的步骤大致相同： Pre-processing of Input Text 预处理输入文章 Sentence Selection 句子选择 Key Selection 选择答案信息 Question Formation 问题生成 Distractor Generation 错误答案生成 Post-Processing 后期处理下面从这六个阶段分别分析：1.Pre-processing of Input Text 用到的技巧（每一个技巧都有对应的文献）： text normalization: 将文本格式变成我们需要的格式，不同的应用领域需要不同的格式化方法 structure analysis：给出段落结构 sentence simplification：把长句子变成短句子 lexical analysis：词汇分析，把文本分隔成单词，符号和数字。同时需要进行词根提取 statistical analysis：统计分析，包括不同的统计手段，比如词频，n元词频等 syntactic analysis：语法分析，包括POS2，NER3，syntactic parsing4。 coreference resolution: 代词通常不作为疑问句的主语，代词解析就是将代词映射到相应的名词。 word sense disambiguation：消除句子中单词的歧义 作者指出，对于text的预处理主要取决于输入文本的性质和下游任务的需求，比如说从web端爬下的文本会包含很多噪音和没必要的内容，那么文本清理就是必须的，再比如wikipedia文档作为输入时常常是一个长句子，需要把长句子简化变成短句子。目前大多数处理文本都不考虑公式、图片和图标等等信息，文本预处理只提取文本信息，今后MCQG的研究需要关注处理嵌入在文本中的信息的能力。2.sentence selection在对输入文本进行处理之后需要挑选出包含questionable fact的句子，我的理解是那些包含事实的句子。 一些技巧： sentence length：给出句子中单词的数量，一般来说，很短的句子不能包含足够的信息来生成问题，同样来说，很长的句子通常包含多个事实和关系，这会给生成问题带来困难。 occurrence of a particular word：查找特殊词汇 parts-of-speech information: 根据一个句子中出现词汇的词性挑选句子，比如说根据名词-形容词对的出来情况来选择句子。 parse information: 根据句子结构挑选，比如主谓宾 semantic information: 文本中包含的语义信息也作为选择句子的标准 machine learning: 使用机器学习算法，比如支持向量机、神经网络等 summarization：基于摘要的方法来选择句子 句子的选择同样需要根据任务的不同来选择。现有的MCQ生成方法侧重于从单个句子生成问题，然而，文本可能通过多个句子来生成句子，所以今后的研究应该侧重于从多个句子中生成问题。3.key Selection选择好句子后，我们从中挑选出关键词。我们不能将一个句子的全部词汇都作为关键词，因此，关键字的选择是确定句子中要被删除的单词（或者短语、n元词元） 一些技巧： frequency count: 统计单词的出现频率作为选择标准 part-of-speech and parse information: 在某些特定的应用领域中，一个特定的词性或者语法可以成为一个潜在的关键字。比如一些研究用动词作为关键字，一些研究用介词作为关键字。 semantic information: 语义信息。 pattern matching：模式匹配，从结构相似的句子中提取出常见的句型，这样有助于句子解析结构来寻找关键字 machine learning：利用机器学习来生成动词或者部分习语或者副词来作为关键字。 关键字的选择同样取决于下游任务或者应用领域，早期的关键字选择依赖于基本的统计和句法信息。最新的研究趋势是使用机器学习或者语义信息作为选择的标准。4.question formation选完关键字后，我们下一个任务就是把陈述句转化为疑问句。 一些技巧： by appropriate wh-word selection: 根据句子的语法结构和关键字来确定使用哪个wh subject-verb-object and their relationship：通过主谓宾结构来生成疑问句 knowledge in sentence：根据句子所包含的知识类型来确定转换规则，例如这个句子是概念，定义，示例等等。 dependency based patterns: 根据句子的依赖关系树来确定主要动词和将被问及的问题部分 syntactic transformation：通过句法结构来生成问题。 discourse connectives：通过不同的关系来转化，比如时间关系，空间关系。 semantic information based：基于语义来转化。 question generation，问题生成也是一个热门的研究方向，该领域的目标是根据输入文本生成问题。在MCQ中，我们首先选取一个句子，然后选择关键字，最后根据关键字转换成问句形式。5.distractor generation错误选项在MCQ中扮演重要的地位，如果错误选项不能很好的迷惑学生，那么这道多选题出的并不好。 一些技巧： parts-of-speech information：错误选项和关键字在语义上很接近，所以他们的词性也要一样 frequency：频率也是一个重要的指标，关键字和错误选项的出现频率应该相近。 wordnet：wordnet是一个词汇数据库，它将单词分组为同义词集并记录这些词集成员的关系。因此可以用wordnet来生成错误选项。 domain ontology：一些文献用web ontology language来寻找错误答案。 distributional hypothesis: 分布假设认为相似的词出现在相似的语境中，那么我们可以基于分布相似度来寻找错误答案。 semantic analysis：基于语义。 错误选项的选择同样与应用领域有关，目前的MCQ生成系统中使用的都是简单的错误选项生成，但在实际情况中，错误选项可以是非常多种类的，可以是不同的命名体，数字大小，多个单词的错误选项等等。作者认为文本的深层语义分析或使用基于神经嵌入的方法可能是复杂错误答案生成的一个可能的方向。6.post-processing后期处理是提高生成MCQ质量的阶段，系统生成的MCQ可能存在各种各样的错误。可能是标点符号错误，疑问词不恰当，问句过长等等，后期处理希望消除这些问题。 一些技巧： question post-editing：有些文献的方法是手动更改， 首先对于问题执行分类，是小问题就更正拼写和标点，如果是大问题就对题干进行重新措辞和替换等等。 question filtering：有些文献设计了一个过滤器来拒绝不对的问题，有的过滤器主要判断错误选项的质量，有的过滤器基于项目信息来过滤。 question ranking：对问题进行排名。 6.MCQ system evalutaion评估MCQ生成好坏，目前大多数系统采用人工评估的办法。由于MCQ生成包含了很多步骤，那么就产生了不同的度量标准。 evaluation of the stem and key: 目前还没有标准的公共数据集来评估MCQ，所以一般都是开发人员创建测试数据，下面有一张图展示了MCQ系统的评估过程，从表中可以看出，并没有一个标准的性能度量标准，开发人员采用了各种指标和参数。我们只能比较基于同一套评价体系下的MCQ。 evaluation of the distractors： 同样的，错误答案的评估也没有标准的数据集和评估指标。在许多应用领域中，MCQ有大量的干扰因素，所以一个标准的数据集可能无法容纳所有。所以目前还是有相关专家来评估。 7.conclusion总结了工作流程中的六个阶段，总结了目前的挑战和今后的研究方向，以及评价标准未确立等等。MCQ领域还有很多地方值得深入研究。 multiple choice question，多选题。 &#8617; part of speech 词性分析 &#8617; 命名实体识别 &#8617; 句子结构分析 &#8617;"
    } ,
  
    {
      "title"       : "练习Markdown",
      "category"    : "",
      "tags"        : "daily",
      "url"         : "./%E7%BB%83%E4%B9%A0Markdown.html",
      "date"        : "2021-11-24 00:00:00 +0800",
      "description" : "练习使用",
      "content"     : "1.标题一级标题二级标题三级标题一共有6级标题2.段落及格式用两个空格加回车表示换行当然也可以直接空一行出来表示换行1)各种文字表示斜体用两个或者两个_把需要斜体的文字围起来比如：*斜体斜体粗体用两个或者两个__把需要粗体的文字围起来比如:**粗体粗体粗斜体用两个或者两个___把需要粗体的文字围起来比如:**粗体*粗体2)分隔线你可以在一行中用三个以上的星号、减号、底线来建立一个分隔线，行内不能有其他东西。你也可以在星号或是减号中间插入空格。下面每种写法都可以建立分隔线：***3)删除线如果段落上的文字要添加删除线，只需要在文字的两端加上两个波浪线即可比如：~~哈哈哈哈4)下划线下划线可以通过HTML的&lt;u&gt;标签来实现比如：下划线5)脚注脚注是对文本的补充说明 创建脚注格式类似这样 1。脚注链接与脚注不能紧挨在一起。注脚默认在最后3.列表1)无序列表无序列表使用星号(*)、加号(+)或是减号(-)作为列表标记，这些标记后面要添加一个空格，然后再填写内容。比如： 第一项 第二项 第三项 第一项 第二项 第三项 第一项 第二项 第三项2)有序列表有序列表使用数字并加上 . 号来表示，如： 第一项 第二项 第三项3)列表嵌套列表嵌套只需在子列表中的选项前面添加四个空格即可。比如： 第一项： 第一项嵌套的第一个元素 第一项嵌套的第二个元素 第二项： 第二项嵌套的第一个元素 第二项嵌套的第二个元素 3.区块1)区块引用Markdown 区块引用是在段落开头使用 &gt; 符号 ，然后后面紧跟一个空格符号：比如： 区块引用2)区块嵌套另外区块是可以嵌套的，一个 &gt; 符号是最外层，两个 &gt; 符号是第一层嵌套，以此类推： 最外层 第一层嵌套 3)区块中使用列表比如： 区块中使用列表 第一项 第二项 第一项 第二项 第三项 4)列表中使用区块如果要在列表项目内放进区块，那么就需要在 &gt; 前添加四个空格的缩进。列表中使用区块实例如下： 第一项 菜鸟教程学的不仅是技术更是梦想 第二项4.使用代码1)代码如果是段落上的一个函数或片段的代码可以用反引号把它包起来(`)，例如：printf()函数2)指定一种语言可以用```包裹一段代码，并指定一种语言（也可以不指定）：def f(): qhr=qhr return qhr5.使用链接1)链接使用方法[链接名称](链接地址)或者比如：这是一个链接 菜鸟教程2)直接使用链接地址用&lt;&gt;把链接括起来。比如:http://www.runoob.com6.图片1)使用图片图片的语法格式：![alt 属性文本](图片地址 “可选标题”)2)链接图片大概长这样：&lt;img src=”http://static.runoob.com/images/runoob-logo.png” width=”50%”&gt;结果：7.表格1)格式Markdown 制作表格使用 | 来分隔不同的单元格，使用 - 来分隔表头和其他行。比如： 表头 表头 单元格 单元格 单元格 单元格 2)对齐方法在---前面加上:表示左对齐，在后面加上:表示右对齐，在两端加上:表示居中 左对齐 右对齐 居中对齐 单元格 单元格 单元格 单元格 单元格 单元格 8.高级技巧1)插入数学公式当你需要在编辑器中插入数学公式时，可以使用两个美元符 $$ 包裹 TeX 或 LaTeX 格式的数学公式来实现。提交后，问答和文章页会根据需要加载 Mathjax 对数学公式进行渲染。比如：$$\\mathbf{V}_1 \\times \\mathbf{V}_2 = \\begin{vmatrix} \\mathbf{i} &amp; \\mathbf{j} &amp; \\mathbf{k} \\frac{\\partial X}{\\partial u} &amp; \\frac{\\partial Y}{\\partial u} &amp; 0 \\frac{\\partial X}{\\partial v} &amp; \\frac{\\partial Y}{\\partial v} &amp; 0 \\end{vmatrix}${$tep1}{\\style{visibility:hidden}{(x+1)(x+1)}}$$输出结果为：\\[\\mathbf{V}_1 \\times \\mathbf{V}_2 = \\begin{vmatrix} \\mathbf{i} &amp; \\mathbf{j} &amp; \\mathbf{k} \\\\\\frac{\\partial X}{\\partial u} &amp; \\frac{\\partial Y}{\\partial u} &amp; 0 \\\\\\frac{\\partial X}{\\partial v} &amp; \\frac{\\partial Y}{\\partial v} &amp; 0 \\\\\\end{vmatrix}${$tep1}{\\style{visibility:hidden}{(x+1)(x+1)}}\\]\\[\\sum_{i=0}N\\int_{a}{b}g(t,i)\\text{d}t\\]还没有整明白，用到的时候在看未完待续 我是脚注脚注脚注注脚 &#8617;"
    } ,
  
    {
      "title"       : "空天报国,敢为人先",
      "category"    : "",
      "tags"        : "daily",
      "url"         : "./%E7%A9%BA%E5%A4%A9%E6%8A%A5%E5%9B%BD%E6%95%A2%E4%B8%BA%E4%BA%BA%E5%85%88.html",
      "date"        : "2021-11-18 00:00:00 +0800",
      "description" : "听党课有感而发",
      "content"     : ""
    } ,
  
    {
      "title"       : "Who owns the copyright for an AI generated creative work?",
      "category"    : "opinion",
      "tags"        : "copyright, creativity, neural networks, machine learning, artificial intelligence",
      "url"         : "./AI-and-intellectual-property.html",
      "date"        : "2021-04-20 00:00:00 +0800",
      "description" : "As neural networks are used more and more in the creative process, text, images and even music are now created by AI, but who owns the copyright for those works?",
      "content"     : "Recently I was reading an article about a cool project that intends to have a neural network create songs of the late club of the 27 (artists that have tragically died at age 27 or near, and in the height of their respective careers), artists such as Amy Winehouse, Jimmy Hendrix, Curt Cobain and Jim Morrison.The project was created by Over the Bridge, an organization dedicated to increase awareness on mental health and substance abuse in the music industry, trying to denormalize and remove the glamour around such illnesses within the music community.They are using Google’s Magenta, which is a neural network that precisely was conceived to explore the role of machine learning within the creative process. Magenta has been used to create a brand new “Beatles” song or even there was a band that used it to write a full album in 2019.So, while reading the article, my immediate thought was: who owns the copyright of these new songs?Think about it, imagine one of this new songs becomes a massive hit with millions of youtube views and spotify streams, who can claim the royalties generated?At first it seems quite simple, Over the Bridge should be the ones reaping the benefits, since they are the ones who had the idea, gathered the data and then fed the neural network to get the “work of art”. But in a second thought, didn’t the original artists provide the basis for the work the neural network generated? shouldn’t their state get credit? what about Google whose tool was used, should they get credit too?Neural networks have been also used to create poetry, paintings and to write news articles, but how do they do it? A computer program developed for machine learning purposes is an algorithm that “learns” from data to make future decisions. When applied to art, music and literary works, machine learning algorithms are actually learning from some input data to generate a new piece of work, making independent decisions throughout the process to determine what the new work looks like. An important feature of this is that while programmers can set the parameters, the work is actually generated by the neural network itself, in a process akin to the thought processes of humans.Now, creative works qualify for copyright protection if they are original, with most definitions of originality requiring a human author. Most jurisdictions, including Spain and Germany, specifically state that only works created by a human can be protected by copyright. In the United States, for example, the Copyright Office has declared that it will “register an original work of authorship, provided that the work was created by a human being.”So as we currently stand, a human author is required to grant a copyright, which makes sense, there is no point of having a neural network be the beneficiary of royalties of a creative work (no bank would open an account for them anyways, lol).I think amendments have to be made to the law to ensure that the person who undertook all the arrangements necessary for the work to be created by the neural network gets the credit but also we need to modify copyright law to ensure the original authors of the body of work used as data input to produce the new piece get their corresponding share of credit. This will get messy if someone uses for example the #1 song of every month in a decade to create the decade song, then there would be as many as 120 different artists to credit.In a computer generated artistic work, both the person who undertook all the arrangements necessary for its creation as well as the original authors of the data input need to be credited.There will still be some ambiguity as to who undertook the arrangements necessary, only the one who gathered the data and pressed the button to let the network learn, or does the person who created the neural network’s model also get credit? Shall we go all the way and say that even the programmer of the neural network gets some credit as well?There are some countries, in particular the UK where some progress has been made to amend copyright laws to cater for computer generated works of art, but I believe this is one of those fields where technology will surpass our law making capacity and we will live under a grey area for a while, and maybe this is just what we need, by having these works ending up free for use by anyone in the world, perhaps a new model for remunerating creative work can be established, one that does not require commercial success to be necessary for artists to make a living, and thus they can become free to explore their art.Perhaps a new model for remunerating creative work can be established, one that does not require commercial success to be necessary for artists to make a living.The Next Rembrandt is a computer-generated 3-D–printed painting developed by a facial-recognition algorithm that scanned data from 346 known paintings by the Dutch painter in a process lasting 18 months. The portrait is based on 168,263 fragments from Rembrandt’s works."
    } ,
  
    {
      "title"       : "So, what is a neural network?",
      "category"    : "theory",
      "tags"        : "neural networks, machine learning, artificial intelligence",
      "url"         : "./back-to-basics.html",
      "date"        : "2021-04-02 00:00:00 +0800",
      "description" : "ELI5: what is a neural network.",
      "content"     : "The omnipresence of technology nowadays has made it commonplace to read news about AI, just a quick glance at today’s headlines, and I get: This Powerful AI Technique Led to Clashes at Google and Fierce Debate in Tech. How A.I.-powered companies dodged the worst damage from COVID AI technology detects ‘ticking time bomb’ arteries AI in Drug Discovery Starts to Live Up to the Hype Pentagon seeks commercial solutions to get its data ready for AITopics from business, manufacturing, supply chain, medicine and biotech and even defense are covered in those news headlines, definitively the advancements on the fields of artificial intelligence, in particular machine learning and deep neural networks have permeated into our daily lives and are here to stay. But, do the general population know what are we talking about when we say “an AI”? I assume most people correctly imagine a computer algorithm or perhaps the more adventurous minds think of a physical machine, an advanced computer entity or even a robot, getting smarter by itself with every use-case we throw at it. And most people will be right, when “an AI” is mentioned it is indeed an algorithm run by a computer, and there is where the boundary of their knowledge lies.They say that the best way to learn something is to try to explain it, so in a personal exercise I will try to do an ELI5 (Explain it Like I am 5) version of what is a neural network.Let’s start with a little history, humans have been tinkering with the idea of an intelligent machine for a while now, some even say that the idea of artificial intelligence was conceived by the ancient greeks (source), and several attempts at devising “intelligent” machines have been made through history, a notable one was ‘The Analytical Engine’ created by Charles Babbage in 1837:The Analytical Engine of Charles Babbage - 1837Then, in the middle of last century by trying to create a model of how our brain works, Neural Networks were born. Around that time, Frank Rosenblatt at Cornell trying to understand the simple decision system present in the eye of a common housefly, proposed the idea of a perceptron, a very simple system that processes certain inputs with basic math operations and produces an output.To illustrate, let’s say that the brain of the housefly is a perceptron, its inputs are whatever values are produced by the multiple cells in its eyes, when the eye cell detects “something” it’s output will be a 1, and if there is nothing a 0. Then the combination of all those inputs can be processed by the perceptron (the fly brain), and the output is a simple 0 or 1 value. If it is a 1 then the brain is telling the fly to flee and if it is a 0 it means it is safe to stay where it is.We can imagine then that if many of the eye cells of the fly produce 1s, it means that an object is quite near, and therefore the perceptron will calculate a 1, it is time to flee.The perceptron is just a math operation, one that multiplies certain input values with preset “parameters” (called weights) and adds up the resulting multiplications to generate a value.Then the magic spark was ignited, the parameters (weights) of the perceptron could be “learnt” by a process of minimizing the difference between known results of particular observations, and what the perceptron is actually calculating. It is this process of learning what we call training the neural network.This idea is so powerful that even today it is one of the fundamental building blocks of what we call AI.From this I will try to explain how this simple concept can have such diverse applications as natural language processing (think Alexa), image recognition like medical diagnosis from a CTR scan, autonomous vehicles, etc.A basic neural network is a combination of perceptrons in different arrangements, the perceptron therefore was downgraded from “fly brain” to “network neuron”.A neural network has different components, in its basic form it has: Input Hidden layers OutputInputThe inputs of a neural network are in their essence just numbers, therefore anything that can be converted to a number can become an input. Letters in a text, pixels in an image, frequencies in a sound wave, values from a sensor, etc. are all different things that when converted to a numerical value serve as inputs for the neural network. This is one of the reasons why applications of neural networks are so diverse.Inputs can be as many as one need for the task at hand, from maybe 9 inputs to teach a neural network how to play tic-tac-toe to thousands of pixels from a camera for an autonomous vehicle. Since the input of a perceptron needs to be a single value, if for example a color pixel is chosen as input, it most likely will be broken into three different values; its red, green and blue components, hence each pixel will become 3 different inputs for the neural network.Hidden layersA “layer” within a neural network is just a group of perceptrons that all perform the same exact mathematical operation to the inputs and produce an output. The catch is that each of them have different weights (parameters), therefore their output for a given input will be different amongst them. There are many types of layers, the most typical of them being a “dense” layer, which is another word to say that all the inputs are connected to all the neurons (individual perceptrons), and as said before, each of these connections have a weight associated with it, so that the operation that each neuron performs is a simple weighted sum of all the inputs.The hidden layer is then typically connected to another dense layer, and their connection means that each output of a neuron from the first layer is treated effectively as an input for the subsequent one, and it is thus connected to every neuron.A neural network can have from one to as many layers as one can think, and the number of layers depends solely on the experience we have gathered on the particular problem we would like to solve.Another critical parameter of a hidden layer is the number of neurons it has, and again, we need to rely on experience to determine how many neurons are needed for a given problem. I have seen networks that vary from a couple of neurons to the thousands. And of course each hidden layer can have as many neurons as we please, so the number of combinations is vast.To the number of layers, their type and how many neurons each have, is what we call the network topology (including the number of inputs and outputs).OutputAt the very end of the chain, another layer lies (which behaves just like a hidden layer), but has the peculiarity that it is the final layer, and therefore whatever it calculates will be the output values of the whole network. The number of outputs the network has is a function of the problem we would like to solve. It could be as simple as one output, with its value representing a probability of an action (like in the case of the flee reaction of the housefly), to many outputs, perhaps if our network is trying to distinguish images of animals, one would have an output for each animal species, and the output would represent how much confidence the network has that the particular image belongs to the corresponding species.As we said, the neural network is just a collection of individual neurons, doing basic math operations on certain inputs in series of layers that eventually generate an output. This mesh of neurons is then “trained” on certain output values from known cases of the inputs; once it has learned it can then process new inputs, values that it has never seen before with surprisingly accurate results.Many of the problems neural networks solve, could be certainly worked out by other algorithms, however, since neural networks are in their core very basic operations, once trained, they are extremely efficient, hence much quicker and economical to produce results.There are a few more details on how a simple neural network operate that I purposedly left out to make this explanation as simple as possible. Thinks like biases, the activation functions and the math behind learning, the backpropagation algorithm, I will leave to a more in depth article. I will also write (perhaps in a series) about the more complex topologies combining different types of layers and other building blocks, a part from the perceptron.Things like “Alexa”, are a bit more complex, but work on exactly the same principles. Let’s break down for example the case of asking “Alexa” to play a song in spotify. Alexa uses several different neural networks to acomplish this:1. Speech recognitionAs a basic input we have our speech: the command “Alexa, play Van Halen”. This might seem quite simple for us humans to process, but for a machine is an incredible difficult feat to be able to understand speech, things like each individual voice timbre, entonation, intention and many more nuances of human spoken language make it so that traditional algorithms have struggled a lot with this. In our simplified example let’s say that we use a neural network to transform our spoken speech into text characters a computer is much more familiarized to learn.2. Understanding what we mean (Natural Language Understanding)Once the previous network managed to succesfuly convert our spoken words into text, there comes the even more difficult task of making sense of what we said. Things that we humans take for granted such as context, intonation and non verbal communication, help give our words meaning in a very subtle, but powerful way, a machine will have to do with much less information to correctly understand what we mean. It has to correctly identify the intention of our sentence and the subject or entities of what we mean.The neural network has to identify that it received a command (by identifying its name), the command (“play music”), and our choice (“Van Halen”). And it does so by means of simple math operations as described before. Of course the network involved is quite complex and has different types of neurons and connection types, but the underlying principles remain.3. Replying to usOnce Alexa understood what we meant, it then proceeds to execute the action of the command it interpreted and it replies to us in turn using natural language. This is accomplished using a technique called speech synthesis, things like pitch, duration and intensity of the words and phonems are selected based on the “meaning” of what Alexa will respond to us: “Playing songs by Van Halen on Spotify” sounding quite naturally. And all is accomplished with neural networks executing many simple math operations.Although it seems quite complex, the process for AI to understand us can be boiled down to simple math operationsOf course Amazon’s Alexa neural networks have undergone quite a lot of training to get to the level where they are, the beauty is that once trained, to perform their magic they just need a few mathematical operations.As said before, I will continue to write about the basics of neural networks, the next article in the series will dive a bit deeper into the math behind a basic neural network."
    } ,
  
    {
      "title"       : "Starting the adventure",
      "category"    : "",
      "tags"        : "general blogging, thoughts, life",
      "url"         : "./starting-the-adventure.html",
      "date"        : "2021-03-24 00:00:00 +0800",
      "description" : "Midlife career change: a disaster or an opportunity?",
      "content"     : "In the midst of a global pandemic caused by the SARS-COV2 coronavirus; I decided to start blogging. I wanted to blog since a long time, I have always enjoyed writing, but many unknowns and having “no time” for it prevented me from taking it up. Things like: “I don’t really know who my target audience is”, “what would my topic or topics be?”, “I don’t think I am a world-class expert in anything”, and many more kept stopping me from setting up my own blog. Now seemed like a good time as any so with those and tons of other questions in my mind I decided it was time to start.Funnily, this is not my first post. The birth of the blog came very natural as a way to “document” my newly established pursuit for getting myself into Machine Learning. This new adventure of mine comprises several things, and if I want to succeed I need to be serious about them all: I want to start coding again! I used to code a long time ago, starting when I was 8 years old in a Tandy Color Computer hooked up to my parent’s TV. Machine Learning is a vast, wide subject, I want to learn the generals, but also to select a few areas to focus on. Setting up a blog to document my journey and share it: Establish a learning and blogging routine. If I don’t do this, I am sure this endeavour will die off soon.As for the focus areas I will start with: Neural Networks fundamentals: history, basic architecture and math behind them Deep Neural Networks Reinforcement Learning Current state of the art: what is at the cutting edge now in terms of Deep Neural Networks and Reinforcement Learning?I selected the above areas to focus on based on my personal interests, I have been fascinated by the developments in reinforcement learning for a long time, in particular Deep Mind’s awesome Go, Chess and Starcraft playing agents. Therefore, I started reading a lot about it and even started a personal project for coding a tic-tac-toe learning agent.With my limited knowledge I have drafted the following learning path: Youtube: Three Blue One Brown’s videos on Neural Networks, Calculus and Linear Algebra. I cannot recommend them enough, they are of sufficient depth and use animation superbly to facilitate the understanding of the subjects. Coursera: Andrew Ng’s Machine Learning course Book: Deep Learning with Python by Francois Chollet Book: Reinforcement Learning: An Introduction, by Richard S. Sutton and Andrew G. BartoAs for practical work I decided to start by coding my first models from scratch (without using libraries such as Tensorflow), to be able to deeply understand the math and logic behind the models, so far it has proven to be priceless.For my next project I think I will start to do the basic hand-written digits recognition, which is the Machine Learning Hello World, for this I think I will start to use Tensorflow already.I will continue to write about my learning road, what I find interesting and relevant, and to document all my practical exercises, as well as news and the state of the art in the world of AI.So far, all I have learned has been so engaging that I am seriously thinking of a career change. I have 17 years of international experience in multinational corporations across various functions, such as Information Services, Sales, Customer Care and New Products Introduction, and sincerely, I am finding more joy in artificial intelligence than anything else I have worked on before. Let’s see where the winds take us.Thanks for reading!P.S. For the geeks like me, here is a snippet on the technical side of the blog.Static Website GeneratorI researched a lot on this, when I started I didn’t even know I needed a static website generator. I was just sure of one thing, I wanted my blog site to look modern, be easy to update and not to have anything extra or additional content or functionality I did not need.There is a myriad of website generators nowadays, after a lengthy search the ones I ended up considering are: wordpress wix squarespace ghost webflow netlify hugo gatsby jekyllI started with the web interfaced generators with included hosting in their offerings:wordpress is the old standard, it is the one CMS I knew from before, and I thought I needed a fully fledged CMS, so I blindly ran towards it. Turns out, it has grown a lot since I remembered, it is now a fully fledged platform for complex websites and ecommerce development, even so I decided to give it a try, I picked a template and created a site. Even with the most simplistic and basic template I could find, there is a lot going on in the site. Setting it up was not as difficult or cumbersome as others claim, it took me about one hour to have it up and running, it looks good, but a bit crowded for my personal taste, and I found out it serves ads in your site for the readers, that is a big no for me.I have tried wix and squarespace before, they are fantastic for quick and easy website generation, but their free offering has ads, so again, a big no for me.I discovered ghost as the platform used by one of the bloggers I follow (Sebastian Ruder), turns out is a fantastic evolution over wordpress. It runs on the latest technologies, its interface is quite modern, and it is focused on one thing only: publishing. They have a paid hosting service, but the software is open sourced, therefore free to use in any hosting.I also tested webflow and even created a mockup there, the learning curve was quite smooth, and its CMS seems quite robust, but a bit too much for the functionalities I required.Next were the generators that don’t have a web interface, but can be easily set up:The first I tried was netlify, I also set up a test site in it. Netlify provides free hosting, and to keep your source files it uses GitHub (a repository keeps the source files where it publishes from). It has its own CMS, Netlify CMS, and you have a choice of site generators: Hugo, Gatsby, MiddleMan, Preact CLI, Next.js, Elevently and Nuxt.js, and once you choose there are some templates for each. I did not find the variety of templates enticing enough, and the set up process was much more cumbersome than with wordpress (at least for my knowledge level). I choose Hugo for my test site.I also tested gatsby with it’s own Gatsby Cloud hosting service, here is my test site. They also use GitHub as a base to host the source files to build the website, so you create a repository, and it is connected to it. I found the free template offerings quite limited for what I was looking for.Finally it came the turn for jekyll, although an older, and slower generator (compared to Hugo and Gatsby), it was created by one of the founders of GitHub, so it’s integration with GitHub Pages is quite natural and painless, so much so, that to use them together you don’t even have to install Jekyll in your machine! You have two choices: keep it all online, by having one repository in Github keep all the source files, modify or add them online, and having Jekyll build and publish your site to the special gh-pages repository everytime you change or add a new file to the source repository. Have a synchronized local copy of the source files for the website, this way you can edit your blog and customize it in your choice of IDE (Integrated Development Environment). Then, when you update any file on your computer, you just “push” the changes to GitHub, and GitHub Pages automatically uses Jekyll to build and publish your site.I chose the second option, specially because I can manipulate files, like images, in my laptop, and everytime I sync my local repository with GitHub, they are updated and published automatically. Quite convenient.After testing with several templates to get the feel for it, I decided to keep Jekyll for my blog for several reasons: the convenience of not having to install anything extra on my computer to build my blog, the integration with GitHub Pages, the ease of use, the future proofing via integration with modern technologies such as react or vue and the vast online community that has produced tons of templates and useful information for issue resolution, customization and added functionality.I picked up a template, just forked the repository and started modifying the files to customize it, it was fast and easy, I even took it upon myself to add some functionality to the template (it served as a coding little project) like: SEO meta tags Dark mode (configurable in _config.yml file) automatic sitemap.xml automatic archive page with infinite scrolling capability new page of posts filtered by a single tag (without needing autopages from paginator V2), also with infinite scrolling click to tweet functionality (just add a &lt;tweet&gt; &lt;/tweet&gt; tag in your markdown. custom and responsive 404 page responsive and automatic Table of Contents (optional per post) read time per post automatically calculated responsive post tags and social share icons (sticky or inline) included linkedin, reddit and bandcamp icons copy link to clipboard sharing option (and icon) view on github link button (optional per post) MathJax support (optional per post) tag cloud in the home page ‘back to top’ button comments ‘courtain’ to mask the disqus interface until the user clicks on it (configurable in _config.yml) CSS variables to make it easy to customize all colors and fonts added several pygments themes for code syntax highlight configurable from the _config.yml file. See the highlighter directory for reference on the options. responsive footer menu and footer logo (if setup in the config file) smoother menu animationsAs a summary, Hugo and Gatsby might be much faster than Jekyll to build the sites, but their complexity I think makes them useful for a big site with plenty of posts. For a small site like mine, Jekyll provides sufficient functionality and power without the hassle.You can use the modified template yourself by forking my repository. Let me know in the comments or feel free to contact me if you are interested in a detailed walkthrough on how to set it all up.HostingSince I decided on Jekyll to generate my site, the choice for hosting was quite obvious, Github Pages is very nicely integrated with it, it is free, and it has no ads! Plus the domain name isn’t too terrible (the-mvm.github.io).Interplanetary File SystemTo contribute to and test IPFS I also set up a mirror in IPFS by using fleek.co. I must confess that it was more troublesome than I imagined, it was definetively not plug and play because of the paths used to fetch resources. The nature of IPFS makes short absolute paths for website resources (like images, css and javascript files) inoperative; the easiest fix for this is to use relative paths, however the same relative path that works for the root directory (i.e. /index.html) does not work for links inside directories (i.e. /tags/), and since the site is static, while generating it, one must make the distinction between the different directory levels for the page to be rendered correctly.At first I tried a simple (but brute force solution):# determine the level of the current file{% assign lvl = page.url | append:'X' | split:'/' | size %}# create the relative base (i.e. \"../\"){% capture relativebase %}{% for i in (3..lvl) %}../{% endfor %}{% endcapture %}{% if relativebase == '' %} {% assign relativebase = './' %}{% endif %}...# Eliminate unecesary double backslashes{% capture post_url %}{{ relativebase }}{{ post.url }}{% endcapture %}{% assign post_url = post_url | replace: \"//\", \"/\" %}This jekyll/liquid code was executed in every page (or include) that needed to reference a resource hosted in the same server.But this fix did not work for the search function, because it relies on a search.json file (also generated programmatically to be served as a static file), therefore when generating this file one either use the relative path for the root directory or for a nested directory, thus the search results will only link correctly the corresponding pages if the page where the user searched for something is in the corresponding scope.So the final solution was to make the whole site flat, meaning to live in a single directory. All pages and posts will live under the root directory, and by doing so, I can control how to address the relative paths for resources."
    } ,
  
    {
      "title"       : "Deep Q Learning for Tic Tac Toe",
      "category"    : "",
      "tags"        : "machine learning, artificial intelligence, reinforcement learning, coding, python",
      "url"         : "./deep-q-learning-tic-tac-toe.html",
      "date"        : "2021-03-19 05:14:20 +0800",
      "description" : "Inspired by Deep Mind's astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe. How hard could it be?",
      "content"     : "BackgroundAfter many years of a corporate career (17) diverging from computer science, I have now decided to learn Machine Learning and in the process return to coding (something I have always loved!).To fully grasp the essence of ML I decided to start by coding a ML library myself, so I can fully understand the inner workings, linear algebra and calculus involved in Stochastic Gradient Descent. And on top learn Python (I used to code in C++ 20 years ago).I built a general purpose basic ML library that creates a Neural Network (only DENSE layers), saves and loads the weights into a file, does forward propagation and training (optimization of weights and biases) using SGD. I tested the ML library with the XOR problem to make sure it worked fine. You can read the blog post for it here.For the next challenge I am interested in reinforcement learning greatly inspired by Deep Mind’s astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe (or noughts and crosses).How hard could it be?Of course the first thing to do was to program the game itself, so I chose Python because I am learning it, so it gives me a good practice opportunity, and PyGame for the interface.Coding the game was quite straightforward, albeit for the hiccups of being my first PyGame and almost my first Python program ever.I created the game quite openly, in such a way that it can be played by two humans, by a human vs. an algorithmic AI, and a human vs. the neural network. And of course the neural network against a choice of 3 AI engines: random, minimax or hardcoded (an exercise I wanted to do since a long time).While training, the visuals of the game can be disabled to make training much faster.Now, for the fun part, training the network, I followed Deep Mind’s own DQN recommendations:The network will be an approximation for the Q value function or Bellman equation, meaning that the network will be trained to predict the \"value\" of each move available in a given game state.A replay experience memory was implemented. This meant that the neural network will not be trained after each move. Each move will be recorded in a special \"memory\" alongside with the state of the board and the reward it received for taking such an action (move).After the memory is sizable enough, batches of random experiences sampled from the replay memory are used for every training roundA secondary neural network (identical to the main one) is used to calculate part of the Q value function (Bellman equation), in particular the future Q values. And then it is updated with the main network's weights every n games. This is done so that we are not chasing a moving target.Designing the neural networkThe Neural Network chosen takes 9 inputs (the current state of the game) and outputs 9 Q values for each of the 9 squares in the board of the game (possible actions). Obviously some squares are illegal moves, hence while training there was a negative reward given to illegal moves hoping that the model would learn not to play illegal moves in a given position.I started out with two hidden layers of 36 neurons each, all fully connected and activated via ReLu. The output layer was initially activated using sigmoid to ensure that we get a nice value between 0 and 1 that represents the QValue of a given state action pair.The many models…Model 1 - the first tryAt first the model was trained by playing vs. a “perfect” AI, meaning a hard coded algorithm that never looses and that will win if it is given the chance. After several thousand training rounds, I noticed that the Neural Network was not learning much; so I switched to training vs. a completely random player, so that it will also learn how to win. After training vs. the random player, the Neural Network seems to have made progress and is steadily diminishing the loss function over time.However, the model was still generating many illegal moves, so I decided to modify the reinforcement learning algorithm to punish more the illegal moves. The change consisted in populating with zeros all the corresponding illegal moves for a given position at the target values to train the network. This seemed to work very well for diminishing the illegal moves:Nevertheless, the model was still performing quite poorly winning only around 50% of games vs. a completely random player (I expected it to win above 90% of the time). This was after only training 100,000 games, so I decided to keep training and see the results:Wins: 65.46% Losses: 30.32% Ties: 4.23%Note that when training restarts, the loss and illegal moves are still high in the beginning of the training round, and this is caused by the epsilon greedy strategy that prefers exploration (a completely random move) over exploitation, this preference diminishes over time.After another round of 100,000 games, I can see that the loss function actually started to diminish, and the win rate ended up at 65%, so with little hope I decided to carry on and do another round of 100,000 games (about 2 hours in an i7 MacBook Pro):Wins: 46.40% Losses: 41.33% Ties: 12.27%As you can see in the chart, the calculated loss not even plateaued, but it seemed to increase a bit over time, which tells me the model is not learning anymore. This was confirmed by the win rate decreasing with respect of the previous round to a meek 46.4% that looks no better than a random player.Model 2 - Linear activation for the outputAfter not getting the results I wanted, I decided to change the output activation function to linear, since the output is supposed to be a Q value, and not a probability of an action.Wins: 47.60% Losses: 39% Ties: 13.4%Initially I tested with only 1000 games to see if the new activation function was working, the loss function appears to be decreasing, however it reached a plateau around a value of 1, hence still not learning as expected. I came across a technique by Brad Kenstler, Carl Thome and Jeremy Jordan called Cyclical Learning Rate, which appears to solve some cases of stagnating loss functions in this type of networks. So I gave it a go using their Triangle 1 model.With the cycling learning rate in place, still no luck after a quick 1,000 games training round; so I decided to implement on top a decaying learning rate as per the following formula:The resulting learning rate combining the cycles and decay per epoch is:Learning Rate = 0.1, Decay = 0.0001, Cycle = 2048 epochs, max Learning Rate factor = 10xtrue_epoch = epoch - c.BATCH_SIZElearning_rate = self.learning_rate*(1/(1+c.DECAY_RATE*true_epoch))if c.CLR_ON: learning_rate = self.cyclic_learning_rate(learning_rate,true_epoch)@staticmethoddef cyclic_learning_rate(learning_rate, epoch): max_lr = learning_rate*c.MAX_LR_FACTOR cycle = np.floor(1+(epoch/(2*c.LR_STEP_SIZE))) x = np.abs((epoch/c.LR_STEP_SIZE)-(2*cycle)+1) return learning_rate+(max_lr-learning_rate)*np.maximum(0,(1-x))c.DECAY_RATE = learning rate decay ratec.MAX_LR_FACTOR = multiplier that determines the max learning ratec.LR_STEP_SIZE = the number of epochs each cycle lastsWith these many changes, I decided to restart with a fresh set of random weights and biases and try training more (much more) games.1,000,000 episodes, 7.5 million epochs with batches of 64 moves eachWins: 52.66% Losses: 36.02% Ties: 11.32%After 24 hours!, my computer was able to run 1,000,000 episodes (games played), which represented 7.5 million training epochs of batches of 64 plays (480 million plays learned), the learning rate did decreased (a bit), but is clearly still in a plateau; interestingly, the lower boundary of the loss function plot seems to continue to decrease as the upper bound and the moving average remains constant. This led me to believe that I might have hit a local minimum.Model 3 - new network topologyAfter all the failures I figured I had to rethink the topology of the network and play around with combinations of different networks and learning rates.100,000 episodes, 635,000 epochs with batches of 64 moves eachWins: 76.83% Losses: 17.35% Ties: 5.82%I increased to 200 neurons each hidden layer. In spite of this great improvement the loss function was still in a plateau at around 0.1 (Mean Squared Error). Which, although it is greatly reduced from what we had, still was giving out only 77% win rate vs. a random player, the network was playing tic tac toe as a toddler!*I can still beat the network most of the time! (I am playing with the red X)*100,000 more episodes, 620,000 epochs with batches of 64 moves eachWins: 82.25% Losses: 13.28% Ties: 4.46%Finally we crossed the 80% mark! This is quite an achievement, it seems that the change in network topology is working, although it also looks like the loss function is stagnating at around 0.15.After more training rounds and some experimenting with the learning rate and other parameters, I couldn’t improve past the 82.25% win rate.These have been the results so far:It is quite interesting to learn how the many parameters (hyper-parameters as most authors call them) of a neural network model affect its training performance, I have played with: the learning rate the network topology and activation functions the cycling and decaying learning rate parameters the batch size the target update cycle (when the target network is updated with the weights from the policy network) the rewards policy the epsilon greedy strategy whether to train vs. a random player or an “intelligent” AI.And so far the most effective change has been the network topology, but being so close but not quite there yet to my goal of 90% win rate vs. a random player, I will still try to optimize further.Network topology seems to have the biggest impact on a neural network's learning ability.Model 4 - implementing momentumI reached out to the reddit community and a kind soul pointed out that maybe what I need is to apply momentum to the optimization algorithm. So I did some research and ended up deciding to implement various optimization methods to experiment with: Stochastic Gradient Descent with Momentum RMSProp: Root Mean Square Plain Momentum NAG: Nezterov’s Accelerated Momentum Adam: Adaptive Moment Estimation and keep my old vanilla Gradient Descent (vGD) ☺Click here for a detailed explanation and code of all the implemented optimization algorithms.So far, I have not been able to get better results with Model 4, I have tried all the momentum optimization algorithms with little to no success.Model 5 - implementing one-hot encoding and changing topology (again)I came across an interesting project in Github that deals exactly with Deep Q Learning, and I noticed that he used “one-hot” encoding for the input as opposed to directly entering the values of the player into the 9 input slots. So I decided to give it a try and at the same time change my topology to match his:So, ‘one hot’ encoding is basically changing the input of a single square in the tic tac toe board to three numbers, so that each state is represented with different inputs, thus the network can clearly differentiate the three of them. As the original author puts it, the way I was encoding, having 0 for empty, 1 for X and 2 for O, the network couldn’t easily tell that, for instance, O and X both meant occupied states, because one is two times as far from 0 as the other. With the new encoding, the empty state will be 3 inputs: (1,0,0), the X will be (0,1,0) and the O (0,0,1) as in the diagram.Still, no luck even with Model 5, so I am starting to think that there could be a bug in my code.To test this hypothesis, I decided to implement the same model using Tensorflow / Keras.Model 6 - Tensorflow / Kerasself.PolicyNetwork = Sequential()for layer in hidden_layers: self.PolicyNetwork.add(Dense( units=layer, activation='relu', input_dim=inputs, kernel_initializer='random_uniform', bias_initializer='zeros'))self.PolicyNetwork.add(Dense( outputs, kernel_initializer='random_uniform', bias_initializer='zeros'))opt = Adam(learning_rate=c.LEARNING_RATE, beta_1=c.GAMMA_OPT, beta_2=c.BETA, epsilon=c.EPSILON, amsgrad=False)self.PolicyNetwork.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])As you can see I am reusing all of my old code, and just replacing my Neural Net library with Tensorflow/Keras, keeping even my hyper-parameter constants.The training function changed to:reduce_lr_on_plateau = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=25)history = self.PolicyNetwork.fit(np.asarray(states_to_train), np.asarray(targets_to_train), epochs=c.EPOCHS, batch_size=c.BATCH_SIZE, verbose=1, callbacks=[reduce_lr_on_plateau], shuffle=True)With Tensorflow implemented, the first thing I noticed, was that I had an error in the calculation of the loss, although this only affected reporting and didn’t change a thing on the training of the network, so the results kept being the same, the loss function was still stagnating! My code was not the issue.Model 7 - changing the training scheduleNext I tried to change the way the network was training as per u/elBarto015 advised me on reddit.The way I was training initially was: Games begin being simulated and the outcome recorded in the replay memory Once a sufficient ammount of experiences are recorded (at least equal to the batch size) the Network will train with a random sample of experiences from the replay memory. The ammount of experiences to sample is the batch size. The games continue to be played between the random player and the network. Every move from either player generates a new training round, again with a random sample from the replay memory. This continues until the number of games set up conclude.The first change was to train only after every game concludes with the same ammount of data (a batch). This was still not giving any good results.The second change was more drastic, it introduced the concept of epochs for every training round, it basically sampled the replay memory for epochs * batch size experiences, for instance if epochs selected were 10, and batch size was 81, then 810 experiences were sampled out of the replay memory. With this sample the network was then trained for 10 epochs randomly using the batch size.This meant that I was training now effectively 10 (or the number of epochs selected) times more per game, but in batches of the same size and randomly shuffling the experiences each epoch.After still playing around with some hyperparameters I managed to get similar performance as I got before, reaching 83.15% win rate vs. the random player, so I decided to keep training in rounds of 2,000 games each to evaluate performance. With almost every round I could see improvement:As of today, my best result so far is 87.5%, I will leave it rest for a while and keep investigating to find a reason for not being able to reach at least 90%. I read about self play, and it looks like a viable option to test and a fun coding challenge. However, before embarking in yet another big change I want to ensure I have been thorough with the model and have tested every option correctly.I feel the end is near… should I continue to update this post as new events unfold or shall I make it a multi post thread?"
    } ,
  
    {
      "title"       : "Neural Network Optimization Methods and Algorithms",
      "category"    : "",
      "tags"        : "coding, machine learning, optimization, deep Neural networks",
      "url"         : "./neural-network-optimization-methods.html",
      "date"        : "2021-03-13 03:32:20 +0800",
      "description" : "Some neural network optimization algorithms mostly to implement momentum when doing back propagation.",
      "content"     : "For the seemingly small project I undertook of creating a machine learning neural network that could learn by itself to play tic-tac-toe, I bumped into the necesity of implementing at least one momentum algorithm for the optimization of the network during backpropagation.And since my original post for the TicTacToe project is quite large already, I decided to post separately these optimization methods and how did I implement them in my code.AdamsourceAdaptive Moment Estimation (Adam) is an optimization method that computes adaptive learning rates for each weight and bias. In addition to storing an exponentially decaying average of past squared gradients \\(v_t\\) and an exponentially decaying average of past gradients \\(m_t\\), similar to momentum. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. We compute the decaying averages of past and past squared gradients \\(m_t\\) and \\(v_t\\) respectively as follows:\\(\\begin{align}\\begin{split}m_t &amp;= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\v_t &amp;= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\end{split}\\end{align}\\)\\(m_t\\) and \\(v_t\\) are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As \\(m_t\\) and \\(v_t\\) are initialized as vectors of 0's, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. \\(\\beta_1\\) and \\(\\beta_2\\) are close to 1).They counteract these biases by computing bias-corrected first and second moment estimates:\\(\\begin{align}\\begin{split}\\hat{m}_t &amp;= \\dfrac{m_t}{1 - \\beta^t_1} \\\\\\hat{v}_t &amp;= \\dfrac{v_t}{1 - \\beta^t_2} \\end{split}\\end{align}\\)We then use these to update the weights and biases which yields the Adam update rule:\\(\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\\).The authors propose defaults of 0.9 for \\(\\beta_1\\), 0.999 for \\(\\beta_2\\), and \\(10^{-8}\\) for \\(\\epsilon\\).view on github# decaying averages of past gradientsself.v[\"dW\" + str(i)] = ((c.BETA1 * self.v[\"dW\" + str(i)]) + ((1 - c.BETA1) * np.array(self.gradients[i]) ))self.v[\"db\" + str(i)] = ((c.BETA1 * self.v[\"db\" + str(i)]) + ((1 - c.BETA1) * np.array(self.bias_gradients[i]) ))# decaying averages of past squared gradientsself.s[\"dW\" + str(i)] = ((c.BETA2 * self.s[\"dW\"+str(i)]) + ((1 - c.BETA2) * (np.square(np.array(self.gradients[i]))) ))self.s[\"db\" + str(i)] = ((c.BETA2 * self.s[\"db\" + str(i)]) + ((1 - c.BETA2) * (np.square(np.array( self.bias_gradients[i]))) ))if c.ADAM_BIAS_Correction: # bias-corrected first and second moment estimates self.v[\"dW\" + str(i)] = self.v[\"dW\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.v[\"db\" + str(i)] = self.v[\"db\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.s[\"dW\" + str(i)] = self.s[\"dW\" + str(i)] / (1 - (c.BETA2 ** true_epoch)) self.s[\"db\" + str(i)] = self.s[\"db\" + str(i)] / (1 - (c.BETA2 ** true_epoch))# apply to weights and biasesweight_col -= ((eta * (self.v[\"dW\" + str(i)] / (np.sqrt(self.s[\"dW\" + str(i)]) + c.EPSILON))))self.bias[i] -= ((eta * (self.v[\"db\" + str(i)] / (np.sqrt(self.s[\"db\" + str(i)]) + c.EPSILON))))SGD MomentumsourceVanilla SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum.Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction \\(\\gamma\\) of the update vector of the past time step to the current update vector:\\(\\begin{align}\\begin{split}v_t &amp;= \\beta_1 v_{t-1} + \\eta \\nabla_\\theta J( \\theta) \\\\\\theta &amp;= \\theta - v_t\\end{split}\\end{align}\\)The momentum term \\(\\beta_1\\) is usually set to 0.9 or a similar value.Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. \\(\\beta_1 &lt; 1\\)). The same thing happens to our weight and biases updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.view on githubself.v[\"dW\"+str(i)] = ((c.BETA1*self.v[\"dW\" + str(i)]) +(eta*np.array(self.gradients[i]) ))self.v[\"db\"+str(i)] = ((c.BETA1*self.v[\"db\" + str(i)]) +(eta*np.array(self.bias_gradients[i]) ))weight_col -= self.v[\"dW\" + str(i)]self.bias[i] -= self.v[\"db\" + str(i)]Nesterov accelerated gradient (NAG)sourceHowever, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We'd like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.Nesterov accelerated gradient (NAG) is a way to give our momentum term this kind of prescience. We know that we will use our momentum term \\(\\beta_1 v_{t-1}\\) to move the weights and biases \\(\\theta\\). Computing \\( \\theta - \\beta_1 v_{t-1} \\) thus gives us an approximation of the next position of the weights and biases (the gradient is missing for the full update), a rough idea where our weights and biases are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current weights and biases \\(\\theta\\) but w.r.t. the approximate future position of our weights and biases:\\(\\begin{align}\\begin{split}v_t &amp;= \\beta_1 v_{t-1} + \\eta \\nabla_\\theta J( \\theta - \\beta_1 v_{t-1} ) \\\\\\theta &amp;= \\theta - v_t\\end{split}\\end{align}\\)Again, we set the momentum term \\(\\beta_1\\) to a value of around 0.9. While Momentum first computes the current gradient and then takes a big jump in the direction of the updated accumulated gradient, NAG first makes a big jump in the direction of the previous accumulated gradient, measures the gradient and then makes a correction, which results in the complete NAG update. This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of Neural Networks on a number of tasks.Now that we are able to adapt our updates to the slope of our error function and speed up SGD in turn, we would also like to adapt our updates to each individual weight and bias to perform larger or smaller updates depending on their importance.view on githubv_prev = {\"dW\" + str(i): self.v[\"dW\" + str(i)], \"db\" + str(i): self.v[\"db\" + str(i)]}self.v[\"dW\" + str(i)] = (c.NAG_COEFF * self.v[\"dW\" + str(i)] - eta * np.array(self.gradients[i]))self.v[\"db\" + str(i)] = (c.NAG_COEFF * self.v[\"db\" + str(i)] - eta * np.array(self.bias_gradients[i]))weight_col += ((-1 * c.BETA1 * v_prev[\"dW\" + str(i)]) + (1 + c.BETA1) * self.v[\"dW\" + str(i)])self.bias[i] += ((-1 * c.BETA1 * v_prev[\"db\" + str(i)]) + (1 + c.BETA1) * self.v[\"db\" + str(i)])RMSpropsourceRMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in Lecture 6e of his Coursera Class.RMSprop was developed stemming from the need to resolve other method's radically diminishing learning rates.\\(\\begin{align}\\begin{split}E[\\theta^2]_t &amp;= \\beta_1 E[\\theta^2]_{t-1} + (1-\\beta_1) \\theta^2_t \\\\\\theta_{t+1} &amp;= \\theta_{t} - \\dfrac{\\eta}{\\sqrt{E[\\theta^2]_t + \\epsilon}} \\theta_{t}\\end{split}\\end{align}\\)RMSprop divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests \\(\\beta_1\\) to be set to 0.9, while a good default value for the learning rate \\(\\eta\\) is 0.001.view on githubself.s[\"dW\" + str(i)] = ((c.BETA1 * self.s[\"dW\" + str(i)]) + ((1-c.BETA1) * (np.square(np.array(self.gradients[i]))) ))self.s[\"db\" + str(i)] = ((c.BETA1 * self.s[\"db\" + str(i)]) + ((1-c.BETA1) * (np.square(np.array(self.bias_gradients[i]))) ))weight_col -= (eta * (np.array(self.gradients[i]) / (np.sqrt(self.s[\"dW\"+str(i)]+c.EPSILON))) )self.bias[i] -= (eta * (np.array(self.bias_gradients[i]) / (np.sqrt(self.s[\"db\"+str(i)]+c.EPSILON))) )Complete codeAll in all the code ended up like this:view on github@staticmethoddef cyclic_learning_rate(learning_rate, epoch): max_lr = learning_rate * c.MAX_LR_FACTOR cycle = np.floor(1 + (epoch / (2 * c.LR_STEP_SIZE)) ) x = np.abs((epoch / c.LR_STEP_SIZE) - (2 * cycle) + 1) return learning_rate + (max_lr - learning_rate) * np.maximum(0, (1 - x))def apply_gradients(self, epoch): true_epoch = epoch - c.BATCH_SIZE eta = self.learning_rate * (1 / (1 + c.DECAY_RATE * true_epoch)) if c.CLR_ON: eta = self.cyclic_learning_rate(eta, true_epoch) for i, weight_col in enumerate(self.weights): if c.OPTIMIZATION == 'vanilla': weight_col -= eta * np.array(self.gradients[i]) / c.BATCH_SIZE self.bias[i] -= eta * np.array(self.bias_gradients[i]) / c.BATCH_SIZE elif c.OPTIMIZATION == 'SGD_momentum': self.v[\"dW\"+str(i)] = ((c.BETA1 *self.v[\"dW\" + str(i)]) +(eta *np.array(self.gradients[i]) )) self.v[\"db\"+str(i)] = ((c.BETA1 *self.v[\"db\" + str(i)]) +(eta *np.array(self.bias_gradients[i]) )) weight_col -= self.v[\"dW\" + str(i)] self.bias[i] -= self.v[\"db\" + str(i)] elif c.OPTIMIZATION == 'NAG': v_prev = {\"dW\" + str(i): self.v[\"dW\" + str(i)], \"db\" + str(i): self.v[\"db\" + str(i)]} self.v[\"dW\" + str(i)] = (c.NAG_COEFF * self.v[\"dW\" + str(i)] - eta * np.array(self.gradients[i])) self.v[\"db\" + str(i)] = (c.NAG_COEFF * self.v[\"db\" + str(i)] - eta * np.array(self.bias_gradients[i])) weight_col += ((-1 * c.BETA1 * v_prev[\"dW\" + str(i)]) + (1 + c.BETA1) * self.v[\"dW\" + str(i)]) self.bias[i] += ((-1 * c.BETA1 * v_prev[\"db\" + str(i)]) + (1 + c.BETA1) * self.v[\"db\" + str(i)]) elif c.OPTIMIZATION == 'RMSProp': self.s[\"dW\" + str(i)] = ((c.BETA1 *self.s[\"dW\" + str(i)]) +((1-c.BETA1) *(np.square(np.array(self.gradients[i]))) )) self.s[\"db\" + str(i)] = ((c.BETA1 *self.s[\"db\" + str(i)]) +((1-c.BETA1) *(np.square(np.array(self.bias_gradients[i]))) )) weight_col -= (eta *(np.array(self.gradients[i]) /(np.sqrt(self.s[\"dW\"+str(i)]+c.EPSILON))) ) self.bias[i] -= (eta *(np.array(self.bias_gradients[i]) /(np.sqrt(self.s[\"db\"+str(i)]+c.EPSILON))) ) if c.OPTIMIZATION == \"ADAM\": # decaying averages of past gradients self.v[\"dW\" + str(i)] = (( c.BETA1 * self.v[\"dW\" + str(i)]) + ((1 - c.BETA1) * np.array(self.gradients[i]) )) self.v[\"db\" + str(i)] = (( c.BETA1 * self.v[\"db\" + str(i)]) + ((1 - c.BETA1) * np.array(self.bias_gradients[i]) )) # decaying averages of past squared gradients self.s[\"dW\" + str(i)] = ((c.BETA2 * self.s[\"dW\"+str(i)]) + ((1 - c.BETA2) * (np.square( np.array( self.gradients[i]))) )) self.s[\"db\" + str(i)] = ((c.BETA2 * self.s[\"db\" + str(i)]) + ((1 - c.BETA2) * (np.square( np.array( self.bias_gradients[i]))) )) if c.ADAM_BIAS_Correction: # bias-corrected first and second moment estimates self.v[\"dW\" + str(i)] = self.v[\"dW\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.v[\"db\" + str(i)] = self.v[\"db\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.s[\"dW\" + str(i)] = self.s[\"dW\" + str(i)] / (1 - (c.BETA2 ** true_epoch)) self.s[\"db\" + str(i)] = self.s[\"db\" + str(i)] / (1 - (c.BETA2 ** true_epoch)) # apply to weights and biases weight_col -= ((eta * (self.v[\"dW\" + str(i)] / (np.sqrt(self.s[\"dW\" + str(i)]) + c.EPSILON)))) self.bias[i] -= ((eta * (self.v[\"db\" + str(i)] / (np.sqrt(self.s[\"db\" + str(i)]) + c.EPSILON)))) self.gradient_zeros()"
    } ,
  
    {
      "title"       : "Machine Learning Library in Python from scratch",
      "category"    : "",
      "tags"        : "machine learning, coding, neural networks, python",
      "url"         : "./ML-Library-from-scratch.html",
      "date"        : "2021-03-01 02:32:20 +0800",
      "description" : "Single neuron perceptron that classifies elements learning quite quickly.",
      "content"     : "It must sound crazy that in this day and age, when we have such a myriad of amazing machine learning libraries and toolkits all open sourced, all quite well documented and easy to use, I decided to create my own ML library from scratch.Let me try to explain; I am in the process of immersing myself into the world of Machine Learning, and to do so, I want to deeply understand the basic concepts and its foundations, and I think that there is no better way to do so than by creating myself all the code for a basic neural network library from scratch. This way I can gain in depth understanding of the math that underpins the ML algorithms.Another benefit of doing this is that since I am also learning Python, the experiment brings along good exercise for me.To call it a Machine Learning Library is perhaps a bit of a stretch, since I just intended to create a multi-neuron, multi-layered perceptron.The library started very narrowly, with just the following functionality: create a neural network based on the following parameters: number of inputs size and number of hidden layers number of outputs learning rate forward propagate or predict the output values when given some inputs learn through back propagation using gradient descentI restricted the model to be sequential, and the layers to be only dense / fully connected, this means that every neuron is connected to every neuron of the following layer. Also, as a restriction, the only activation function I implemented was sigmoid:With my neural network coded, I tested it with a very basic problem, the famous XOR problem.XOR is a logical operation that cannot be solved by a single perceptron because of its linearity restriction:As you can see, when plotted in an X,Y plane, the logical operators AND and OR have a line that can clearly separate the points that are false from the ones that are true, hence a perceptron can easily learn to classify them; however, for XOR there is no single straight line that can do so, therefore a multilayer perceptron is needed for the task.For the test I created a neural network with my library:import Neural_Network as nninputs = 3hidden_layers = [2, 1]outputs = 1learning_rate = 0.03NN = nn.NeuralNetwork(inputs, hidden_layers, outputs, learning_rate)The three inputs I decided to use (after a lot of trial and error) are the X and Y coordinate of a point (between X = 0, X = 1, Y = 0 and Y = 1) and as the third input the multiplication of both X and Y. Apparently it gives the network more information, and it ends up converging much more quickly with this third input.Then there is a single hidden layer with 2 neurons and one output value, that will represent False if the value is closer to 0 or True if the value is closer to 1.Then I created the learning data, which is quite trivial for this problem, since we know very easily how to compute XOR.training_data = []for n in range(learning_rounds): x = rnd.random() y = rnd.random() training_data.append([x, y, x * y, 0 if (x &lt; 0.5 and y &lt; 0.5) or (x &gt;= 0.5 and y &gt;= 0.5) else 1])And off we go into training:for data in training_data: NN.train(data[:3].reshape(inputs), data[3:].reshape(outputs))The ML library can only train on batches of 1 (another self-imposed coding restriction), therefore only one “observation” at a time, this is why the train function accepts two parameters, one is the inputs packed in an array, and the other one is the outputs, packed as well in an array.To see the neural net in action I decided to plot the predicted results in both a 3d X,Y,Z surface plot (z being the network’s predicted value), and a scatter plot with the color of the points representing the predicted value.This was plotted in MatPlotLib, so we needed to do some housekeeping first:fig = plt.figure()fig.canvas.set_window_title('Learning XOR Algorithm')fig.set_size_inches(11, 6)axs1 = fig.add_subplot(1, 2, 1, projection='3d')axs2 = fig.add_subplot(1, 2, 2)Then we need to prepare the data to be plotted by generating X and Y values distributed between 0 and 1, and having the network calculate the Z value:x = np.linspace(0, 1, num_surface_points)y = np.linspace(0, 1, num_surface_points)x, y = np.meshgrid(x, y)z = np.array(NN.forward_propagation([x, y, x * y])).reshape(num_surface_points, num_surface_points)As you can see, the z values array is reshaped as a 2d array of shape (x,y), since this is the way Matplotlib interprets it as a surface:axs1.plot_surface(x, y, z, rstride=1, cstride=1, cmap='viridis', vmin=0, vmax=1, antialiased=True)The end result looks something like this:Then we reshape the z array as a one dimensional array to use it to color the scatter plot:z = z.reshape(num_surface_points ** 2)scatter = axs2.scatter(x, y, marker='o', s=40, c=z.astype(float), cmap='viridis', vmin=0, vmax=1)To actually see the progress while learning, I created a Matplotlib animation, and it is quite interesting to see as it learns. So my baby ML library is completed for now, but still I would like to enhance it in several ways: include multiple activation functions (ReLu, linear, Tanh, etc.) allow for multiple optimizers (Adam, RMSProp, SGD Momentum, etc.) have batch and epoch training schedules functionality save and load trained model to fileI will get to it soon…"
    } ,
  
    {
      "title"       : "Conway&#39;s Game of Life",
      "category"    : "",
      "tags"        : "coding, python",
      "url"         : "./conways-game-of-life.html",
      "date"        : "2021-02-11 03:32:20 +0800",
      "description" : "Taking on the challenge of picking up coding again through interesting small projects, this time it is the turn of Conway's Game of Life.",
      "content"     : "I&nbsp;am lately trying to take on coding again. It had always been a part of my life since my early years when I&nbsp;learned to program a Tandy Color Computer at the age of 8, the good old days.Tandy Color Computer TRS80 IIIHaving already programed in Java, C# and of course BASIC, I&nbsp;thought it would be a great idea to learn Python since I&nbsp;have great interest in data science and machine learning, and those two topics seem to have an avid community within Python coders.For one of my starter quick programming tasks, I&nbsp;decided to code Conway's Game of Life, a very simple cellular automata that basically plays itself.The game consists of a grid of n size, and within each block of the grid a cell could either be dead or alive according to these rules:If a cell has less than 2 neighbors, meaning contiguous alive cells, the cell will die of lonelinessIf a cell has more than 3 neighbors, it will die of overpopulationIf an empty block has exactly 3 contiguous alive neighbors, a new cell will be born in that spotIf an alive cell has 2 or 3 alive neighbors, it continues to liveConway’s rules for the Game of LifeTo make it more of a challenge I&nbsp;also decided to implement an \"sparse\" method of recording the game board, this means that instead of the typical 2d array representing the whole board, I&nbsp;will only record the cells which are alive. Saving a lot of memory space and processing time, while adding some spice to the challenge.The trickiest part was figuring out how to calculate which empty blocks had exactly 3 alive neighbors so that a new cell will spring to life there, this is trivial in the case of recording the whole grid, because we just iterate all over the board and find the alive neighbors of ALL&nbsp;the blocks in the grid, but in the case of only keeping the alive cells proved quite a challenge.In the end the algorithm ended up as follows:Iterate through all the alive cells and get all of their neighborsdef get_neighbors(self, cell): neighbors = [] for x in range(-1, 2, 1): for y in range(-1, 2, 1): if not (x == 0 and y == 0): if (0 &amp;lt;= (cell[0] + x) &amp;lt;= self.size_x) and (0 &amp;lt;= (cell[1] + y) &amp;lt;= self.size_y): neighbors.append((cell[0] + x, cell[1] + y)) return neighborsMark all the neighboring blocks as having +1 neighbor each time a particular cell is encountered. This way, for each neighboring alive cell the counter of the particular block will increase, and in the end it will contain the total number of live cells which are contiguous to it.def next_state(self): alive_neighbors = {} for cell in self.alive_cells: if cell not in alive_neighbors: alive_neighbors[cell] = 0 neighbors = self.get_neighbors(cell) for neighbor in neighbors: if neighbor not in alive_neighbors: alive_neighbors[neighbor] = 1 else: alive_neighbors[neighbor] += 1The trick was using a dictionary to keep the record of the blocks that have alive neighbors and the cells who are alive in the current state but have zero alive neighbors (thus will die).With the dictionary it became easy just to add cells and increase their neighbor counter each time it was encountered as a neighbor of an alive cell.Having the dictionary now filled with all the cells that have alive neighbors and how many they have, it was just a matter of applying the rules of the game:for cell in alive_neighbors: if alive_neighbors[cell] &amp;lt; 2 or alive_neighbors[cell] &gt; 3: self.alive_cells.discard(cell) elif alive_neighbors[cell] == 3: self.alive_cells.add(cell)Notice that since I am keeping an array of the coordinates of only the cells who are alive, I could apply just 3 rules, die of loneliness, die of overpopulation and become alive from reproduction (exactly 3 alive neighbors) because the ones who have 2 or 3 neighbors and are already alive, can remain alive in the next iteration.I&nbsp;found it very interesting to implement the Game of Life like this, it was quite a refreshing challenge and I am beginning to feel my coding skills ramping up again."
    } ,
  
    {
      "title"       : "Single Neuron Perceptron",
      "category"    : "",
      "tags"        : "machine learning, coding, neural networks",
      "url"         : "./single-neuron-perceptron.html",
      "date"        : "2021-01-26 03:32:20 +0800",
      "description" : "Single neuron perceptron that classifies elements learning quite quickly.",
      "content"     : "As an entry point to learning python and getting into Machine Learning, I decided to code from scratch the Hello World! of the field, a single neuron perceptron.What is a perceptron?A perceptron is the basic building block of a neural network, it can be compared to a neuron, And its conception is what detonated the vast field of Artificial Intelligence nowadays.Back in the late 1950’s, a young Frank Rosenblatt devised a very simple algorithm as a foundation to construct a machine that could learn to perform different tasks.In its essence, a perceptron is nothing more than a collection of values and rules for passing information through them, but in its simplicity lies its power.Imagine you have a ‘neuron’ and to ‘activate’ it, you pass through several input signals, each signal connects to the neuron through a synapse, once the signal is aggregated in the perceptron, it is then passed on to one or as many outputs as defined. A perceptron is but a neuron and its collection of synapses to get a signal into it and to modify a signal to pass on.In more mathematical terms, a perceptron is an array of values (let’s call them weights), and the rules to apply such values to an input signal.For instance a perceptron could get 3 different inputs as in the image, lets pretend that the inputs it receives as signal are: $x_1 = 1, \\; x_2 = 2\\; and \\; x_3 = 3$, if it’s weights are $w_1 = 0.5,\\; w_2 = 1\\; and \\; w_3 = -1$ respectively, then what the perceptron will do when the signal is received is to multiply each input value by its corresponding weight, then add them up.\\(\\begin{align}\\begin{split}\\left(x_1 * w_1\\right) + \\left(x_2 * w_2\\right) + \\left(x_3 * w_3\\right)\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}\\left(0.5 * 1\\right) + \\left(1 * 2\\right) + \\left(-1 * 3\\right) = 0.5 + 2 - 3 = -0.5\\end{split}\\end{align}\\)Typically when this value is obtained, we need to apply an “activation” function to smooth the output, but let’s say that our activation function is linear, meaning that we keep the value as it is, then that’s it, that is the output of the perceptron, -0.5.In a practical application, the output means something, perhaps we want our perceptron to classify a set of data and if the perceptron outputs a negative number, then we know the data is of type A, and if it is a positive number then it is of type B.Once we understand this, the magic starts to happen through a process called backpropagation, where we “educate” our tiny one neuron brain to have it learn how to do its job.The magic starts to happen through a process called backpropagation, where we \"educate\" our tiny one neuron brain to have it learn how to do its job.For this we need a set of data that it is already classified, we call this a training set. This data has inputs and their corresponding correct output. So we can tell the little brain when it misses in its prediction, and by doing so, we also adjust the weights a bit in the direction where we know the perceptron committed the mistake hoping that after many iterations like this the weights will be so that most of the predictions will be correct.After the model trains successfully we can have it classify data it has never seen before, and we have a fairly high confidence that it will do so correctly.The math behind this magical property of the perceptron is called gradient descent, and is just a bit of differential calculus that helps us convert the error the brain is having into tiny nudges of value of the weights towards their optimum. This video series by 3 blue 1 brown explains it wonderfuly.My program creates a single neuron neural network tuned to guess if a point is above or below a randomly generated line and generates a visualization based on graphs to see how the neural network is learning through time.The neuron has 3 inputs and weights to calculate its output:input 1 is the X coordinate of the point,Input 2 is the y coordinate of the point,Input 3 is the bias and it is always 1Input 3 or the bias is required for lines that do not cross the origin (0,0)The Perceptron starts with weights all set to zero and learns by using 1,000 random points per each iteration.The output of the perceptron is calculated with the following activation function: if x * weight_x + y weight_y + weight_bias is positive then 1 else 0The error for each point is calculated as the expected outcome of the perceptron minus the real outcome therefore there are only 3 possible error values: Expected Calculated Error 1 -1 1 1 1 0 -1 -1 0 -1 1 -1 With every point that is learned if the error is not 0 the weights are adjusted according to:New_weight = Old_weight + error * input * learning_ratefor example: New_weight_x = Old_weight_x + error * x * learning rateA very useful parameter in all of neural networks is teh learning rate, which is basically a measure on how tiny our nudge to the weights is going to be.In this particular case, I coded the learning_rate to decrease with every iteration as follows:learning_rate = 0.01 / (iteration + 1)this is important to ensure that once the weights are nearing the optimal values the adjustment in each iteration is subsequently more subtle.In the end, the perceptron always converges into a solution and finds with great precision the line we are looking for.Perceptrons are quite a revelation in that they can resolve equations by learning, however they are very limited. By their nature they can only resolve linear equations, so their problem space is quite narrow.Nowadays the neural networks consist of combinations of many perceptrons, in many layers, and other types of “neurons”, like convolution, recurrent, etc. increasing significantly the types of problems they solve."
    } 
  
]
