[
  
    {
      "title"       : "扩散模型",
      "category"    : "",
      "tags"        : "notes",
      "url"         : "./Diffusion-Model.html",
      "date"        : "2022-10-12 00:00:00 +0800",
      "description" : "collect and arrange information and principle of diffusion model",
      "content"     : "1. 扩散模型简介 2. 模型 2.1. 前向扩散 2.2. 反向扩散过程 2.3. 损失函数 2.4. DDPM中给出的训练与采样(生成)过程 2.5. 小结 3. 一些技巧和主要网络结构 3.1. $\\beta_t$和$\\Sigma_\\theta$的取值 3.2. 加速扩散模型采样的技巧 3.3. U-Net 4. 条件生成 4.1. Classifier Guided Diffusion 4.2. Classifier-Free Guidance 4.3. Scale up Generation Resolution and Quality 1. 扩散模型简介扩散模型(Diffusion Model)是深度生成模型中的SOTA，相比于GAN、VAE、Flow-based这些生成模型而言，扩散模型可以取得更好的效果。扩散模型受非平衡热力学启发，它定义了一条多时间步的马尔可夫链来逐步给图片添加噪声，如果时间步够大，最终图片会变成纯噪声，扩散模型的目的是学习反向的扩散过程，也就是输入随机噪声，能返回一张图片，相比于之前提到的各种生成模型而言，扩散模型具有相对固定的学习步骤，同时隐变量维度更高(和输入数据同样的维度)扩散模型的早在2015年便提出了(论文链接)，但在当时没有引起广泛的关注，直到2019年NCSN和2020年DDPM的出现才将扩散模型引入了新高度，2022年火爆的text2image模型GLIDE、DALLE2、Latent Diffusion、Imagen的相继提出，让扩散模型火出了圈，这篇博客将对扩散模型的前向计算、反向训练、训练、生成步骤及其数学原理做详细的整理，会列出很多数学公式，同时该博客也参考了很多相关资料，这里我一并列出 Lil blog, 一篇整理相当详尽的博客，也是我主要的参考对象 huggingface的一篇解释简单明了的博客 知乎上一篇中文博客 DDPM 一篇综述 Improved DDPM Diffusion Models Beat GANs GLIDE Cascaded Diffusion Model2. 模型2.1. 前向扩散从原始数据分布中采样$x_0$, 假设$x_0\\sim q(x)$，前向扩散过程就是在每一个时间步都加上一个高斯噪声，这样就可以从最初的$x_0$生成长度为T的噪声序列$x_1, x_2, x_3,…, x_T$，每一步都用variance schedule$\\beta_t$控制，其中$\\beta_t\\in (0, 1)$，每一步的后验分布(预定义好的)为:\\begin{equation}q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1}, \\beta_t\\mathbf{I}) \\quadq(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0) = \\prod^T_{t=1} q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})\\end{equation}这个前向传播的过程中有一个非常好的性质，就是我们可以在任意时间步采样得到$x_t$，为了实现这个技巧，我们需要用到reparameterization技巧(该技巧也在VAE中出现过)，重参数化技巧的本质就是将随机采样的z通过引入高斯噪声$\\epsilon$变成确定性的z，也就是上面的$x_t$可以表示为$x_t=\\sqrt{1-\\beta_t}x_{t-1}+\\sqrt{\\beta_t}\\epsilon$，这样有利于梯度的逆传播，那么我们可以推出以下公式:\\begin{equation}\\begin{aligned}\\mathbf{x}_t &amp;= \\sqrt{\\alpha_t}\\mathbf{x}_{t-1} + \\sqrt{1 - \\alpha_t}\\boldsymbol{\\epsilon}_{t-1} \\\\&amp;= \\sqrt{\\alpha_t \\alpha_{t-1}} \\mathbf{x}_{t-2} + \\sqrt{1 - \\alpha_t \\alpha_{t-1}} \\bar{\\boldsymbol{\\epsilon}}_{t-2} \\\\&amp;= \\dots \\\\&amp;= \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon} \\\\q(\\mathbf{x}_t \\vert \\mathbf{x}_0) &amp;= \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t)\\mathbf{I})\\end{aligned}\\end{equation}其中$\\epsilon_t$都是均值为0，方差为1的高斯噪声，$\\alpha_t=1-\\beta_t$, $\\bar{\\alpha_t}=\\prod_{i=1}^t\\alpha_i$, 注:两个均值相同高斯噪声可以合并成一个高斯噪声，方差为之前方差的平方和开根号，一般来说，$\\beta_1&lt;\\beta_2&lt;…&lt;\\beta_T$2.2. 反向扩散过程如果我们可以将前向传播的过程反向，那么我们就可以获得后验分布$q(x_{t-1}|x_{t})$，那么我们就可以利用马尔科夫链的性质，输入高斯噪声，然后获得生成的照片，但是，我们无法高效地得到$q(x_{t-1}|x_{t})$，于是我们希望学习出分布$p_\\theta$来模拟后验分布$q(x_{t-1}|x_{t})$，由于前向扩散的过程中我们假设后验分布是高斯分布，所以这里我们也假设$p_\\theta$是高斯分布，于是我们有:\\begin{equation}p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T) \\prod^T_{t=1} p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) \\quadp_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t))\\end{equation}其中分布$p_\\theta$中的均值$\\mu$和方差$\\Sigma$与时间步t和输入$x_t$有关虽然我们不知道$q(x_{t-1}|x_t)$的分布情况，但是我们可以知道$q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)$的分布情况，推导过程如下:\\begin{equation}\\begin{aligned}q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0\\right) &amp;=q\\left(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}, \\mathbf{x}_0\\right) \\frac{q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_0\\right)}{q\\left(\\mathbf{x}_t \\mid \\mathbf{x}_0\\right)} \\\\&amp; \\propto \\exp \\left(-\\frac{1}{2}\\left(\\frac{\\left(\\mathbf{x}_t-\\sqrt{\\alpha_t} \\mathbf{x}_{t-1}\\right)^2}{\\beta_t}+\\frac{\\left(\\mathbf{x}_{t-1}-\\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0\\right)^2}{1-\\bar{\\alpha}_{t-1}}-\\frac{\\left(\\mathbf{x}_t-\\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0\\right)^2}{1-\\bar{\\alpha}_t}\\right)\\right) \\\\&amp;=\\exp \\left(-\\frac{1}{2}\\left(\\frac{\\mathbf{x}_t^2-2 \\sqrt{\\alpha_t} \\mathbf{x}_t \\mathbf{x}_{t-1}+\\alpha_t \\mathbf{x}_{t-1}^2}{\\beta_t}+\\frac{\\mathbf{x}_{t-1}^2-2 \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0 \\mathbf{x}_{t-1}+\\bar{\\alpha}_{t-1} \\mathbf{x}_0^2}{1-\\bar{\\alpha}_{t-1}}-\\frac{\\left(\\mathbf{x}_t-\\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0\\right)^2}{1-\\bar{\\alpha}_t}\\right)\\right) \\\\&amp;=\\exp \\left(-\\frac{1}{2}\\left(\\left(\\frac{\\alpha_t}{\\beta_t}+\\frac{1}{1-\\bar{\\alpha}_{t-1}}\\right) \\mathbf{x}_{t-1}^2-\\left(\\frac{2 \\sqrt{\\alpha_t}}{\\beta_t} \\mathbf{x}_t+\\frac{2 \\sqrt{\\bar{\\alpha}_{t-1}}}{1-\\bar{\\alpha}_{t-1}} \\mathbf{x}_0\\right) \\mathbf{x}_{t-1}+C\\left(\\mathbf{x}_t, \\mathbf{x}_0\\right)\\right)\\right)\\end{aligned}\\end{equation}其中函数$C(x_t, x_0)$与$x_{t-1}$无关，根据上述式子我们可以得出$q(x_{t-1}|x_t,x_0)$满足正态分布，均值和标准差分别为$\\tilde{\\mu_t}$和$\\tilde{\\beta_t}$，表达式分别为:\\begin{equation}\\begin{aligned}\\tilde{\\beta}_t &amp;= 1/(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}}) = 1/(\\frac{\\alpha_t - \\bar{\\alpha}_t + \\beta_t}{\\beta_t(1 - \\bar{\\alpha}_{t-1})})= \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\beta_t \\\\\\tilde{\\boldsymbol{\\mu}}_t (\\mathbf{x}_t, \\mathbf{x}_0)&amp;= (\\frac{\\sqrt{\\alpha_t}}{\\beta_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1} }}{1 - \\bar{\\alpha}_{t-1}} \\mathbf{x}_0)/(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}}) \\\\&amp;= (\\frac{\\sqrt{\\alpha_t}}{\\beta_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1} }}{1 - \\bar{\\alpha}_{t-1}} \\mathbf{x}_0) \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\beta_t \\\\&amp;= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t} \\mathbf{x}_0\\\\\\end{aligned}\\end{equation}于是最终可以得到$q(x_{t-1}|x_t,x_0)$:\\begin{equation}q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\tilde{\\boldsymbol{\\mu}}(\\mathbf{x}_t, \\mathbf{x}_0), \\tilde{\\beta}_t \\mathbf{I})\\end{equation}在根据马尔可夫链我们有: $\\mathbf{x}_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}}(\\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_t)$，注意这里的$\\epsilon_t$并不是任意的一个噪声，而是让$x_0$变成$x_t$的噪声，那么$\\tilde{\\mu_t}$可以表示为:\\begin{equation}\\begin{aligned}\\tilde{\\boldsymbol{\\mu}}_t&amp;= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t} \\frac{1}{\\sqrt{\\bar{\\alpha}_t}}(\\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_t) \\\\&amp;= \\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_t \\Big)\\end{aligned}\\end{equation}2.3. 损失函数和VAE类似，也可以用Variational Lower Bound来最大边缘似然函数$p_\\theta(x_0)$:\\begin{equation}\\begin{aligned}- \\log p_\\theta(\\mathbf{x}_0) &amp;\\leq - \\log p_\\theta(\\mathbf{x}_0) + D_\\text{KL}(q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0) \\| p_\\theta(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0) ) \\\\&amp;= -\\log p_\\theta(\\mathbf{x}_0) + \\mathbb{E}_{\\mathbf{x}_{1:T}\\sim q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0)} \\Big[ \\log\\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T}) / p_\\theta(\\mathbf{x}_0)} \\Big] \\\\&amp;= -\\log p_\\theta(\\mathbf{x}_0) + \\mathbb{E}_q \\Big[ \\log\\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} + \\log p_\\theta(\\mathbf{x}_0) \\Big] \\\\&amp;= \\mathbb{E}_q \\Big[ \\log \\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} \\Big] \\\\\\text{Let }L_\\text{VLB} &amp;= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\Big[ \\log \\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} \\Big] \\geq - \\mathbb{E}_{q(\\mathbf{x}_0)} \\log p_\\theta(\\mathbf{x}_0)\\end{aligned}\\end{equation}由于这里取了-log，所以目标变成了最小化VLB损失函数，经过一系列漫长的推导，我们可以得到(中间步骤其后就是用马尔科夫链和贝叶斯定理把条件概率拆开):\\begin{equation}\\begin{aligned}L_\\text{VLB} &amp;= L_T + L_{T-1} + \\dots + L_0 \\\\\\text{where } L_T &amp;= D_\\text{KL}(q(\\mathbf{x}_T \\vert \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_T)) \\\\L_t &amp;= D_\\text{KL}(q(\\mathbf{x}_t \\vert \\mathbf{x}_{t+1}, \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_t \\vert\\mathbf{x}_{t+1})) \\text{ for }1 \\leq t \\leq T-1 \\\\L_0 &amp;= - \\log p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)\\end{aligned}\\end{equation}参数化损失函数中的$L_t$: 反向扩散的目标是用神经网络来拟合后验分布$p_\\theta(x_{t-1} \\vert x_t) = N(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$，根据损失函数$L_t$可知，反向扩散训练的目标是: 给定t和$x_t$, $\\mu_\\theta$的结果和$\\tilde{\\mu_t}$更接近，因为任意$x_t$在给定$x_0$的情况下都可以求出，我们可以参数化高斯噪声，把反向扩散的目标变成让$\\epsilon_t$和$\\epsilon_\\theta$更接近\\begin{equation}\\begin{aligned}\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) &amp;= \\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\Big) \\\\\\end{aligned}\\end{equation}损失函数$L_t$为:\\begin{equation}\\begin{aligned}L_t &amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{1}{2 \\| \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t) \\|^2_2} \\| \\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0) - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) \\|^2 \\Big] \\\\&amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{1}{2 \\|\\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\| \\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_t \\Big) - \\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{x}_t, t) \\Big) \\|^2 \\Big] \\\\&amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) \\| \\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2 \\Big] \\\\&amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) \\| \\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_t, t)\\|^2 \\Big] \\end{aligned}\\end{equation}2.4. DDPM中给出的训练与采样(生成)过程训练过程: 采样一个$x_0$ 任选一个时间t 随机采样一个高斯噪声$\\epsilon$ 计算损失函数的梯度，更新参数$\\theta$采样过程: 采样一个高斯噪声$x_T$ 从时间T开始，每一步采样一个高斯噪声z，利用重参数化，得到上一步的$x_{t-1}$，重复T次，最终得到生成的$x_0$注意这里还没有给出$\\epsilon_\\theta(x_t, t)$的网络结构，DDPM使用U-Net作为其网络结构(后面会具体展开)2.5. 小结简单来说，扩散模型的前向扩散过程都是定义好的马尔可夫链，每一步都需要使用重参数化技巧来添加噪声，这里每一步的后验分布的参数都是预定义好的。反向扩散过程就是用噪声生成原始图片的过程，和VAE类似，用分布$p_\\theta(x_{t-1}|x_t)$来拟合真实的后验分布$q(x_{t-1}|x_t)$，所以生成过程最重要的就是训练出合适的分布来拟合，通过VLB和重参数化的技巧，最终可以把训练过程看成给一个高斯噪声，拟合成前向扩散的噪声。这里的网络结构一般使用的是U-Net3. 一些技巧和主要网络结构3.1. $\\beta_t$和$\\Sigma_\\theta$的取值关于$\\beta_t$的取值，DDPM的做法是$\\beta_1=10^{-4}$到$\\beta_T=0.02$线性取值，这样的扩散模型取得的效果不算最好，Improved DDPM提出了一种新的取值方法:\\begin{equation}\\beta_t = \\text{clip}(1-\\frac{\\bar{\\alpha}_t}{\\bar{\\alpha}_{t-1}}, 0.999) \\quad\\bar{\\alpha}_t = \\frac{f(t)}{f(0)}\\quad\\text{where }f(t)=\\cos\\Big(\\frac{t/T+s}{1+s}\\cdot\\frac{\\pi}{2}\\Big)\\end{equation}关于$\\Sigma_\\theta$的取值方法，DDPM采用固定的$\\Sigma_\\theta$(不学习)，可以取$\\beta_t$或者$\\tilde{\\beta_t}=\\frac{1 - \\bar{\\alpha_{t-1}}}{1 - \\bar{\\alpha_t}} \\cdot \\beta_t$，Improved DDPM采用可学习的$\\Sigma_\\theta$参数，利用线性插值的方法:\\begin{equation}\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t) = \\exp(\\mathbf{v} \\log \\beta_t + (1-\\mathbf{v}) \\log \\tilde{\\beta}_t)\\end{equation}由于损失函数中没有关于$\\Sigma_\\theta$的梯度，所以需要对损失函数进行一点更改3.2. 加速扩散模型采样的技巧DDPM的作者对比了扩散模型和其他生成模型的生成速度，发现DDPM的生成速度远小于其他生成模型，有一些加速模型采样的技巧: 缩短采样步骤，比如每隔T/S步才采样一次 DDIM论文里提出的技巧，在采样过程中只需要采样一个子集的步骤便可做生成 Latent Diffusion Model论文提出让扩散过程在隐空间中进行，而不是在像素空间中进行，这样可以让训练代价更小，推断过程更快，后续会整理LDM论文3.3. U-Net很多扩散模型的噪声网络结构都是基于U-Net，U-Net的网络结构如下图:模型可以分为两个部分，左边用于特征的抽取，右边部分用于上采样，由于网络结构酷似字母U而得名。U-Net网络结构又可以看成AutoEncoder的结构，它的bottleneck就是中间的低纬度特征表示，U-Net要保证输出的噪声和输入的噪声有相同的维度，是一个自回归模型。DDPM使用的是PixelCNN++的backbone，也就是基于Wide Resnet的U-Net，也就是说encoder和decoder之间是残差连接，输入$x_t$返回噪声(残差思想)4. 条件生成条件生成就是conditioned generation，通过输入额外的conditioning information来生成图片，比如一段提示词或者生成图片的类别4.1. Classifier Guided Diffusion博客上讲解的关于classifier guidance的部分不太详尽，于是我去翻看了Classifier Guidance的论文Diffusion Models Beat GANs:classifier guidance的思路来源于GAN模型的条件生成，将这种条件生成应用于扩散模型后，发现效果非常好。作者提出可以训练一个分类器$p_\\phi(y|x_t, t)$，然后把$\\nabla_{x_t} \\log p_\\phi\\left(y \\mid x_t, t\\right)$的加到总的梯度公式里面，来指导扩散模型采样的过程偏向于生成类别为y的图片没有classifier guidance之前的反向扩散分布函数为: $p_\\theta(x_t|x_{t+1})$，但是有了classifier guidance之后，反向扩散的后验分布函数变成了:\\begin{equation}p_{\\theta, \\phi}(x_t|x_{t+1}, y) = Zp_\\theta(x_t|x_{t+1})p_\\phi(y|x_t)\\end{equation}其中Z是正则化的常数，接下来我们需要化简上面这个公式，首先我们知道$p_\\theta(x_t|x_{t+1})$本质上就是正态分布:\\begin{equation}p_\\theta(x_t|x_{t+1})=\\mathcal{N}(\\mu, \\Sigma)\\end{equation}然后我们对$log_\\phi p(y|x_t)$在$x=\\mu$处进行泰勒展开:\\begin{equation}\\begin{aligned}\\log p_\\phi(y \\mid x_t) &amp; \\approx log p_\\phi (y \\mid x_t) \\mid _{x_t=\\mu}+(x_t-\\mu)\\nabla_{x_t}logp_\\phi(y \\mid x_t)\\mid _{x_t=\\mu} \\\\&amp;=(x_t-\\mu)g+C_1\\\\\\end{aligned}\\end{equation}这里$g=\\nabla_{x_t}logp_\\phi(y \\mid x_t)\\mid _{x_t=\\mu}$\\begin{equation}\\begin{aligned}\\log \\left(p_\\theta\\left(x_t \\mid x_{t+1}\\right) p_\\phi\\left(y \\mid x_t\\right)\\right) &amp; \\approx-\\frac{1}{2}\\left(x_t-\\mu\\right)^T \\Sigma^{-1}\\left(x_t-\\mu\\right)+\\left(x_t-\\mu\\right) g+C_2 \\\\&amp;=-\\frac{1}{2}\\left(x_t-\\mu-\\Sigma g\\right)^T \\Sigma^{-1}\\left(x_t-\\mu-\\Sigma g\\right)+\\frac{1}{2} g^T \\Sigma g+C_2 \\\\&amp;=-\\frac{1}{2}\\left(x_t-\\mu-\\Sigma g\\right)^T \\Sigma^{-1}\\left(x_t-\\mu-\\Sigma g\\right)+C_3 \\\\&amp;=\\log p(z)+C_4, z \\sim \\mathcal{N}(\\mu+\\Sigma g, \\Sigma)\\end{aligned}\\end{equation}那么反向扩散过程就可以看成均值为$\\mu+\\Sigma g$，方差为$\\Sigma$的正态分布，那么我们有以下采样算法:另一种思路是修改正态分布中的噪声函数，原本的梯度为:\\begin{equation}\\nabla _{x_t} log p_\\theta (x_t) = - \\frac{1}{\\sqrt{1-\\overline{\\alpha_t}}} \\epsilon_\\theta (x_t)\\end{equation}修改反向扩散函数后的梯度为:\\begin{equation}\\begin{aligned}\\nabla _{x_t} log (p_\\theta (x_t) p_\\phi (y \\mid x_t)) &amp;= \\nabla _{x_t} log p_\\theta (x_t) + \\nabla _{x_t} log p_\\phi (y \\mid x_t) \\\\&amp;= - \\frac{1}{\\sqrt{1-\\overline{\\alpha_t}}} \\epsilon_\\theta (x_t) + \\nabla _{x_t} log p_\\phi (y \\mid x_t)\\end{aligned}\\end{equation}那么根据梯度，我们可以定义一个新的噪声预测函数$\\hat{\\epsilon}$:\\begin{equation}\\hat{\\epsilon(x_t)} := \\epsilon_\\theta(x_t) - \\sqrt{1-\\overline{\\alpha_t}} \\nabla _{x_t} log p_\\phi(y \\mid x_t)\\end{equation}该方法对应的算法为:4.2. Classifier-Free Guidance上一小节提到的classifier guidance的技巧是需要单独使用一个分类器(参与训练或者不参与训练的情况都有)来获得$x_t$的类别，根据不同的class可以使用不同的分类器，比如resnet可以进行图片类别的guidance，CLIP可以进行文本的guidance等等。如果我们没有这个单独的分类器，我们也可以利用classifier-free guidance的技巧来实现条件生成，同样地，为了更详尽地了解这个技巧，我去翻看了GLIDE论文中关于classifier-free guidance的介绍classifier-free guidance并不需要模型去单独给出一个分类器，而是将条件生成与非条件生成都用同一个函数表示，即$\\epsilon(x_t\\mid y)$，如果我们希望这个函数表示非条件生成，那么我们只需要将y替换成空集即可，在训练过程中，我们以相同的概率随机替换y为空集。采样时，反向扩散函数为$\\epsilon_\\theta(x_t \\mid y)$和$\\epsilon_\\theta(x_t \\mid \\emptyset)$的线性插值:\\begin{equation}\\hat{\\epsilon_\\theta}(x_t \\mid y)=\\epsilon_\\theta(x_t\\mid \\emptyset) + s \\cdot (\\epsilon_\\theta(x_t\\mid y)-\\epsilon_\\theta(x_t\\mid \\emptyset))\\end{equation}式子中的s是guidance scale，s越大代表生成的图片越靠近y，guidance-free的技巧出现后，大家发现它的效果非常好，于是后续的模型基本上都运用了该技巧，比如GLIDE、DALLE2、Imagen4.3. Scale up Generation Resolution and Quality为了生成更高质量和更高分辨率的图片，可以将扩散模型与超分辨率的技术相结合，论文Cascaded Diffusion Model中提出用层级式的扩散模型来做图片的超分辨率生成，模型的结构如下图:模型由三个子模型组成，分别是一个基础的扩散模型和两个超分辨率扩散模型，注意这里的每个子模型都需要输入类别，超分辨率子模型还需要输入上一个子模型的低分辨率结果，这些子模型都是conditional的。超分辨率扩散模型与普通的扩散模型的区别是损失函数和反向扩散函数不同，具体来说就是U-Net的结构不同，超分辨率的U-Net的输出维度比输入维度要大，而且输入为低分辨率的图片、高分辨率的图片、类别:一个two-stage的cascaded模型的训练算法为:一个two-stage的cascaded模型的采样算法为:"
    } ,
  
    {
      "title"       : "短文本评估论文阅读整理",
      "category"    : "",
      "tags"        : "notes",
      "url"         : "./short-answer-assessment.html",
      "date"        : "2022-10-10 00:00:00 +0800",
      "description" : "read and arrange paper about short answer assessment",
      "content"     : "1. 论文简介 2. BERT-Based Deep Neural Networks 2.1. Abstract 2.2. Introduction 2.3. Related Work 2.3.1. Applications of Deep Learning in ASAG Tasks 2.3.2. BERT Model and Its Application in Education 2.4. Methodology 2.4.1. Task Definition 2.4.2. Model 2.4.2.1. BERT layer 2.4.2.2. Semantic Refinement Layer 2.4.2.3. Semantic Fusion Layer 2.4.2.4. Prediction Layer 2.4.2.5. Loss Function 2.5. Experiments 2.5.1. Datasets 2.5.2. Experimental Settings 2.5.3. Ablation Studies 2.5.4. Comparison With Baseline Systems 2.6. Discussions 2.7. Conclusion 3. Semantic Facets1. 论文简介 short answer grading model semantic facets这是两篇关于short-answer assessment的论文，所谓short-answer assessment就是对简答题的答案进行评估(和参考答案对比)，第一篇提出了利用BERT解决这个问题，第二篇提出了改进了评估过程，用多个semantic facets来评估short-answer，这篇博客对这两篇论文进行简单的整理2. BERT-Based Deep Neural Networks2.1. AbstractAutomatic short-answer grading(ASAG)，即自动短文本评分任务，是智慧辅导系统的重要组成部分。ASAG目前还存在很多挑战，作者提出了两个主要原因: 1)高精度评分任务需要对answer text有很深的语义理解; 2)ASAG任务的语料一般都很小，不能为深度学习提供足够的训练数据。为了解决这些挑战，作者提出用BERT-based网络来解决ASAG任务的挑战: 1)用预训练模型BERT来encoder答案文本就可以客服语料太小的问题。2)为了生成足够强的语义理解，作者在BERT输出层后加上了一个精炼层(由LSTM和Capsule网络串联组成) 3)作者提出一种triple-hot loss来处理ASAG的回归问题。实验结果表明模型的效果在SemEval-2013和Mohler数据集上表现比SOTA要好。模型在github上开源2.2. Introduction考试和评估是智慧辅导系统(intelligent tutoring systems, ITSs)的重要组成部分，可以获得学生们的实时知识认知水平，也能为学生们提供个性化的学习方案。多选题是考试的重要组成部分，但是多选题有两个明显的短板: 1)多选题只提供部分选择 2)有些学生的答案可能是蒙出来的。ASAG可以解决上述问题，学生们为简答题提供一个short text，然后ASAG来评估short text是否正确，具体来说评估结果有五类: Correct、Partically correct、Contradictory、Irrelevant、Nondomain。以往的研究中，Feature engineering是ASAG的主要解决方法，有很多稀疏特征应用于ASAG: token overlap features、syntax and dependency features、knowledge-based features(WordNet)… 但是这种特征工程为基础的方法存在以下问题: 首先，稀疏特征一般需要很多预处理步骤，这些步骤会产生一定的误差，可能会涉及到误差累积和误差传递的后果。此外，缺乏有效地encode文本句的手段随着deeplearning的发展，出现了很多deep net，比如LSTM-based model、CNN and LSTM-based model、transformer-based model出现在了ASAG任务中。这些模型都从answer text中挖掘语义信息，然后将answer text转化成word enbedding，所以这些方法都是end-to-end的。但是这些方法存在以下问题: 1)学生的答案非常free，也就是说在句子结构、语言风格、段落长度这些方面可能会有很大的区别，所以作者认为需要用更先进的technique去获得text的语义理解。2)由于数据很难标注，所以ASAG任务的数据集语料很小，可能只有few thousand。所以说主要的挑战就是如何在小语料库上训练一个稳定高效的模型论文的主要贡献: 提出了用预训练模型BERT微调，然后连接一个精炼层的模型表现超过SOTA(在SemEval-2013数据集和Mohler数据集上) 精炼层由Bi-LSTM和Capsule network(with position information)串联组成，LSTM来抽取全局的context信息，Capsule来抽取局部context信息 用多头注意力层来连接全局和局部context来生成语义表示 提出了triple-hot loss策略2.3. Related Work2.3.1. Applications of Deep Learning in ASAG Tasks根据deep learning和训练策略的不同，作者将deeplearning在ASAG的应用分为以下三种: Participator: deep learning参与feature-based方法中 Contractor: deeplearning独立地在ASAG任务中实现end-to-end 迁移学习，经典的预训练模型+scaling语料库接下来分别介绍了利用稀疏特征的方法与Deeplearning来来解决ASAG任务: 稀疏特征，也即feature engineering的应用有: Marvaniya等人和Saha等人使用预训练的神经网络InferSent对答案文本进行编码，这弥补了标记重叠(token overlap)方法中上下文表示的不足，其中InferSent是使用Bi-LSTM网络的预训练句子嵌入模型 Tan等人提出了一种将图卷积网络(GCNs)与几种稀疏特征相结合的评分方法。他们首先为答案文本构建了一个无向异构文本图，其中包含句子级节点、单词/bigam级节点和节点之间的边。然后，他们使用两层GCN模型对图结构进行编码，得到图的表示形式。 Zhang等人使用深度信念网络作为分类器，而不是传统的机器学习，对学生由六个稀疏特征组成的答案表示进行分类 Deeplearning的方法有: Kumar等人提出了ASAG的Bi-LSTM框架。他们的框架由三个级联的神经模块组成:分别应用于参考和学生答案的Siamese Bi-LSTMs，使用earth-mover distance(EMD)与LSTMs的隐状态交互的池化层，以及用于输出分数的回归层 Uto和Uchida将LSTM网络与项目反应理论(item response theory)相结合进行短文本答案评分 Tulu等人改进了基于LSTM的评分方法，通过引入感觉向量并将池化层替换为曼哈顿距离 Riordan等人结合CNN和LSTM网络进行短文本答案评分 Liu等人在一个大型K-12数据集上提出了一个具有多路注意的模型 上面提到的deeplearning方法需要大语料库支撑的数据集，但是ASAG缺少足够的大语料库，于是出现了用预训练模型来解决ASAG任务，比如ELMo、BERT、GPT、GPT-2，在这些模型中，BERT表现最好2.3.2. BERT Model and Its Application in EducationBERT吸收了ELMo和GPT的优点，模型如下图所示:BERTstack了12个transformer块接下来介绍了BERT在智慧教育领域的应用: Wang等人提出了分层课程BERT模型，以更好地捕捉每门课程的课程结构质量和语言特征，预测在线教育中教师的绩效 Khodeir等人将BERT与多层双向GRU相结合，构建了一个紧急分类模型，用于教师快速挑选和响应大规模开放在线课程(MOOC)论坛中最紧急的学生帖子，该模型在三个Stanford MOOC Post数据集上实现了紧急帖子分类，加权F-score分别为91.9%、91.0%和90.0% Sung等人利用BERT构建了一个多标签分类模型，用于快速评估学生在探索热力学的过程中的多模态的表征思维关于ASAG的应用有: Sung等人分析比较了BERT与多种网络结构在short-answer grading的效果 Leon等人分析比较了BERT、ALBERT、RoBERTa在short-answer grading的效果2.4. Methodology2.4.1. Task DefinitionASAG问题有两种形式: 回归问题: 连续的分数来评估学生的答案 分类问题: 将学生的答案分为五类: Correct、Partically correct、Contradictory、Irrelevant、Nondomain作者的做法是用分数来对类别进行分类，比如0-0.5属于类别1，所以问题的本质还是分类问题，那么ASAG的预测类别$y^*$可以表示为:\\begin{equation}y^*=\\underset{y \\in Y}{\\operatorname{argmax}}(\\operatorname{Pr}(y \\mid(q, p)))\\end{equation}其中Y表示类别集，Pr()表示预测的概率分布，q是学生答案，p是参考答案2.4.2. Model作者解释为什么即要用BERT，也要用refinement: BERT获得word embedding结果，利用了所有词元之间的关系，但是没有考虑顺序和距离，所以需要用Bi-LSTM来生成更精细的全局context，同时弥补BERT时序信息的缺失，然后利用Capsule或者CNN来生成BERT每个隐层的局部信息 BERT可以获得动态的词嵌入(对比GloVe获得静态的词嵌入)，这样可以获得更丰富的general-purpose knowledge，所以BERT即使在小语料库上也能有不错的效果 一些研究表明，在BERT上应用经典的神经网络可以在小数据集上获得更好的效果，比如Liao等人结合RoBERTa和CNN来提升情感分析的效果，Yang等人在BERT上应用多头注意力层来添加距离权重在aspect polarity classification上获得更好的效果主题网络模型如下图所示:接下来从模型的各个板块来分别介绍:2.4.2.1. BERT layer首先BERT layer的参数初始化成BERTbase的参数，微调。BERT layer层的输入是学生和参考答案的token embedding，输出是BERT的隐层\\begin{equation}O_{BERT}=BERT(s)=\\left\\{h_1^b,h_2^b,...,h_n^b\\right\\}\\in \\mathbb{R}^{n\\times d_b}\\end{equation}2.4.2.2. Semantic Refinement LayerRefinement层由Bi-LSTM和Capsule network(with position information)串联组成，输出结果如下所示:\\begin{equation}\\begin{aligned}&amp;\\overrightarrow{O_{\\mathrm{LSTNS}}}=\\overrightarrow{\\operatorname{LSTMS}}\\left(O_{\\text {BERT }}\\right)=\\left\\{\\overrightarrow{h_1^L}, \\overrightarrow{h_2^L}, \\ldots, \\overrightarrow{h_n^L}\\right\\} \\in \\mathbb{R}^{n \\times d_L} \\\\&amp;\\overleftrightarrow{O_{\\mathrm{LSTMs}}}=\\overleftarrow{\\operatorname{LSTMs}}\\left(O_{\\mathrm{BERT}}\\right)=\\left\\{\\overleftrightarrow{h_1^r}, \\overleftrightarrow{h_2^r}, \\ldots, \\overleftrightarrow{h_n^r}\\right\\} \\in \\mathbb{R}^{n \\times d_L} \\\\&amp;O_{\\mathrm{Caps}}=\\operatorname{Capsules}\\left(O_{\\text {BERT }}\\right)=\\left\\{h_1^c, h_2^c, \\ldots, h_n^c\\right\\} \\in \\mathbb{R}^{n \\times d_c}\\end{aligned}\\end{equation}输出结果后面都跟了一个层归一化(保证数据分布的稳定，加速收敛)这里提到了Capsule network，我对Capsule network进行一定的补充: Capsule网络主要想解决卷积神经网络（Convolutional Neural Networks）存在的一些缺陷，比如说信息丢失，视角变化等。Capsule网络结构如下图所示:以数字图片分类为例，Capsule一共包含3层，2层卷积层和1层全连接层。与普通网络的区别是输出的每个类别都是一个向量，向量的长度表示实体存在的概率大小，向量在空间中的方向表示实体的实例化参数，Capsule网络和CNN还是比较相似的2.4.2.3. Semantic Fusion Layer在refinement层后，需要有一个融合层来融合LSTM和Capsule的结果，先用矩阵来stackLSTM、Capsule的结果:\\begin{equation}X^{(e)}=\\{x_1^{(e)}, x_2^{(e)},...,x_n^{(e)}\\}\\in \\mathbb{R}^{n\\times d}\\end{equation}其中$x_i^{(e)}=[h_i^L;h_i^r;h_i^c]$，然后再把矩阵X送入多头自注意力层，注意力评分函数是scaled dot-product attention，具体细节如下:\\begin{equation}\\begin{aligned}&amp; \\text{MultiHead}(Q,K,V)=[\\text{head}_1;\\text{head}_2;...;\\text{head}_h]\\omega^R \\\\&amp; \\text{head}_i=\\text{Attention}(Q_i;K_i;V_i)=\\text{Attention}(Q\\omega ^Q, K\\omega ^K, V\\omega ^V) \\\\&amp; \\text{Attention}(Q_i;K_i;V_i)=\\text{softmax}(\\frac{Q_iK_i^T}{\\sqrt{d_K}})V_i \\\\ X^{(h)}&amp;=\\text{MultiHead}(X^{(e)}, X^{(e)}, X^{(e)}) \\\\&amp;=\\{x_1^{(h)}, x_2^{(h)},..., x_n^{(h)}\\}\\end{aligned}\\end{equation}为了让全局context和局部context不互相干扰，作者对多头自注意力层做以下约束: 让LSTM的输出维度和Capsule的输出维度相同，即$d_c=2d_L$ head数取2 让局部context和全局context不互相干扰(用参数调整)，如下图所示:最后再连接一个层归一化2.4.2.4. Prediction Layer预测层，首先用最大池化层获得pair(q,p)的语义表示，其实就是在每个头上选择最大的值:\\begin{equation}\\begin{aligned}Z &amp;=\\text{Maxpooling}(X^{(h)})={z_1,z_2,...,z_d}\\in \\mathbb{R}^d \\\\z_j &amp;=\\text{Max}(x_{1j}^{(h)}, x_{2j}^{(h)},..., x_{nj}^{(h)}), j=1,2,...,d\\end{aligned}\\end{equation}然后将语义表示Z输入线性层(加上一个dropout防止overfit)，然后用softmax表示输出的概率分布:\\begin{equation}\\begin{aligned}o &amp;=MZ+b \\\\p(y\\mid Z)&amp;=\\frac{exp(o_y)}{\\sum_{i}^{d_y}exp(o_i)}\\end{aligned}\\end{equation}2.4.2.5. Loss Function为了适应两种ASAG tasks，作者提出了两种损失函数的策略: 第一种就是常规的交叉熵，分类结果用one-hot编码\\begin{equation}L(\\theta)=-\\sum_{i=1}^{|\\Omega|}\\log \\left(p\\left(y_i \\mid Z_i, \\theta\\right)\\right)\\end{equation} 第二种就是作者提出用triple-hot编码y，就是在y对应位置的左右也置1，那么损失函数为:\\begin{equation}\\begin{aligned}L(\\theta)=&amp;-\\sum_{i=1}^{|\\Omega|}\\left(\\log \\left(p\\left(y_i^{-1} \\mid Z_i, \\theta\\right)\\right)+\\log \\left(p\\left(y_i \\mid Z_i, \\theta\\right)\\right)\\right.\\\\&amp;\\left.+\\log \\left(p\\left(y_i^{+1} \\mid Z_i, \\theta\\right)\\right)\\right)\\end{aligned}\\end{equation}2.5. Experiments2.5.1. Datasets作者在两个ASAG主流数据集上进行评估，分别是SemEval-2013和Mohler数据集 SemEval-2013: 作者使用SemEval-2013中的SciEntsBank语料，SciEntsBank语料包含15个不同科学领域的197个问题和10000个答案，这个语料库是ASAG分类任务的一个benchmark，他包含三种分类类别，分别是two-way(Correct and Incorrect)，three-way (Correct, Contradictory, and Incorrect)，five-way (Correct, Partially correct, Contradictory, Irrelevant, and Non-domain)，为了提供多方面的evaluation，测试数据集分为了三个子集: Unseen Answers(UA): 和训练集有相同的题目和参考答案，但是学生的回答不同 Unseen Questions(UQ): 和训练集的问题不同，但是属于同一个领域 Unseen Domains(UD): 和训练集的问题不同，且不属于同一个领域 对于这个数据集，作者用三个性能度量(accuracy, weighted-F1, macro-average F1)来评估两个子任务(three-way, five-way) Mohler dataset: 数据集开源。数据集由Mohler团队从University of North Texas的一门计算机科学课程的两个考试和十个测试收集整理。它包含80个问题和2273个学生的答案，每个答案都由两名老师打分(0-5, integer)，由于是平均而来，所以一共由11种分类结果，Mohler数据集同样是ASAG任务的一个benchmark，作者可以将数据集变成了11个类别的分类数据集。由于数据集只有2273个答案对，太小，所以需要对数据集进行扩充，Kumar等人通过把训练集中正确的学生答案作为额外的参考答案，这样就把数据集的答案-问题对扩充到30000对。作者为了避免过拟合，采取了折中的策略，对每个问题只挑选一个学生的正确答案作为额外的参考答案，这样就把2083个答案对扩充至3300个答案对。针对Mohler数据集，作者采用了12折交叉验证的方法，用来评估的性能度量有Cohen’s kappa coefficient(kappa), Pearson correlation coefficient(Pearson’s r), mean absolute error(MAE), root-mean-square error(RMSE)，Mohler的标签只有11类，作者既可以把它当作了回归任务来评估(就是把分类结果用分数表示)，也可以把它当作分类任务来评估，其中kappa系数是分类任务的性能度量，其他的性能度量都是回归任务的性能度量2.5.2. Experimental SettingsBERT采用base版本(12层，768个单元，12个head，110M参数)，LSTM的隐层个数设置为200并且在最后一个时间步返回所有的hidden state，Capsule的卷积核个数设置为400，卷积核大小为3，dynamic route设置为3。在融合层，attention head设置为2，每个头400维参数，dropout参数都设置为0.1，使用adam优化器，学习率设置为2e-5，一个小批量64个输入，训练周期为102.5.3. Ablation Studies为了分析每一层的作用，从六个角度来做ablation studies: W/O refinement: 无refienment W/O multihead: 无multihead以此类推，得到如下结果:2.5.4. Comparison With Baseline Systems与众多模型进行对比，主要的实验结果如下: Mohler数据集 SemEval-2013: PR曲线: AUC值和PR曲线平衡点:2.6. Discussions从Ablation实验结果可以看出，refinement层提升了模型的在Sem-UQ和Mohler数据集上的精度，说明refinement层提高了BERT模型在相同问题领域的泛化性，同时也可以看出LSTM在refinement层也很重要，可以提取更丰富的全局context信息。Capsule的有无也说明了它可以提取局部context信息，同时发现它的效果比一般的CNN要好。Triple-loss的设计也确实提升了模型的性能接着简单分析了一下表4(Mohler数据集)与表5(SemEval-2013)的结果，其实就是对比了不同模型的性能度量，说哪个模型更好什么的然后分析PR曲线的结果，作者认为在大多数情况下，模型的PR曲线远高于其他模型的PR曲线，这与表6中模型在所有图中AUC最大的结论是一致的。除此之外，平衡点的值也更高，说明模型性能最好模型可以应用于智慧教育系统的两个场景: ITSs领域，即智慧辅导系统，可以让评估变成自动化的 MOOC线上平台，可以替代老师的手动评估，快速精准地为大量的free-text answer进行评分2.7. Conclusion作者提出了一种新的BERT-based网络结构来解决ASAG问题，进行了大量的实验，得出了一下的结论: 基于词嵌入的网络，如CNN、LSTM、Capsule无法在小数据集上取得很好的结果 预训练网络BERT可以很好的适配ASAG任务 利用LSTM和Capsule网络可以进一步挖掘语义信息模型局限性: 在开放领域的问答中，小数据集训练出来的模型无法取得预期的效果，比如Sem-UD 目前来说，模型无法消除或者替代学生答案中的大量的代词，作者计划在后续通过BERT模型来消除学生答案中的代词来提升模型的性能3. Semantic Facets"
    } ,
  
    {
      "title"       : "AutoEncoder系列整理",
      "category"    : "",
      "tags"        : "notes",
      "url"         : "./AutoEncoder-Series.html",
      "date"        : "2022-10-08 00:00:00 +0800",
      "description" : "Information about autoencoder series",
      "content"     : "1. AE简介AutoEncoder，即AE，自编码器，是一类在半监督学习和非监督学习中使用的人工神经网络，其功能是通过将输入信息作为学习目标，对输入信息进行表征学习(representation learning)，编码其实就是特征表示半监督学习(semi-supervised learning)的训练数据一部分是有标签的，另一部分没有标签，而没标签的数量一般大于有标签数据的数量自编码器的原理如下图所示，encoder首先读取input，将输入转换成高效的内部表示(code)，然后再由decoder输出输入数据的类似物自编码器属于自监督学习的范畴，算法把输入作为监督信号来学习，encoder的作用其实就是对输入向量进行特征降维，常见的降维算法有主成分分析法PCA，但PCA本质上是一种线性变换，提取特征的能力有限自编码器利用神经网络来学习输入的特征表达，AE利用数据x本身作为监督信号来指导神经网络的训练，即希望神经网络能学到映射$f_\\theta$:x-&gt;x把网络切分为两个部分，前面的子网络尝试学习映射关系$g_{\\theta1}:x-&gt;z$，后面的子网络尝试学习映射关系$h_{\\theta2}:z-&gt;x$，即编码器和解码器，编码器和解码器共同完成了输入数据x的编码、解码过程，把整个网络模型叫做AutoEncoder，模型根据输出与输入的距离函数作为损失函数来优化AE，随机梯度下降假设输入为x，中间层为y，最终输出为z，那么y=s(Wx+b)，s是激活函数，z=s(W’y+b’)接下来我将整理AE家族的一些模型，有DAE、VAE、VQVAE，当然不可能涵盖所有的模型，尽量介绍一些使用较多的模型2. DAE通过Auto-Encoder得到的模型往往存在过拟合的风险，为了学习到较鲁棒的特征，可以在网络的输入层引入随机噪声，这种方法称为降噪自编码器(Denoising autoencoder, DAE)，为了更了解模型的原理和架构，我去阅读了DAE的论文作者的想法是让网络从corrputed的输入还原出原始输入，通过这个方法来提高模型的鲁棒性。corrputed的方法: 对于每一个输入x，随机选取$v_d$个元素置零，其他的部分保持不变，那么网络的目标就变成了对这些位置进行填空(fill-in)，这和BERT中Masked LM的思想差不多论文的其他部分着重介绍为什么这种denoising的思想有用以及背后的数学原理3. VAEAutoEncoder被指责只能简单地记住数据，在生成数据的能力上很差，于是变分自编码器VAE(Variational auto-encoder)出现了。论文链接作者提出了一种autoencoding variational bayesian(AEVB)算法，在AEVB算法中，通过使用SGVB(stochastic gradient variational bayes)估计器优化识别模型，使推断和学习更加有效，该识别模型允许我们使用简单的采样来获得非常有效的近似后验分布3.1. 数学原理数据集X由独立同分布采样的N个x组成，即$X={x^{(i)}}_{i=1}^N$。我们假设x的生成过程由两部分组成: $z^{(i)}$采样自先验分布$p_{\\theta*}(z)$ $x^{(i)}$由似然函数$p_{\\theta*}(x|z)$生成作者说不会对边缘概率分布和后验概率分布做一般的近似假设，所以一些常用的方法可能不行，作者提出用辨别模型$q_\\Phi(z|x)$作为真实后验概率$p_\\theta(z|x)$的近似，这里的$\\Phi$也就是变分参数(variational parameters)，variational有两个作用，一个是可以用q(z|x)来近似p(z|x)，另一个是优化了lower bound的梯度计算3.1.1. variational lower bound通过最大边缘似然函数$p_\\theta(x)$来获得参数$\\theta$的估计值，对数似然函数log$p_\\theta(x)$可写成:(x都指$x^{(i)}$)推导过程为:等式右边的第一项是KL散度，第二项是Evidence lower bound(ELBO)，因为KL散度非负，所以有:ELBO可以进一步推导，有:推导和前面类似，省略我们希望最大化ELBO来获得参数$\\Phi$和$\\theta$，可以用梯度上升法，然而对ELBO求梯度有点困难，作者提出了SGVB estimator来解决这个问题3.1.2. SGVB estimator and AEVB algorithm首先介绍一下蒙特卡洛gradient estimator:简单来说，蒙特卡洛就是将期望外的梯度符号移到了期望内接下来我们需要应用一种再参数化(reparameterization)的trick来实现似然函数梯度的计算，具体来说就是服从q(z|x)分布的随机变量z可以写成可微的形式:其中$\\epsilon$就是噪声，那么再经过一系列的推导，就可以得出似然函数的梯度AEVB算法:3.2. VAE模型VAE是AEVB算法的一个实例，VAE在AEVB算法的基础上做了以下约束: 使用encoder来模拟后验分布$q_\\Phi(z|x)$，并假设其满足多元混合高斯其中$\\mu^{(i)}$和$\\sigma^{(i)}$是输入$x^{(i)}$经过MLP后得到的结果，模型使用两个网络分别来估计每个样本对应的隐状态$z^{(i)}$的均值和方差 假设先验概率$p_\\theta(z)$满足多元正态分布模型，即$p_\\theta(z)~N(z;0,I)$ 用decoder来模拟$p_\\theta(x|z)$ 使用重参数的trick来获得z，因为这里假设的是正态分布，所以z可以简单理解成$z=\\mu+\\sigma * \\epsilon$，其中$\\epsilon$采样自均值为0，方差为1的正态分布这样我们就获得了VAE模型3.3. 总结这是我自己画的一张VAE模型图，encoder和decoder可以都是MLP。encoder的目的是从输入数据x中学习到z重参数化需要的变量$\\mu$和$\\sigma$，之后我们就可以采样噪声来获得z的先验分布，然后经过decoder得到重构的输出x，训练的过程其实就是学习x分布的过程。训练完成后，我们就可以得到生成模型$p_\\theta(x|z)p_\\theta(z)$，其中$p_\\theta(x|z)$就是decoder，先验分布$p_\\theta(z)$为正态分布，从先验分布$p_\\theta(z)$随机采样一个z送入decoder，就可以得到与训练数据类似的输出，所以VAE是一个生成模型一篇介绍VAE很详细的博客4. VQVAE4.1. 简介论文链接，VQVAE是DeepMind于2017年提出的一种基于离散隐变量(Discrete Latent variables)的生成模型，相比于VAE，VQVAE有两个重要的区别: 首先VQVAE采用离散隐变量(VAE采用的是连续隐变量)，其次VQVAE需要单独训练一个基于自回归的模型(比如PixelCNN)来学习先验概率，而不是像VAE那样采用一个固定的先验分布。VQVAE是一个强大的无监督表征学习模型，它学习的离散编码有很强的表征能力，DALLE第一版就是基于VQVAE的4.2. VQVAE模型VQ-VAE的全称是Vector Quantised Variational AutoEncoder，与VAE的主要区别是使用了离散的隐变量和学习出来的prior(非固定)4.2.1. 离散隐变量首先定义一个embedding space，记为e$\\in R^{K*D}$，K就是embedding space的大小，D就是每个embedding vector的大小，embedding space由K个长度为D的向量组成。原始输入x经过encoder后变成$z_e(x)$，然后我们需要向量化$z_e(x)$，即把它变成一个向量，这里就需要embedding space发挥作用，我们从embedding space的K个向量中选择一个作为$z_e(x)$向量化的结果，作者的做法是最临近查找(nearest neighbour look-up，即选择欧式距离最小的向量$e_{k}$作为向量化结果)，将向量化的结果$z_q(x)$输入decoder:模型在训练过程中需要调整的参数就是encoder的参数、decoder的参数以及embedding space的参数。在训练过程中VQ-VAE其实没有用到先验分布prior，所以后面需要单独训练一个先验模型来生成数据。这里的z对于不同的任务是不同维度的，为了简便，用一维来举例，那么z的后验分布$q(z|x)$可以看成一个多类分布:4.2.2. 模型模型如下图所示:由于上面提到的argmin操作不可导，所以梯度就无法逆传播到encoder，论文采用了一种straight-through estimator的方法来解决这个问题，所谓straight-through estimator其实就是计算梯度时，忽略它而采用上游得到的梯度，在这里，就是用$z_q(x)$的梯度作为$z_e(x)$的梯度。损失函数的定义如下:损失函数的第一项是重建误差，它用来优化encoder和decoder的参数，由于在计算梯度时采用了straight-through estimator的方法，所以重建误差不涉及embedding space的参数的更新，为了学习embedding space的参数，用L2误差来移动embedding space的特征向量$e_i$，也就是损失函数的第二项，这里的sg指的时stop gradient的操作，意味着这个L2损失只会更新embedding space的参数，不会传递到encoder。除此之外，论文还额外增加了一个commitment loss(损失函数第三项)，其主要目的是约束encoder的输出和embedding space保持一致，$\\beta$的取值对算法的效果不会产生很大的影响，论文中取值0.25关于先验模型prior，在训练VQVAE的主体过程中保持一致，在VQVAE训练完成后，作者为prior训练一个自回归分布p(z)，对于z是图片的情况下，用PixelCNN，对于z是音频的情况下，用WaveNet。VQVAE适用于多种模态的数据。embedding space也叫做codebook。4.2.3. 生成过程VAE的目的是训练完成后, 丢掉encoder, 在prior上直接采样, 再加上decoder就能做生成。如果我们现在独立地采样HxW个z, 然后查表(codebook)得到维度为HxWxD的$z_q(x)$, 那么生成的图片在空间上的每块区域之间几乎就是独立的。因此我们需要让各个z之间有关系, 因此用PixelCNN, 对这些z建立一个autoregressive model: $p(z_1, z_2, z_3, …)=p(z_1)p(z_2|z_1)p(z_3|z_1, z_2)…$4.3. VQVAE2VQVAE2是VQVAE的升级版, 可以生成非常清晰的高分辨率图片. 主要变化就是把VQ-VAE的encoder和decoder进行了分层, bottom层对local feature进行建模, top层对global feature进行建模; 为了让top层能更有效地提取global信息, 在网络中加入了self attention4.4. 其他 知乎上两个讲解很详细的博客: 知乎1 知乎2 VQVAE的特点是可学习的prior和codebook作离散化的设计，影响了很多后续的模型，包括BEiT和DALLE"
    } ,
  
    {
      "title"       : "GAN",
      "category"    : "",
      "tags"        : "note",
      "url"         : "./GAN.html",
      "date"        : "2022-09-27 00:00:00 +0800",
      "description" : "arrange notes",
      "content"     : "目录 目录 1. 博客简介 2. GAN简介 3. Adversarial nets 4. 理论原理 5. 其他1. 博客简介GAN的全称是generative adversarial nets，是Goodfellow于2014年提出的新的生成模型框架，这种全新的生成模型框架有很多应用和变种，这篇博客主要介绍最开始的GAN的原理和论文整理，这里阅读的论文不是最终版(区别在于related work不同)，下面列出一些链接 论文链接 李沐讲解2. GAN简介GAN是一种全新的生成模型框架，它包含两个部分，生成模型G和辨别模型D，G的作用是捕捉数据的分布，D的作用是辨别数据来源于真实数据分布还是G生成的数据分布。生成模型训练过程就是让D犯错的可能性更高。GAN框架其实就是一个minmax game，如果G和D都是MLP的话，那么整个系统可以用逆传播机制训练。GAN的作者举了一个简单的例子介绍模型训练过程，生成模型可以看成印假钞的团伙，辨别模型可以看成警察，双方都在训练中提升自己的能力，最终希望达到的效果是警察无法分辨出一张假钞是真币还是假币。论文只介绍了一种特殊情况，就是G和D都是MLP的情况，作者把这种情况称为Adversarial nets3. Adversarial nets为了让生成模型学习到分布$p_g$(分布尽量和原始数据x的分布一致)，需要定义输入噪音的先验分布$p_z(z)$，$G(z;\\theta_g)$表示噪音z输入生成模型的结果，G是一个可微分的函数，这里是MLP，参数为$\\theta_g$。$D(x;\\theta_d)$表示输入x后的辨别模型的结果，输出是一个标量，表示x来自于真实数据分布的概率。也就是说，D和G的价值函数V(G, D)可表示为:辨别模型D的目标是最大化价值函数的值，D(x)的取值在0-1之间，所以价值函数越大说明辨别模型D的效果越好，生成模型G的目标是最小化价值函数的值。GAN训练生成模型和辨别模型的过程为:绿色的线是生成模型，蓝色虚线是辨别模型，黑色的散点线是原始数据分布4. 理论原理算法原理由下面这一张图片展示:在每个迭代周期的每个批量中，我们有m个取自先验分布的噪音z，其中z$\\sim$ $p_g(z)$和m个取自真实分布的x，其中x$\\sim$ $p_{data}(x)$，先训练辨别器D，沿着梯度上升的方向更新参数，然后在沿着log(1-D(G($z^{(i)}$)))的梯度下降的方向更新参数。接下来介绍了一些命题和证明和一些定理，证实了GAN用到的价值函数和目标函数的可行性5. 其他 GAN在刚提出的时候还是有很多缺点的，比如模型还是比较难训练的，但是后续有很多很多的工作来优化原始的GAN模型，所以GAN更像是抛出了一个引子，让后续模型来优化它 GAN本质上就是左右手互博，目标函数设计的也很好"
    } ,
  
    {
      "title"       : "概率统计",
      "category"    : "",
      "tags"        : "notes",
      "url"         : "./Probability-and-Statistics.html",
      "date"        : "2022-09-27 00:00:00 +0800",
      "description" : "Information about autoencoder series",
      "content"     : "1. 链接 贝叶斯、先验、后验、似然等基础知识2. KL散度2.1. 简介KL散度就是相对熵，是两个概率分布间差异的非对称性度量2.2. 定义设$P(x)$与$Q(x)$是随机变量X上的两个概率分布，则在离散和连续变量的情形下，KL散度的定义分别为:$KL(P||Q)=\\sum P(x)log\\frac{P(x)}{Q(x)}$$KL(P||Q)=\\int P(x)log\\frac{P(x)}{Q(x)}dx$"
    } ,
  
    {
      "title"       : "重要时间点及事件",
      "category"    : "",
      "tags"        : "daily",
      "url"         : "./Important-Event.html",
      "date"        : "2022-09-26 00:00:00 +0800",
      "description" : "for query",
      "content"     : ""
    } ,
  
    {
      "title"       : "技术分享会笔记(关于text2image)",
      "category"    : "",
      "tags"        : "note",
      "url"         : "./%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB%E4%BC%9A%E7%AC%94%E8%AE%B0.html",
      "date"        : "2022-09-19 00:00:00 +0800",
      "description" : "note",
      "content"     : ""
    } ,
  
    {
      "title"       : "T5模型",
      "category"    : "",
      "tags"        : "paper",
      "url"         : "./T5%E6%A8%A1%E5%9E%8B.html",
      "date"        : "2022-09-19 00:00:00 +0800",
      "description" : "arrange notes",
      "content"     : "1. T5简介 2. 读论文 2.1. Introduction 2.2. Setup 2.2.1. Model 2.2.2. THE Colossai Clean Crawled Corpus(C4) 2.2.3. Downstream Tasks 2.2.4. Input and Output Format 2.3. Experiments 2.3.1. Baseline 2.3.1.1. Model 2.3.1.2. Training 2.3.1.3. Vocabulary 2.3.1.4. Unsupervised Objective 2.3.1.5. Baseline Performance 2.3.2. Architectures 2.3.2.1. Model Structures 2.3.2.2. Comparing Different Model Structures 2.3.2.3. Objectives 2.3.2.4. Results 2.3.3. Unsupervised Objectives 2.3.4. Pre-training Data Set 2.3.5. Training Strategy 2.3.6. Scaling 2.3.7. Putting It All Together 2.4. Reflection 3. 个人总结1. T5简介T5的全称是text-to-text transfer transformer，是google于2019年推出的NLP领域的大型预训练模型，T5模型将NLP领域的任务均看成text to text类型，在众多任务的表现十分优异，模型本身的结构就是transformer的encoder-decoder结构，但是预训练目标以及其他细节有所区别相关链接: 论文 github google博客 huggingface文档2. 读论文摘要: 将所有的以文本为基础的语言任务变成text to text格式的任务，论文比较了不同的预训练目标、架构、无标签数据集、迁移方式在NLU任务上的表现。论文还新建了数据集C4，T5模型在很多benchmark上能做到SOTA，包括总结、QA、文本分类等。此外，T5模型和C4数据集均开源2.1. Introduction把所有的文本处理问题看成”text-to-text”问题，也即输入一段文本，输出一段文本。2.2. Setup2.2.1. ModelTransformer架构一开始用于机器翻译任务，自注意力可以看成将一段序列的每个词元替换成其他词元的加权平均。T5模型的架构和Transformer的encoder-decoder结构基本一致，区别在于T5模型去除了层归一偏差，将层归一化放在残差路径外，使用了一种不同的位置嵌入方案。2.2.2. THE Colossai Clean Crawled Corpus(C4)这一部分主要介绍了C4数据集的相关内容。Common Crawl是一个公开的数据集网站，它可以提供从网页爬取的文本，但是这些文本数据存在很多问题，论文提出了以下的几种方法来让数据集更clean: 只保留以终点符号(即句点，感叹号，问号或引号)结尾的行 丢弃少于五个句子的page，只保留超过3个单词的句子 删除任何包含有在List-of-Dirty网站中出现的单词的网页 删除包含Javascript的行 删除出现“lorem ipsum”短语的page 删除所有包含大括号的页面 对数据集进行重复数据删除，当连续的三句话重复出现时只保留一个 使用langdetect工具过滤掉非英文的页面2.2.3. Downstream TasksT5模型为了测量总体的语言学习能力，在很多benchmark上测试性能，比如机器翻译、QA、摘要总结、文本分类。在GLUE和SuperGLUE上测试文本分类能力，在CNN/Daily Mail上测试摘要总结能力，在SQuAD上测试QA能力…2.2.4. Input and Output Format正如在introduction中提及的一样，论文将所有的task看成text-to-text格式。这种框架为预训练和微调提供了一致的训练目标。模型用极大似然目标训练(教师强制)。为了区分不同任务，给input前加上task-specific前缀。比如为英翻德加上前缀“translate English to German: ”，论文附录里有各种任务的前缀与相关处理方法。2.3. Experiments论文搭建模型的出发点是比较不同的预训练目标、模型架构、无标签数据集等方面，从中选择表现最好的部分组成T5模型。每次只改变baseline的一部分，其余部分保持不变。BERT不太好做生成任务，比如机器翻译和摘要总结2.3.1. Baseline也即基准2.3.1.1. Model模型选用Transformer的Encoder-Decoder架构，相比于只使用Encoder来说，该架构在分类和生成任务上取得更好的效果2.3.1.2. Training所有的任务都是text-to-text类型，这让作者能用极大似然法和交叉熵损失来训练模型，优化器选择AdaFactor。在测试阶段，选用概率最高的词元作为输出。在预训练阶段，采用逆平方根学习率策略，即学习率会随着迭代周期下降。预训练阶段，模型迭代524288步。在微调阶段，模型迭代262144步，同时使用固定的学习率。2.3.1.3. Vocabulary由于模型任务包含了翻译任务，所以词表不仅包含了英语词汇，还包括德语、法语和罗马尼亚语词汇。词表是预定义的，所以模型输出不会出现超出词表的词汇2.3.1.4. Unsupervised Objective模型预训练过程需要无标签的数据。过往的预训练模型训练过程都采用masked language modeling(denosing objectives)作为预训练目标，大家发现这种处理方式能取得很好的结果。对于去噪目标，模型需要预测被遮掩的词元。借鉴于BERT的经验，模型随机采样并选择丢弃了15%的词元(作为masked)，并且连续的掩蔽词元只被一个sentinel词元替代。下面展示了一个掩蔽的例子2.3.1.5. Baseline Performance展示了baseline模型在不同benchmark上的表现，不同的benchmark使用不同的指标2.3.2. Architectures比较不同框架在benchmark上的表现2.3.2.1. Model Structures作者选择了三种不同的架构进行对比，第一种架构是传统的Transformer的encoder-decoder架构，第二种是language modeling(encoder)架构，BERT用的就是这个架构，下一步的输出依赖于前一步的预测，第三种是Prefix Language Model，为text-to-text任务提供任务的前缀，比如翻译任务就是加上前缀translate English to German:2.3.2.2. Comparing Different Model Structures比较了不同模型的层数，参数和FLOPS2.3.2.3. Objectives除了架构的区别外，还比较了不同预训练目标带来的区别，比如使用Denosing Objectives时，LM架构需要把输入和输出连接起来进行连续的预测，使用LM目标时，LM架构需要从头预测到尾2.3.2.4. Results直接看表格，可以发现第一种encoder-decoder架构的表现最好2.3.3. Unsupervised Objectives本章从以下几个角度比较Unsupervised Objectives，实验得出结论，选取BERT-style，Corruption Strategies选择Replace spans，Corruption rate选择15%，Corrupted span length选择对每个词元都决定是否corrupted(独立)，也即i.i.d.，这样得出的效果最好2.3.4. Pre-training Data Set最终选择了全size的C4数据集作为预训练数据集2.3.5. Training Strategy比较了微调的不同方案、比较了多任务同时训练和单任务训练的效果，最终发现baseline的效果最好，即预训练加下游任务微调2.3.6. Scaling尝试了扩大模型规模的几种方式，最后发现baseline选择的预训练规模是最合适的，使用较大的模型可能会使下游的微调和推断变得更加昂贵2.3.7. Putting It All Together这一部分介绍了模型最终的一些调整内容 预训练目标: 掩蔽片段平均长度为3，同时掩蔽比率为15% 更长的训练过程: C4数据集够大，让训练过程可以不用重复数据，因此增加批量大小、增加训练步数会更好 模型大小: 有好几个版本的T5模型，Base、Small、Large、3B and 11B 多任务预训练: 使用多任务预训练会为下游任务带来好处展示一下最终的效果2.4. Reflection这一部分总结了模型的创新部分，同时提出了模型的缺点以及展望3. 个人总结T5模型是google继bert之后推出的一个大型预训练模型，先说说T5模型的特点，T5模型的架构是transformer的encoder-decoder架构，预训练数据集选用google自制的C4数据集，数据集也相当大，作者希望做出一个大统一的预训练模型，所以采用text-to-text任务类型也是它的一大特点，具体来说就是把所有的NLP任务变成输入一段文本，模型输出一段文本的形式，模型的预训练目标也很有特色，采用了类似bert的掩蔽预训练目标。论文做了很多很多很贵的实验，对比了很多方面，最后得到了这个模型，论文里的实验都说明的很详细，同时它也刷了很多榜，比如GLUE等，效果是比之前的预训练模型都好，是google财大气粗的表现。模型在github上开源，在tensorflow上可以直接实现"
    } ,
  
    {
      "title"       : "动手学深度学习",
      "category"    : "",
      "tags"        : "note",
      "url"         : "./%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html",
      "date"        : "2022-09-05 00:00:00 +0800",
      "description" : "d2l note",
      "content"     : "0. 简介 1. 预备知识 2. 线性神经网络 3. 多层感知机 5. 卷积神经网络 6. 现代卷积神经网络 7. 循环神经网络 8. 现代循环神经网络 9. 注意力机制 12. 计算机视觉 13. 自然语言处理: 预训练 14. 自然语言处理: 应用0. 简介 《动手学深度学习》的笔记 各种链接: bilibili book_zh book_en 这本书笼统的介绍了深度学习所需要的各种知识，从线性神经网络开始讲起，然后到CNN，最后到RNN，介绍了CV和NLP领域的较新的网络结构。同时这本书不止有理论内容，每一小节都有代码实践内容，可以边写代码边了解知识，同时bilibili上有李沐老师的网课配合学习，很适合初学者进行学习。1. 预备知识这一章主要介绍了深度学习的一些前置知识，这里对比较重要的点做备注 张量(Tensor)包含了一维张量(向量)和二维张量(矩阵) torch中A*B是哈达玛积，表示矩阵元素按元素相乘 torch.dot()是点积 torch.cat(…, dim=0)表示在行上延伸，比如(3, 4)和(3, 4)变成(6, 4) A.sum(axis=0)表示把每一列的数据都相加，比如(5, 4)变成(4) 范数是norm，L1范数为每个元素的绝对值相加，L2范数为元素的平方和开根号，torch中默认L2范数，一般也是L2范数用的最多 梯度: 连接多元函数的所有偏导数: 梯度是一个向量 常用的梯度计算公式: torch中自动求导的步骤: 第一步 为x分配内存空间: x.requires_grad_(True) 第二步 链式反向传播，希望求哪个函数的梯度，就对那个函数反向传播，比如y.backward() 第三步 求x的梯度，x.grad，如果我们需要重新求梯度，需要清零梯度，x.grad.zero_() 注意torch中只能对标量输出求梯度，所以常见操作是sum 标量对向量的偏导是向量，向量对向量的偏导是矩阵 贝叶斯公式: P(A|B)P(B)=P(B|A)P(A)2. 线性神经网络本章主要介绍了线性回归网络和softmax回归网络，接下来是一些笔记 随机梯度下降和梯度下降的区别: 梯度下降一般而言是针对所有的样本而言，而随机梯度下降是针对单个样本而言，同样地，小批量随机梯度下降是针对一个批量的样本而言 可以调整但是在训练过程中不更新的参数叫做超参数 极大似然法: $\\theta$是需要估计的值，在写似然函数时只需要把$\\theta$看成参数，最大化似然函数即$\\theta$的估计值 每个输入与每个输出相连的层成为全连接层 with torch.no_grad()的作用是让输出结果之后不构建计算图 本章的训练过程: 计算y的预测值-&gt;计算损失函数-&gt;累加loss并反向传播(记得每个批量在梯度更新前需要清零梯度并反向传播loss)-&gt;更新参数 训练过程中重要的组成部分: 数据迭代器、损失函数、优化器(updater/trainer)、网络(记得初始化参数) softmax为分类服务，softmax本质上是将输出规范成概率数值，方便选取预测概率最大的类作为预测类: 分类的标签可以用独热编码定义 网络模型用nn.sequential()定义 softmax回归的损失函数可以用极大似然法推出，普通的极大似然法是最大化似然函数，但是在这里我们加上-log就变成了最小化损失函数 softmax回归的损失函数是交叉熵损失:3. 多层感知机本小节主要介绍了多层感知机的实现以及面对各种问题的解决方法，比如解决过拟合的权重衰退(weight decay)和暂退法(dropout)，解决梯度爆炸与消失的Xavier初始化。 激活函数的作用是将线性网络变成非线性，常见的有ReLU、Sigmoid、tanh ReLU: max(x, 0) Sigmoid: $\\frac{1}{1+e^{-x}}$ tanh: $\\frac{1-e^{-2x}}{1+e^{-2x}}$ 在torch中可以用@来简单表示矩阵乘法 用nn.Sequential()来实例化网络时，nn.ReLU()单独算一层 过拟合问题可以用正则化技术解决，比如权重衰退 权重衰退就是L2正则化，它在计算损失函数时增加了权重的惩罚项，比如L($\\omega$, b)+$\\frac{\\lambda}{2}$||$\\omega$||，其中$\\lambda$是超参数 torch框架中把权重衰退放在优化器的实例化中(torch.optim)，只需要将weight_decay的超参数输入即可 暂退法(Dropout): 在前向传播中，计算每一内部层的同时注入噪音，就好像在训练过程中丢弃了一些神经元 中间层活性值: 只有在训练过程中才有权重衰退和暂退法 在torch中简单实现dropout的方法: 在构建net时将nn.Dropout(dropout)加入nn.Sequential()，其中dropout作为丢弃概率输入Dropout中 网络架构顺序: linear-&gt;relu-&gt;dropout torch中实现tensor对tensor求梯度的方法是在backward()里面加入torch.ones_like() 不正常的参数初始化可能会导致梯度爆炸和梯度消失 Xavier初始化是解决梯度爆炸和消失的好手段5. 卷积神经网络这章主要介绍了CNN的基础知识，包括卷积计算以及汇聚层和简单的卷积神经网络LeNet 卷积运算即互相关运算，卷积核函数沿着输入矩阵滑动计算，一般的卷积层除了核运算外，还需要加上偏置 二维卷积层的输入格式: (批量大小, 通道数, 高, 宽)，卷积层又被称为特征映射(feature map) 感受野(Receptive Field)的定义是卷积神经网络每一层输出的特征图上的像素点在输入图片上映射的区域大小，也就是一个像素点对应的上一层图像的区域大小 填充(padding)与步幅(stride): 填充的作用是在输入图像的边界填充元素(通常为0)，添加$p_h$行与$p_w$列，基本是一半在左一半在右 一般在定义卷积层nn.Convd()时可以加上填充与步幅 步幅包括垂直步幅$S_h$与水平步幅$S_w$ 在经过卷积层后，二维图像变成了$[(n_h - k_h + p_h + 1)/S_h, (n_w - k_w + p_w + 1)/S_w]$ 多通道输入，只需把各通道输出结果加起来即可 多通道输出，为每个输出通道创建一个卷积核函数$(c_i, k_h, k_w)$，假设输入通道个数$c_i$，输出通道个数为$c_o$，那么卷积核形状为$(c_o, c_i, k_h, k_w)$ torch.stack(): 沿一个新维度对输入张量进行连接 汇聚层(pooling)包括最大汇聚层和平均汇聚层，汇聚层是直接返回输入图像的一个小窗口的最大值或者平均值 汇聚层没有可学习的参数 汇聚层同样有填充与步幅，默认情况下步幅与窗口大小相同，nn.MaxPool2d() 每个卷积块的基本单元是: 卷积层-&gt;激活函数-&gt;汇聚层 nn.Conv2d(1, 6, kernel_size=5)其中1表示输入通道数，6表示输出通道数 在CNN的最后都需要连接全连接层来变成预测类别 在训练过程中如果想好好利用GPU，那么需要将网络的参数与数据集数据传入GPU，具体方法是net.to(device)、X,y.to(device) 多输入多输出通道的图片示例:6. 现代卷积神经网络这一节主要介绍了CNN的各种网络的发展历程，LeNet之后，于2012年出现深度CNN网络AlexNet，之后出现了NiN与VGG，然后是GoogleNet，之后有很大进步的网络是ResNet，直到现在，ResNet用的也很多，残差思想也持续影响后续模型的搭建 促进CV有更深的网络的两大关键因素: 数据与硬件(主要是GPU) AlexNet(深度卷积网络)于2012年ImageNet挑战赛上夺冠，第一次学习到的特征超越了手工设计的特征 相比于LeNet而言，AlexNet更深、激活函数用ReLU、对训练数据进行了增广 VGG(使用块的网络): VGG块由一系列卷积层(包含ReLU)+汇聚层组成，VGG网络由VGG块组成 NiN(网络中的网络): NiN块结构是卷积层(含ReLU)+两个1x1卷积层(相当于全连接层)，NiN网络结构是: NiN块+汇聚层+NiN块+汇聚层+…+平均汇聚层 GoogleNet(含并行连结的网络)是google花费了很多money实验出的网络，特点是参数值特殊，参数以及网络结构都是经过了很多实验得出的结果 GoogleNet中基本的卷积块被称为Inception块，Inception块的架构如下图所示: GoogleNet架构: 卷积块+Inception块+最大汇聚层+Inception块+最大汇聚层+Inception块+平均汇聚层+全连接层 批量规范化(Batch Normalization)是一种trick，可加速深层网络的收敛速度 正则化在深度学习中非常重要 对于一个批量来说，首先规范化输入(减去其均值并除以标准差)，再应用比例系数与比例偏移，就是对当前批量进行了批量规范化上述式子中x是输入，$\\hat{\\mu}_B$是这个批量的均值，$\\hat{\\sigma}_B$是批量的标准差，$\\gamma$是比例系数，$\\beta$是比例偏移，$\\gamma$与$\\beta$与x的形状相同，是需要学习的参数 应用于全连接层的BN: $h=\\Phi(BN(Wx+b))$ 应用于卷积层的BN: 在每个输出通道的m*p*q个元素上同时执行BN 可以发现，BN的作用位置为权重层后，激活函数前 BN在训练和预测时有所不同，在预测时，直接使用模型传入的移动平均所得的均值与方差 用pytorch架构简单实现BN: nn.BatchNorm2d(通道数) 直观地说，BN可以使优化更加平滑 残差网络ResNet于2015年在ImageNet上夺冠 残差思想: 每个附加层都应该更容易地包含原始函数作为其元素之一，残差块不是为了学习输出f(x)，而是学习输出与输入的差别f(x)-x 残差块的架构: ResNet的架构: 和GoogleNet很像，就是Inception变成了残差块，同时多了BN 稠密链接网络DenseNet: 是ResNet的继承，DenseNet的输出是连结，而不是如ResNet那样的简单相加7. 循环神经网络这小节主要介绍了文本数据集如何制作，RNN的网络结构与实现 序列数据就是与时间相关的数据 马尔可夫模型: 用定时间跨度的观测序列预测$x_t$ $P(x_1,…,x_T)=\\prod_{t=1}^TP(x_t|x_{t-1},…,x_{t-\\tau})$ 一些名词: 文本序列、词元(token)、词表(vocabulary)、语料(corpus) 文本预处理过程: 读取数据集成列表-&gt;将列表词元化，变成包含多行的词元列表-&gt;构建词表(词表将词元与数字对应) 文本预处理中的词元可以是单词，也可以是字符，这里采用字符 语言模型(language model)的目标是估计联合概率$P(x_1,…,x_T)$ 涉及一个、两个、三个变量的概率公式分别被称为一元语法、二元语法、三元语法 zip()的作用是将可迭代对象打包成一个个元组，然后返回元组组成的列表 构建文本序列数据集的两种方法: 随机采样: 随机选取，特征是原始序列，标签是原始序列右移一位 顺序分区: 保证每个批量中子序列再原语料中相邻 相比与马尔可夫模型，隐变量模型更能体现过往序列的影响: $P(x_t|x_{t-1},…,x_1)=P(x_t|h_{t-1})$ RNN的示意图以及推导公式: 循环神经网络中循环的是H(Hidden state) 度量语言模型的质量的性能度量是困惑度(perplexity): 一个序列中n个词元的交叉熵损失来衡量语言模型的质量 最好的情况下，困惑度为1，最差的情况下，困惑度为无穷大 独热编码将(批量大小, 时间步数)转变成(批量大小, 时间步数, 词表大小)，但为了方便计算，最终转变成(时间步数, 批量大小, 词表大小) 梯度裁剪的作用是保证梯度不会爆炸 RNN的网络结构与之前差别不大，只是在更新梯度前需要进行梯度裁剪 隐藏状态形状: (隐藏层个数, 批量大小, 隐层参数个数) nn.RNN()返回的Y为隐层参数个数，需要再加上全连接层8. 现代循环神经网络这一章介绍了拥有记忆单元的LSTM模型，以及后续新的NLP任务机器翻译，介绍了数据集处理过程和编码器解码器结构的网络seq2seq，用来处理序列转换任务 长短期记忆网络LSTM(long short term memory) LSTM相较于普通的RNN多了很多元素，最主要的设计是记忆单元，它可以影响下一步的隐藏状态: 输入、输出、遗忘门均与$H_{t-1}$和$X_t$有关 记忆单元C类似于隐状态，时时更新 总的来说，LSTM中$H_t$与$H_{t-1}$、$X_t$、$C_t$都有关 RNN的延伸: 多层与双向RNN，其中多层很好理解，就是把单向隐藏层的神经网络变成多层，双向的作用是让序列用到上下文信息，在预测下一个词元的任务中双向RNN表现不佳，但是在NER中表现很好 nn.LSTM(num_inputs, num_hiddens, num_layers) 接下来的内容变成了机器翻译任务(序列转换) 机器翻译中使用单词级词元化 机器翻译数据集处理过程: 读取数据集-&gt;词元化列表-&gt;将数据集分割成source(源语言)与target(目标语言)-&gt;序列末端加上&lt;eos&gt;，同时针对长短不一的序列填充&lt;pad&gt;与截断 处理序列转换任务可以用编码器-解码器结构 编码器的作用是将长短可变序列变成固定形状的状态，解码器的作用是将固定形状的状态变成长度可变序列 编码器为解码器输入一个状态，在seq2seq中是编码器编码过程中的隐状态，这个隐状态既作为解码器的初始state，在每个时间步中也作为上下文变量和输入concatenate之后一起输入解码器 采用嵌入层将词元进行向量化，嵌入层是一个矩阵，(词表大小，特征向量维度) 编码器与解码器是两个GRU permute()可以改变张量维度的位置 rnn()的输入形状一般为(num_steps, batch_size, embed_size) 解码器的最后同样需要一个全连接层输出 解码器的第一个输入为&lt;bos&gt; 由于序列存在很多&lt;pad&gt;，计算损失时不能计算pad那一部分，可以mask这一部分，所以损失函数需要重新改一下 在seq2seq训练时，解码器net的输入为cat(&lt;bos&gt;，真实序列少一时间步)，这种训练机制叫做强制教学 预测的时候解码器net的输入仅为&lt;bos&gt;，用每一步的预测作为下一步的输入 机器翻译的性能度量为BLEU(bilingual evaluation understudy)，可用来预测输出序列的质量，当预测序列与标签序列完全相同时，BLEU为1，公式如下: 编码器的功能主要是为解码器提供上下文变量c和解码器的初始隐状态9. 注意力机制这章主要介绍了注意力机制，介绍了注意力机制的组成部分，比如查询、键、值、评分函数，后面又介绍了与RNN结合的Bahdanau注意力以及自注意力和多头注意力，最后介绍了transformer 注意力机制的主要成分是查询(query)、键(key)、值(value)，q和k交互形成注意力权重，然后与v相乘得到注意力汇聚结果 注意力汇聚结果计算公式:其中x是查询，$x_i$是key，$y_i$是value，$\\alpha$的作用是将x与$x_i$之间的关系建模，且权重总和为1，有点像softmax unsqueeze()的作用是在指定位置添加一个维度，squeeze()的作用是在指定位置删除一个维度，torch.bmm()是批量矩阵乘法 评分函数a同样是对q和k的关系进行建模，q、k、v都可以是向量，而且长度可以不同 这里介绍了两种评分函数: 加性注意力和缩放点积注意力 加性注意力: 可以处理长度不同的q与k 缩放点积注意力(计算效率高): 要求q与k长度相同 Bahdanau注意力模型也是编码器解码器结构，与之前的seq2seq不同，这里的上下文变量在解码器的每一步都不相同，上下文变量$c_{t’}$与解码器的上一步隐状态有关，同时在解码器和编码器的输入位置都有嵌入层 多头注意力: 对q、k、v使用线性变换得到h组不同的q-k-v来输入h个注意力汇聚层，得到h个输出，这h个输出再线性变换得到最终输出 自注意力就是q-k-v都是相同的一组元素 自注意力无法使用序列的位置信息，可以给输入concatenate一个位置编码，比如X∈$R^{nxd}$表示n个词元的d维嵌入，P∈$R^{nxd}$表示位置嵌入矩阵，那么X+P即输入，位置编码可以基于正弦函数和余弦函数的固定位置编码 transformer模型与Bahdanau模型不同，它完全基于注意力机制来构建模型 transformer每块都由多头注意力和基于位置的前馈神经网络组成，其中还有残差连接，即x+sublayer(x)，再层规范化(层规范化和批量规范化类似，都是使得网络中每层输入数据的分布相对稳定，加速模型学习，批量规范化是一组输入数据对一个神经元而言，层规范化是一个输入数据对多个神经元而言)。在解码器的注意力层中，q是上个解码器层的输出，k和v是编码器输出(每个源序列的位置的编码代表一个键值对)。基于位置的前馈神经网络，简称ffn，即两层MLP12. 计算机视觉这一章主要介绍了计算机视觉领域的两大任务: 目标检测于语义分割。目标检测从boundingbox讲起，然后介绍如何生成锚框和锚框的标签，然后介绍了SSD、RCNN等实现目标检测的网络，语义分割主要介绍了定义以及网络，最后介绍了风格迁移的一种实现方法 就目前的趋势来说，数据集越大，模型的表现效果越好 图像增广的作用是扩大数据集的规模，图像增广在对训练图像进行一系列的随机变换后，生成相似但不同的训练样本 常见的图像增广的方法有两种: 翻转和裁剪: 降低模型对目标位置的敏感性 改变颜色(这里指改变RGB): 亮度、饱和度、对比度 torchvision.transforms.RandomVerticalFlip() 上下翻转 torchvision.transforms.RandomHorizontalFlip() 左右翻转 torchvision.transforms.RandomResizedCrop() 裁剪，但最终的图片大小都相同 torchvision.transforms.ColorJitter() 微调(fine-tune)约等于迁移学习(transfer learning)，从源数据集学到的知识迁移到目标数据集 源模型的输出层从头训练，其余层微调，技巧就是让输出层的学习率变大 torchvision.datasets里包含很多常用的数据集 torchvision.models里包含很多常用的模型和预训练模型 数据集实例化后一般还需要load到内存中变成iterator 图像分类只是基础的CV任务，接下来介绍新的CV任务: 目标检测 目标检测(object detection/recognition): 不仅想知道类别，还想知道它们在图像中具体位置 边界框(bounding box)有两种表示方法: 由矩形左上角及右下角的横纵坐标决定(corner表示法) 用中心点+高宽表示(center表示法) 对于一张图像来说，左上角的点为原点，向右为x轴正方向，向下为y轴正方向 锚框(anchor box): 为了预测目标位置，在输入图像中采样大量的区域，然后判断这些区域中是否包含感兴趣的目标，这些区域就是锚框 这一节主要关注采样方法，也就是锚框的生成方法，以每个像素为中心，生成多个缩放比和宽高比的boundingbox，这些边界框被称为锚框 torch.meshgrid()的作用是让输入的两个向量变成网格状矩阵，一个作为x轴坐标，另一个作为y轴坐标 交互比IoU: 量化锚框与真实boundingbox的相似性，杰卡德系数的定义如下:$J(A, B)=\\frac{\\|A\\cap B\\|}{\\|A\\cup B\\|}$对于两个bbox而言，将它们的杰卡德系数称为交互比IoU(intersection over union) 如何利用训练集中的标签标注锚框(标注锚框的类别和偏移量): 数据集中含有真实边界框的位置及类别，对于任意生成的锚框而言，可以利用真实边界框的信息去标注锚框，得到锚框的类别和偏移量(offset)，其中一种将真实边界框的类别分配给锚框的算法:简单来说就是先找IoU值最大的一组真实边界框和锚框，然后删除它的行和列，直到真实边界框分配完，然后遍历没有分配的锚框的行，找到IoU最大的真实边界框，然后判断是否大于阈值，只有大于阈值时，才将真实边界框分配给该锚框 除了显而易见的锚框的类别，还需要标注锚框的偏移量，偏移量即该锚框与真实边界框的偏移，可以用下面这个式子来表示:其中$\\mu$和$\\sigma$都是常量 如果一个锚框没有被分配真实边界框，则将它标记为background(即负类锚框) 在生成锚框的时候，还会返回掩码，掩码的作用是去掉不关心的负类锚框的偏移量 当有许多锚框时，会输出许多相似但有明显重叠的预测边界框，为了简化计算，用非极大值抑制(non-maxmum suppression, NMS)来合并属于同一目标的类似预测边界框。对于一个预测边界框B，目标检测模型会计算每个类别的预测概率。假设最大的预测概率为p，则该概率所对应的类别B即为预测的类别。具体来说，我们将p称为预测边界框B的置信度(confidence)。在同一张图像中，所有预测的非背景边界框都按置信度降序排序，以生成列表L。然后我们通过以下步骤操作排序列表L: 从L中选取置信度最高的预测边界框$B_1$作为基准，然后将所有与$B_1$的IoU超过预定阈值$\\epsilon$的非基准预测边界框从L中移除。这时，L保留了置信度最高的预测边界框，去除了与其太过相似的其他预测边界框。简而言之，那些具有非极大值置信度的边界框被抑制了 从L中选取置信度第二高的预测边界框$B_2$作为又一个基准，然后将所有与$B_2$的IoU大于$\\epsilon$的非基准预测边界框从L中移除 重复上述过程，直到L中的所有预测边界框都曾被用作基准。此时，L中任意一对预测边界框的IoU都小于阈值$\\epsilon$；因此，没有一对边界框过于相似 输出列表L中的所有预测边界框 由于逐个像素生成锚框太多了，可以有针对性地多尺度生成锚框，检测较小物体时，用小锚框，采样更多的区域，检测较大物体时，用大锚框，采样更少的区域 目标检测数据集领域里，没有小型数据集，所以李沐团队自行标注了1000张大小角度不同的banana图像，然后在一些背景图片上的随机位置放一张香蕉的图片，然后为这些图像标记边界框和类别 batch[0]图片(x): (批量大小，通道数，h，w) batch[1]标签(y): (批量大小，m，5) 上面式子中m表示所有图片中最多可能出现的锚框数，如果一张图片的锚框数少于m个，用非法锚框填充(类别为-1) 5表示边界框的信息，分别为类别, $x_min$, $y_min$, $x_max$, $y_max$ 单发多框检测SSD(single-shot detection)的模型如下图所示: 模型由基础网络(比如VGG、ResNet等等)和多尺度特征块组成，基础网络的输出将高宽扩大，这样就可以生成更多的锚框(可用于检测尺寸较小的物体)，然后后续的多尺度特征块将高宽缩小，这样便可以实现多尺度的目标检测，每一层都预测每个锚框的类别与offset，接下来介绍一下类别预测层和offset预测层 类别预测层: 用卷积层的通道来输出类别预测的方法，比如一个特征图像素上有a个锚框，则输出a(q+1)个通道 offset预测层: 与类别预测层的设计思路类似，但每个锚框预测4个偏移量 可以通过合并多个通道的预测来实现不同尺度预测的简化 网络记录每个块的前向输出Y，生成的锚框anchors以及类别预测结果cls(Y)，offset预测bbox(Y)，最后合并不同层的anchors(Y)、cls(Y)、bbox(Y) 这个网络的损失函数为: 有关锚框类别的损失(用交叉熵)，有关正类偏移量的损失(回归问题，用L1范数损失)，掩码变量可以让负类锚框与填充锚框不参与计算 训练时，真实值来源于Y的标签信息为每个锚框生成的类别和offset，然后用预测出来的锚框的类别和offset进行损失函数的计算 预测阶段，输入x，得到anchors，cls_pred，bbox_pred，然后用NMS来去掉相似的锚框，得到最终的预测结果，output的形状: (批量大小, 锚框个数, 6)，6代表类别、概率、bbox的四个坐标 接下来介绍目标检测的另一类算法: 区域卷积神经网络R-CNN系列(Region-based CNN)，模型如下图所示: RCNN的四个步骤: 对输入图像选取多个高质量的提议区域，标注其类别和真实bbox 选择一个预训练的卷积神经网络，并将其在输出层之前截断。将每个提议区域变形为网络需要的输入尺寸，并通过前向传播输出抽取的提议区域特征 将每个提议区域的特征连同其标注的类别作为一个样本。训练多个支持向量机对目标分类，其中每个支持向量机用来判断样本是否属于某一个类别 将每个提议区域的特征连同其标注的边界框作为一个样本，训练线性回归模型来预测真实边界框 庞大的计算量使R-CNN在现实中难以广泛应用，于是出现了Fast R-CNN:与R-CNN的区别在于，Fast R-CNN在整张图上执行CNN前向传播来抽取特征 还有Faster R-CNN:与Fast R-CNN区别在于: 将选择性搜索替换成区域提议网络 接下来介绍语义分割任务: 语义分割(semantic segmentation)就是将图像分割成属于不同语义类别的区域(像素级) 语义分割数据集: Pascal VOC 2012 feature x: 图像 label y: 尺寸与图像相同，但是每个像素的RGB值表示它们所属的类别，类别相同的颜色也相同 转置卷积(上采样): 与卷积核运算相反(卷积核运算是用核窗口滑动输入图像)转置卷积是输入图像的每个像素的核函数上滑动，而卷积核运算时用核窗口在输入图像上滑动，转置卷积相当于使图像的高宽变大 转置卷积可以通过nn.ConvTranspose2d()简单实现 接下来介绍解决语义分割的网络: 全卷积网络(fully convolutional network, FCN) 语义分割本质上就是对图像的每个像素进行分类，FCN实现了像素到像素的变换，FCN将中间层特征图的高宽变换回输入图像的尺寸(用转置卷积实现)，输出的类别预测与输入图像在像素级别上有一一对应的关系 FCN模型结构如下图:FCN先用卷积神经网络(可以用ResNet)抽取图像特征，然后通过1x1卷积层将通道变换为类别个数，最后通过转置卷积将特征图的高宽变换为输入图像的尺寸 转置卷积是一种上采样(upsampling)方法，双线性插值也是上采样方法之一，可以用转置卷积实现双线性插值(通过改变核函数) 风格迁移(style transfer): 将一个图像中的风格应用于另一个图像上，即风格迁移，输入是一张内容图像(content)和一张风格图像(style)从content中用预训练模型抽取图像的内容特征(不更新参数)，从style图像中抽取风格特征，损失函数的设计很有意思13. 自然语言处理: 预训练这一章主要介绍了NLP领域的预训练模型，NLP领域的预训练模型都是encoder，即用文本特征来表示词元(一般都是单词)，首先介绍了word2vec，然后介绍了全局向量的词嵌入，之后介绍了子词嵌入模型fastText与字节对编码(BPE)，之后介绍了BERT(双向Transformer编码器) 在介绍RNN模型时，介绍了用独热向量来表示词元，但是这有个很严重的缺点: 不同词的独热向量的余弦相似度为0。所以接下来会介绍很多词嵌入模型，即用一个词向量来表示单词 word2vec: 将词映射到固定长度的向量，这里介绍了两种模型: 跳元模型(skip-gram)与连续词袋CBOW 跳元模型: 假设一个词可以用来在文本序列中生成其周围的词，对于每个索引为i的单词，可以用$u_i$与$v_i$分别表示其作为上下文词和中心词的向量，可以用softmax对生成概率进行建模，对于给定中心词$w_c$，生成上下文词$w_o$的概率为: 那么跳元模型的似然函数为(上下文窗口大小为m): 然后通过极大似然估计法来训练 连续词袋: 与跳元模型相反，CBOW是基于上下文词生成中心词，连续词袋模型用$v_i$和$u_i$分别表示一个词的上下文词向量与中心词向量(与跳元模型相反)，同样用softmax建模(上下文词向量相加): 连续词袋模型的似然函数: 由于词表过大，使用softmax来建模的话计算成本过大，可采用两种近似训练办法来优化: 负采样与分层softmax 负采样建模: 直接用内积加上激活函数来表示概率，负采样即在似然函数中加上负例(从预定义分布中采样噪声词) 层序softmax: 用二叉树来表示概率模型，同样使用了激活函数sigmoid，时间复杂度变低 接下来用负采样跳元模型训练(自监督训练)来展示word2vec的效果，数据集用PTB，语料库取自华尔街时报。数据集处理时用到了下采样方法: 高频词有概率被丢弃:上述式子中f($w_i$)是词在整个语料库中出现的比率，t是超参数。这样高频词就不会太影响模型效果，毕竟不太关注类似a和the与其他词共同出现的概率 在下采样与负采样完毕后，一个小批量中第i个样本包括中心词及其$n_i$个上下文词和$m_i$个噪声词，数据集还需要返回mask与label，分别用来遮掩&lt;pad&gt;与标记正例 word2vec本质上其实是训练一个权重矩阵(词表大小, 嵌入维度)，就是一个nn.Embedding()。跳元模型的前向传播就是计算内积矩阵torch.bmm(embed_v, embed_u)损失函数是带掩码的交叉熵损失 跳元模型在预训练完毕后，可以用来找出语义相似的单词，即计算中心词与其余所有词的余弦相似度，计算结果最高的词即为语义最相似的单词 无论是word2vec的哪个模型，都着眼于中心词与上下文词的关系 全局向量的词嵌入GloVe: word2vec只考虑了局部的上下文词，GloVe则考虑了全局语料库统计来设计模型，训练GloVe是为了降低以下损失函数:上述式子中，$x_{ij}$是中心词i与上下文词j在一个上下文窗口出现的次数，h($x_{ij}$)是每个损失项的权重，当x小于c时，h(x)=$\\frac{x}{c}^\\alpha$，当x大于c时h(x)=1，$b_i$与$c_j$是可学习的偏置 子词嵌入模型: 对词的内部结构进行研究(比如dog和dogs的关系) fastText模型: 每个中心词由其子词的向量之和表示(子词就是单词的某些连续字符) 其余部分与跳元模型相同 字节对编码(Byte Pair Encoding, BPE): 一种算法，用来提取子词。BPE的本质就是对训练数据集进行统计分析，找出单词的公共符号，这些公共符号将作为划分子词的依据，对于每个单词，都将返回最长的子词划分结果(这样就可以获得任意长度的子词) 从大型语料库中训练的词向量可用于下游的自然语言处理任务，预训练的词向量可应用到词的类比性和相似性任务中，比如GloVe和fastText torch.topk(k)的作用是返回列表中的最大值(前k个) 词相似: 利用余弦相似度 词类比: a:b::c:d，比如man:woman::son:daughter，词类比任务就是给出a、b、c，找到d，即让d的词向量尽量靠近vec(c)+vec(b)-vec(a) word2vec于GloVe都是上下文无关的，即对于一个词元编码，只需要输入该词元即可 NLP的六种任务: 情感分析、自然语言推断、语义角色标注、共指消解，NER和QA GPT的缺点: 自回归，单向 BERT使用双向Transformer编码器来编码文本，BERT同样是预训练模型，可以基于双向上下文来表示任意词元 针对不同的上下文任务，BERT需要对架构进行微调 BERT可输入单个文本，也可以输入文本序列对，当输入单个文本时，BERT的输入序列是&lt;cls&gt;文本&lt;sep&gt;；当输入文本对时，BERT的输入序列是&lt;cls&gt;文本1&lt;sep&gt;文本2&lt;sep&gt; 在BERT中，有三个嵌入层，分别是词元嵌入(普通的embedding)、段嵌入(两个片段序列的输入，用来区分不同的句子)和位置嵌入(可学习)，之后再把结果输入encoder中 BERT通过两个预训练任务来优化双向Transformer编码器，分别是掩蔽语言模型(masked language modeling 即填空)和下一句预测(next sentence predicition) 掩遮语言模型: 随机掩蔽词元并使用来自双向上下文的词元以自监督的方式预测掩蔽词元。预测阶段使用单隐藏层的MLP来进行预测，输入BERT的编码结果和用于预测词元的位置，然后根据预测词元的位置得到预测词元的编码结果，然后输入mlp中得到预测结果 下一句预测: 二分类任务，判断文本序列对是否是连续句子，预测时同样使用MLP，输入编码后的&lt;cls&gt;词元 这两个任务在制作数据集的时候都需要加上一些负例或者噪声 BERT的预训练数据集有很多(针对不同的应用领域，使用不同的数据集进行训练)，最开始使用的是图书语料库和wiki BERT本质上就是一个双向Transformer编码器14. 自然语言处理: 应用这一章主要介绍了如何将自然语言预处理模型应用到下游任务中，首先是传统的GloVe模型和子词嵌入模型，针对这种预训练模型，需要设计特定的网络结构在适配任务，但是BERT的出现，让下游任务应用更简单，有时候只需要加一个全连接层就行，参数也只需要微调，这一章主要介绍了两个下游任务，分别是情感分析和自然语言推断 情感分析任务(sentiment analysis): 本质上就是文本序列分类任务，数据集采用Imdb的电影评论集(评论+情感) 处理长短不一的序列时，使用截断与填充来预处理数据集，可以将其变成长短一致的序列 一般的预训练模型应用于下游任务的方式都是: 预训练模型(词元的文本表示embed)+架构(网络)+应用(各种任务) 这里介绍了两种非BERT的架构，分别是用双向LSTM和CNN来处理情感分析任务: 双向LSTM: 双向LSTM的初始与最终步的隐状态连结起来作为文本序列的表示，然后连接一个全连接层，输出 CNN: 这里使用了一种名为textCNN的网络架构，把文本序列看成一维图像进行处理，采用一维卷积来获得局部特征 一维卷积是二维卷积的一个特例，同样是核函数沿着输入滑动。多通道输入的一维互相关等同于单输入通道的二维互相关 自然语言推断(nature language inference, NLI)任务是文本对分类任务，对文本对进行判断，决定一个句子能否由另一个句子推断出，即假设(hypothesis)能否由前提(premise)推出 两个句子的三种关系: 蕴涵(entailment): 假设可以从前提推出 矛盾(contradiction): 假设的否定可以从前提推出(我感觉本质上就是假设不能由前提推出) 中性(neutral): 所有其他的情况 NLI使用的数据集是斯坦福自然语言推断数据集(SNLI) 接下来介绍两种进行NLI的方法，分别是使用注意力机制(包含MLP)和BERT微调: 使用注意力机制: 利用注意力机制将两个文本序列的词元对齐，然后比较、聚合这些信息，那么本质上就是三个步骤：注意、比较、聚合: 注意: 与attention机制类似 比较: 比较软对齐的hypothesis与premise相比较 聚合: 将比较结果concat之后送入mlp 这中间涉及到很多mlp层 另一种方法就是利用BERT进行微调，后面会具体介绍 这种使用非BERT的应用都是将嵌入层的权重替换成预训练模型的权重 BERT可处理的一些下游任务: 单文本分类(比如情感分析、句子在语法上是否可接受) 文本对分类/回归(NLI、语义文本相似度) 词元级任务: 比如文本标注(每个词元经过相同的全连接层后，返回词性标签)、问答(QA, 使用数据集SQuAD，给定段落与问题后，预测用段落的哪个片段进行回答(也即文本片段的开始与结束位置的预测)) 加载bert模型时，可以将预训练好的参数直接放到定义好的网络架构中 之前很多预训练模型在处理下游任务时，都需要为下游任务设定特定的框架，但是BERT却并不需要设置特定的框架，有时只需要添加一个额外的全连接层即可 微调只更新部分参数"
    } ,
  
    {
      "title"       : "Web速成",
      "category"    : "",
      "tags"        : "note",
      "url"         : "./Web%E9%80%9F%E6%88%90.html",
      "date"        : "2022-08-03 00:00:00 +0800",
      "description" : "速成HTML、CSS、JS",
      "content"     : "1. HTML学习2. CSS学习3. JS学习"
    } ,
  
    {
      "title"       : "课程总结",
      "category"    : "",
      "tags"        : "school",
      "url"         : "./%E5%A4%A7%E5%9B%9B%E4%B8%8B%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93.html",
      "date"        : "2022-04-29 00:00:00 +0800",
      "description" : "记录大四下课程和课程笔记",
      "content"     : "Introduction to corporate finance 课程简介 课程内容 工业科学 课程简介 课程内容 量子力学 课程简介 课程内容 统计物理 课程简介 课程内容 信号处理 课程简介 课程内容 结构力学2 课程简介 课程内容 传热学(Heat Transfert) 课程简介 课程内容 过程工程(Process Engin) 课程简介 课程内容 Press 课程简介 课程内容 Audiovisuel 课程简介 课程内容 毕业设计 毕设内容 Introduction to corporate finance课程简介 授课老师: Danielle Levi-Feunteun 授课形式: 线上 授课材料: 电子讲义 考核形式: 每节课都有作业需要提交，算平时分，最后还有一个开卷的考试课程内容围绕着财报进行简单的介绍工业科学课程简介 授课老师: 付小尧 授课形式: 工业科学由两部分组成，实验和正课，都在线下完成 授课材料: 实验课有电子讲义，正课有讲义和TD 考核形式: 实验和正课都有线下考试，各占五十课程内容实验一共有8节课，都在二号楼上，实验围绕PID控制原理展开，我影响比较深的有停车场栏杆控制、云台、装乒乓球、给羽毛球拍上弦正课讲了一点二进制、逻辑电路量子力学课程简介 授课老师: 付小尧 授课形式: 线下授课 授课材料: 电子讲义和TD 考核形式: 线上考试课程内容一共有8章，第一章介绍了光电效应、黑体，第二章介绍了薛定谔方程，后面的忘了统计物理课程简介 授课老师: Philippe Ribiere 授课形式: 线下授课 授课材料: 电子讲义和TD 考核形式: 大作业(统计物理与人工智能的关系)课程内容一共四章，对量子力学里用到的统计知识进行了补充，统计物理根据对物质微观结构及微观粒子相互作用的认识，用概率统计的方法，对由大量粒子组成的宏观物体的物理性质及宏观规律作出微观解释的理论物理学分支。信号处理课程简介 授课老师: Antoine Roueff 授课形式: 线上授课 授课材料: 电子讲义、TD、TP(Matlab) 考核形式: 线上考试课程内容没咋听结构力学2课程简介 授课老师: Olivier Bareille、黄行蓉 授课形式: 线上授课 授课材料: 电子讲义和TD 考核形式: 大作业(做题)课程内容Olivier讲正课，黄老师讲TD，由于疫情原因，最终在家完成大作业传热学(Heat Transfert)课程简介 授课老师: Nelson IBASETA、张堇 授课形式: 线上授课 授课材料: 电子讲义和TD 考核形式: 线上考试课程内容Nelson讲正课，张老师讲TD，主要介绍了三种传热方式，分别是热传导，热对流和热辐射过程工程(Process Engin)课程简介 授课老师: Dominique Pareau、唐宏哲 授课形式: 线上授课 授课材料: 电子讲义和TD 考核形式: 两个大作业，Dominique的是做题，唐老师的是设计一个蒸馏的方案课程内容Dominique主要讲了工业流程中各组分的物料守恒和能量守恒，唐宏哲主要讲了蒸馏Press课程简介 授课老师: Vanessa 授课形式: 线下和线上授课 授课材料: 电子讲义 考核形式: 大作业课程内容法语课Audiovisuel课程简介 授课老师: Fabien 授课形式: 线下和线上授课 授课材料: 电子讲义 考核形式: 大作业课程内容法语课毕业设计毕设内容大四最重要的一门课，我的毕设是设计了一个成绩预测模型，包含了风险学科预测和成绩区间预测，主要用了一些机器学习的算法和数据处理的知识，写论文花了一周时间，剩下的就是不停的答辩"
    } ,
  
    {
      "title"       : "毕设记录",
      "category"    : "",
      "tags"        : "record",
      "url"         : "./Graduation-Project.html",
      "date"        : "2022-04-20 00:00:00 +0800",
      "description" : "毕设流程记录",
      "content"     : "1. 数据预处理1.1. 使用MySQL对原始数据进行处理 目标: 生成每行为一个学生，第一列为学号，第二列到最后一列都是课程名称 第一步: 创建表格 首先遇到的问题是创建列名时有MySQL关键字，所以对KCMC两端加上了反引号 使用Group_concat时有内容长度限制，需要使用以下代码来暂时增大限制: SET GLOBAL group_concat_max_len = 4294967295; SET SESSION group_concat_max_len = 4294967295; 列名长度硬性要求: 不能超过64个字符，所以我采用了将英文翻译为中文的方法减少长度，有以下几门学科名称做过修改: UPDATE grade_original SET KCMC = ‘网格生成方法及软件简介’ WHERE KCMC=’An Introduction to Mesh Generation Methods &amp; Software for Scientific Computing’ UPDATE grade_original SET KCMC = ‘经典论文鉴赏:电磁学顶级论文精选’ WHERE KCMC=’Appreciation of Classical Papers: The Selected Top Papers in Electromagnetism’ UPDATE grade_original SET KCMC = ‘动脉硬化的脆弱性评估:从体内成像到生物力学’ WHERE KCMC=’Atherosclerosis Vulnerability Assessment: From In Vivo Imaging To Biomechanics’ UPDATE grade_original SET KCMC = ‘计算机建模和仿真基础:方法、技术和应用’ WHERE KCMC=’Basics of Computer-Based Modelling and Simulation: Methodologies, Technologies and Applications’ UPDATE grade_original SET KCMC = ‘当代中国外交政策及其全球治理途径’ WHERE KCMC=’Contemporary Chinese Foreign Policy and Its Global Governance Approach’ UPDATE grade_original SET KCMC = ‘灵活的中英文语言:成功的必要条件’ WHERE KCMC=’Elastic Language in Chinese and English: Essential for Successful’ UPDATE grade_original SET KCMC = ‘自然界中的功能结构材料:从保护到传感’ WHERE KCMC=’Functional Structural Materials in Nature: From Protection to Sensing’ UPDATE grade_original SET KCMC = ‘国际商法-在中国经商的法律环境’ WHERE KCMC=’International Business Law - The Legal Environment of Doing Business in China’ UPDATE grade_original SET KCMC = ‘航空航天工程疲劳与损伤容限导论’ WHERE KCMC=’Introduction to Fatigue and Damage Tolerance in Aerospace Engineering’ UPDATE grade_original SET KCMC = ‘模型检查定时系统导论:理论与实践’ WHERE KCMC=’Introduction to Model-Checking Timed Systems: Theory and Practice’ UPDATE grade_original SET KCMC = ‘功能薄膜磁控溅射的研究现状与发展趋势’ WHERE KCMC=’Magnetron Sputtering of Functional Thin Films: Present Status and Trends’ UPDATE grade_original SET KCMC = ‘材料表征热分析原理及应用’ WHERE KCMC=’Principles and Applications of Thermal Analysis for Materials Characterization’ UPDATE grade_original SET KCMC = ‘从英语学习到口译翻译能力的发展:原则与策略’ WHERE KCMC=’Progression from English Study to Interpreting and Translation Competence: Principles and Strategies’ 一共十三门课名有做修改 MySQL对列数有硬性要求: 其中InnoDB引擎要求不超过1024，其余引擎不超过4096，但是我的列数一共有1425，所以我改用了MyISAM引擎 MySQL命令行代码: SELECT CONCAT( 'CREATE TABLE grade_student (', GROUP_CONCAT(DISTINCT CONCAT('\\`', KCMC, '\\`', ' FLOAT', CHAR(10)) SEPARATOR ','), ')', 'ENGINE=MyISAM DEFAULT CHARSET=gbk;') FROM grade_original INTO @sql; PREPARE stmt_name FROM @sql; EXECUTE stmt_name; 目前完成: 列名创建出来，XH一行填满，但是成绩没有填 先放着，后面有时间再试试，先用python1.2 利用Python的pandas库对数据进行预处理 首先实现了从原始的data_original转变成data_student_0: 里面数据格式如下: 每行代表一名学生，第一列为学号，第二列至最后一列列名是学科名称，数值是成绩 然后实现了去除大家都有的学科，把参加课程设为1，未参加课程设为0，生成data_student_1.csv文件，用来进行聚类 聚类算法: kmeans，选了4个簇，生成的结果总体上来说非常不错，但是我希望计算一下性能度量(待办) 更细化来说，首先我可以手动针对学生选课来确定学院，最好是通过大三或者大四的课程来确定，以免有人中途转系等等 聚类后生成了data_student_2.csv，里面新增了XY列，0代表国通，1代表计院，2代表航院，3代表中法 然后需要针对学年进行进一步细分: 目前聚类后的结果分布如下"
    } ,
  
    {
      "title"       : "算法基础",
      "category"    : "",
      "tags"        : "note",
      "url"         : "./Algorithms.html",
      "date"        : "2022-04-13 00:00:00 +0800",
      "description" : "郭炜老师算法网课记录",
      "content"     : "目录 目录 0. 简介 1. 第一周 枚举 1.1. 完美立方 1.2. 生理周期 1.3. 称硬币 1.4. 熄灯问题 2. 第二周 递归(一) 2.1. 求阶乘 2.2. 汉诺塔问题 2.3. n皇后问题 2.4. 逆波兰表达式求值 3. 第三周 递归(二) 3.1. 表达式求值 3.2. 上台阶问题 3.3. 放苹果问题 3.4. 算24问题 4. 第四周 二分算法 4.1. 程序或算法的时间复杂度 4.2. 二分查找的原理和实现 4.3. 二分法求方程的根 5. 第五周 分治 5.1. 归并排序 5.2. 快速排序 5.3. 例题: 输出前m大的数 5.4. 例题: 求排序的逆序数 6. 第六周 动态规划(一) 6.1. 数字三角形 6.2. 动态规划解题的一般思路 6.3. 最长上升子序列 6.4. 最长公共子序列 6.5. 最佳加法表达式 7. 第七周 动态规划(二) 7.1. Help Jimmy 7.2. 滑雪 7.3. 神奇的口袋 7.4. 0-1背包问题 7.5. 分蛋糕 8. 第八周 深度优先搜索(一) 8.1. 在图上寻找路径和遍历 8.2. 图的表示方法: 邻接矩阵和邻接表 8.3. 城堡问题: 8.4. 踩方格 9. 第九周 深度优先搜索(二) 9.1. 寻路问题 9.2. 生日蛋糕 10. 第十周 广度优先搜索 10.1. 抓住那头牛 10.2. 迷宫问题 10.3. 鸣人和佐助 10.4. 八数码问题 11. 第十一周 贪心算法 11.1. 圣诞老人的礼物 11.2. 电影节 11.3. 分配畜栏 11.4. 放置雷达 11.5. 钓鱼 0. 简介 课程来源于北大郭炜老师的MOOC，在中国大学MOOC平台上有网课，课程名为程序设计与算法(二)算法基础，第九次开课，课程有附带的习题，该博客记录了我的随堂笔记，课件在MOOC网站有1. 第一周 枚举1.1. 完美立方 枚举: 基于逐个尝试答案的一种问题求解策略 例如: 求小于N的最小素数 完美立方: 解题思路:1.2. 生理周期 题干: 解题思路:1.3. 称硬币 题干: 解题思路1.4. 熄灯问题 题干: 解题思路:局部的思想，化繁为简 可以用0-31的十进制数来表示第一列的数据，因为其二进制数刚好对应开关的状态2. 第二周 递归(一)2.1. 求阶乘 递归的基本概念: 一个函数调用其自身就是递归 递归和普通函数调用一样是通过栈实现 递归的作用 替代多重循环 解决本来就是递归形式定义的问题 将问题分解为规模更小的问题进行求解: 比如n！变成n * (n-1) 2.2. 汉诺塔问题 任务描述: 解决思路: 把盘子从A移动到C的过程分解为三个小问题，分别是移动n-1个盘子从A到B，然后移动1个盘子从A到C，最后移动n-1个盘子从B到C，这就是一个递归问题 递归的核心思想是将大问题分解为规模更小的问题，同时还要保证是从n变成n-1 代码实现:2.3. n皇后问题 问题描述:用递归代替多重循环，皇后的攻击范围是横竖斜 解决思路: 同样是从第1行开始逐个往后摆放，但是这里的n是未知数，所以循环的层数不确定，这个时候就可以用递归代替循环，构造一个函数，表示从第k行开始摆放棋子，然后在循环内部判断每一列的位置是否能摆放，就是一个穷举问题了 代码实现:这个代码设计很巧妙的地方是，它会遍历所有的情况，只要是满足条件的就会输出，所以会返回所有可能的结果2.4. 逆波兰表达式求值 问题描述: 输入输出例子: 解决思路: 一个数也可以看成一个逆波兰表达式，那么就可以直接用递归求解，实现过程中需要边输入边递归 代码实现:3. 第三周 递归(二)3.1. 表达式求值 问题描述: 解决思路: 先看看表达式递归的定义即然把表达式的递归过程弄清楚了，那么只需要定义表达式、项、因子的函数即可 代码实现:3.2. 上台阶问题 问题描述: 输入输出样例: 解决思路: 将n级台阶的走法看成n-1级台阶的走法+n-2级台阶的走法，分别代表在第一步走一阶还是两阶，这里需要设置边界条件来防止无限递归 代码实现:3.3. 放苹果问题 问题描述: 解决思路: 又是计算方法的总数，那么和上台阶问题一样，用表达式来表示递归，分类讨论。假设i个苹果，k个盘子，如果k&gt;i，那么等价于把i个苹果放到i个盘子里，因为一定有k-i个盘子空着；如果k&lt;=i，那么又将问题分为有没有空盘子，如果有空盘子，那么至少有一个空盘子，表示为把i个苹果放到k-1个盘子里，如果没有空盘子，那么等价于把i-k个苹果放到k个盘子里 代码实现:3.4. 算24问题 问题描述: 输入输出样例: 判断两个浮点数是否相等，用两个浮点数的差是否小于某个值 解决思路: 不论给了多少个数计算24，都需要首先计算出两个数的计算结果，这个计算过程可以是加减乘除任意，然后得到的结果再和剩下的n-1个数算24，这样就可以变成一个递归问题，边界条件是只剩一个数的时候是否是24 代码实现:4. 第四周 二分算法4.1. 程序或算法的时间复杂度 时间复杂度的定义:重点是明白程序中固定的操作是什么 复杂度有平均复杂度和最坏复杂度两种，两者可能一致，也可能不一致，一般来说只要平均复杂度不太高，算法的效率就还可以 常见的时间复杂度:4.2. 二分查找的原理和实现 首先可以看这么一个问题: 二分查找的实现: 时间复杂度是O(log(n)) 查找比待查找数小的最大坐标的函数实现: 二分查找的问题前提是序列必须是递增或者递减的，即有序的 为了防止数据溢出，写中点的时候要这么写: int mid = L + (R - L) / 2 整型在转型的时候是向下取整4.3. 二分法求方程的根 二分法求方程的根需要方程满足一定的条件，不是所有的方程都可以用二分法求根 问题描述及求解思路: 代码实现: 如果一个序列不是有序的，可以用排序算法对序列先进行排序然后二分查找5. 第五周 分治5.1. 归并排序 分治的基本概念: 分治的典型应用: 归并排序 归并排序的思路就是先分治，然后归并，代码实现如下： 归并排序的时间复杂度:5.2. 快速排序 快速排序的思想: 代码实现: 快速排序的时间复杂度是O(nlog(n))，这是在运气不坏的情况下得出的结果(平均复杂度)，运气最坏的情况下时间复杂度为O($n^2$)(最坏复杂度)5.3. 例题: 输出前m大的数 问题描述: 解决思路:用分治的思想解决问题，先把前m个元素移到数组的最右边，然后在对这m个元素进行快排 具体解决方法: 时间复杂度计算:5.4. 例题: 求排序的逆序数 问题描述: 解决思路:分治一般都使用了递归6. 第六周 动态规划(一)6.1. 数字三角形 问题描述: 输入格式: 解题思路: 看成递归问题 递归程序代码实现: 虽然说在代码逻辑这一方面，递归算法没有问题，但是这个算法的时间复杂度太高，程序很容易超时: 之前的递归算法中存在过多的重复计算，如果能把每一步的计算结果保存起来，那么即可避免重复计算，算法的时间复杂度为O($n^2$) 记忆递归型动规程序:用一个二维数组存储每一个结点的max值，那么读取到这个结点时，就可以直接获得数值，避免了重复计算 也可以用递推的思想解决问题，先把最后一行的结果计算出来，然后从下到上逐步计算，用一个双重循环解决 代码实现: 还可以对空间进行优化，因为下一层的数值在计算上一层的数值后就没有用了，那么完全不需要用一个二维数组存储maxsum，完全可以用一个一维数组存放。再进一步来说，连maxSum数组都可以不要，直接用D的第n行替代maxSum 空间优化后的代码实现:6.2. 动态规划解题的一般思路 递归到动规的一般转化方法: 动规解题的一般思路: 第一步: 将原问题分解为子问题 第二步: 确定状态 第三步: 确定一些初始状态的值 第四步: 确定状态转移方程 能用动规解决的问题的特点:6.3. 最长上升子序列 问题描述: 输入输出格式: 解题思路: 找子问题: 确定状态: 找出状态转移方程: 代码实现: 动规的常用两种形式:6.4. 最长公共子序列 问题描述: 输入输出样例: 解题思路:重要的还是找到一个合适的子问题与状态 状态转移方程 证明一下这个递推公式是正确的: 代码实现:6.5. 最佳加法表达式 问题描述: 解题思路:7. 第七周 动态规划(二)7.1. Help Jimmy 问题描述: 输入输出样例: 解题思路:板子的顺序其实没有关系，重要的关注点是当前板子的左侧或右侧正下方的板子是哪个板子，然后计算出从每个板子的左侧或者右侧下降需要的最短时间，也就是说这里的状态值得是不同的板子 伪代码实现:将下落点看成宽度为0的板子是一种很好的思路 时间复杂度:7.2. 滑雪 问题描述: 输入输出样例: 解题思路:这个题目递推的顺序很奇怪，如果按照二维数组的排序顺序来递推会出现问题，这里比较好的解决思路是把点按照高度排序，因为如果高度低的点值没求出来的话，高度高的点的值一定求不出来。然后这里排完序后有两种解决思路，一种是按顺序把每个点的值根据周围四个低的点求出，另一种思路是按照顺序每次更新周围四个点的值7.3. 神奇的口袋 问题描述: 输入输出样例: 当然可以用枚举的方法暴力求解 也可以用递推的方法求解:设计一个递推的函数，代表前k个物品凑w体积的方法个数，那么在新出现这个物品时有两个选择，即选或不选 动规解法:用二维数组来表示状态7.4. 0-1背包问题 问题描述: 解题思路: 和上一小节的思路类似，状态同样用二维数组表示，然后找出状态转移方程求解即可可以用滚动数组的思想来优化空间，递推的过程是从右往左替换滚动数组的过程7.5. 分蛋糕 问题描述: 输入输出样例: 解题思路:由于这里存在高宽，所以状态需要用三维数组表示 递推公式:8. 第八周 深度优先搜索(一)8.1. 在图上寻找路径和遍历 问题描述: 利用这种策略，可能出现的情况: 深度优先搜索(Depth-First-Search)的定义: 刚才那个问题的伪代码实现: 如果要记录路径，代码实现:8.2. 图的表示方法: 邻接矩阵和邻接表 邻接矩阵表示图: 邻接表表示图:8.3. 城堡问题: 问题描述: 输入输出样例: 解题思路: 这种比较抽象的问题可以通过建模转换成相对简单的题目，把城堡的方块看成节点，然后如果两个方块连接，则连接这两个节点求房间个数等价于求极大连通子图个数 代码实现:8.4. 踩方格 问题描述: 解决思路: 用递归的思路解决问题，第一步的选择有三种，那么走n步的方案数等于这三种走法的方案和 代码实现:9. 第九周 深度优先搜索(二)9.1. 寻路问题 问题描述: 解题思路: 从城市1开始深度优先遍历整个图，找到能到达城市N的最优路线(在不超过开销情况下的最短路径)但是这种方法会超时，所以我们需要对算法进行改进(剪枝)，最容易想到的方法是最优性剪枝，但是发现这种剪枝方法还是超时了这种剪枝方法能保存中间计算结果，效果比之前的最优性剪枝要好 代码实现:9.2. 生日蛋糕 下棋也是一个深度搜索的过程 问题描述:蛋糕的高和半径是递减的 解题思路: 代码实现: 剪枝(剪枝在深度优先搜索中很重要)10. 第十周 广度优先搜索10.1. 抓住那头牛 问题描述: 解决思路:第一种想法是用深度优先搜索解决问题，让农夫尝试所有的走法，不能走重，不能往下走了就回溯第二种想法是用广度优先搜索的思路解决问题，对所有的节点进行分层，广搜的优点是确保可以找到最优解，但是因为拓展出来的节点比较多，且多数节点都需要保存，因此需要的存储空间较大，用队列保存节点 广搜算法:光看文字可能有些晦涩难懂，下面有open表和close表变化的实例 代码实现在了解广度优先搜索的概念后，代码实现的难点在于队列queue的使用，queue的实例化可以用queue&lt;T&gt;来实现，q.front()是取当前元素，q.push()是在队列尾部添加元素，q.pop()是删除头部元素10.2. 迷宫问题 问题描述: 解决思路: 广搜这里队列不能用STL的queue实现(因为要给出最短路径)，要自己写，可以用一维数组实现其实就是记录了每个节点的父节点，在达到重点的时候可以一路返回过去得到路径10.3. 鸣人和佐助鸣人和佐助问题是迷宫问题的一个变种 问题描述: 解决思路:由于每个位置的查克拉不同也会导致状态的不同，所以状态用三个参数表示，然后根据条件拓展节点 问题变种:10.4. 八数码问题 问题描述: 解决思路:用广度优先搜索来解决问题，优先拓展浅层节点，在逐渐深入 广度优先搜索的代码框架: 这个题目的关键点在于判重，状态数目大，如何存储才能较快判断一个状态是否重复 一些可能的编码方案: 继续优化问题的方法: 判定八数码问题是否有解移动0的位置，不改变排列的奇偶性 代码实现(单向广搜，用set判重) 其余优化问题的方法: 双向广搜、针对本题的预处理、A*算法 广搜和深搜的比较:11. 第十一周 贪心算法11.1. 圣诞老人的礼物 问题描述: 样例输入输出:第一行表示箱子总数和可携带的最大重量，其余行都表示箱子的信息，第一列是价值，第二列是重量 解决思路:因为要携带尽可能价值高的糖果，所以可以把所有箱子的价值重量比算出来，然后排列，按顺序装糖果。这种方法就是贪心算法的思路 代码实现:这里实现candy的结构体时，结构体里面重载了运算符&lt;，operator是转换运算符，这样就可以直接调用sort对candies进行排序 证明这种方法是正确的: 贪心算法:每一步行动总是按照某种指标选取最优的操作来进行11.2. 电影节 问题描述: 样例输入输出: 解决思路: 证明贪心算法的正确性:11.3. 分配畜栏 问题描述: 解决思路:按照奶牛的开始挤奶时间来分配奶牛的畜栏，可以用队列存储最早结束的畜栏的时间 代码实现:priority是优先队列，在优先队列中，优先级高的元素先出队列，并非按照先进先出的顺序11.4. 放置雷达 问题描述: 解决思路 首先把题目问题转换一下: 接下来我们通过观察得到一个重要的结论，那就是如果一个雷达可以覆盖多个雷达，那么这个雷达可以在所覆盖雷达的最右边的起点 贪心算法实现步骤: 11.5. 钓鱼 问题描述: 解决思路:"
    } ,
  
    {
      "title"       : "C语言程序设计",
      "category"    : "",
      "tags"        : "note",
      "url"         : "./C-Language.html",
      "date"        : "2022-02-25 00:00:00 +0800",
      "description" : "网课记录",
      "content"     : "目录 目录 1. 第一章 C语言快速入门 1.1. 信息在计算机中的表示 1.1.1. 用0和1表示各种信息 1.1.2. 十进制到二进制的互相转换 1.1.3. K进制小数 1.1.4. 十六进制数到二进制数的相互转换 1.2. C语言快速入门 1.3. 变量和数据类型初探 1.3.1. 什么是变量 1.3.2. 变量的命名规则 1.3.3. C++的基本数据类型 1.4. 变量和数据类型进阶 1.4.1. 数据类型的自动转换 1.5. 常量 1.5.1. 整型常量 1.5.2. 字符型常量 1.5.3. 符号常量 2. 第二章 输入输出和基本运算 2.1. 输入输出进阶 2.1.1. 输入输出控制符 2.1.2. 用scanf读入不同类型的变量 2.1.3. 控制printf输出整数的宽度 2.1.4. 用C++的cout进行输出 2.1.5. 用C++的cin进行输入 2.2. 算术运算符和算术表达式 2.2.1. 赋值运算符 2.2.2. 算术运算符 2.2.3. 模运算 2.2.4. 自增运算符 ++ 2.3. 关系运算符和逻辑表达式 2.3.1. 关系运算符 2.3.2. 逻辑运算符和逻辑表达式 2.4. 其他运算符及运算符优先级 2.4.1. 强制类型转换运算符 2.4.2. 部分运算符的优先级 3. 分支语句和循环语句 3.1. if语句 3.1.1. 条件分支结构 3.1.2. if语句 3.2. switch语句 3.3. for循环 3.3.1. for循环语句 3.4. while循环和do while循环 3.4.1. while循环 3.4.2. do while循环 4. 第四章 循环综合应用 4.1. break语句和continue语句 4.1.1. break语句 4.1.2. continue语句 4.2. OJ输入数据的处理 4.2.1. scanf表达式的值 4.2.2. 用freopen重定向输入 5. 第五章 数组 5.1. 数组 5.2. 筛法求素数 5.3. 数组初始化 5.4. 数组越界 5.5. 二维数组 6. 第六章 函数和位运算 6.1. 函数 6.2. 递归初步 6.3. 库函数和头文件 6.4. 位运算 6.4.1. 按位与&amp; 6.4.2. 按位或| 6.4.3. 按位异或^ 6.4.4. 按位非~ 6.4.5. 左移运算符« 6.4.6. 右移运算符» 7. 第七章 字符串 7.1. 字符串的形式和存储 7.2. 输入字符串 7.3. 字符串库函数 8. 第八章 指针(一) 8.1. 指针的基本概念和用法 8.2. 指针的意义和相互赋值 8.3. 指针的运算 8.4. 指针作为函数参数 8.5. 指针和数组 9. 第九章 指针(二) 9.1. 指针和二维数组、指向指针的指针 9.2. 指针和字符串 9.3. 字符串库函数 9.4. void指针和内存操作函数 9.5. 函数指针 10. 第十章 程序结构和简单算法 10.1. 结构 10.2. 全局变量、局部变量、静态变量 10.3. 变量的作用域和生存周期 10.4. 选择排序和插入排序 10.4.1. 选择排序 10.4.2. 插入排序 10.5. 冒泡排序 10.6. 程序或算法的时间复杂度 11. 第十一章 文件读写 11.1. 文件读写概述 11.1.1. 打开文件的函数 11.2. 文本文件读写 11.2.1. 文本文件读写 11.2.1. 文本文件读写(另一种函数) 11.3. 二进制文件读写概述 11.3.1. 文件的读写指针 11.3.2. 二进制文件读写 11.4. 创建和读取二进制文件 11.5. 修改二进制文件 11.6. 文件拷贝程序 12. C++的STL 12.1. STL排序算法sort 12.2. STL二分查找算法 12.2.1. 用binary_search进行二分查找 12.2.2. 用lower_bound二分查找下界 12.2.3. 用upper_bound二分查找上界 12.3. multiset 12.4. set 12.5. multimap 12.6. map 1. 第一章 C语言快速入门1.1. 信息在计算机中的表示1.1.1. 用0和1表示各种信息 计算机中的所有信息都是用0、1表示 二进制数的一位，称为一个比特(bit), 简写b 八个二进制位称为一个字节(byte), 简写B 1KB, 1MB, 1GB, 1TB ASCII编码方案：用8个连续的0或1来表示一个字母数字和标点符号，一共有256种不同的组合1.1.2. 十进制到二进制的互相转换 十进制数是数的十进制表示形式的简称 短除法，每次除以进制，余数就是这个进制的最小位数1.1.3. K进制小数 K进制小数和整数的定义类似，只不过变成了K的负次方，比如小数后的第一位是$K^{-1}$1.1.4. 十六进制数到二进制数的相互转换1.2. C语言快速入门 空格也是一个字符 C语言中输入输出: scanf、printf 程序的注释: 多行注释: /* … */ 单行注释: // 1.3. 变量和数据类型初探1.3.1. 什么是变量 变量就是一个代号, 程序运行时系统会自动为变量分配内存空间，于是变量就代表了系统分配的那片内存空间 变量有名字和类型两种属性，变量的类型决定了一个变量占用多少个字节 变量的定义要在使用之前 一个变量不能定义两次1.3.2. 变量的命名规则 变量不能以数字开头 变量只能由大小写字母、数字和下划线组成 变量名不能和C++系统预留的一些保留字重复1.3.3. C++的基本数据类型 float的取值范围是绝对值的范围 整型、实数型、布尔型、字符型 用sizeof()可以返回数据类型所占的字节数 变量在定义的时候可以给它指定一个初始值1.4. 变量和数据类型进阶 整型可以分为有符号整型和无符号整型 有符号整数的表示方式 将最左边的位看作符号位，符号位为0表示非负数，其绝对值就是除去符号位以外的部分 符号位为1，则表示是负数，其绝对值是除符号位以外的部分取反后加1 将一个负整数转化为有符号整数是: 符号位取1，其余部分取该负整数的绝对值的二进制表示取反加1 1.4.1. 数据类型的自动转换 有些不同的数据类型之间是相容的，可以相互赋值 int a = 11.34, 其实就是a=11 整型数据也可以转换为字符型数据, 但只会留下最右边的一个字节 看一个例子1.5. 常量1.5.1. 整型常量 十六进制整型常量以0x开头 一个十六进制位正好对应四个二进制位 0开头的是八进制数1.5.2. 字符型常量 字符型常量表示一个字符，用单引号括起来 字符型常量和变量都占一个字节，内部存放的是ASCII编码 小写字母的ASCII编码比大写字母大 字符型常量中有一部分以‘\\’开头, 被称为转义字符 字符串常量用双引号括起来，字符常量用单引号括起来1.5.3. 符号常量 为了阅读和修改方便, 常用一个由字母和数字组成的符号来代表某个常量 #define 常量名 常量值 尽量多用符号常量，少用数值常量2. 第二章 输入输出和基本运算2.1. 输入输出进阶2.1.1. 输入输出控制符 在printf和scanf中可以使用以%开头的控制符，指明要输入和输出的数据类型 常用的格式控制符2.1.2. 用scanf读入不同类型的变量 输入字符时，不会跳过空格 如果在输入中有scanf中出现的非控制字符，则这些字符会被跳过2.1.3. 控制printf输出整数的宽度 比如用%nd和%0nd控制输出整型的长度 用%.nf控制输出浮点数的精度2.1.4. 用C++的cout进行输出 cout « … endl可以进行换行2.1.5. 用C++的cin进行输入 cin » … cin、cout的速度比printf、scanf慢，输入输出数据量大的时候用后者 一个程序不要同时使用cout和printf2.2. 算术运算符和算术表达式2.2.1. 赋值运算符 a += b 等同于 a = a + b2.2.2. 算术运算符 加减乘除 %表示取余数 两个整数进行加减乘都可能导致计算结果超出了结果类型所能表示的范围，这种情况就是溢出 如果溢出，则直接丢弃溢出的部分 有时计算的最终结果似乎不会溢出，但中间结果可能溢出，这也会导致程序出错 解决溢出的办法是尽量使用高精度的数据类型 除法的结果、类型和操作数中精度高的类型相同2.2.3. 模运算 求余数的运算符%也称为模运算符，两个操作数都是整数类型2.2.4. 自增运算符 ++ 自增运算符有前置用法和后置用法 前置用法: ++ a 表示将a的值加1，表达式返回a+1后的值 后置用法: a ++ 表示将a的值加1，表达式返回值为a加1前的值2.3. 关系运算符和逻辑表达式2.3.1. 关系运算符 一共有六种关系运算符用于数值的比较 比较的结果是bool类型2.3.2. 逻辑运算符和逻辑表达式 逻辑运算符用于表达式的逻辑操作，有&amp;&amp;、||、!这三种，操作结果为true或false 逻辑表达式是短路运算，即对逻辑表达式的计算在整个表达式的值已经能够断定的时候停止2.4. 其他运算符及运算符优先级2.4.1. 强制类型转换运算符 (int)、(char)这样的运算符就是强制将操作数转换为指定类型2.4.2. 部分运算符的优先级3. 分支语句和循环语句3.1. if语句3.1.1. 条件分支结构 有时候我们希望满足一个条件执行一种语句，另一个条件执行另一种语句3.1.2. if语句 if语句可以没有else if，也可以没有else 如果语句组只有一条语句，则不需要{} if语句可以嵌套 else总是和离它最近的if配对，加一个花括号可以解决这个问题3.2. switch语句 可以没有default语句 注意常量表达式不能带变量3.3. for循环3.3.1. for循环语句 注意是先执行语句组然后执行表达式3 表达式1和表达式3都可以是用逗号连接的若干个表达式 for循环可以嵌套，形成多重for循环 for语句括号里面的表达式1、表达式2、表达式3可以任何一个都不写，但是分号必须保留3.4. while循环和do while循环3.4.1. while循环3.4.2. do while循环 如果希望循环至少要执行一次，那么可以用do while循环4. 第四章 循环综合应用4.1. break语句和continue语句4.1.1. break语句 break语句出现在循环体中，其作用是跳出循环 在多重循环中，break语句只能跳出直接包含它的那一重循环4.1.2. continue语句 continue可以出现在循环体中，其作用是立即结束本次循环，并回到循环开头判断是否要进行下一次循环4.2. OJ输入数据的处理4.2.1. scanf表达式的值 scanf()表达式其实是有返回值的，返回值为int类型，表示成功读入的变量个数 scamf()值为EOF则说明输入数据已经结束 ctrl+z表示输入结束 这样就可以处理无结束标记的OJ题目输入4.2.2. 用freopen重定向输入 调试程序时，每次运行程序都要输入测试数据，太麻烦 可以将测试数据存入文件，然后用freopen将输入由键盘重定向为文件，则运行程序时不再需要输入数据5. 第五章 数组5.1. 数组 数组可以用来表达类型相同的元素的集合，集合的名字就是数组名 一维数组的定义方法如下： 元素个数必须是常量或常量表达式 sizeof()可以访问数组所占字节 数组名代表数组的地址 数组一般不要定义在main里面，尤其是大数组5.2. 筛法求素数 之前我们判断一个数n是不是素数，使用2到根号n之间的所有整数去除n，也就是穷举 筛法：把2到n中所有的数都列出来，然后从2开始，先划掉n内所有2的倍数，然后每次从下一个剩下的数开始，划掉其n内的所有倍数，最后剩下的数就是素数 筛法会稍微快一点，用空间换时间 代码如下：5.3. 数组初始化 在定义一个一维数组的同时，可以给数组中的元素赋初值 如果在定义数组的时候，如给全部元素赋值，则可以不给出数组元素的个数 可以用数组取代复杂分支结构 使用string须包含头文件5.4. 数组越界 数组元素的下标，可以是任何整数，可以是负数，也可以大于数组的元素个数，不会导致编译错误 但如果将越界写入了别的变量的内存空间，就很有可能出错5.5. 二维数组 二维数组的定义： 二维数组的访问可以直接用下标访问 二维数组的初始化也是用{} 二维数组初始化时，如果对每行都进行初始化，则不用写行数或列数6. 第六章 函数和位运算6.1. 函数 函数可以实现某一功能，当程序中需要使用该项功能时，只需要写一条语句，调用实现该功能的函数即可 函数的定义: 函数的调用: 函数名(参数1, 参数2…) 函数中至少含有一个return, 如果函数的类型为void, 则用return; 定义函数的参数叫做形参, 调用函数时的参数叫做实参 函数的定义一般在调用之前 但是函数的调用语句前面有函数的声明即可，不一定要有定义 C/C++程序从main函数开始 函数的形参是实参的一个拷贝，形参的改变一般不会影响到实参 一维数组作为形参时不用写出元素的个数，这时候形参的改变会影响实参 二维数组作为形参时，必须写明数组有多少列，不用写明有多少行6.2. 递归初步 一个函数，自己调用自己，就是递归 递归函数得有终止条件6.3. 库函数和头文件 头文件&lt;cmath&gt;中包含很多数学库函数的声明 库函数的定义一般在.lib文件中 库函数: C/C++标准规定, 编译器自带的函数 头文件: C++编译器提供许多头文件, 比如: iostream、cmath、string 头文件内部包含很多库函数的声明以及其他信息, 比如cin、cout的定义6.4. 位运算 位运算: 用于对整数类型变量中的某一位(bit)或者若干位进行操作 C++提供了六种位运算符来进行位运算操作6.4.1. 按位与&amp; 比如表达式(21 &amp; 18)的结果是16 通常用来将某变量中的某些位清0且同时保留其他位不变6.4.2. 按位或| 比如“21|18”的结果是23 按位或运算通常用来将某些变量中的某些位置1且保留其他位不变6.4.3. 按位异或^ 异或是逻辑运算, 如果两个值相同返回0, 如果两个值不同返回1 异或运算通常用来将某变量中的某些位取反 异或运算的特点:6.4.4. 按位非~ 按位非运算符~是单目运算符，其功能是将操作数中的二进制位0变成1，1变成06.4.5. 左移运算符« 9 « 4 表示将9的二进制表示左移4位6.4.6. 右移运算符» 右移时，移出最右边的位就被丢弃 对于有符号数，在右移时，符号位将一起移动，并且大多数C++编译器规定，如果圆符号位为1，则右移时高位就补充1，原符号位为0，则右移时高位就补充07. 第七章 字符串7.1. 字符串的形式和存储 字符串常量占据内存的字节数等于字符串中字符数目加1，多出来的是结尾字符’\\0’ 空串”“也是合法的字符串常量 包含’\\0’字符的一维char数组，就是一个字符串，其中存放的字符串即为’\\0’前面的字符组成 可以给一维数组这么赋值: char title[] = “Prison Break” ‘\\0’可以视为字符数组结束标志7.2. 输入字符串 用scanf也可以将字符串读入字符数组 scanf会自动添加结尾’\\0’ scanf读入到空格为止 scanf(“%s”, line) 不用取地址符 读入一行到字符串组: cin.getline(char buf[], int bufsize), 读入一行，自动添加’\\0’, 回车换行符不会写入buf, 但是会从输入流中去掉 也可以用gets(char buf[])来读入一行到字符数组，回车换行符不会写入buf，但是会从输入流中去掉，可能导致数组越界7.3. 字符串库函数 使用字符串库函数需要 #include &lt;cstring&gt; 形参为char []类型，则实参可以是char数组或字符串常量 字符串拷贝 strcpy(char[] dest, char[] src) 拷贝src到dest 字符串比较大小 int strcmp(char[] s1, char[] s2) 是根据字符的ASCII码值进行比较，大写字母比小写字母小 求字符串长度 int strlen(char[] s) 字符串拼接 strcat(char[] s1, char[] s2) 将s2拼接到s1后面 字符串转成大写 strupr(char []) 字符串转成小写 strlwr(char [])8. 第八章 指针(一)8.1. 指针的基本概念和用法 指针也称作指针变量，大小为4个字节(或8个字节)的变量，其内容代表一个内存地址 通过指针，能够对该指针指向的内存区域进行读写 指针的定义: 类型名 * 指针变量名 比如: int * p = (int *) 40000 p指向地址40000，地址p就是地址40000 * p就代表地址40000开始处的若干个字节的内容 我们可以通过指针访问其指向的内存空间 指针定义总结 指针用法，一般是让指针指向一个变量的地址8.2. 指针的意义和相互赋值 有了指针，就有了自由访问内存空间的手段 不同类型的指针，如果不经过强制类型转换，不能直接互相赋值8.3. 指针的运算 两个同类型的指针变量，可以比较大小 两个同类型的指针变量，可以相减 指针变量加减一个整数的结果是指针 指针变量可以自增自减 指针可以用下标运算符[]进行运算8.4. 指针作为函数参数 地址0不能访问，指向地址0的指针就是空指针 可以用NULL关键字对任何类型的指针进行赋值，NULL实际上就是整数0.值为NULL的指针就是空指针 指针可以作为条件表达式使用，如果指针的值为NULL，则相当于为假，值不为NULL，就相当于为真8.5. 指针和数组 数组的名字是一个指针常量，指向数组的起始地址 作为函数形参时， T *p与 T p[] 等价9. 第九章 指针(二)9.1. 指针和二维数组、指向指针的指针 二维数组的每一行都是一维数组，也就是指针 指向指针的指针:9.2. 指针和字符串 字符串常量的类型就是char * 字符数组名的类型也是char *9.3. 字符串库函数 字符串操作库函数 这些字符串操作库函数都需要include&lt;cstring&gt;9.4. void指针和内存操作函数 void指针: void * p 可以用任何类型的指针对void指针进行赋值或初始化 对于void指针，*p没有定义，++p、–p，p += n、p+n、p-n均无定义 内存操作库函数memset 内存操作库函数memcpy9.5. 函数指针 程序运行期间，每个函数都会占用一段连续的内存空间。而函数名就是该函数所占内存区域的起始地址(也称入口地址) 我们可以将函数的入口地址赋给一个指针变量，使该指针变量指向该函数，然后通过指针变量就可以调用这个函数，这种指向函数的指针变量称为函数指针 定义形式: 使用方法: 函数指针和qsort库函数 pfcompare是比较函数10. 第十章 程序结构和简单算法10.1. 结构 在现实问题中，常常需要用一组不同类型的数据来描述一个事物 C++允许程序员自己定义新的数据类型。因此针对“学生”这种事物，可以定义一种新名为Student的数据类型，一个student类型的变量就能描述一个学生的全部信息，同理，还可以定义数据类型worker来表示工人 结构(struct): 用struct关键字来定义一个结构，也就定义了一个新的数据类型 student即成为自定义的类型的名字，可以用来定义变量 两个同类型的结构变量，可以相互赋值，结构变量之间不能用比较运算符进行计算 一般来说，一个结构变量所占的内存空间的大小就是结构中所有成员变量大小之和 一个结构的成员变量可以是任何类型的，包括可以是另一个结构类型 结构的成员变量可以是指向本结构类型的变量的指针 访问结构变量的成员变量: 一个结构变量的成员变量完全可以和一个普通变量一样来使用，也可以取得其地址 结构变量名.成员变量名 结构变量可以在定义时进行初始化:(使用花括号和逗号) 结构数组也可以定义，就是把结构体名字看作变量类型使用 指向结构变量的指针，通过指针访问其指向的结构变量的成员变量10.2. 全局变量、局部变量、静态变量 定义在函数内部的变量叫局部变量(函数的形参也是局部变量) 定义在所有函数的外面的变量叫做全局变量 全局变量在所有函数中均可以使用，局部变量只能在定义它的内部函数中使用 静态变量: 全局变量都是静态变量，局部变量定义时如果前面加了static关键字，则该变量也成为静态变量 静态变量在整个程序运行期间都是固定不变的 局部变量在函数每次调用时地址都可能不同 如果未明确初始化，则静态变量会被自动初始化为全0，局部非静态变量的值则随机 静态变量只初始化一次，也就是下次调用函数的时候不进行初始化10.3. 变量的作用域和生存周期 变量名、函数名、类型名统称为标识符 一个标识符能够起作用的范围，叫做该标识符的作用域 使用标识符的语句，必须出现在它们的声明或者定义之后 在单文件的程序中，结构、函数和全局变量的作用域是其定义所在的整个文件 函数的形参的作用域是整个函数 局部变量的作用域，是从定义它的语句开始，到包含它的最内层的那一对大括号{}的右大括号为止 for循环里定义的循环控制变量，其作用域是整个for循环 同名标识符的作用域，可能一个被另一个包含，则在小的作用域里，作用域大的那个标识符被屏蔽，不起作用 所谓变量的生存期，值的是在此期间，变量占有内存空间，其占有的内存空间只能归它使用，不会用来存放别的东西 而变量的生存期终止，就意味着该变量不再占有内存空间，它原来占有的内存空间，随时可能被派作他用 全局变量的生存期，从程序被装入内存开始，到整个程序结束 静态局部变量的生存期，从定义它的语句第一次被执行开始，直到程序结束 函数形参的生存期从函数执行开始，到函数返回时结束，非静态局部变量的生存期，从执行到定义它的语句开始，一旦程序执行了它的作用域之外，其生存期即告终止10.4. 选择排序和插入排序10.4.1. 选择排序 排序问题: 编程接收键盘输入的若干个整数，排序后从小到大输出，先输入一个整数n，表明有n个整数需要排序，接下来再输入待排序的n个整数 选择排序: 选择最小的整数，与第i位的整数更换位置10.4.2. 插入排序 插入排序就是将数组分为有序和无序，每次让无序最左边的元素与有序分别比较，插入到合适的位置10.5. 冒泡排序 冒泡排序同样是将数组分为有序和无序两组，无序在左边，有序在右边，每次将无序部分两两比较，较大的在右边 上面三种简单排序算法，都要做$n^2$量级次数的比较，其中n是元素个数 而比较好的排序算法，如快速排序，归并排序等，只需要做$n*log_2n$量级次数的比较10.6. 程序或算法的时间复杂度 一个程序或算法的时间效率，也称为时间复杂度，有时简称复杂度 复杂度常用大的字母O和小写字母n来表示，比如O(n), n代表问题的规模 复杂度也有平均复杂度和最坏复杂度两种，两种可能一致，也可能不一致 如果复杂度是多个n的函数之和，则只关心随n的增长增长得最快的那个函数 一些例子11. 第十一章 文件读写11.1. 文件读写概述 二进制文件: 本质上所有文件都是0、1串，因此都是二进制文件。但是一般将内容不是文字，记事本打开看是乱码的文件，称为二进制文件 文本文件: 内容是文字，用记事本打开能看到文字的文件 文件读写相关函数在头文件cstdio中声明: #include &lt;cstdio&gt; fopen函数打开文件，返回FILE * 指针，指向和文件相关的一个FILE变量，FILE是一个struct 文件读写结束后，一定要fclose关闭文件，否则可能导致数据没被保存，或者无法打开其他文件 一些读写函数都需要FILE *指针进行11.1.1. 打开文件的函数 打开文件的模式 二进制打开和文本打开的区别: 主要是二进制打开的话会有换行符的区别，最好还是用二进制打开 文件名的绝对路径和相对路径:11.2. 文本文件读写11.2.1. 文本文件读写 我们希望写一个文件读写程序:11.2.1. 文本文件读写(另一种函数) fgets是读取一行 读取整个文本文件并输出 fputs是输出一行11.3. 二进制文件读写概述11.3.1. 文件的读写指针 这都是C语言读写的规则 fseek的作用是将读写指针定位到距离origin位置offset字节处11.3.2. 二进制文件读写 用fread进行二进制读文件 用fgetc进行二进制读文件 fgetc是用来读取一个字节 用fwrite二进制写文件 用fputc二进制写文件11.4. 创建和读取二进制文件 用二进制文件存学生信息比用文本方式存的好处: 可能节约空间、便于快速读取、改单个学生信息11.5. 修改二进制文件 用r+b打开文件既读又写时，如果做了读操作，则做写操作之前一定要用fssek重新定位文件读写指针11.6. 文件拷贝程序 文件拷贝程序mycopy示例12. C++的STL12.1. STL排序算法sort STL: standard template library 标准模板库 包含一些常用的算法如排序查找，还有常用的数据结构如可变长数组、链表、字典等 要使用其中的算法，需要#include &lt;algorithm&gt; 用sort进行排序(用法一) 用sort进行排序(用法二) 用sort进行排序(用法三)，用自定义的排序规则对任何类型T的数组进行排序 几个自定义排序规则例子12.2. STL二分查找算法12.2.1. 用binary_search进行二分查找 用法一: 用法二:12.2.2. 用lower_bound二分查找下界 用法一: 用法二:12.2.3. 用upper_bound二分查找上界 用法一: 用法二:12.3. multiset STL中的平衡二叉树数据结构 有时需要在大量增加、删除数据的同时，还要进行大量数据的查找 可以使用平衡二叉树数据结构存放数据，体现在STL中，就是以下四种排序容器: multiset、set、multimap、map multiset的用法: multiset上的迭代器 自定义排序规则的multiset用法:12.4. set set和multiset的区别在于容器里面不能有重复元素 set插入元素可能不成功 pair模板的用法 set的例子12.5. multimap multimap容器里面的元素，都是pair形式的 multimap的应用 代码实现细节12.6. map map和multimap的区别: 不能有关键字重复的元素, 可以使用[], 下标为关键字, 返回值为first和关键字相同的元素的second 插入元素可能失败"
    } ,
  
    {
      "title"       : "机器学习",
      "category"    : "",
      "tags"        : "note",
      "url"         : "./Machine-Learning.html",
      "date"        : "2021-12-22 00:00:00 +0800",
      "description" : "《机器学习》周志华读书笔记",
      "content"     : "目录 目录 1. 第1章 绪论 2. 第2章 模型评估与选择 2.1. 思维导图 2.2. 经验误差与过拟合 2.3. 评估方法 2.3.1. 留出法 2.3.2. 交叉验证法 2.3.3. 自助法 2.4. 性能度量 2.4.1. 错误率与精度 2.4.2. 查准率、查全率与F1 2.4.3. ROC与AUC 2.4.4. 代价敏感错误率与代价曲线 2.5. 比较检验 2.5.1. 假设检验 2.5.2. 交叉验证t检验 2.5.3. McNemar检验 2.5.4. Friedman检验与Nemenyi后续检验 2.6. 偏差与方差 3. 第3章 线性模型 3.1. 思维导图 3.2. 基本形式 3.3. 线性回归 3.4. 对数几率回归 3.5. 线性判别分析 3.6. 多分类学习 3.7. 类别不平衡问题 4. 第4章 决策树 4.1. 思维导图 4.1.1. 章节导图 4.1.2. 如何生成一棵决策树 4.2. 基本流程 4.3. 划分选择 4.3.1. 信息增益 4.3.2. 增益率 4.3.3. 基尼指数 4.4. 剪枝处理 4.4.1. 预剪枝 4.4.2. 后剪枝 4.5. 连续与缺失值 4.5.1. 连续值处理 4.5.2. 缺失值处理 4.6. 多变量决策树 4.7. 阅读材料 5. 第5章 神经网络 5.0. 思维导图 5.1. 神经元模型 5.2. 感知机与多层网络 5.3. 误差逆传播算法 5.4. 全局最小与局部极小 5.5. 其他常见神经网络 5.5.1. RBF网络 5.5.2. ART网络 5.5.3. SOM网络 5.5.4. 级联相关网络 5.5.5. Elman网络 5.5.6. Boltzmann机 5.6. 深度学习 6. 第6章 支持向量机 6.0. 思维导图 6.1. 间隔与支持向量 6.2. 对偶问题 6.3. 核函数 6.4. 软间隔与正则化 6.5. 支持向量回归 6.6. 核方法 7. 第7章 贝叶斯分类器 7.0. 思维导图 7.1. 贝叶斯决策论 7.2. 极大似然估计 7.3. 朴素贝叶斯分类器 7.4. 半朴素贝叶斯分类器 7.5. 贝叶斯网 7.5.1. 结构 7.5.2. 学习 7.5.3. 推断 7.6. EM算法 1. 第1章 绪论2. 第2章 模型评估与选择2.1. 思维导图2.2. 经验误差与过拟合定义： 错误率(error rate): 如果m个样本中有a个样本分类错误，则错误率E=a/m。 精度(accuracy)：精度=1-错误率。 训练误差：学习器在训练集上的误差称为训练误差或者经验误差。 泛化误差(generalization error)：学习器在新样本上的误差称为泛化误差。 过拟合(overfitting)：当学习器把训练样本学得太好了的时候，很可能把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这就会导致泛化性能下降。 欠拟合(underfitting)：对训练样本的一般性质尚未学好。2.3. 评估方法通常我们可通过实验测试来对学习器的泛化误差进行评估而做出选择，于是我们需要一个测试集(testing set)，然后以测试集上的测试误差作为泛化误差的近似。下面介绍一些常见的处理数据集D的方法(数据集D-&gt;训练集S+测试集T)12.3.1. 留出法 留出发(hold-out)直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T。在S上训练出模型后，用T来评估其测试误差。 以采样的角度来看待数据集划分的过程，则保留类别比例的采样方式通常称为分层采样。比如1000个样本，50%的正例，50%负例，以7：3划分数据集，那么训练集包含350个正例和350个负例就是分层采样。 在使用留出法评估结果时，一般要采用若干次随即划分、重复进行实验评估后取平均值作为留出法的评估结果。 常用做法时将大约2/3~4/5的样本用于训练。2.3.2. 交叉验证法 交叉验证法(cross validation)先将数据集D划分为k个大小相似的互斥子集，每个子集Di都尽可能保持数据分布的一致性。然后每次用k-1个自己的并集作为训练集，余下的那个子集作为测试集，这样就可以获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。 交叉验证也称为k折交叉验证。k的常见取值有10、5、20。 留一法就是k=m，其中数据集D有m个样本。2.3.3. 自助法 自助法(bootstrapping)：给定包含m个样本的数据集D，每次随机从D中挑选一个样本，将其拷贝放入D’, 然后再将该样本放回最初的数据集D中，这个过程重复执行m次后，我们获得了包含m个样本的数据集D’。我们将D’作为训练集，D\\D’作为测试集。 不难发现大概有36.8%(m趋于无限大时)的样本在m次采样中始终不被采到。$\\lim\\limits_{m\\rightarrow\\infty}(1-\\frac{1}{m})^m = \\frac{1}{e} ≈ 0.368$ 缺点：自助法产生的数据集改变了初始数据集的分布，这样会引入估计偏差。因此，在初始数据量足够时，留出法和交叉验证法更常用一些。2.4. 性能度量 对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量(performance measure)。 在预测任务中，给定D = {(x1,y1), (x2,y2)…(xm,ym)}, 其中yi是xi的真实标记。学习器f。 均方误差(mean squared error)：回归任务最常用的性能度量是均方误差：$E(f;D)=\\frac{1}{m}\\sum_1^m(f(x_i)-y_i)^2$接下来我将介绍分类任务中常用的性能度量2.4.1. 错误率与精度本章开头提到了错误率和精度，这是分类任务中最常用的两种性能度量，既适用于二分类任务，也适用于多分类任务。2.4.2. 查准率、查全率与F1 针对二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例(true positive)、假正例(false positive)、真反例(true negative)、假反例(false negative)，分别记为TP、FP、TN、FN。 混淆矩阵(confusion matrix) 真\\预 正例 反例 正例 TP FN 反例 FP TN 查准率(precision)，记为P，它表示选择的好瓜中有多少是真正的好瓜$P=\\frac{TP}{TP+FP}$ 查全率(recall)，记为R，它表示好瓜中有多少被选出来了$R=\\frac{TP}{TP+FN}$ 一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。 P-R曲线：在很多情形下，我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器认为最可能是正例的样本，排在最后的则是学习器认为最不可能是正例的样本，按此顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率、查准率，也得到了P-R图。 如果一个学习器的P-R曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者。如果两者曲线相交，则可以通过比较平衡点(break-even-point)来比较，越大越好。 F1度量：F1综合考虑了查准率和查全率，是他们的调和平均 $F1=\\frac{2*P*R}{P+R}$ F1的一般形式：有时候我们希望赋予查准率和查重率不同的权重：$F_\\beta=\\frac{(1+\\beta^2)*P*R}{\\beta^2*P+R}$其中β大于1表示查全率有更大影响 有时候我们有多个二分类混淆矩阵，我们希望在这n个混淆矩阵上综合考虑查准率和查全率，那么我们有以下的度量 宏查准率macro-P，宏查全率macro-R，宏F1： $macro-P=\\frac{1}{n}\\sum_1^nP_i$$macro-R=\\frac{1}{n}\\sum_1^nR_i$ 微查准率micro-P，微查全率micro-P，微F1：对TP、FP、TN、FN进行平均$micro-P=\\frac{\\overline{TP}}{\\overline{TP}+\\overline{FP}}$$micro-P=\\frac{\\overline{TP}}{\\overline{TP}+\\overline{FN}}$2.4.3. ROC与AUC 很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为反类。 ROC曲线是从排序角度来出发研究学习器的泛化性能的工具。ROC的全称是Receiver Operating Characteristic。 与P-R曲线类似，我们根据学习器的预测结果进行排序，按此顺序逐个把样本作为正例进行预测，计算出真正例率TPR(true positive rate)与假正例率FPR(false positive rate)并以它们分别为横纵坐标画出ROC曲线 $TPR=\\frac{TP}{TP+FN}$$FPR=\\frac{FP}{FP+TN}$ 同样的，如果一个学习器的ROC曲线被另一个学习器完全包住，则认为后者的性能优于前者。若两个曲线相交，则比较曲线下的面积AUC(area under curve) 形式化的看，AUC考虑的是样本预测的排序质量，那么我们可以定义一个排序损失lrank AUC=1-lrank2.4.4. 代价敏感错误率与代价曲线 有时候将正例误判断为负例与将负例误判断为正例所带来的后果不一样，我们赋予它们非均等代价 代价矩阵(cost matrix) 真\\预 第0类 第1类 第0类 0 cost01 第1类 cost10 0 其中costij表示将第i类样本预测为第j类样本的代价 代价敏感错误率 在非均等代价下，ROC曲线不能直接反映出学习器的期望总代价，而代价曲线则可达到此目的，横轴是正例概率代价，纵轴是归一化代价  2.5. 比较检验我们有了实验评估方法与性能度量，是否就可以对学习器的性能进行评估比较了呢？实际上，机器学习中性能比较这件事比大家想象的要复杂很多，我们需要考虑多种因素的影响，下面介绍一些对学习器性能进行比较的方法，本小节默认以错误率作为性能度量。(但我觉得似乎同一个数据集下就可以比较)2.5.1. 假设检验 假设检验中的假设是对学习器泛化错误率分布的某种猜想或者判断，例如“ε=ε0”这样的假设 现实任务中我们并不知道学习器的泛化错误率，我们只能获知其测试错误率$\\hat{\\epsilon}$，但直观上，两者接近的可能性比较大，因此可根据测试错误率估推出泛化错误率的分布。 泛化错误率为$\\epsilon$的学习器被测得测试错误率为$\\hat{\\epsilon}$的概率： 我们发现$\\epsilon$符合二项分布 二项检验：我们可以使用二项检验(binomial test)来对“$\\epsilon$&lt;0.3”这样的假设进行检验，即在$\\alpha$显著度下，$1-\\alpha$置信度下判断假设是否成立。 t检验：我们也可以用t检验(t-test)来检验。 上面介绍的都是针对单个学习器泛化性能的假设进行检验 2.5.2. 交叉验证t检验 对于两个学习器A和B，若我们使用k折交叉验证法得到的测试错误率分别是$\\epsilon_1^A$, $\\epsilon_2^A$…$\\epsilon_k^A$和$\\epsilon_1^B$, $\\epsilon_2^B$…$\\epsilon_k^B$。其中$\\epsilon_i^A$和$\\epsilon_i^B$是在相同第i折训练集上得到的结果。则可以用k折交叉验证“成对t检验”来进行比较检验。 我们这里的基本思想是若两个学习器的性能相同，则它们使用相同的训练测试集得到的错误率应该相同，即$\\epsilon_i^A=\\epsilon_1^B$ $\\Delta_i$ = $\\epsilon_i^A$ - $\\epsilon_i^B$，然后对$\\Delta$进行分析 2.5.3. McNemar检验 对于二分类问题，使用留出法不仅可以估计出学习器A和B的测试错误率，还可获得两学习器分类结果的差别，即两者都正确、都错误…的样本数 若我们假设两学习器性能相同，则应有e01=e10，那么变量|e01-e10|应该服从正态分布/卡方分布，然后用McNemar检验2.5.4. Friedman检验与Nemenyi后续检验 交叉验证t检验与McNemar检验都是在一个数据集上比较两个算法的性能，但是很多时候，我们会在一组数据集上比较多个算法。 当有多个算法参与比较时，一种做法是在每个数据集上本别列出两两比较的结果。另一种方法则是基于算法排列的Friedman检验 假定我们用D1, D2, D3, D4四个数据集对算法A、B、C进行比较。首先，使用留出法或交叉验证法得到每个算法在每个数据集上的测试结果。然后在每个数据集上根据测试性能由好到坏排序，并赋予序值1,2,…若算法的测试性能相同，则平分序值。 然后使用Fredman检验来判断这些算法是否性能都相同，若相同，那么它们的平均序值应当相同。ri表示第i个算法的平均序值，那么它的均值和方差应该满足… 若“所有算法的性能相同”这个假设被拒绝，则说明算法的性能显著不同，这时需要后续检验(post-hoc test)来进一步区分各算法，常用的有Nemenyi后续检验 Nemenyi检验计算出平均序值差别的临界值域 在表中找到k=3时q0.05=2.344，根据公式计算出临界值域CD=1.657，由表中的平均序值可知A与B算法的差距，以及算法B与C的差距均未超过临界值域，而算法A与C的差距超过临界值域，因此检验结果认为算法A与C的性能显著不同，而算法A与B以及算法B与C的性能没有显著差别。2.6. 偏差与方差 对学习算法除了通过实验估计其泛化性能，人们往往还希望了解它“为什么”具有这样的性能。偏差-方差分解是解释学习算法泛化性能的一种重要工具 偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力 方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响 噪声则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度 一般来说，偏差与方差是有冲突的，也就是偏差大的方差小，偏差小的方差大3. 第3章 线性模型3.1. 思维导图3.2. 基本形式 给定由d个属性描述的示例$x=(x_1;x_2;…x_d)$，这是一个列向量，其中$x_i$是$x$在第i个属性上的取值。 线性模型(linear model)试图学得一个通过属性线性组合来进行预测的函数： 向量形式：$f(x)=\\omega^Tx+b$其中$\\omega=(\\omega_1;\\omega_2…\\omega_d)$ 当$\\omega$和b学得后，模型就得以确定 线性模型的优点：形式简单，易于建模，良好的可解释性(comprehensibility) 3.3. 线性回归 对离散属性的处理，若属性值存在“序”的关系，可通过连续化将其转化为连续值，比如高、矮变成1、0；2.若不存在序的关系，可转化为k维向量。 均方误差是回归任务中最常用的性能度量，试图让均方误差最小化： 均方误差有非常好的几何意义，它对应了欧氏距离。基于均方误差最小化来进行模型求解的方法称为最小二乘法(least square method)。在线性回归中，最小二乘法就是试图找到一条直线，使得样本到直线上的欧氏距离之和最小 首先观察一个属性值的情况。求解$\\omega$和$b$使得均方误差最小化的过程，称为线性回归的最小二乘“参数估计”，我们令均方误差分别对$\\omega$和$b$求导令其为零，可以得到最优解的闭式解(closed-form),即解析解   更一般的情况是d个属性，称其为“多元线性回归”，同样的步骤，只是$\\omega$变成向量形式，自变量写成m*(d+1)的矩阵形式，m对应了m个样本，d+1对应了d个属性和偏置。同样的求导为0然后得出$\\hat{\\omega}$最优解的闭式解，其中$\\hat{\\omega}=(\\omega;b)$。当$X^TX$为满秩矩阵2或正定矩阵时，有唯一的解：     然而现实任务中往往不是满秩矩阵，例如在许多任务中我们会遇到大量的变量其数目甚至超过样例数。那么我们会求出$\\hat{\\omega}=(\\omega;b)$的多个解，它们都能使均方误差最小化。选择哪一个解作为输出与学习算法的归纳偏好决定，常见的做法是引入正则化(regularization) 广义线性模型(generalized linear model): $g(y)=\\omega^Tx+b$其中g()单调可微，被称为联系函数。比如当g()=ln()时称为对数线性回归3.4. 对数几率回归 上一节讨论的是线性回归进行回归学习。那么如果我们要做的是分类任务应该怎么改变？我们只需要找到一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。 考虑二分类问题，那么我们可以用单位阶跃函数/Heaviside函数联系y与z，其中y是真是标记，z是预测值，$z=\\omega^Tx+b$ 但是它有个问题就是不连续，所以我们希望找到能在一定程度上近似它的替代函数。 对数几率函数(logistic function)就是一个替代函数： $y=\\frac{1}{1+e^{-z}}$ 那么这个式子可以转化为：可以把y视为样本x作为正例的可能性，则1-y是其反例的可能性，两者的比值称为几率(odds)： 若将y视为类后验概率估计。则式子可以重写为： 接下来我们可以通过极大似然法(maximum likelihood method)来估计$\\omega$和$b$。给定数据集，对数似然函数3为：即每个样本属于其真实标记的概率越大越好。 推导过程：上面有个式子应该有问题，(3.26)应该是$p(y_i|x_i;\\omega,b) = p_1(\\hat{x_i};\\beta)^{y_i}p_0(\\hat{x_i};\\beta)^{1-y_i}$因为$\\beta$是高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法如梯度下降法(gradient descent method)和牛顿法都可以求得最优解3.5. 线性判别分析 线性判别分析(Linear Discriminant Analysis，简称LDA)是一种经典的线性学习方法。 LDA的思想非常朴素: 给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离;在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。 令$X_i$、$\\mu_i$、$\\Sigma_i$分别表示第i类示例的集合、均值向量4、协方差矩阵5。 欲使同类样例的投影点尽可能接近，可以让同类样例投影点的协方差尽可能小，即$\\omega^T\\Sigma_0\\omega+\\omega^T\\Sigma_1\\omega$尽可能小;而欲使异类样例的投影点尽可能远离，可以让类中心之间的距离尽可能大，即$||\\omega^T\\mu_0-\\omega^T\\mu_1||$尽可能大: 剩余推导过程：  值得一提的是，LDA可从贝时斯决策理论的角度来阐释，并可证明，当两类数据同先验、满足高斯分布且协方差相等时，LDA可达到最优分类。3.6. 多分类学习 现实中常遇到多分类学习任务，有些二分类学习方法可直接推广到多分类，但在更多情形下，我们是基于一些基本策略，利用二分类学习器来解决多分类问题。 不失一般性，考虑N个类别$C_1$、$C_2$…$C_N$，多分类学习的基本思路是”拆解法”，即将多分类任务拆为若干个二分类任务求解。具体来说，先对问题进行拆分，然后为拆出的每个二分类任务训练一个分类器;在测试时，对这些分类器的预测结果进行集成以获得最终的多分类结果。 这里我们着重介绍如何拆分，最经典的拆分策略有三种：一对一(One vs One)、一对其余(One vs Rest)、多对多(Many vs Many) 一对一：将这N个类别两两配对，从而产生 N(N-1)/2个二分类任务。在测试阶段，新样本将同时提交给所有分类器，于是我们将得到N(N-1)/2个分类结果，最终结果可通过投票产生:即把被预测得最多的类别作为最终分类结果。 一对其余：OvR则是每次将一个类的样例作为正例，所有其他类的样例作为反例来训练N个分类器。在测试时若仅有一个分类器预测为正类，则对应的类别标记作为最终分类结果。若有多个分类器预测为正类，则通常考虑各分类器的预测置信度，选择置信度最大的类别标记作为分类结果。 多对多MvM是每次将若干个类作为正类，若干个其他类作为反类。这里我们介绍一种最常用的MvM技术：纠错输出码(ECOC) ECOC是将编码的思想引入类别拆分，主要分为两步：   1.编码： 对N个类别做M次划分，每次划分将一部分类别划为正类，一部分划为反类，从而形成一个二分类训练集;这样一共产生M个训练集，可训练出M个分类器  2.解码：:M个分类器分别对测试样本进行预测，这些预测标记组成一个编码.将这个预测编码与每个类别各自的编码进行比较，返回其中距离最小的类别作为最终预测结果。 类别划分通过编码矩阵指定。编码矩阵有多种形式，常见的有二元码和三元码，前者将每个类别分别指定为正类和反类，后者在正、反类之外，还可指定“停用类”3.7. 类别不平衡问题 前面介绍的分类学习方法都有一个共同的基本假设：即不同类别的训练样例数目相当。如果不同类别的样例数差别很大，会对学习过程造成困扰。 类别不平衡(class-imbalance)就是指分类任务中不同类别的训练样例数目差别很大的情况。 再缩放(rescaling)是类别不平衡中的一个基本策略：比如在最简单的二分类问题中，我们假设y大于0.5为正例，y小于0.5为负例，但是在类别不平衡时，我们可以改变阈值来达到再平衡：   将  变成 现有的解决类别不平衡的技术大体上有三类做法(这里我们均假设正例样本少):  1.第一类是直接对训练集里的反类样例进行”欠采样” (undersampling)，即去除一些反例使得正、反例数日接近，然后再进行学习;  2.第二类是对训练集里的正类样例进行”过采样” (oversampling)，即增加一些正例使得正、反例数目接近，然后再进行学习;  3.第三类则是直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将上面的公式(改变阈值)嵌入到其决策过程中，称为”阔值移动” (threshold-moving) 需注意的是，过采样法不能简单地对初始正例样本进行重复来样，否则会招致严重的过拟合；过采样法的代表性算法SMOTE是通过对训练集里的正例进行插值来产生额外的正例.另一方面，欠采样法若随机丢弃反例可能丢失一些重要信息;欠采样法的代表性算法EasyEnsemble则是利用集成学习机制，将反例划分为若干个集合供不同学习器使用，这样对每个学习器来看都进行了欠采样，但在全局来看却不会丢失重要信息。4. 第4章 决策树4.1. 思维导图4.1.1. 章节导图4.1.2. 如何生成一棵决策树4.2. 基本流程 决策树是基于树结构来进行决策，其中包含一个根结点，多个内部结点和多个叶结点 叶结点对应决策结果，其他每个结点都对应一个属性测试 每个结点包含的样本集合根据属性测试被划分到子结点中，那么根结点包含样本全集 决策树学习的目的是为了产生一棵泛化能力强的决策树 决策树的生成是一个递归过程，下面这张图展示了递归的过程：对于每个结点，首先判断该结点的样本集是否属于同一个类别C，如果是，则将该结点标记为C类叶结点。再判断该结点的样本集的属性值是否完全相同(或者是否为空集)，如果是，则将该结点标记为D类叶结点，其中D类是这些样本中最多的类别。如果该结点即不是同属于一个类别也不是属性值取值相同，那么则需要继续划分，选择一个最优的划分属性$a_*$,创建新的分支，对于每个分支结点首先判断是否为空，如果为空则判定为E类叶结点，其中E类是父结点中类别最多的类。如果子结点不为空则递归。4.3. 划分选择可以发现生成决策树最关键的步骤就是选择最优划分属性，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的纯度越来越高。我们有很多指标来确定选择哪一个属性作为最优划分选择，下面将分别介绍：4.3.1. 信息增益 信息熵(information entropy)是度量样本集合纯度最常用的一种指标。下面是信息熵的定义公式：  其中$p_k$表示第k类样本在样本集D中所占比例，信息熵越小表示D的纯度越高。 假设离散属性a有V个可能的取值${a^1,a^2…a^V}$,那么我们可以计算出在使用a作为划分属性前后的信息熵差别，也就是信息增益(information gain)：  对每一个子结点$D^v$都赋予权重同时相加。 著名的ID3决策树学习算法就是以信息增益作为准则来选择划分属性，我们希望找到信息增益最大的属性。 书上使用信息增益划分的例子： 4.3.2. 增益率 信息增益对可取值数目较多的属性有所偏好，比如我们使用编号这一属性来划分，每一个编号都只有一个样本，那么信息增益肯定增大了，但是决策树的泛化能力显然下降了。 增益率(gain ratio)，我们通过对信息增益除以IV来平衡属性数目带来的影响，增益率的定义如下：   IV(intrinsic value)的定义如下：  IV是属性a的固有值，属性a可取的数值数目越多，那么IV就越大 但是增益率也有问题，那就是对于可取值数目较少的属性有偏好，所以著名的C4.5决策树算法并不是直接使用增益率，而是先从候选划分属性中找出信息增益高于平均水平的属性，然后再从中选择增益率最高的属性4.3.3. 基尼指数 CART决策树使用基尼指数来选择划分属性 数据集D的纯度定义如下  直观来说，Gini反映了从数据集随便抽取两个样本，它们类别不一致概率 基尼指数定义如下：  很明显，我们希望基尼指数越小越好，所以我们选择基尼指数最小的属性最为最优划分属性。4.4. 剪枝处理 不难发现，上面对于属性的划分很容易过拟合，所以针对过拟合现象，决策树选择剪枝(pruning)来对付过拟合 剪枝就是去掉一些分支来降低过拟合的风险，剪枝可以分为预剪枝和后剪枝 预剪枝(prepruning):在决策树生成的过程中，对每个结点在划分前进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分 后剪枝(postpruning):从训练集生成了一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能的提升，则将该子树替换为叶结点。4.4.1. 预剪枝 如何判断决策树泛化性能？可以使用留出法预留一部分数据用作验证集进行性能评估，性能度量可以用之前介绍的那些，本小节使用精度作为性能度量 预剪枝生成的决策树： 可以发现预剪枝显著减少了分支的数量，这样可以减少决策树的训练时间开销。但是这样也有一个问题，就是有些分支的当前划分虽然不能提升泛化性能，但是后续划分却有可能导致性能显著提高，这样就带来了欠拟合的风险4.4.2. 后剪枝 首先生成决策树，然后对每个结点进行评估是否需要剪枝 虽然后剪枝决策树的欠拟合风险小，泛化性能也往往优于预剪枝决策树，但是后剪枝的时间开销大4.5. 连续与缺失值4.5.1. 连续值处理 到目前为止仅讨论了基于离散属性来生成决策树，但是现实学习任务中通常会遇到连续属性 很明显连续属性不能根据连续属性的可取值来对结点进行划分，我们需要用到连续属性离散化的技术，最简单的策略是二分法 给定样本集D和和连续属性a，假定a在D上出现了n个不同的取值，将这些值从小到大排序，然后每次选择每两个数的中位数t作为划分点，那么连续值就可以当作离散值来处理了，分出的子样本集分别记作$D_t^+$和$D_t^-$ 划分结果： 需要注意的是：连续属性在划分后并不会被丢失，后续划分仍然可以使用4.5.2. 缺失值处理 在实际数据中一般都有很多缺失值，所以我们需要考虑如何对含有缺失值的数据进行学习 给几个定义：$\\tilde{D}$表示D中属性a上没有缺失值的样本子集，假设属性值a可取值{$a^1$,$a^2$…$a^V$}, $\\tilde{D}^v$表示$\\tilde{D}$中属性值a取值为$a^v$的子集，$\\tilde{D}_k$表示样本子集，我们为每个样本赋予权重$\\omega_x$(决策树开始阶段，根结点中权重初始化为1)并定义： 直观地看，对属性a，$\\rho$表示无缺失值样本所占比例，$\\tilde{p}_k$表示无缺失样本中第k类样本所占的比例，$\\tilde{r}_v$表示无缺失值样本在属性上取值为$a^v$所占的比例 那么我们可以将信息增益的公式推广为   那么对于那些在该属性上缺失的值如何处理呢？分两种情况：若样本$x$在划分属性$a$上的取值己知, 则将$x$划入与其取值对应的子结点，且样本权值在于结点中保持为$\\omega_x$, 若样本$x$在划分属性$a$上的取值未知，则将$x$同时划入所有子结点, 且样本权值在与属性值$a^v$对应的子结点中调整为$\\tilde{r}_v*\\omega_x$，直观地看，这就是让同一个样本以不同的概率划入到不同的子结点中去。 C4.5就是使用了上述的解决方法 4.6. 多变量决策树 若我们把每个属性视为坐标空间中的一个坐标轴，则d个属性描述的样本就对应了d维空间中的一个数据点，对样本分类则意味着在这个坐标空间中寻找不同类样本之间的分类边界，决策树生成的分类边界有个明显的特点：轴平行，即它的分类边界由若干个与坐标轴平行的分段组成 这样的决策树由于要进行大量的属性测试，预测时间开销会很大，所以我们希望使用如下图红线所示的斜划分。多变量决策树就是能实现这样斜划分甚至更复杂划分的决策树 以实现斜划分的决策树为例，非叶结点不再是仅对某一个属性，而是对属性的线性组合进行测试4.7. 阅读材料 多变量决策树算法主要有OC1，还有一些算法试图在决策树的叶结点上嵌入神经网络，比如感知机树在每个叶结点上训练一个感知机 有些决策树学习算法可进行“增量学习”(incrementallearning)，即在接收到新样本后可对己学得的模型进行调整，而不用完全重新学习。主要机制是通过调整分支路径上的划分属性次序来对树进行部分重构，代表性算法ID4、ID5R、ITI等。增量学习可有效地降低每次接收到新样本后的训练时间开销，但多步增量学习后的模型会与基于全部数据训练而得的模型有较大差别。 5. 第5章 神经网络5.0. 思维导图5.1. 神经元模型 神经网络定义: 神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应 神经网络中最基本的成分是神经元模型(neuron) M-P神经元模型: 在这个模型中，神经元接收来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过激活函数处理以产生神经元的输出 激活函数: 有阶越函数和Sigmoid函数等等5.2. 感知机与多层网络 感知机(perceptron)由两层神经元组成，如下图所示 权重$\\omega_i$以及阈值$\\theta$可通过学习得到 感知机的学习规则非常简单，对训练样例(x, y)，若当前感知机的输出为$\\hat{y}$, 则感知机权重将这样调整 其中$\\eta$称为学习率(learning rate) 感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元，其学习能力非常有限 要解决非线性可分问题，需考虑使用多层功能神经元，比如下图，输入层和输出层之间的一层神经元被称为隐含层(hidden layer)，隐含层和输出层神经元都是拥有激活函数的功能神经元 多层前馈神经网络(multi-layer feedforward neural networks): 每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接5.3. 误差逆传播算法 误差逆传播算法(error BackPropagation)简称BP算法，可用于很多类型的神经网络 误差采用均方误差: BP算法基于梯度下降(gradient descent)策略，以目标的负梯度方向对参数进行调整 学习率控制着算法每一轮迭代中的更新步长，若太大则容易振荡，太小则收敛速度又会过慢 上面介绍的标准BP算法每次仅针对一个训练样例更新连接权和阈值，我们也可以简单推出基于累积误差最小化的更新规则，就得到了累积BP算法 正是由于其强大的表达能力，BP神经网络经常遭遇过拟合，其训练误差持续降低，但测试误差却可能上升，由两种策略常用来缓解BP网络的过拟合。 第一种策略是早停(early stopping): 若训练集误差降低但验证集误差升高，则停止训练 第二种策略是正则化，其基本思路是在误差目标函数中增加一个用于描述网络复杂度的部分，例如连接权和阈值的平方和，那么误差目标函数变成: 5.4. 全局最小与局部极小 我们常常会谈到两种最优: 局部极小(local minimum)和全局最小(global minimum) 直观地看，局部极小解是参数空间中的某个点，其领域点的误差函数值均不小于该点的函数值 全局最小解是指参数空间中所有点的误差函数值均不小于该点的误差函数值 在现实任务中，人们常采用以下策略来试图跳出局部极小，从而进一步实现全局最小 以多组不同参数值初始化多个神经网络 使用“模拟退火”(simulated annealing)技术，每一步都以一定概率接受比当前解更差的结果，从而有助于跳出局部极小 随机梯度下降，即每次使用随机的样本进行误差计算 5.5. 其他常见神经网络5.5.1. RBF网络 RBF(Radial Basis Function, 径向基函数)网络是一种单隐层前馈神经网络，它使用径向基函数作为隐层神经元激活函数，而输出层则是对隐层神经元输出的线性组合 具有足够多隐层神经元的RBF网络能以任意精度逼近任意连续函数5.5.2. ART网络 竞争型学习(competitive learning)是神经网络中一种常用的无监督学习策略，在使用该策略时，网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元的状态被抑制。这种机制亦称胜者通吃(winner-take-all)原则 ART(Adaptive Resonance Theory，自适应谐振理论)网络是竞争型学习的重要代表。该网络由比较层、识别层、识别阔值和重置模块构成。其中，比较层负责接受输入样本，并将其传递给识别层神经元。识别层每个神经元对应一个模式类，神经元数目可在训练过程中动态增长以增加新的模式类5.5.3. SOM网络 SOM(Self-Organizing Map，自组织映射)网络是一种竞争学习型的无监督神经网络，它能将高维输入数据映射到低维空间(通常为二维) ，同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元5.5.4. 级联相关网络 一般的神经网络模型通常假定网络结构是事先固定的，训练的目的是利用训练样本来确定合适的连接权、阈值等参数。与此不同，结构自适网络则将网络结构也当作学习的目标之一，并希望能在训练过程中找到最符合数据特点的网络结构-级联相关(Cascade-Correlation)网络是结构自适应网络的重要代表5.5.5. Elman网络 与前馈神经网络不同递归神经网络(recurrent neural networks)允许网络中出现环形结构，从而可让一些神经元的输出反馈回来作为输入信号。这样的结构与信息反馈过程，使得网络在t时刻的输出状态不仅与t时刻的输入有关，还与t-1时刻的网络状态有关，从而能处理与时间有关的动态变化 Elman网络是最常用的递归神经网络之一，其结构如图所示，它的结构与多层前馈网络很相似，但隐层神经元的输出被反馈回来，与下一时刻输入层神经元提供的信号一起，作为隐层神经元在下一时刻的输入。隐层神经元通常采用Sigmoid激活函数，而网络的训练则常通过推广的BP算法进行5.5.6. Boltzmann机 神经网络中有一类模型是为网络状态定义一个能量(energy)，能量最小化时网络达到理想状态，而网络的训练就是在最小化这个能量函数。Boltzmann机就是一种基于能量的模型(energy-based model)，常见结构如图所示，其神经元分为两层:显层与隐层。显层用于表示数据的输入与输出，隐层则被理解为数据的内在表达5.6. 深度学习 理论上来说，参数越多的模型复杂度越高、 “容量” (capacity) 越大，这意味着它能完成更复杂的学习任务。但一般情形下，复杂模型的训练效率低，易陷入过拟合，因此难以受到人们青睐。而随着云计算、大数据时代的到来，计算能力的大幅提高可缓解训练低效性，训练数据的大幅增加则可降低过拟合风险，因此，以”深度学习”(deep learning)为代表的复杂模型开始受到人们的关注。 无监督逐层训练(unsupervised layer-wise training)是多隐层网络训练的有效手段，其基本思想是每次训练一层隐结点，训练时将上一层隐结点的输出作为输入，向本层隐结点的输出作为下一层隐结点的输入，这称为预训练(pre-training);在预训练全部完成后，再对整个网络进行微调(fine-tuning)训练 另一种节省训练开销的策略是权共享，即让一组神经元使用相同的连接权，比如CNN 以往在机器学习用于现实任务时，描述样本的特征通常需由人类专家来设计，这称为特征工程(feature engineering)，深度学习则通过机器学习技术自身产生好特征 神经网络是一种难以解释的黑箱模型6. 第6章 支持向量机6.0. 思维导图6.1. 间隔与支持向量 给定训练样本集$D={(x_1, y_1),(x_2, y_2),…,(x_m, y_m)}$, 其中$y_i\\in${$-1, +1$}, 也就是一个传统的分类问题。分类学习最基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开，但能将训练样本分开的划分超平面可能有很多，如下图 直观上看，应该去找位于两类训练样本正中间的划分超平面，因为该划分超平面对训练样本局部扰动的容忍性最好 在样本空间中，划分超平面可通过如下线性方程来描述: $\\omega^Tx+b=0$, 其中$\\omega$为法向量, 决定了超平面的方向, b为位移项, 决定了超平面与原点的距离, 显然划分超平面可被法向量$\\omega$和b确定 样本空间中任意点x到超平面的距离可写为 假设超平面能将训练样本正确分类, 即对($x_i, y_i$)$\\in$D, 我们有以下结果: 支持向量: 距离超平面最近的几个训练样本点使得上述条件等号成立，它们就被称为支持向量(support vector), 两个异类支持向量到超平面的距离之和为间隔$\\gamma$ 图示: 欲找到具有最大间隔的划分超平面，也就是找到能满足约束的最大的$\\gamma$ 为了最大化间隔，仅需最大化||$\\omega$||$^{-1}$, 这等价于最小化||$\\omega$||$^{2}$, 于是上述式子可重写为: 这就是支持向量机(Support Vector Machine, SVM)的基本型6.2. 对偶问题 我们注意到SVM的基本型本身是一个凸二次规划问题6, 能直接用现成的优化计算包求解，但我们可以有更高效的方法 对上述式子使用拉格朗日乘子法7可得到其对偶问题，则该问题的拉格朗日函数可写为 其中$\\alpha$=($\\alpha_1$;$\\alpha_2$…;$\\alpha_m$), 令L($\\omega$, b, $\\alpha$)对$\\omega$和b的偏导为零可得 将这两个条件带入之前的拉格朗日函数中，即可将$\\omega$和b消去，得到以下的对偶问题: 从对偶问题中解出的$\\alpha_i$是拉格朗日乘子，它恰对应着训练样本($x_i$, $y_i$), 在主问题中有不等式约束，因此上述过程需满足KTT条件, 即要求: 解出$\\alpha$后，求出$\\omega$与b即可得到模型: 于是, 对任意训练样本($x_i$, $y_i$), 总有$\\alpha_i=0$或者$y_if(x_i)=1$。若$\\alpha_i=0$，则该样本将不会在最终的模型的求和中出现，也就不会对f(x)有任何影响;若$\\alpha_i&gt;0$, 则必有$y_if(x_i)=1$，所对应的样本点位于最大间隔边界上，是一个支持向量。这显示出支持向量机的一个重要性质:训练完成后, 大部分的训练样本都不需保留，最终模型仅与支持向量有关 那么如何求解对偶问题解出$\\alpha_i$呢？这是一个二次规划问题, 可使用通用的二次规划算法求解，然而，该问题的规模正比于训练样本数，这会在实际任务中造成很大的开销，为了避开这个障碍，人们通过利用问题本身的特性，提出了很多高效算法, SMO是其中一个著名的代表 SMO的基本思路是先固定$\\alpha_i$之外的所有参数，然后求$\\alpha_i$上的极值，由于存在约束$\\sum_1^m\\alpha_iy_i=0$,若固定$\\alpha_i$之外的其他变量，则$\\alpha_i$可由其他变量导出，于是SMO每次选择两个变量$\\alpha_i$和$\\alpha_j$，并固定其他参数。这样，在参数初始化后，SMO不断执行如下两个步骤直至收敛 选取一对需更新的变量$\\alpha_i$和$\\alpha_j$ 固定$\\alpha_i$和$\\alpha_j$以外的参数，求解获得更新后的$\\alpha_i$和$\\alpha_j$ 如何确定偏移项b呢？注意到对任意支持向量($x_s$, $y_s$)都有$y_sf(x_s)=1$: 其中S为所有支持向量的下标集，理论上可以选取任意支持向量来求出b，但现实任务中常采用一种更鲁棒的做法: 使用所有支持向量求解的平均值6.3. 核函数 在本章前面的讨论中，我们假设训练样本是线性可分的，然而在现实任务中，原始样本空间内也许并不存在一个能正确划分两类样本的超平面，例如异或问题就不是线性可分的 对这样的问题，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分，例如将异或问题的原始二维空间映射到一个合适的三维空间，就能找到一个合适的划分超平面。如果原始空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本可分 令$\\phi(x)$表示将x映射后的特征向量，于是在特征空间中划分超平面所对应的模型可表示为 其中$\\omega$和b是模型参数，我们有以下二次规划: 其对偶问题是: 我们观察到在对偶问题中涉及到计算$\\phi(x_i)^T\\phi(x_j)$, 这是样本$x_i$与$x_j$映射到特征空间之后的内积。由于特征空间维数可能很高，甚至是无穷维，因此直接计算$\\phi(x_i)^T\\phi(x_j)$通常是困难的，为了避开这个障碍，可以设想这样一个函数: 即$x_i$与$x_j$在特征空间的内积等于它们在原始样本空间中通过函数$\\kappa(·,·)$计算的结果, 有了这样的函数，我们就不必直接去计算高维甚至无穷维特征空间中的内积，于是式子可重写为 求解后可得到: 上面式子显示出模型最优解可通过训练样本的核函数展开，这一展式亦称支持向量展式 显然，若已知合适映射$\\phi(·)$的具体形式，则可写出核函数$\\kappa(·,·)$，但在现实任务中我们通常不知道$\\phi(·)$是什么形式，那么合适的核函数是否一定存在呢？什么样的函数能做核函数呢？我们有以下定理: 核函数选择成为支持向量机的最大变数，若核函数选择不合适，则意味着将样本映射到一个不合适的特征空间，很可能导致性能不佳 常用核函数: 此外，还可通过函数组合得到: 两个核函数的线性组合 两个核函数的直积 若$\\kappa_1$为核函数，则对任意函数g(x): 也是核函数 6.4. 软间隔与正则化 在前面的讨论中，我们一直假定训练样本在样本空间或特征空间中是线性可分的，即存在一个超平面能将不同类的样本完全划分开。然而，在现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分 缓解该问题的一个方法是允许支持向量机在一些样本上出错，为此，要引入软间隔(soft margin)的概念 具体来说，前面介绍的支持向量机形式都是要求所有样本均满足约束，即所有样本必须划分正确，这称为硬间隔，而软间隔则允许某些样本不满足约束 优化目标可写为: 其中C&gt;0是一个常数，$l_{0/1}$是0/1损失函数 显然，当C无穷大时，上面式子迫使所有样本均满足约束，当C取有限值时，式子允许一些样本不满足约束 然而0/1损失函数非凸、非连续，数学性质不太好，于是人们通常用其他一些函数来替代它，下面给出三个常用的替代损失函数： 若采用hinge损失，则式子变成: 引入松弛变量$\\xi_i$≥0，可将式子重写为 这就是常用的软间隔支持向量机，可以看出每个样本都有一个对应的松弛变量，用以表征样本不满足约束的程度 这仍是一个二次规划的问题，于是，通过拉格朗日乘子法可得到拉格朗日函数: 其中$\\alpha_i≥0$, $\\mu_i≥0$是拉格朗日乘子 偏导为0后可得到对偶问题: KTT条件: 可以发现软间隔支持向量机的最终模型仅与支持向量有关 我们还可以把0/1损失函数替换成别的替代损失函数以得到其他学习模型，这些模型的性质与所用的替代函数直接相关，但它们具有一个共性: 优化目标中的第一项用来描述划分超平面的间隔大小，另一项用来描述训练集上的误差 其中$\\Omega(f)$称为“结构风险”(structural risk)，用于描述模型f的某些性质;第二项$\\sum_i^ml(f(x_i),y_i)$称为“经验风险”(empirical risk)，用于描述模型与训练数据的契合程度; C用于对二者进行折中。从经验风险最小化的角度来看，$\\Omega(f)$表述了我们希望获得具有何种性质的模型(例如希望获得复杂度较小的模型)，这为引入领域知识和用户意图提供了途径; 另一方面，该信息有助于削减假设空间，从而降低了最小化训练误差的过拟合风险。从这个角度来说，上面的公式为“正则化”(regularization)问题，$\\Omega(f)$称为正则化项，C则称为正则化常数。$L_p$范数(norm)是常用的正则化项，其中$L_2$范数$||\\omega||_2$倾向于$\\omega$的分量取值尽量均衡，即非零分量个数尽量稠密，而$L_0$范数$||\\omega||_0$和$L_1$范数$||\\omega||_1$则倾向于$\\omega$的分量尽量稀疏，即非零分量个数尽量少6.5. 支持向量回归 现在我们来考虑回归问题，给定训练样本$D={(x_1, y_1), (x_2, y_2), …, (x_m, y_m))}，希望学得一个形如$f(x)=\\omega^Tx+b$的回归模型，使得f(x)与y尽可能接近。 传统回归模型通常直接基于输出f(x)与真实输出y之间的差别来计算损失，当且仅当f(x)与y完全相同时，损失才为0。与此不同，支持向量回归(Support Vector Regression, 简称SVR)假设我们能容忍f(x)与y之间最多有$\\epsilon$的偏差，如下图所示，若样本落入此间隔带，则被认为预测正确 于是SVR问题可形式化为: 其中C为正则化常数，$l_{\\epsilon}$是$\\epsilon$-不敏感损失函数: 引入松弛变量$\\xi_i$和$\\hat{\\xi_i}$，可将上述式子重写为: 类似地，通过引入拉格朗日乘子可得到拉格朗日函数: 偏导为零得到对偶问题: KTT条件: 将上述条件代入，SVR的解形如: 能使式子中$(\\hat{\\alpha}_i-\\alpha_i)≠0$的样本即为SVR的支持向量，它们必落在$\\epsilon$-间隔带之外 由KTT条件可看出，对每个样本($x_i, y_i$)都有$(C-\\alpha_i)\\xi_i=0$且$\\alpha_i(f(x_i)-y_i-\\epsilon-\\xi_i)=0$，于是在得到$\\alpha_i$后，若0&lt;$\\alpha_i$&lt;C, 则必有$\\xi_i=0$,进而有: 理论上说，可任意选取满足的$\\alpha_i$来得到b，但是实践中常采用一种更鲁棒的方法: 选取多个满足0&lt;$\\alpha_i$&lt;C的样本通过上述公式求解b后取平均值 若考虑特征映射，则:6.6. 核方法 若不考虑偏移项b，则无论SVM还是SVR，学得的模型总能表示成核函数的线性组合。不仅如此，事实上我们有下面这个称为“表示定理”(representer theorem)的更一般的结论 人们发展出一系列基于核函数的学习方法，统称为核方法，最常见的是通过引入核函数来将线性学习器拓展为非线性学习器 支持向量机是针对二分类任务设计的，对多分类任务要进行专门的推广7. 第7章 贝叶斯分类器7.0. 思维导图7.1. 贝叶斯决策论 贝叶斯决策论是概率框架下实施决策的基本方法。对于分类任务来说，在所有相关概率都已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。 以多分类任务为例: 假设有N种可能的类别标记，即$Y=${$c_1, c_2,…,c_N$}, $\\lambda_{ij}$是将真实标记为$c_j$的样本误分类为$c_i$所产生的损失。基于后验概率8$P(c_i|x)$可获得将样本x分类为$c_i$所产生的期望损失，即在样本上的条件风险: 我们的任务是寻找一个判定准则h: X -&gt; Y以最小化总体风险 显然，对每个样本x，若h能最小化条件风险R(h(x)|x)，则总体风险R(h)也将被最小化。这就产生了贝叶斯判定准则: 为最小化总体风险，只需在每个样本上选择那个能使条件风险R(c|x)最小的类别标记，即: 此时，$h^*$称为贝叶斯最优分类器，与之对应的总体风险R($h^*$)称为贝叶斯风险。1-R($h^*$)反映了分类器所能达到的最好性能。 具体来说，若目标是最小化分类错误率，则误判损失$\\lambda_{ij}$可写为: 此时条件风险为: 于是，最小化分类错误率的贝叶斯最优分类器为: 即对每个样本，选择能使后验概率$P(c|x)$最大的类别标记 不难看出，欲使用贝叶斯判定准则来最小化决策风险，首先要获得后验概率$P(c|x)$。然而，在现实任务中这通常难以直接获得，从这个角度来看，机器学习所要实现的是基于有限的训练样本集尽可能准确地估计出后验概率$P(c|x)$。大体来说，主要有两种策略: 给定x，可通过直接建模$P(c|x)$来预测c，这样得到的是判别式模型。也可以先对联合概率分布P(x,c)建模，然后再由此获得$P(c|x)$，这样得到的是生成式模型。 对于生成式模型来说，必须考虑: 基于贝叶斯定理，$P(c|x)$可写为: 其中，P(c)是类先验概率，P(x|c)是样本x相对于类标记c的类条件概率，或称为似然(likelihood)。P(x)是用于归一化的证据因子，P(x)对所有类标记均相同。因此估计P(c|x)的问题就转化为如何基于训练数据D来估计先验概率P(c)和似然P(x|c). 类先验概率P(c)表达了样本空间中各类样本所占的比例，根据大数定律，当训练集包含充足独立同分布的样本时，P(c)可通过各类样本出现的频率进行估计 对类条件概率P(x|c)来说，由于它涉及关于x所有属性的联合概率，直接根据样本出现的频率来估计将会遇到严重的困难，例如样本d个属性都是二值的，则样本空间将有$2^d$种可能取值，在现实应用中，这个值往往远大于训练样本数m，因此不能用频率估计概率7.2. 极大似然估计 估计类条件概率的一种常用策略是先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布进行参数估计。具体来说，记关于类别c的类条件概率为P(x|c)，假设P(x|c)具有确定的形式且被参数向量$\\theta_c$唯一确定，则我们的任务就是利用训练集D估计参数$\\theta_c$ 概率模型的训练过程就是参数估计的过程，统计学界的两个学派提出了不同的解决方案，频率主义学派认为参数虽然未知，但却客观存在固定值，因此可以通过优化似然函数等准则来确定参数值。本节介绍源自频率主义学派的极大似然估计9 令$D_c$表示训练集D中第c类样本组成的集合，假设这些样本是独立同分布的，则参数$\\theta_c$对于数据集$D_c$的似然是: 对$\\theta_c$进行极大似然估计，就是去寻找最大化似然$P(D_c|\\theta_c)$的参数值$\\hat{\\theta_c}$ 上述似然中的连乘操作易造成下溢，通常使用对数似然(log-likelihood) 此时参数$\\hat{\\theta_c}$的极大似然估计$\\hat{\\theta_c}$为 例如，再连续属性情形下，假设概率密度函数p(x|c)~N($\\mu_c, \\sigma_c^2$)， 则参数$\\mu_c$和$\\sigma_c^2$的极大似然估计为: 也就是说，通过极大似然法得到的正态分布均值就是样本均值，方差就是$(x-\\hat{mu_c})(x-\\hat{\\mu_c})^T$的均值 虽然这种参数化方法虽然能使类条件概率估计变得相对简单，但估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布7.3. 朴素贝叶斯分类器 不难发现，基于贝叶斯公式来估计后验概率P(c|x)的主要困难在于: 类条件概率P(x|c)是所有属性上的联合概率，难以从有限的训练样本直接估计而得。为了避开这个障碍，朴素贝叶斯分类器(naïve Bayes classifier)采用了属性条件独立性假设 基于这个假设，贝叶斯公式可以重写为: 对于所有类别来说P(x)相同，因此基于贝叶斯判定准则有: 对于P(c)和$P(x_i|c)$都用频率估计概率(对于离散属性而言)，对于连续属性则考虑概率密度函数为正态分布，参数由极大似然估计求出 但是这种方法存在一个问题: 如果某个属性值在训练样本中没有出现过，那么对概率进行连乘的时候会乘上一个零，这样显然不太合理。解决方法: 可以在估计概率值时进行平滑处理，常用拉普拉斯修正。具体来说，令N表示训练集D中所有的类别数，$N_i$表示第i个属性可能的取值数，则修正为: 在现实任务中朴素贝叶斯分类器有多种使用方式。例如，若任务对预测速度要求较高，则对给定训练集，可将朴素贝叶斯分类器涉及的所有概率估值事先计算好存储起来，这样在进行预测时只需”查表”即可进行判别;若任务数据更替频繁，则可采用”懒惰学习”(lazy learning)方式，先不进行任何训练，待收到预测请求时再根据当前数据集进行概率估值;若数据不断增加，则可在现有估值基础上，仅对新增样本的属性值所涉及的概率估值进行计数修正即可实现增量学习7.4. 半朴素贝叶斯分类器 朴素贝叶斯的假设在现实任务中往往难以实现，人们尝试对属性条件独立性假设进行一定程度的放松，由此产生了一类称为半朴素贝叶斯分类器的方法 半朴素贝叶斯分类器的基本想法是适当考虑一部分属性间的相互依赖信息，从而既不需要进行完全联合概率，又不至于彻底忽略了比较强的属性依赖关系。独依赖估计(one-dependent estimator 简称ODE)是半朴素贝叶斯分类器最常用的一种策略。独依赖就是假设每个属性在类别之外最多仅依赖于一个其他属性，即: 其中$pa_i$为属性$x_i$所依赖的属性，称为$x_i$的父属性，此时如果每个属性的父属性已知，则可采用之前的方法估计概率值，但是问题的关键在于如何确定每个属性的父属性，不同的做法产生不同的独依赖分类器 最直接的做法是假设所有属性都依赖于同一个属性，称为超父，然后通过交叉验证等模型选择方法来确定超父属性，由此形成了SPODE(super parent ODE)方法。例如下图中，$x_1$是超父属性 另一种TAN(Tree Augmented naïve Bayes)则是最大带权生成树算法的基础上，通过以下步骤将属性间依赖关系简约为如图所示的树形结构: 1.计算任意两个属性间的条件相互信息 2.以属性为结点构建完全图，任意两个结点之间边的权重设为$I(x_i,x_j)|y$ 3.构建此完全图的最大带权生成树，挑选根变量，将边置为有向 4.加入类别结点y，增加从y到每个属性的有向边 TAN实际保留了强相关属性之间的依赖性 AODE(averaged one-dependent estimator)是一种基于集成学习机制、更为强大的独依赖分类器，于SPODE通过模型选择确定超父属性不同，AODE尝试将每个属性作为超父来构建SPODE，然后将那些具有足够训练数据支撑的SPODE集成起来作为最终结果: 与朴素贝叶斯分类器类似，AODE的训练过程也是计数，即在训练数据上对符合条件的样本进行计数的过程 既然将属性条件独立性假设放松为独依赖假设可能获得泛化性能的提升，那么，能否通过考虑属性间的高阶依赖来进一步提升泛化性能呢?也就是说，将式中的属性$pa_i$替换为包含k个属性的集合$pa_i$，从而将ODE拓展为kDE。需注意的是，随着k的增加，准确估计概率$P(x_i|y,pa_i)$所需的训练样本数量将以指数级增加。因此，若训练数据非常充分，泛化性能有可能提升;但在有限样本条件下，则又陷入估计高阶联合概率的泥沼7.5. 贝叶斯网 贝叶斯网亦称信念图，它借助有向无环图来刻画属性之间的依赖关系，并使用条件概率表来描述属性的联合概率分布，一个贝叶斯网B由结构G和参数$\\Theta$两部分组成，即$B=&lt;G,\\Theta&gt;$7.5.1. 结构 贝叶斯网结构有效地表达了属性间的条件独立性，给定父结点集，贝叶斯网假设每个属性与它的非后裔属性独立，于是联合概率分布定义为: 上图中，$x_3$和$x_4$在给定$x_1$的取值时独立，简记为$x_3$⊥$x_4$|$x_1$ 下图给出了贝叶斯网的三个变量之间的典型依赖关系 在顺序结构中，给定x的值，则y与z条件独立。V型结构(V-structure)亦称冲撞的结构，给定子结点$x_4$的取值，$x_1$与$x_2$必不独立，奇妙的是，若$x_4$的取值完全未知，则V型结构下$x_1$和$x_2$却是相互独立的，我们做一个简单的验证: 这样的独立性称为边际独立性，记为$x_1$╨$x_2$ 为了分析有向图中变量间的条件独立性，可使用有向分离，将有向图变为无向图: 找出有向图中的所有V型结构，在V型结构的两个父结点之间加上一条无向边 将所有有向边改为无向边 这样产生的无向图称为道德图。基于道德图能直观、迅速地找到变量间的条件独立性。假定道德图中有变量x, y和变量集合z={$z_i$}，若变量x和y能在图上被z分开，即从道德图中将变量集合z去除后，x和y分属两个连通分支，则称变量x和y被z有向分离，x⊥y|z成立7.5.2. 学习 若网络结构已知，即属性间的依赖关系已知，则贝叶斯网的学习过程相对简单，只需要通过对训练样本计数，估计出每个结点的条件概率表即可。但在现实应用中我们往往并不知晓网络结构，于是，贝叶斯网学习的首要任务就是根据训练数据集来找出结构最”恰当”的贝叶斯网。”评分搜索”是求解这一问题的常用办法。具体来说，我们先定义一个评分函数(score function)，以此来评估贝叶斯网与训练数据的契合程度，然后基于这个评分函数来寻找结构最优的贝叶斯网。显然，评分函数引入了关于我们希望获得什么样的贝叶斯网的归纳偏好 细节参见书中7.5.3. 推断 贝叶斯网训练好之后就能用来回答”查询”(query)，即通过一些属性变量的观测值来推测其他属性变量的取值。例如在西瓜问题中，若我们观测到西瓜色泽青绿、敲声烛响、根蒂蜷缩，想知道它是否成熟、甜度如何。这样通过已知变量观测值来推测待查询变量的过程称为”推断”(inference)，已知变量观测值称为”证据” (evidence) 最理想的是直接根据贝叶斯网定义的联合概率分布来精确计算后验概率，不幸的是，这样的精确推断已被证明是NP难，在现实应用中，贝叶斯网的近似推断常使用吉布斯采样来完成7.6. EM算法 在前面的讨论中，我们一直假设训练样本所有属性变量的值都已被观测到，即训练样本是完整的，但在现实应用中往往会遇到不完整的训练样本，在这种存在未观测变量的情形下，是否仍能对模型参数进行估计呢？未观测变量的学名是隐变量(latent variable) 令X表示已观测变量集，Z表示隐变量集，$\\Theta$表示模型参数，若欲对$\\Theta$做极大似然估计，则应最大化对数似然: 然而由于Z是隐变量，上式无法直接求解，因此我们可通过对Z计算期望，来最大化已观测数据的对数边际似然(marginal likelihood) EM(Expectation-Maximization)算法是常用的估计隐变量的利器，它是一种迭代式的方法。其基本想法是:若参数$\\Theta$己知，则可根据训练数据推断出最优隐变量Z的值(E步);反之，若Z的值已知，则可方便地对参数$\\Theta$做极大似然估计(M步) 于是，以初始值$\\Theta^0$为起点，可迭代执行以下步骤直至收敛: 基于$\\Theta^t$推断隐变量Z的期望，记为$Z^t$; 基于已观测变量X和$Z^t$对参数$\\Theta$做极大似然估计，记为$\\Theta^{t+1}$; 这就是EM算法的原型，进一步，若我们不是取Z的期望，而是基于$\\Theta^t$计算隐变量Z的概率分布$P(Z|X,\\Theta^t)$, 则算法的步骤为: 注意这里的测试集其实是验证集，我们通常把实际情况中遇到的数据集称为测试集。 &#8617; 满秩矩阵指方阵的秩等于矩阵的行数/列数，满秩矩阵有逆矩阵且对于y=Xb有唯一的解 &#8617; 统计学中，似然函数是一种关于统计模型参数的函数。给定输出x时，关于参数θ的似然函数L(θ|x)（在数值上）等于给定参数θ后变量X的概率：L(θ|x)=P(X=x|θ)。 &#8617; 随机变量的期望组成的向量称为期望向量或者均值向量 &#8617; 协方差矩阵的每个元素是各个向量元素之间的协方差。协方差就是Covariance &#8617; 二次规划(Quadratic Programming)是一种典型的优化问题，在此类问题中，目标函数是变量的二次函数，而约束条件是变量的线性不等式，假定变量个数为d，约束条件的个数为m，则标准的二次规划问题形如: 其中x为d维向量，Q为实对称矩阵，A为实矩阵，若Q为半正定矩阵，则目标函数为凸函数，相应的二次规划是凸二次优化问题。半正定矩阵即满足: A是n阶方阵，如果对任何非零向量X，都有X’AX≥0，其中X’表示X的转置，就称A为半正定矩阵。正定矩阵定义类似，只是把大于等于变成大于。此时若约束条件Ax≤b定义的可行域不为空，且目标函数在此可行域有下界，则该问题将有全局最小值。若Q为正定矩阵，则该问题有唯一全局最小值 &#8617; 拉格朗日乘子法是一种寻找多元函数在一组约束下的极值的方法，通过引入拉格朗日乘子, 可将d个变量与k个约束条件的最优化问题转化为具有d+k个变量的无约束优化问题求解。首先考虑等式约束的优化问题，假定x为d维向量，欲寻找x的某个取值$x^*$, 使目标函数f(x)最小且同时满足g(x)=0的约束，从几何角度看，该问题的目标是在由方程g(x)=0确定的d-1维曲面上寻找能使目标函数f(x)最小化的点，此时不难得到如下结论: 1.对于约束曲面上的任意点x, 该点的梯度▽g(x)正交于约束曲面; 2.在最优点$x^*$，目标函数在该点的梯度▽f($x^*$)正交于约束曲面 由此可知，在最优点$x^*$，存在$\\lambda$≠0使得$\\lambda$称为拉格朗日算子，定义拉格朗日函数现在考虑不等式约束g(x)≤0,如图所示 此时最优点$x^*$或在g(x)&lt;0的区域中，或在边界g(x)=0上。对于g(x)&lt;0的情形，约束g(x)≤0不起作用，可直接通过条件▽f(x)=0来获得最优点，这相当于将$\\lambda$置零，g(x)=0的情形类似于上面等式约束的分析。因此，在约束g(x)≤0下最小化f(x)，可转化为在如下约束下最小化拉格朗日函数: 上述约束条件称为KTT条件。一个优化问题可以从两个角度来考察，即主问题和对偶问题，主问题就是没有引入拉格朗日乘子的原始目标函数，对偶问题就是加上了拉格朗日乘子的目标函数，这个目标函数是原始目标函数的最优值的一个下界，所以我们希望找到最好的下界，也就是最大的下界，所以对偶函数是寻找最大值 &#8617; 提到后验概率，不得不提到先验概率，先验概率是根据以往经验和分析得到的概率，而后验概率可以视为条件概率，它可以通过贝叶斯公式算出 &#8617; 它是建立在极大似然原理的基础上的一个统计方法，极大似然原理的直观想法是，一个随机试验如有若干个可能的结果A，B，C，…，若在一次试验中，结果A出现了，那么可以认为实验条件对A的出现有利，也即出现的概率P(A)较大。极大似然原理的直观想法我们用下面例子说明。设甲箱中有99个白球，1个黑球；乙箱中有1个白球．99个黑球。现随机取出一箱，再从抽取的一箱中随机取出一球，结果是黑球，这一黑球从乙箱抽取的概率比从甲箱抽取的概率大得多，这时我们自然更多地相信这个黑球是取自乙箱的。一般说来，事件A发生的概率与某一未知参数$\\theta$有关，$\\theta$取值不同，则事件A发生的概率$P(A|\\theta)$也不同，当我们在一次试验中事件A发生了，则认为此时的$\\theta$值应是t的一切可能取值中使$P(A|\\theta)$达到最大的那一个，极大似然估计法就是要选取这样的t值作为参数t的估计值，使所选取的样本在被选的总体中出现的可能性为最大 &#8617;"
    } ,
  
    {
      "title"       : "制作类RACE数据集",
      "category"    : "",
      "tags"        : "work",
      "url"         : "./RACElike-datasets.html",
      "date"        : "2021-12-21 00:00:00 +0800",
      "description" : "帮助学长制作RACE数据集",
      "content"     : "目录 目录 RACE 简介 RACE数据集格式 RACE数据集分布 RACE数据集中的长度 RACE数据集中的问题的统计信息 GaoRACE Gao他们对于RACE数据集的处理 Gao处理后的RACE数据集统计信息 Gao处理后的数据集格式 预处理 updated 预处理代码 MRC 阅读理解数据集 简介 Title Abstract Table 一张十分完整的表格 值得关注的地方 自制数据集 大型题库 方法 RACE简介RACE数据集包含了中国初高中阅读理解题目，最初发布在2017年，一共含有28k短文和100k个问题，最开始发布的目的是为了阅读理解任务。它的特点是包含了很多需要推理的问题。 原RACE数据集地址 下载地址url 论文地址：RACE: Large-scale ReAding Comprehension Dataset From ExaminationsRACE数据集格式Each passage is a JSON file. The JSON file contains following fields: article: A string, which is the passage. 文章 questions: A string list. Each string is a query. We have two types of questions. First one is an interrogative sentence. Another one has a placeholder, which is represented by _. 四个问题题干 options: A list of the options list. Each options list contains 4 strings, which are the candidate option. 四个题目的四个选项 answers: A list contains the golden label of each query.四个题目的正确答案 id: Each passage has a unique id in this dataset.RACE数据集分布RACE-M表示初中题目，RACE-H表示高中题目RACE数据集中的长度RACE数据集中的问题的统计信息GaoRACEGao他们对于RACE数据集的处理 去掉了那些误导选项和文章语义不相关的数据 去掉了那些需要world knowledge生成的选项 githuburl,上面有预处理RACE数据集的代码Gao处理后的RACE数据集统计信息Gao处理后的数据集格式预处理首先把数据集规整到一个json文件里，分为dev,test,train三个json文件。每一行包含以下信息：article, sent(sentence), question(问题有两种，一种是疑问句，一种是填空), answer_text, answer, id, word_overlap_score, word_overlap_count, article_id, question_id, distractor_id.那么一个问题会有2-3个误导选项，一篇文章又会有3-4个问题。相比于原本的数据集多了word-overlap指标，word-overlap就是词重叠率，交集比上并集。updatedupdated数据集和original数据集格式类似，少了overlap，内容上去掉了一些语义不相关的题目。预处理代码利用torchtext框架预处理文本，流程大概如下： 定义Field：声明如何处理数据 定义 Dataset：得到数据集，此时数据集里每一个样本是一个 经过 Field声明的预处理 预处理后的 wordlist 建立vocab：在这一步建立词汇表，词向量(word embeddings) 构造迭代器：构造迭代器，用来分批次训练模型Gao说有去掉一些语义不相关的误导选项，但是在代码中并没有看见这步操作？？MRC 阅读理解数据集简介发现了一篇很好的综述，里面涵盖了2021年之前用到的所有MRC数据集。现在对这篇综述简单介绍一下TitleEnglish Machine Reading Comprehension Datasets: A SurveyAbstract文献收集了60个英语阅读理解数据集，分别从不同维度进行比较，包括size, vocabulary, data source, method of creation, human performance level, first question word。调研发现维基百科是最多的数据来源，同时也发现了缺少很多why,when,where问题。Table 一张十分完整的表格首先我简单解释以下这个表格，这个表格一个收录了18个Multiple Choice Datasets,也就是说这18个数据集都着眼于多选题。 第一列是数据集的名称。 第二列表示数据集中问题的个数(size)。 第三列表示数据集中文章的来源，其中ER表示education resource, AG表示automatically generated即自动生成,CRW表示crowdsourcing。 第四列表示答案的来源(answer)，其中UG表示user generated。 第五列LB表示leader board available，即是否有排行榜，带*表示排行榜在网站上发布。 第六列表示人在该数据集上的表现。 第七列表示该数据集是否有被解决，也就是说是否有比较好的模型能在该数据集上表现良好。 第八列表示问题第一个单词出现最频繁的是哪个？比如what,how,which这样的单词。 第九列PAD表示是否开源。值得关注的地方这么多数据集中，来源于考试题目的有RACE,RACE-C,DREAM,ReClor,这些数据集的收集方法可以借鉴。自制数据集大型题库泸江，星火英语…方法Python爬取网页"
    } ,
  
    {
      "title"       : "计算机图形学",
      "category"    : "",
      "tags"        : "note",
      "url"         : "./Computer_Graphics.html",
      "date"        : "2021-12-21 00:00:00 +0800",
      "description" : "Games 101 introduction to computer graphics 课程笔记",
      "content"     : "1. Lecture 01 Overview of Computer Graphics 1.1. 课程情况 1.2. 什么是好的画面 1.3. 应用场景 1.4. Rasterization 光栅化 1.5. 计算机视觉 1.6. 推荐书籍 2. Lecture 02 Review of Linear Algebra 2.1. 图形学依赖学科 2.2. 向量 2.3. 矩阵 3. Lecture 03 Transformation 3.1. why transformation 为什么要变换 3.2. D变换 3.3. 齐次坐标 homogeneous coordinate 4. Lecture 04 Transformation Cont. 4.1. D Transformations 4.2. view transformation 视图变换 4.3. projection transformation 投影变换 5. Lecture05 Rasterization 1(Triangles) 5.1. Perspective Projection 透视投影 5.2. Canonical Cube to Screen 光栅化 5.3. Different Raster Displays 不同的成像设备 5.4. 三角形光栅化 6. Lecture 06 Rasterization 2(Antialiasing and Z-Buffering) 6.1. sampling 采样原理 6.2. Frequency domaine 信号处理频率 6.3. antialiasing 反走样/抗锯齿 6.4. antialiasing today 目前反走样的方法 7. Lecture 07 Shading(Illumination, Shading, and Graphics Pipeline) 7.1. Painter’s Algorithm 画家算法 7.2. Z-buffer 深度缓存 7.3. 目前为止学到了什么 7.4. shading 着色 8. Shading 2(Shading, Pipeline, Texture Mapping) 8.1. Specular Term 高光项 8.2. Ambient Term 环境项 8.3. Shading Frequencies 着色频率 8.4. Graphics Pipeline 图像管线/实时渲染管线 8.5. Texture Mapping 纹理映射 9. Lecture 09 Shading 3 (Texture Mapping) 9.1. Barycentric Coordinates重心坐标系 9.2. Interpolate 插值 9.3. Simple Texture Mapping 简单的纹理映射模型 9.4. Texture Magnification 纹理放大 9.5. Point Sampling Textures 9.6. Mipmap 范围查询 10. Lecture 10 Geomrtry 1(introduction) 10.1. 纹理的应用 10.1.1. Environment Map 环境光映射 10.1.2. Spherical Environment Map 球形环境光映射 10.1.3. 纹理凹凸贴图bump mapping 10.1.4. 位移贴图 displacement mapping 10.1.5. 三维纹理 10.2. 几何 10.2.1. 分类 10.2.2. 隐式几何 10.2.3. 显式几何 10.2.4. 隐式的表达方式 11. Lecture 11 Geometry 2(Curves and Surfaces) 11.1. 显式几何的表示方法 11.1.1. Point Cloud 点云 11.1.2. Polygone Mesh 11.1.3. 一个例子 11.2. Curves 曲线 11.2.1. 贝塞尔曲线 11.2.2. 如何画一条贝塞尔曲线 11.2.3. Piecewise Bézier Curves 逐段的贝塞尔曲线 11.2.4. Spline 样条 11.3. 曲面 11.3.1. 贝塞尔曲面 11.3.2. 曲面细分 12. Lecture 12 Geometry 3 12.1. Mesh Subdivision(upsampling) 网格细分 12.2. Mesh Simplification 网格简化 12.3. 阴影 Shadow mapping 13. Lecture 13 Ray Tracing 1 13.1. Why ray tracing 13.2. Light Rays 13.3. Ray Casting 光线投射 13.4. Recursive Ray Tracing 递归光线追踪 13.5. Ray-Surface interaction 光线和表面相交 13.5.1. Ray Equation 13.5.2. 与圆相交的交点 13.5.3. intersection with implicit surface 13.5.4. intersection with triangle mesh 13.5.5. accelerating ray-surface intersection 14. Lecture 14 Ray Tracing 2 14.1. Uniform Spatial Partitions (Grids) 14.2. Spatial Partitions 空间划分 14.2.1. 一些划分示例 14.2.2. KD-Tree 14.3. Object Partitions 物体划分 14.3.1. Bounding Volume Hierarchy(BVH) 14.3.2. Building BVH 14.3.3. 与空间划分的对比 14.4. Whitted style 14.5. Radiometry 辐射度量学 14.5.1. 一些物理量 14.5.2. Radiant Energy and Flux 14.5.3. Radiant Intensity 15. Lecture 15 Ray Tracing 15.1. Radiometry cont. 辐射度量学 15.1.1. 继续上节课的内容 15.1.2. Irradiance 15.1.3. Radiance 15.2. Bidirectional Reflectance Distribution Function (BRDF) 15.3. Rendering Equation 渲染方程 15.3.1. 如何理解渲染方程 16. Lecture 16 Ray Tracing 4 16.1. Monte Carlo Integration 蒙特卡洛积分 16.2. Path Tracing 路径追踪 16.2.1. 解渲染方程 16.2.2. 最终的代码 16.3. 路径追踪 1. Lecture 01 Overview of Computer Graphics1.1. 课程情况 授课老师：闫令琪 授课形式：网课（B站）1.2. 什么是好的画面画面亮1.3. 应用场景电影，游戏，动画，设计，可视化，虚拟现实，增强现实，模拟，GUI图形用户接口。电影中里程碑：阿凡达，大量应用面部捕捉技术。1.4. Rasterization 光栅化实时，FPS&gt;30离线, FPS&lt;301.5. 计算机视觉计算机图形学离不开计算机视觉，但是视觉一般是对图像的处理。1.6. 推荐书籍Tiger虎书2. Lecture 02 Review of Linear Algebra2.1. 图形学依赖学科Optics, Mechanics, Linear algebra, statics, Singal processing, numerical analysis数值分析2.2. 向量向量的定义单位向量向量计算，向量加法用笛卡尔坐标系表示向量向量乘法，点乘和叉乘，点乘在笛卡尔坐标系中就是对应元素相乘。在图形学中，点乘是为了寻找两个向量的夹角(夹角可以判断两个向量方向的接近程度)，或者获得一个向量在另一个向量的投影，还可以获得向量的分解。叉乘，叉积结果垂直于这两个向量所在的平面，满足右手定则。向量的叉乘可以写成矩阵形式。在图形学中的应用：判断左右关系，比如a^b&gt;0，说明b在a的左边。还可以判断内外，比如判断一个点是否在一个三角形内。坐标系的定义，右手坐标系2.3. 矩阵矩阵定义矩阵乘法矩阵乘法没有交换律，但是有结合律矩阵转置，矩阵的逆向量的点乘和叉乘都可以写成矩阵乘法形式3. Lecture 03 Transformation3.1. why transformation 为什么要变换viewing: 3D to 2D projection3.2. D变换 缩放 scale transform 非均匀缩放 scale(non-uniform) 翻转 reflection matrix 切变 shear matrix竖直方向上没有变化，水平方向上发生了变化 旋转 Rotate旋转默认绕零点逆时针旋转二维旋转矩阵R上述所有的变化都可以写成x$\\prime$=Mx，也就是线性变换3.3. 齐次坐标 homogeneous coordinate 为什么要引入齐次坐标，因为对于简单的平移操作并不能写成线性变换的形式，但是人们也不想认为平移是一种特殊的变换，所以引入齐次坐标 齐次坐标 注意点和向量的表示方法不同 仿射变换 affine transformations 2D Transformations 逆变换就是乘以逆矩阵 复杂的变换都是简单的变换的组合，变换的组合顺序很重要 绕着某一个点（非原点）旋转的分解 4. Lecture 04 Transformation Cont.4.1. D Transformations 齐次坐标对于w不等于1，每一个坐标除以w 正交矩阵一个矩阵的逆等于矩阵的转置，旋转矩阵就是一个正交矩阵 仿射变换（旋转+平移）仿射变换是先进行旋转再进行平移 矩阵表示（缩放，平移） 旋转绕着某一个轴旋转一般的旋转（分解成三个坐标轴的旋转）Rodrigues’ Rotation Formula, 用向量n表示旋转轴，最终推出这个公式4.2. view transformation 视图变换 观测变换viewing，包括了视图变化和投影变化 MVP变换(model-&gt;view-&gt;projection) view transformation(不等于viewing) 视图变换视图变换是把相机放到标准位置上，located at origin, look at -Z利用逆变换，先平移再旋转一般把model和view变换统称为view transformation4.3. projection transformation 投影变换 orthographic vs perspectiive projection orthographic projection 正交投影平移，缩放（不考虑旋转） perspective projection 透视投影满足近大远小透视投影就是先把物体挤压成立方体，然后对立方体进行正交投影5. Lecture05 Rasterization 1(Triangles)5.1. Perspective Projection 透视投影 首先是对上节课的透视投影的一些补充, 其中l=left, r=right, b=bottom, t=top, n=near, f=far，这些量可以描述视锥Frustum 视锥Frustum的描述还可以用fovY(field of view)垂直视角和aspect ratio宽高比5.2. Canonical Cube to Screen 光栅化 把物体的数学描述以及与物体相关的颜色信息转换为屏幕上用于对应位置的像素及用于填充像素的颜色，这个过程称为光栅化。 屏幕是最常见的光栅设备，每一个像素都是一个小方块，像素是最小的单位，一个像素的颜色可以用rgb三种颜色表示 屏幕空间screen space 把之前投影后的小方块变成屏幕空间5.3. Different Raster Displays 不同的成像设备 Oscilloscope 示波器 Cathode Ray Tube 阴极射线管成像原理。早期电视屏幕就是这样实现成像，扫描成像。 Frame Buffer: Memory for a Raster Display 内存中的一块区域存储图像信息。 LCD(liquid crystal display)液晶显示器，光的波动性原理。 LED发光二极管5.4. 三角形光栅化 三角形是最基本的多边形，有很多好的性质。 sampling 采样。三角形离散化。在不同的像素中心，确定是0还是1,表示在三角形里还是外 如何判断点和三角形关系，利用叉积，边界上的点自己定义。 jaggies锯齿，走样aliasing6. Lecture 06 Rasterization 2(Antialiasing and Z-Buffering)6.1. sampling 采样原理 视频就是对时间进行采样 采样的artifact(瑕疵)：锯齿，摩尔纹，轮胎效应(在时间上采样) 反走样采样：可以对原始的图像进行滤波(模糊处理)然后再采样。 采样速度跟不上信号变化的速度就会走样(aliasing)6.2. Frequency domaine 信号处理频率 傅里叶变换：所有的周期函数都可以写成不同平吕的正弦函数的组合。傅里叶变换就是频域和时域/空间域的变换 走样的原因(时域)：高频信号欠采样，高频信号和低频信号在某一采样速度下没有差别，就会产生走样 滤波：抹掉特定的频率。比如高通滤波(过滤到低频信号) 卷积：图形学上的简化定义，见下图 卷积定律：时域上的卷积等于频域上的乘积 采样：重复频域上的内容 走样在频率上的解释：采样频率小会让频域上发生重叠6.3. antialiasing 反走样/抗锯齿 第一种解决方法：增加采样率，相当于增加了频域上的两个信号的距离 第二种解决方法：反走样。即先对信号进行滤波再采样 比如对于之前三角形的问题 但是这种反走样的方法比较复杂，有一种更简单的近似方法(对滤波这一步的近似)：supersampling，就是在对每个像素点变成更多的小点6.4. antialiasing today 目前反走样的方法介绍了两种新的抗锯齿的操作：FXAA和TAA。FXAA的做法是把边界找到然后对边界进行处理。7. Lecture 07 Shading(Illumination, Shading, and Graphics Pipeline)7.1. Painter’s Algorithm 画家算法 首先画出远处的物体，然后再画近处的物体。画近处的物体再覆盖远处的物体。 需要定义深度信息，根据深度信息排序7.2. Z-buffer 深度缓存 对每个像素都有最小的z值，除了一个frame buffer储存颜色信息外，还需要z-buffer储存深度信息。 假设每个像素最开始的时候深度为无限远 特点是在像素维度进行操作 7.3. 目前为止学到了什么7.4. shading 着色 着色：对不同物体应用不同的材质 一个简单的着色模型(Blinn-Phong Reflection model) 局部着色，不考虑阴影 diffuse reflection 漫反射，一个物体有多亮与接收到多少光的能量有关。点光源的能量随距离缩减。在点光源的光线到达物体表面时被物体接受多少能量又与光线和法线的夹角的cos值有关，也就是说直射时接受的能量最大(相同距离)。漫反射表示不论观测角度在哪，你观测到的亮度应该是一样的。8. Shading 2(Shading, Pipeline, Texture Mapping)8.1. Specular Term 高光项 着色包括三部分：漫反射，高光，环境光 高光就是观测方向和镜面反射方向相同，即半程向量是否和法向量接近 通常高光都是白色的8.2. Ambient Term 环境项 环境光就是一些其他物体反射的光照亮背光物体 这里介绍非常简化的模型 最终结果8.3. Shading Frequencies 着色频率 之前介绍的着色是应用在着色点，对应在屏幕空间是如何的呢？ 第一种：Shading ecah triangle 对每个三角形着色 第二种：shading each vertex 对顶点着色，然后插值 第三种：shading each pixel 对每个像素点着色 如何定义顶点的法向量呢？对周围的面的法向量求平均 如何定义像素的法向量？8.4. Graphics Pipeline 图像管线/实时渲染管线 一个实时渲染的流程/流水线 现代的GPU允许写入顶点着色部分与片段着色部分的代码8.5. Texture Mapping 纹理映射 希望在物体的不同位置定义不同的属性，比如漫反射系数等等 3维物体的表现都是一个平面 纹理映射就是对于一个平面定义不同的属性，有一个映射关系 纹理也有坐标系9. Lecture 09 Shading 3 (Texture Mapping)9.1. Barycentric Coordinates重心坐标系9.2. Interpolate 插值 重心坐标系插值9.3. Simple Texture Mapping 简单的纹理映射模型9.4. Texture Magnification 纹理放大9.5. Point Sampling Textures 就是走样问题9.6. Mipmap 范围查询 生成不同分辨率的图片 任何一个像素可以映射到纹理区域的一个点，mipmap可以让像素点快速查阅，因为他又很多层，不同的纹理区域的面积对应不同的层 mipmap也不是最好的方法，只是一种折中的办法 anisotropic filtering 各向异性过滤 10. Lecture 10 Geomrtry 1(introduction)10.1. 纹理的应用10.1.1. Environment Map 环境光映射 纹理可以用来映射环境光 假设环境光来自无限远10.1.2. Spherical Environment Map 球形环境光映射 将环境光信息存在球上 但是在边缘部分会有扭曲，解决方法有环境光存在正方体上10.1.3. 纹理凹凸贴图bump mapping 纹理不仅可以表示颜色，还可以应用一个复杂的纹理来定义高度，也就改变了法线的方向 凹凸贴图只增加表面细节，不添加新的三角形 10.1.4. 位移贴图 displacement mapping 和凹凸贴图很像，但是移动了顶点10.1.5. 三维纹理 定义了空间中任意一个点的纹理坐标 广泛应用于体积渲染 10.2. 几何10.2.1. 分类 隐式几何 显式几何10.2.2. 隐式几何 不给出点的具体坐标，而是给出点的坐标关系，比如$x^2+y^2+z^2=1$ 推广到一般形式, $f(x,y,z)=0$ 缺点：不直观，不好采样 优点：可以很容易的判断点在不在几何体内10.2.3. 显式几何 直接给出或者参数映射的方式给出 优点：采样方便，直观 缺点：不好判断点是否在几何体内还是外10.2.4. 隐式的表达方式 公式定义 通过几何体的布尔组合，目前有很多建模软件就是这么表示的 距离函数定义，SDF有向距离场11. Lecture 11 Geometry 2(Curves and Surfaces)11.1. 显式几何的表示方法11.1.1. Point Cloud 点云 点的集合 优点：可以表示任何几何体11.1.2. Polygone Mesh 使用顶点和图形表示(三角形，正方形)11.1.3. 一个例子里面定义了顶点坐标，法线，纹理坐标和哪几个点组成一个三角形11.2. Curves 曲线11.2.1. 贝塞尔曲线 用一系列控制点定义曲线 曲线不一定要经过控制点11.2.2. 如何画一条贝塞尔曲线 Casteljau Algorithm：这个算法的核心是画出每个时间t的点的位置(递归)其中$b_0^2$就是时间t的点的位置 大致流程 代数形式 生成的曲线只能在控制点的凸包内11.2.3. Piecewise Bézier Curves 逐段的贝塞尔曲线 每四个控制点定义一条贝塞尔曲线 C0连续(点连续)，C1连续(切线连续)11.2.4. Spline 样条 样条是用一系列的点画出线条11.3. 曲面11.3.1. 贝塞尔曲面 使用贝塞尔曲线生成贝塞尔曲面 竖直方向生成四条曲线，然后对于t来说四个点再作为控制前生成曲线11.3.2. 曲面细分 使用很多三角形网格来表示曲面12. Lecture 12 Geometry 312.1. Mesh Subdivision(upsampling) 网格细分 引入更多三角形，微调它们的位置 Loop Subdivision：第一步增加三角形的数量，第二部调整三角形的位置 Loop细分规则： 另一种细分规则：Catmull-Clark Subdivision奇异点是这个点的度不是4的点(就是连接的边数不等于4) 这种细分方法可以用于任何面12.2. Mesh Simplification 网格简化 基本思路是为了减少网格数目但是保持它的基本形状 一种方法：Collapsing an edge 边坍缩。删除一些点 判断标准：quadric error metrics 二次误差度量 实际效果12.3. 阴影 Shadow mapping 光栅化着色的时候是局部的，但是有时候会有问题，比如有东西挡在shading point和光源之间时，所以需要在这种情况下生成阴影 光栅化生成阴影的方法叫做shadow mapping shadow mapping 的两步 第一步：从光源出发，看向shading point，记录能看见的点的深度 第二步：从摄像机出发，看向shading point，如果看见的点的深度和光源所看见的深度相同，那么这个点不在阴影内，否则，它在阴影内。 具体的例子： 问题：走样，阴影分辨率，只能做硬阴影(hard shadow)…13. Lecture 13 Ray Tracing 113.1. Why ray tracing 光栅化的缺点：无法表示全局的光照、毛玻璃效果无法很好表示、阴影处理不算好 光纤追踪很精准但是比较慢，经常做离线(电影制作)13.2. Light Rays 光线沿直线传播 光线不会交叉 光线是不断折回然后打到人眼 光路可逆性13.3. Ray Casting 光线投射 从眼睛到像素点出发，到虚拟世界，再到光源(Local) 从眼睛到像素点到虚拟世界的线叫做eye ray13.4. Recursive Ray Tracing 递归光线追踪 如果在shading point 处可以折射，能量损失，则继续折射然后对每个点都算着色值 对每个点都要计算是否处在阴影中13.5. Ray-Surface interaction 光线和表面相交13.5.1. Ray Equation13.5.2. 与圆相交的交点 一个交点就是相切，两个交点就是相交13.5.3. intersection with implicit surface 与隐式表面相交13.5.4. intersection with triangle mesh 也就是与显式表面(三角形网格)相交 第一种想法就是光线与每个三角形进行计算，但这样计算量太大 第二种想法是光线与三角形所在的平面相交，然后判断交点是不是在三角形内 如何定义平面？一个点+法线 然后将光线方程带入平面方程中，就可以得出光线与平面的交点 如何简化判断交点与三角形的位置关系？MT算法： 这个算法的核心就是利用重心坐标系：解出重心坐标后，如果它们都为正，那么点在三角形内13.5.5. accelerating ray-surface intersection 加速交点(一般指与三角形网格的交点)计算过程 bounding volume 包围盒 引入包围盒的思路是：如果光线与包围盒都不相交，那么肯定不会与里面的几何体有交点 包围盒由三个对面的交集轴对齐包围盒(就是对面与坐标轴平行)axis-aligned bounding box 先考虑二维的情况Ray intersection with aabb找到最大的时间和最小的时间 三维：对于三组对面，计算$t_{min}$和$t_{max}$，然后找到$t_{enter}$和$t_{exit}$。那么我们就知道了进入的时间和出去的时间，如果进去的时间小于出去的时间，那么光线进入了aabb，表示光线在盒子里呆过一段时间 还要要保证进入的时间和出去的时间都要大于014. Lecture 14 Ray Tracing 214.1. Uniform Spatial Partitions (Grids) 继续上节课的加速计算话题 一种加速方法：生成grid找到aabb后，创建网格，存储aabb内几何体 然后光线沿着这些小格子相交14.2. Spatial Partitions 空间划分14.2.1. 一些划分示例八叉树Oct-Tree，KD-Tree，BSP-Tree14.2.2. KD-Tree 每次划分都沿着坐标轴移动，对于中间的结点都有子节点，只存储叶子结点的数据 缺点：一个物体可能存在在多个叶子节点里 14.3. Object Partitions 物体划分14.3.1. Bounding Volume Hierarchy(BVH) 这种方法是目前图形学中使用较多的方法 沿着物体不断细分出bbox bvh的缺点：两部分bbox可能相交 14.3.2. Building BVH 如何划分结点？选择一个维度进行划分，每次找最长的结点进行细分，细分的结点在中位数，当结点处图形较少，则停止14.3.3. 与空间划分的对比14.4. Whitted style 到目前为止，已经讲了国内光线追踪会讲的内容。也就是讲完了Whitted style光线追踪14.5. Radiometry 辐射度量学14.5.1. 一些物理量 new terms: radiant flux, intensity, irradiance, radiance14.5.2. Radiant Energy and Flux randiant flux就是单位时间能量/功率14.5.3. Radiant Intensity 辐射强度就是单位立体角(solid angle)的功率 那么立体角是什么呢？立体角就是二维空间的角在三维空间的沿伸，就是球面面积除以半径的平方15. Lecture 15 Ray Tracing15.1. Radiometry cont. 辐射度量学15.1.1. 继续上节课的内容 微分立体角，就是球坐标系上对$\\theta$和$\\phi$的微分15.1.2. Irradiance 单位面积的功率 面积是投影的面积15.1.3. Radiance randiance就是单位投影面积单位立体角的功率 irradiance和radiance的区别：irradiance是某一个面积上接受的能量，而radiance是某一个面积某一个角度上接受的能量15.2. Bidirectional Reflectance Distribution Function (BRDF) 双向反射分布方程BRDF是描述光线传播的方程 某一个方向$\\omega_i$的光线打到某一个表面然后被吸收同时从另一个方向$\\omega_r$反射出去 反射方程 观察某一个物体的反射光线不止从光源有光线，还有其他物体反射的光 渲染方程Rendering Equation 渲染方程两部分组成，一部分是自身发光，另一部分是接受的光线的反射光线(半球上每个方向)15.3. Rendering Equation 渲染方程15.3.1. 如何理解渲染方程 反射的光线由两个个部分组成：自身的emission和从各个方向的反射光 如何考虑物体反射的光？把物体看作一个光源，也就是看作一个递归的过程 通过数学式子简化渲染方程： 然后通过逆矩阵可以解出L 光线弹射一次叫做直接光照、弹射两次及以上叫做间接光照 那么就可以发现与光栅化的区别 在多次弹射后场景会趋于一个固定的亮度16. Lecture 16 Ray Tracing 416.1. Monte Carlo Integration 蒙特卡洛积分 有些函数不太好用解析式写出来 蒙特卡洛积分就是数值积分的方法 就是采样值除以采样密度16.2. Path Tracing 路径追踪 与whitted sytle的区别：whitted sytle没有考虑全局光照16.2.1. 解渲染方程 考虑一个简单的模型，只有直接光照 每一个$\\omega_i$都看作采样，那么可以应用蒙特卡洛积分 应用全局光照，将物体反射面也看做光源，做一个递归 但是这样会出现一个问题，那就是爆炸，如果我取多个X，那么弹射很多次后就会爆炸 解决方法，对每个点只取一个方向，也就是N=1，所以它叫做路径追踪 这样噪声会比较大，但是从每个像素点有多个路径，所以还是可以接受 第二个问题是递归不会停止？解决方法：俄罗斯轮盘赌，即在某一个程度停止递归 那么我们可以设定一个概率P来决定每个点是否打出一条光线，同时保证期望不变 到目前为止已经是一个正确的path tracing的渲染方法，但是这样效率比较低 效率低的原因：每个点打到或者打不到光源是随机的，也就是说浪费了很多光线 可以在光源上采样，这样没有光线会浪费，渲染方程就需要写成在光源上采样 那么我们就可以将渲染方程分为两部分，一部分是光源直接光照，方法使用上面提到的在光源上采样，另一部分是间接光照，保持不变16.2.2. 最终的代码 但还有一个小问题，就是中间有物体遮挡，需要添加一个判断16.3. 路径追踪 在之前，ray tracing主要指whitted-style ray tracing 但现在，只要设计了光线传播方法，就是ray tracing，路径追踪只是其中的一个方法"
    } ,
  
    {
      "title"       : "组会记录",
      "category"    : "",
      "tags"        : "note",
      "url"         : "./1221%E7%BB%84%E4%BC%9A.html",
      "date"        : "2021-12-21 00:00:00 +0800",
      "description" : "组会笔记",
      "content"     : "VAEAEAuto-Encoder自动编码器，比如Seq2seq模型。VAE(Variational Auto-Encoder)在实际情况中，我们需要在模型的准确率上与隐含向量服从标准正态分布之间做一个权衡，所谓模型的准确率就是指解码器生成的图片与原图片的相似程度。我们可以让网络自己来做这个决定，非常简单，我们只需要将这两者都做一个loss，然后在将他们求和作为总的loss，这样网络就能够自己选择如何才能够使得这个总的loss下降。另外我们要衡量两种分布的相似程度，如何看过之前一片GAN的数学推导，你就知道会有一个东西叫KL-divergence来衡量两种分布的相似程度，这里我们就是用KL-divergence来表示隐含向量与标准正态分布之间差异的loss，另外一个loss仍然使用生成图片与原图片的均方误差来表示。KL消失KL消失后，VAE就变成了AE原因： KL项本身太容易被优化 一旦崩塌，Decoder会忽视Zx Zx的表示学习依赖于Decoder解决KL消失的思路…Analyze Pretraining Language ModelPerspective of knowledge Syntacitic/Semantic/lexical 句法，语义，词汇 重构语法树 Attention中很多头可能没有用，学到了很多冗余的信息 Analyze Feed Forward Neural Network 浅层词汇信息，深层语义信息 Prompt"
    } ,
  
    {
      "title"       : "推荐系统",
      "category"    : "",
      "tags"        : "note",
      "url"         : "./Recommender_system.html",
      "date"        : "2021-12-16 00:00:00 +0800",
      "description" : "d2l推荐系统笔记",
      "content"     : "1. 推荐系统总览 1.1. 协同过滤 Collaborative Filtering 1.2. 显式反馈和隐式反馈 1.3. 推荐任务 2. 矩阵分解 Matrix Factorization 3. AutoRec 3.1. overview 3.2. formula 4. Personalized Ranking for Recommender System 4.1. overview 4.2. Bayesian Personalized Ranking loss 贝叶斯损失 4.3. Hinge Loss 5. Neural Collaborative Filtering for Personalized Ranking 使用协同过滤网络个性化排序 5.1. The NeuMF model 5.2. Evaluator 5.3. 代码 6. Sequence-Aware Recommender Systems 6.1. Model Architectures 6.2. Negative Sampling 负采样 7. Feature-Rich Recommender Systems 8. Factorization Machines 因子分解机 8.1. 2-Way Factorization Machines 8.2. An Efficient Optimization Citerion 9. Deep Factorization Machines 深度因子分解机DeeoFM 9.1. Model Architectures 模型架构 1. 推荐系统总览1.1. 协同过滤 Collaborative Filtering协同过滤最早出现在1992年Tapestry system，“人们相互协作，相互帮助，执行过滤程序，以处理大量的电子邮件和张贴到新闻组的信息。”现在协同过滤的概念更加广泛，从广义上讲，它是利用涉及多个用户、代理和数据源之间协作的技术来过滤信息或模式的过程。协同过滤模型可以分为:1.memory-based CF; 2.model-based CF. 其中Memory-based CF又可以分为item-based和user-based CF。model-based CF有矩阵分解模型。总的来说，协同过滤就是利用用户-物品的数据来预测和推荐。1.2. 显式反馈和隐式反馈为了学习用户的偏好，系统需要收集用户的反馈feedback。反馈可以分为显式和隐式。显式反馈就是需要用户主动提供兴趣偏好。比如点赞、点踩。隐式反馈则是间接反映用户的喜好，比如购物历史记录，浏览记录，观看记录甚至是鼠标移动。1.3. 推荐任务电影推荐、新闻推荐、评分预测rating prediction task、top-n reommendation。如果使用了时间戳信息，那么我们构建了sequence-aware recommendation。针对新用户推荐新物品称为cold-start recommendation冷启动推荐。2. 矩阵分解 Matrix FactorizationThe Matrix Factorization Model矩阵分解模型R是user-item矩阵，行数是用户数量，列数是物品数量,那么R∈Rmxn。P是user latent matrix，P∈Rmxk，Q是item latent matrix，Q∈Rnxk矩阵分解就是把R分解成P和Q，那么预测的评分就是：但是上面这个式子没有考虑偏置，我们会有下面这个完整的式子：那么目标函数可以定义为：右边那一串是正则项，为了避免过拟合下面这张图值观的展示了矩阵分解过程：3. AutoRec3.1. overview使用autoencoder预测评分，上小节介绍的矩阵分解模型是线性模型，它不能捕捉复杂的非线性关系，比如用户的偏好。这一小节介绍一个非线性协同过滤神经网络模型AutoRec。AutoRec是基于自编码器的结构，自编码器是一种特殊的神经网络架构，他的输入和输出的架构是相同的，自编码器通过无监督学习来训练获取输入数据在较低维度的表达，在神经网络的后段，这些低纬度的信息再次被重构回高维的数据表达。所以AutoRec的架构也是输入层，隐藏层和重构输出层。它的目的是输入一个只有部分兴趣矩阵，输出一个完整的兴趣矩阵。AutoRec可以分为user-based 和 item-based3.2. formula针对item-based：$R_{*i}$表示兴趣矩阵的第i列，不知道的项填为0。那么神经网络的构架可以定义为：h()表示最终的输出，输出一个完整的兴趣矩阵，那么误差定义为：4. Personalized Ranking for Recommender System4.1. overview在上一节中，我们用到了显式反馈，同时模型只在能观察到的评分上训练。那么这种模型有两个缺点：第一个是很多的反馈并不是显式的。第二个是没有观察到的评分被完全忽略了。个性化推荐可以分为:1.pointwise;2.pairwise;3.listwise。Pointwise表示每次预测单个偏好，pairwise则是预测出一系列的偏好然后进行排序，listwise则是预测所有的item并进行排序。4.2. Bayesian Personalized Ranking loss 贝叶斯损失 贝叶斯损失是一种pairwise个性化推荐损失。它被广泛应用于多种推荐系统中。它假设用户相对于无观察项，更加喜欢positive item 训练集格式是(u, i, j)表示用户u喜欢i超过j，BPR希望最大化下面这个后验概率： 其中$\\Theta$表示推荐系统的参数，$&gt;_u$表示用户u对所有item的排序。4.3. Hinge Loss 数学表达式其中m表示安全系数，它的目的是让不喜欢的项离喜欢的项更远。它和贝叶斯都是为了优化positive sample和negative sample之间的距离。5. Neural Collaborative Filtering for Personalized Ranking 使用协同过滤网络个性化排序本小节重新将目光聚集到隐式反馈中，介绍协同过滤推荐系统NeuMF。NeuMF利用隐式反馈，它由两个子结构组成，分别是generalized matrix factorization(GMF)和MLP。不同于评分的预测如AutoRec，它将生成一系列的推荐，它根据用户是否看过这场电影来区分为正例和反例5.1. The NeuMF modelNeuMF的网络结构由两部分组成。 一部分是GMF，也就是matrix factorization的类似形式，输入用户向量$p_u$和物品向量$q_i$，返回x 另一部分是MLP，输入和GMF一样，但是用不同的字母表示，具体公式如下： 最后对这两个子结构concatenate一下，就是最终的输出 大体的网络结构如下5.2. Evaluator有两个性能度量指标 hit rate at given cutting off l，记作Hit@l这个式子的主题思路是判断推荐的物品是否在top l中，m表示用户的数量，$rank_{u,g_u}$表示对于用户u和物品$g_u$的排名，1表示指标函数 AUC，即ROC曲线下的面积，也是模型泛化能力的一个指标其中$S_u$表示模型对于u的推荐物品集，I表示item set，AUC越大越好5.3. 代码网络结构就是上面介绍的那样，net的输出是用户和物品匹配出的一个推荐值(我的想法)。在进行训练的时候，会给出正例物品(即用户有过评分的物品)和反例物品(用户没有评分，也就是没有看过)分别与用户得到一个推荐值，然后利用上一小节介绍的贝叶斯损失来优化(让评分过的物品有更高的推荐值)，然后最终我们希望返回一系列的推荐物品，这些推荐物品都是没有负例物品，然后根据推荐值进行排序。性能指标是hit或者auc。hit的思想是让真实评分的物品在推荐列表中。6. Sequence-Aware Recommender Systems之前的模型都没有考虑时序信息，这小节的Caser模型将会考虑用户的时序信息。6.1. Model Architectures模型的输入$E^{(u,t)}$表示用户u的近期L个评价的物品，Caser模型有横向和纵向的卷积层，输入矩阵分别与卷积层作用后，结果concatenate变成$z$，$z$再和用户的一般信息结合，也就是$z$和$p_u$concatenate最终输出$\\hat{y}_{uit}$，其中$p_u$表示用户u的item信息6.2. Negative Sampling 负采样我们需要对数据集进行重新处理，比如一个人喜欢9部电影，同时我们的L=5，那么我们将最近的一部电影留出来作为test，其余的都作为训练集，可以划分出3个训练集。同时我们也需要进行负采样(采样没有评分的item)7. Feature-Rich Recommender Systems之前的模型大都用到了用户物品的交互矩阵，但是很少有用到一些额外的信息，比如物品的特征，用户的简介，发生交互的背景等等…利用这些信息可以获得用户的兴趣特征。本节提出了一个新的任务CTR(click-through rate)，也就是点击率任务，对象可以是广告、电影等等。8. Factorization Machines 因子分解机Factorization machines(FM)是一个监督算法，可用于分类，回归和排名任务。它有两个优点：1.它能处理稀疏的数据；2.它能减少时间复杂度和线性复杂度8.1. 2-Way Factorization Machines$x$表示样本的特征值，而$y$表示它的标签值，即click/non-click。第二项表示线性项，第三项表示矩阵分解项8.2. An Efficient Optimization Citerion上面式子的第三项时间复杂度太高，我们可以简化一下9. Deep Factorization Machines 深度因子分解机DeeoFM上小节提到的因子分解机用到的都是线性模型(单线性和双线性)，这种模型在真实数据表现并不好。这里我们就可以结合因子分解机和深度神经网络，比如我们这小节即将介绍的DeepFM。9.1. Model Architectures 模型架构DeepFM由两部分组成，FM component和deep component，FM部分和上小节提到的2-way FM做法一样，主要是处理低纬度特征，而deep部分用到的MLP来处理高维度和非线性。这两部分使用相同的输入/嵌入层然后它们的结果整合成最终的预测。模型结构如下图："
    } ,
  
    {
      "title"       : "Robotics",
      "category"    : "",
      "tags"        : "note",
      "url"         : "./Robotics.html",
      "date"        : "2021-12-13 00:00:00 +0800",
      "description" : "《机器人》课程随堂笔记",
      "content"     : "报告报告内容用solidworks设计一个至少三个自由度的机械臂，并且描述它的动能，移动能力等等。提交的是截图，源文件等等。报告格式 标题，下面有姓名学号电话等等 摘要 正文"
    } ,
  
    {
      "title"       : "数据挖掘",
      "category"    : "",
      "tags"        : "note",
      "url"         : "./datamining.html",
      "date"        : "2021-12-10 00:00:00 +0800",
      "description" : "《Python数据挖掘入门与实践》笔记",
      "content"     : "总体情况 第一章 开始数据挖掘之旅 1.1 亲和性分析 1.2 分类 第二章 用scikit-learn估计器分类 2.1 scikit-learn 2.2 邻近算法KNN 第三章 用决策树预测获胜球队 3.1 决策树 3.2 随机森林 第四章 用亲和性分析方法推荐电影 4.1 亲和性分析 4.2 Apriori算法 第五章 用转换器抽取特征 5.1 抽取特征 5.2 特征选择 5.3 创建特征 第六章 使用朴素贝叶斯进行社会媒体挖掘 6.1 消歧 6.2 文本转换器 6.3 朴素贝叶斯 6.4 F1值 第九章 作者归属问题 9.1 作者归属 9.2 支持向量机 9.3 基础SVM的局限性 第十章 新闻语料分类 10.1 新闻语料聚类 10.2 K-means算法 总体情况 书籍:Python数据挖掘入门与实践 github_url:https://github.com/LinXueyuanStdio/PythonDataMining 配套代码和笔记，很适合迅速上手 这篇博客主要记录一些比较重要的算法第一章 开始数据挖掘之旅1.1 亲和性分析 亲和性分析根据样本个体（物体）之间的相似度，确定它们关系的亲疏。 例子：商品推荐。 我们要找出“如果顾客购买了商品X，那么他们可能愿意购买商品Y”这样的规则。简单粗暴的做法是，找出数据集中所有同时购买的两件商品。找出规则后，还需要判断其优劣，我们挑好的规则用。 规则的优劣有多种判断标准，常用的有支持度(support)和置信度(confidence) 支持度：数据集中规则应验的次数，统计起来很简单。有时候，还需要对支持度进行规范化，即再除以规则有效前提下的总数量。 置信度是衡量规则的准确性如何。1.2 分类 根据特征分出类别 例子：Iris植物分类数据集，通过四个特征分出三个类别 特征连续值变成离散值 OneR算法：它根据已有数据中，具有相同特征值的个体最可能属于哪个类别进行分类。比如对于某一个特征值来说，属于A的类别有80个，属于B的类别有20个，那么对于这个特征值来说，取值为1代表为A类别，错误率有20％。给出所有特征值，找出错误率最小的特征值作为判断标准。第二章 用scikit-learn估计器分类2.1 scikit-learnscikit-learn里面已经封装好很多数据挖掘的算法现介绍数据挖掘框架的搭建方法： 转换器（Transformer）用于数据预处理，数据转换 流水线（Pipeline）组合数据挖掘流程，方便再次使用（封装） 估计器（Estimator）用于分类，聚类，回归分析（各种算法对象） 所有的估计器都有下面2个函数 fit() 训练 用法：estimator.fit(X_train, y_train)， estimator = KNeighborsClassifier() 是scikit-learn算法对象 X_train = dataset.data 是numpy数组 y_train = dataset.target 是numpy数组 predict() 预测 用法：estimator.predict(X_test) estimator = KNeighborsClassifier() 是scikit-learn算法对象 X_test = dataset.data 是numpy数组 2.2 邻近算法KNN邻近算法，或者说K最邻近（KNN，K-NearestNeighbor）分类算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是K个最近的邻居的意思，说的是每个样本都可以用它最接近的K个邻近值来代表。近邻算法就是将数据集合中每一个记录进行分类的方法。例子：分类，Ionosphere数据集第三章 用决策树预测获胜球队3.1 决策树例子：预测NBA球队获胜情况决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。分类树（决策树）是一种十分常用的分类方法。它是一种监督学习，所谓监督学习就是给定一堆样本，每个样本都有一组属性和一个类别，这些类别是事先确定的，那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类。这样的机器学习就被称之为监督学习。scikit-learn库实现了分类回归树（Classification and Regression Trees，CART）算法并将其作为生成决策树的默认算法，它支持连续型特征和类别型特征。3.2 随机森林随机森林指的是利用多棵树对样本进行训练并预测的一种分类器。在机器学习中，随机森林是一个包含多个决策树的分类器， 并且其输出的类别是由个别树输出的类别的众数而定。第四章 用亲和性分析方法推荐电影4.1 亲和性分析亲和性分析就是分析两个样本之间的疏密关系，常用的算法有Apriori，Apriori算法的一大特点是根据最小支持度生成频繁项集（frequent itemest），它只从数据集中频繁出现的商品中选取共同出现的商品组成频繁项集。其他亲和性分析算法有Eclat和频繁项集挖掘算法（FP-growth）。4.2 Apriori算法Apriori算法主要有两个阶段，第一个阶段是根据最小支持度生成频繁项集，第二个阶段是根据最小置信度选择规则，返回规则。本章的例子是电影推荐。第一个阶段，算法会先生成长度较小的项集，再将这个项集作为超集寻找长度较大的项集。第二个阶段是从频繁项集中抽取关联规则。把其中几部电影作为前提，另一部电影作为结论。组成如下形式的规则：如果用户喜欢前提中的所有电影，那么他们也会喜欢结论中的电影。第五章 用转换器抽取特征5.1 抽取特征抽取数据集的特征是重要的一步，在之前的学习中我们都获得了数据集的特征，但很多没有处理的文本特征并不是很明显，比如一段文本等等。特征值可以分为连续特征，序数特征，类别型特征。5.2 特征选择通常特征有很多，但我们只想选择其中一部分。选用干净的数据，选取更具描述性的特征。判断特征相关性：书中列举的例子是判断一个人的收入能不能超过五万，利用单变量卡方检验(或者皮尔逊相关系数)判断各个特征的相关性，然后给出了三个最好的特征，分别是年龄，资本收入和资本损失。5.3 创建特征主成分分析算法（Principal Component Analysis，PCA）的目的是找到能用较少信息描述数据集的特征组合。第六章 使用朴素贝叶斯进行社会媒体挖掘6.1 消歧本章我们将处理文本，文本通常被称为无结构格式。文本挖掘的一个难点来自于歧义，比如bank一词多义。本章将探讨区别Twitter消息中Python的意思。6.2 文本转换器Python中处理文本的库NLTK(Natural Language Toolkit)。据作者说很好用，可以作自然语言处理。N元语法是指由连续的词组成的子序列。6.3 朴素贝叶斯朴素贝叶斯概率模型是以对贝叶斯统计方法的朴素解释为基础。贝叶斯定理公式如下：$ P(A|B) = \\frac {P(B|A)P(A)}{P(B)} $贝叶斯公式可以用它来计算个体属于给定类别的概率。朴素贝叶斯算法假定了各个特征之间相互独立，那么我们计算文档D属于类别C的概率为P(D|C)=P(D1|C)*P(D2|C)…P(Dn|C)。贝叶斯分类器是输入数据来更新贝叶斯的先验概率和后验概率，输入贝叶斯模型后，返回不同类别中概率的最大值。示例： 举例说明下计算过程，假如数据集中有以下一条用二值特征表示的数据：[1, 0, 0, 1]。训练集中有75%的数据属于类别0，25%属于类别1，且每个特征属于每个类别的似然度如下。类别0：[0.3, 0.4, 0.4, 0.7] 类别1：[0.7, 0.3, 0.4, 0.9] 拿类别0中特征1的似然度举例子，上面这两行数据可以这样理解：类别0中有30%的数据，特征1的值为1。我们来计算一下这条数据属于类别0的概率。类别为0时，P(C=0) = 0.75。朴素贝叶斯算法用不到P(D)，因此我们不用计算它。我们来看下计算过程。P(D|C=0) = P(D1|C=0) x P(D2|C=0) x P(D3|C=0) x P(D4|C=0)= 0.3 x 0.6 x 0.6 x 0.7 = 0.0756 现在，我们就可以计算该条数据从属于每个类别的概率。需要提醒的是，我们没有计算P(D)，因此，计算结果不是实际的概率。由于两次都不计算P(D)，结果具有可比较性，能够区分出大小就足够了。来看下计算结果。P(C=0|D) = P(C=0) P(D|C=0) = 0.75 * 0.0756 = 0.05676.4 F1值F1值是一种评价指标。F1值是以每个类别为基础进行定义的，包括两大概念：准确率（precision）和召回率（recall）。准确率是指预测结果属于某一类的个体，实际属于该类的比例。召回率是指被正确预测为某个类别的个体数量与数据集中该类别个体总量的比例。F1值是准确率和召回率的调和平均数。第九章 作者归属问题9.1 作者归属作者归属（authorship attribution）是作者分析的一个细分领域，研究目标是从一组可能的作者中找到文档真正的主人。利用功能词进行分类，功能词是指本身含义很少，但是是组成句子必不可少的部分。9.2 支持向量机支持向量机（SVM）分类算法背后的思想很简单，它是一种二类分类器（扩展后可用来对多个类别进行分类）。假如我们有两个类别的数据，而这两个类别恰好能被一条线分开，线上所有点为一类，线下所有点属于另一类。SVM要做的就是找到这条线，用它来做预测，跟线性回归原理很像。下图中有三条线，那么哪一条线的分类效果最好呢？直觉告诉我们从左下到右上的这一条线效果最好，因为每一个点到这条线的距离最远，那么寻找这条线就变成了最优化问题。对于多种类别的分类问题，我们创建多个SVM分类器，其中每个SVM分类器还是二分类。连接多个分类器的方法有很多，比如说我们可以将每个类别创建一对多分类器。把训练数据分为两个类别——属于特定类别的数据和其他所有类别数据。对新数据进行分类时，从这些类别中找出最匹配的。9.3 基础SVM的局限性最基础的SVM只能区分线性可分的两种类别，如果数据线性不可分，就需要将其置入更高维的空间中，加入更多伪特征直到数据线性可分。寻找最佳分隔线时往往需要计算个体之间的内积。我们把内核函数定义为数据集中两个个体函数的点积。常用的内核函数有几种。线性内核最简单，它无外乎两个个体的特征向量的点积、带权重的特征和偏置项。多项式内核提高点积的阶数（比如2）。此外，还有高斯内核（rbf）、Sigmoind内核。第十章 新闻语料分类10.1 新闻语料聚类之前我们研究的都是监督学习，在已经知道类别的情况下进行分类。本章着眼于无监督学习，聚类。10.2 K-means算法k-means聚类算法迭代寻找最能够代表数据的聚类质心点。算法开始时使用从训练数据中随机选取的几个数据点作为质心点。k-means中的k表示寻找多少个质心点，同时也是算法将会找到的簇的数量。例如，把k设置为3，数据集所有数据将会被分成3个簇。k-means算法分为两个步骤：为每一个数据点分配簇标签，更新各簇的质心点。k-means算法会重复上述两个步骤；每次更新质心点时，所有质心点将会小范围移动。这会轻微改变每个数据点在簇内的位置，从而引发下一次迭代时质心点的变动。这个过程会重复执行直到条件不再满足时为止。通常是在迭代一定次数后，或者当质心点的整体移动量很小时，就可以终止算法的运行。有时可以等算法自行终止运行，这表明簇已经相当稳定——数据点所属的簇不再变动，质心点也不再改变时。"
    } ,
  
    {
      "title"       : "软件方法",
      "category"    : "",
      "tags"        : "school",
      "url"         : "./%E8%BD%AF%E4%BB%B6%E6%96%B9%E6%B3%95.html",
      "date"        : "2021-11-30 00:00:00 +0800",
      "description" : "课堂记录",
      "content"     : "目录 目录 软件方法 课程要求 随记 PPT整理 1. 对象，类 2.面向对象 3.JAVA 4.数据结构 5. 常用数据结构方法 TP整理 软件方法课程要求学习面向对象这种软件开发方法（目前概念越来越广），通过java来了解面向对象的编程具体怎么实现。随记 类，对象： 对象是类的一个实例 c语言可以构建面向对象所有的结构 类集合了属性和方法 面向对象的三大特征： 封装（encapsulation）: private, protected, public 可作用于属性和方法，一般构造方法和成员方法都是public, 属性都是private 一般是隐藏对象的属性和实现细节，但是提供方法的接口 提供公开的方法 提高了软件开发的效率 继承（inheritance）： 子类与父类 子类自动具有父类属性和方法，添加自己特有的属性和方法，并且子类使用父类的方法也可以覆盖/重写父类方法 可以实现代码的复用（当然功能不止于此） 多态（polymorphism）： 父类有多个子类 子类覆盖/重写父类方法 相当于是根据实际创建的对象类型动态决定使用哪个方法 所有的子类都可以看成父类的类型，运行时，系统会自动调用各种子类的方法 UML可以画出类之间的关系 java程序设计 百分百面向对象 不存在类以外代码 只能采用面向对象方法编程 java文件命名规范 必须以.java结尾 源文件中如果只有一个类，文件类必须与该类名相同 如果有多个类，且没有public类，文件名可与任一类名相同 有多个类，且有public类，文件名必须与该类名相同 一个JAVA源文件只能有一个public类，一个文件中只能有一个main主函数 静态方法/static，可以直接用类和函数名直接调用，和普通方法的区别是不用new一个示例 static 方法可以直接调用，abstract方法存在的类肯定是抽象类 抽象方法不定义具体内容 多态的实现，先定义抽象的（abstract）父类，然后子类继承父类然后定义父类的抽象方法 通过抽象方法固定通用接口 子类通过强制实现抽象方法实现多态 抽象父类可以定义属性和构造函数 抽象父类不能实例化，只能通过向上转型的方法定义 抽象父类可以向下转型成子类 父类的方法一般是抽象方法，不定义具体内容，留给子类定义，父类出现的抽象方法子类必须全部定义 多态的主要特点就是父类的方法全部是抽象的 多态例子public class Test { public static void main(String[] args) { show(new Cat()); // 以 Cat 对象调用 show 方法 show(new Dog()); // 以 Dog 对象调用 show 方法 Animal a = new Cat(); // 向上转型 a.eat(); // 调用的是 Cat 的 eat Cat c = (Cat)a; // 向下转型 c.work(); // 调用的是 Cat 的 work } public static void show(Animal a) { a.eat(); // 类型判断 if (a instanceof Cat) { // 猫做的事情 Cat c = (Cat)a; // 向下转型 c.work(); } else if (a instanceof Dog) { // 狗做的事情 Dog c = (Dog)a; c.work(); } } } abstract class Animal { abstract void eat(); } class Cat extends Animal { public void eat() { System.out.println(\"吃鱼\"); } public void work() { System.out.println(\"抓老鼠\"); } } class Dog extends Animal { public void eat() { System.out.println(\"吃骨头\"); } public void work() { System.out.println(\"看家\"); } }PPT整理1. 对象，类 使用对象之前要先声明和创建 类定义了对象的类型，所有对象都是类的实例，所有的类描述了属性和定义了方法2.面向对象 面向对象的编程有4个特点 封装：保护类的属性和方法, 类里面的属性的数据是private的, public的方法定义了对象的接口。权限修饰符: private, default, protected, public。 继承：B继承A，重用，修改，添加，A所有的属性都存在于B中，A的方法可以在B中重新定义，B方法的改动不会影响A 多态：一个对象属于多个类，通过使用不同类中的方法属于不同的类，父类是抽象类，各个子类继承父类并定义方法，调用的时候根据不同子类调用方法。判断类型是否相同instanceof，声明的时候可以这么声明: A a = new B(),其中B是A的子类, 这种声明方法叫做向上转型。向下转型: B b = (B) a 动态链接: 通过PPT上的例子，我感觉和继承很像，这部分需要更深入了解才能明白。3.JAVA main的格式: 数据类型: int, float, double, char, string, boolean, byte, long, short, JAVA里面也有这些数据类型的类：其中Characte应该为Character JAVA关键字this, super x = bool ? a : b，表示如果bool为true，执行a，如果为false执行b for(Point p : this.getVect())表示遍历 exception 异常: 还有异常的抛出throwstry-catch-finallytry { // 可能会发生异常的程序代码 } catch (Type1 id1){ // 捕获并处置try抛出的异常类型Type1 } catch (Type2 id2){ //捕获并处置try抛出的异常类型Type2 }finally { // 无论是否发生异常，都将执行的语句块 }自定义异常：class NombreNegatifException extends Exception{ public NombreNegatifException() { System.out.println(\"Vous avez un nombre négatif !\"); } }在类的方法中抛出新的异常：public int Count() throws Exception{ if (...){ throw new Exception(\"...\"); }} 文件读写：类FileReader,FileWriter,使用里面的方法read()和write(x)和close()比如： 枚举类型enum，举例说明public enum Jour { LUNDI, MARDI, MERCREDI, JEUDI, VENDREDI, SAMEDI, DIMANCHE;}class EssaiJour { public static void main(String[] args) { Jour jour = Jour.valueOf(args[0]); if (jour == Jour.SAMEDI) System.out.print(\"fin de semaine : \"); switch(jour) { case SAMEDI : case DIMANCHE : System.out.println(\"se reposer\"); break; default : System.out.println(\"travailler\"); break; } }} 接口interface, 迭代器iterator举例：class Main { @FunctionalInterface public interface maFonction { Integer appliquer(Integer p); } public static Vector&lt;Integer&gt; transforme(Vector&lt;Integer&gt; v, maFonction function) { Vector&lt;Integer&gt; nouveauVect = new Vector&lt;Integer&gt;(); for (int i = 0; i &lt; 3; i++) { nouveauVect.add(function.appliquer(v.get(i))); } nouveauVect.add(v.get(3)); return nouveauVect; } public static void main(String[] args) { Vector&lt;Integer&gt; vi = new Vector&lt;Integer&gt;(4); vi.add(1); vi.add(4); vi.add(83); vi.add(18); System.out.println(\"Les valeurs du vecteur initial : \"); System.out.print(vi.get(0)+\" \"); System.out.print(vi.get(1)+\" \"); System.out.print(vi.get(2)+\" \"); System.out.println(vi.get(3)); vi = transforme(vi,s -&gt; (s * 2)); System.out.println(\"Les valeurs du vecteur modifié : \"); Iterator iter = vi.iterator(); while (iter.hasNext()) { System.out.print(iter.next() + \"\"); } System.out.println(); }}4.数据结构 数据结构一般含有以下功能：创建，插入，寻找，删除，排序 二维数组,举例说明：String tab[][]={ {\"a\", \"e\", \"i\", \"o\", \"u\"}, {\"1\", \"2\", \"3\", \"4\"} };int i = 0, j = 0;for(String sousTab[] : tab) { j = 0; for(String str : sousTab) { System.out.println(\"Valeur du tableau à l'indice [\"+i+\"][\"+j+\"]: \" + tab[i][j]); j++; } i++;}声明数组：数组类型加变量名或者int tabEntiers[];tabEntiers = new int[40]; // création effective du tableau précédent 列表，包含ArrayList, LinkedListArrayList是实现了基于动态数组的数据结构，LinkedList基于链表的数据结构。对于随机访问get和set，ArrayList优于LinkedList，因为ArrayList可以随机定位，而LinkedList要移动指针一步一步的移动到节点处。（参考数组与链表来思考）。对于新增和删除操作add和remove，LinedList比较占优势，只需要对指针进行修改即可，而ArrayList要移动数据来填补被删除的对象的空间。public class Liste&lt;T&gt; { protected T valeur; protected Liste&lt;T&gt; succ; protected Liste&lt;T&gt; pred; public T valeur() { return valeur; } public void changerValeur(T x) { valeur = x; } public Liste&lt;T&gt; succ() { return succ; } public void changerSucc(Liste&lt;T&gt; y) { succ = y; } public void changerPred(Liste&lt;T&gt; y) { pred = y; }} 这是一个链表的简写，每一层包含了上一个元素，这一个元素，下一个元素 哈希表，通过建立KV关系查找，相比于之前的顺序访问或者其他指数访问要快。import java.util.HashMap;public class TestHash { public static void main(String[] args) { HashMap&lt;String,String&gt; annuaire = new HashMap&lt;String,String&gt;(); // ajout des valeurs annuaire.put(\"Alfred\",\"2399020806\"); annuaire.put(\"Daniel\", \"2186000000\"); // obtention d'un numéro if (annuaire.containsKey(\"Danielle\")) { String num = annuaire.get(\"Danielle\"); System.out.println(Danielle : \"+num\"); } else { System.out.println(\"pas trouve\"); } }} 树状结构tree一般包含结点，结点的度(该结点下有多少子树的数目)，树的度不同的遍历方法： 前序遍历，首先结点，然后左子树，右子树中序遍历，左子树，结点，右子树后序遍历，左子树，右子树，结点层序遍历，从上到下，从左到右class Arbre { protected &lt;T&gt; valeur; protected Arbre filsGauche, filsDroit; public &lt;T&gt; valeur() { return valeur; } public boolean existeFilsGauche() { return filsGauche != null; } public boolean existeFilsDroit() { return filsDroit != null; } public Arbre filsGauche() { return filsGauche; } public Arbre filsDroit() { return filsDroit; } public void affecterValeur(&lt;T&gt; c) { valeur = c; } public void affecterFilsGauche(Arbre g) { filsGauche = g; } public void affecterFilsDroit(Arbre d) { filsDroit = d;} public boolean feuille() {return (filsDroit==null &amp;&amp; filsGauche==null); }public int hauteur() { int g = existeFilsGauche() ? filsGauche.hauteur() : 0; int d = existeFilsDroit() ? filsDroit.hauteur() : 0; return Math.max(g,d) + 1 ;}// Constructeurspublic Arbre(T val) { valeur = val; filsGauche = filsDroit = null;}public Arbre(T val, Arbre&lt;T&gt; g, Arbre&lt;T&gt; d) { valeur = val; filsGauche = g; filsDroit = d;}// Affichagepublic void afficherPrefixe() { System.out.print(valeur+\"\\t\"); if (existeFilsGauche()) filsGauche.afficherPrefixe(); if (existeFilsDroit()) filsDroit.afficherPrefixe();}public void afficherInfixe() { if (existeFilsGauche()) filsGauche.afficherInfixe(); System.out.print(valeur+\"\\t\"); if (existeFilsDroit())filsDroit.afficherInfixe();}public void afficherPostfixe() { if (existeFilsGauche()) filsGauche.afficherPostfixe(); if (existeFilsDroit())filsDroit.afficherPostfixe(); System.out.print(valeur+\"\\t\");}二叉排序树是指左子树小于结点小于右子树，而且结点值不重复。判断是否为二叉排序树：public boolean superieur(char x) {// vrai si x est supérieur à tous les éléments de l’arbre if (feuille()) return (x&gt;=valeur); else return (((this.existeFilsGauche())? (this.filsGauche).superieur(x):true) &amp; ((this.existeFilsDroit())? (this.filsDroit).superieur(x):true));} public boolean inferieur(char x) {//similaire a superieur ... }public boolean binrech() { if (feuille()) return true; else return ((existeFilsGauche()?(filsGauche.superieur(valeur) &amp;&amp; filsGauche.binrech()):true) &amp; (existeFilsDroit()?(filsDroit.inferieur(valeur) &amp;&amp; filsDroit.binrech()):true));} 5. 常用数据结构方法二维数组array[][]的定义和访问 数据结构 vector&lt;String&gt; ArrayList&lt;String&gt; LinkedList&lt;String&gt; HashMap&lt;String,int&gt; 添加 add(i, str) add(i,str) add(i,str) put(str,i) 查找 get(i) get(i) get(i) get(str) 索引 indexOf(str) indexOf(str) indexOf(str)   删除 remove(i)/remove(str) remove(i/str) remove(i/str) remove(i) 清除 clear() clear() clear()   查看大小 size() size() size() size() 迭代器 iterator() iterator() iterator()   变成数组   toArray() toArray()   hashmap还可以返回键值对entryset()，也可以判断是否含有key和value，containsKey(),containsValue()Hashmap创建的时候需要继承TP整理 定义构造函数时也需要声明权限修饰符，构造函数不需要返回值，调用的时候直接在声明一个对象的时候调用 访问成员变量的时候用. 方法中除了构造函数都需要权限修饰符和返回类型名 在类中调用成员变量可以直接调用 字符串之间用+连接 当一个类中有main函数时，一般这么定义: public static void main(String[] a){} 在定义类的时候不要有() 向量的创建和声明: Vector&lt;T&gt; mesObjectsG = new Vector&lt;T&gt;(); 打印的使用: System.out.println() 成员函数在类的开头只是声明，并没有创建，所以在方法中创建 for循环的使用: for(Point p : seg.getVecP()) Vector.add(), 向量的添加 Vector.get(i), i从0开始, 向量的访问 Vector.size()返回int值，是vector的大小 TP1中为了让图形可视化，每个类都需要extends ObjectGraphique 什么时候需要在类定义方法时new一个对象: 当这个对象来自其他类同时改变这个对象会对新创建的类产生影响，所以一般还是new一个对象比较好 基本的数据类型: int, string, float, boolean不需要new, 声明的时候也不需要构造函数 继承的时候可以用super()来调用继承类的构造函数 异常的抛出: 在定义类中的方法的时候，需要抛出的方法需要在创建时添加一句throws Exception, 调用的时候使用throw new Exception(“…”) 也可以throws IOException这样已经定义好的抛出类型 文件读入: Reader reader = new FileReader(textname.txt) 逐行读入: BufferedReader in = new BufferedReader(reader); String line = in.readline() 分词器: StringTokenizer st = new StringTokenlizer(line, DeleteChar); 然后就可以像迭代器一样调用, if(st.hasMoreTokens()){st.nextToken()} 文件写入: Writer writer = new FileWriter(newtextname); writer.write(“…”) 将链表中所有的单词写入一个文件: for (object k : this){writer.write(k+”\\n”);writer.flush()} 写入完毕后, writer.close() 哈希表判断key是否在哈希表中: HashMap.containsKey(…), 返回值为布尔类型 哈希表中添加元素: HashMap.put(key, value) 哈希表可以返回键值对，键对和值对，调用HashMap.entrySet()、HashMap.keySet()、HashMap.valueSet() 通过键来访问哈希表中的值: HashMap.get(key) EntrySet同样可以通过getKey()和getValue()来访问键值 在创建一个新类时，对于类的名字可以这么取: Arbre&lt;T&gt;, 这样就表示Arbre类中的数据类型是T java中的空是null，小写 记住二叉树高度是怎么写的 记住二叉树前序中序后序遍历是如何写的 在进行没有变量x时怎么计算数的结果: 先用迭代访问出左值和右值，然后根据根结点的符号决定进行什么运算 Integer.parseInt(String a)的作用是将整数的String表示转化为Integer 对于含变量x的二叉树如何在输入x的值的情况下算出结果: 首先遍历左子树，如果左子树的左边有x，那么将输入的值赋给这个结点的x，然后迭代回来计算左结点的值，具体的可以看网站截图 java中final关键字是保证了变量无法修改，只能访问 字符串的比较用.equals() Vector.contains(…)可以判断一个值是否在这个向量里 String.charAt(i), 可以将字符串第i个字符赋给char instanceOf用来判断一个对象是不是一个类的实例: if (a instanceOf A) 多态的一种应用是: 抽象父类，子类继承抽象父类"
    } ,
  
    {
      "title"       : "RACE数据集相关文献",
      "category"    : "",
      "tags"        : "paper",
      "url"         : "./RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html",
      "date"        : "2021-11-30 00:00:00 +0800",
      "description" : "文献整理",
      "content"     : "目录 目录 文献整理 要求 搜集到相关文献标题和地址 第一篇 Title Author Abstract Introduction BERT distractor generation 1)BERT-based distractor generation(BDG) 2)Multi-task with Parallel MLM 3)Answer Negative Regularization Multiple Distractor Generation 1)Selecting Distractors by Entropy Maximization 2)BDG-EM Performance Evaluation 1)datasets 2)implementation details 3)compared methods 4)token score comparison 5)MCQ Model Accuracy Comparison 6）Parameter Study on γ Conclusion 我的看法 第二篇 Title Author Abstract Method 1)question generation 2)distractor generation 3)QA filtering Results 1)quantitative evaluation 2)question answering ability 3)human evaluation conclusion 第三篇 Title Author Abstract Framework Description 网络结构 1)Task Definition 2)Framework overview 3)Hierarchical encoder 4)static attention mechanism 5)encoding layer 6)matching layer 7)nomalization layer 8)distractor decoder 9)question-based initializer 10)dynamic hierarchical attention mechanism 11)training and inference experimental setting 实验设置 1)dataset 2)implementation details 3)baselines and ablations results and analysis 结果与分析 我的看法 第四篇 Title Author Abstract Proposed Framework 网络结构 1)notations and task definition 2)model overview 3)encoding article and question 4)Co-attention between article and question 5)Merging sentence representation 6)question initialization 7)hierarchical attention 8)semantic similarity loss Experimental Settings 1)dataset 2)baselines and evaluation metrics 3)implementation details Results and Analysis 结果与分析 我的看法 补充 RACE数据集简介 BLEU ROUGE 文献整理要求搜集到相关文献标题和地址 A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies Better Distractions: Transformer-based Distractor Generation and Multiple Choice Question Filtering Generating Distractors for Reading Comprehension Questions from Real Examinations Co-attention hierarchical network: Generating coherent long distractors for reading comprehension Automatic Distractor Generation for Multiple Choice Questions in Standard Tests Distractor Generation for Multiple Choice Questions Using Learning to Rank Knowledge-Driven Distractor Generation for Cloze-style Multiple Choice Questions第一篇TitleA BERT-based Distractor Generation Scheme with Multi-tasking andNegative Answer Training StrategiesAuthorHo-Lam Chung, Ying-Hong Chan, Yao-Chung FanAbstract现有的DG1局限在只能生成一个误导选项，我们需要生成多个误导选项，文章中提到他们团队用multi-tasking和negative answer training技巧来生成多个误导选项，模型结果达到了学界顶尖。IntroductionDG效果不好，文章提出了两个提升的空间： DG质量提升： BERT模型来提升误导选项质量 多个误导选项生成： 运用了覆盖的方法来选择distractor，而不是选择概率最高但是语义很相近的distractorBERT distractor generation1)BERT-based distractor generation(BDG)输入：段落P，答案A，问题Q，用C表示这三者concatenate后的结果。BDG模型是一个自回归模型，在预测阶段，每次输入C和上一次预测的词元，BDG迭代预测词元，直到预测出特殊词元[S]停止。下面这张图简单介绍了这个过程。网络结构简单介绍：h[M]表示bert输出的隐藏状态，隐藏状态再输入到一个全连接层中用来预测词元。2)Multi-task with Parallel MLMMLM全称masked language model，遮蔽语言模型,通过并行BDG和P-MLM来训练模型让模型有更好的效果。上图中左边的sequential MLM就是之前提到的BDG，BDG模型是一个词接一个词的预测，P-MLM是对所有的masked token进行预测，最后的损失函数是这两者相加2，公式如下：作者如此设计的思路是：BDG可能会忽略整体语义语义信息，但是会过拟合单个词预测。那么并行一个P-MLM可以防止过拟合。3)Answer Negative Regularization目前机器预测的distractor和answer有很高的相似度，下面一张表可以展示相似度。其中PM表示机器，Gold表示人工，作者将这类问题称为answer copying problem。为了解决这个问题，作者提出了answer negative loss来让机器更多的选择与answer不同的词来表示新的distractor，公式如下：可以看出BDG的loss替换成了AN的loss，每一项都减去了Answer negative loss。Multiple Distractor Generation1)Selecting Distractors by Entropy Maximization选择语义不同的distractor set。文章借鉴了MRC3的方法，让BDGmodel生成很多distractor组成 $\\hat{D}$ = {$\\hat{d}$1, $\\hat{d}$2, $\\hat{d}$3…}，然后找出最好的一组选项，一般情况下由三个误导选项和一个答案组成。选择的一句是最大化下面这个公式：2)BDG-EM我们可以通过不同的BDG模型来生成不同的误导选项最后组合，不同的模型区别是有没有answer negative/multi-task training，比如我们有这几个模型:$\\hat{D}$,$\\hat{D}$PM,$\\hat{D}$PM+AN，它们分别代表含PM4和含AN5Performance Evaluation1)datasetsRACE,沿用了Gao那篇论文的处理,后面也会梳理那篇论文2)implementation details tokenizer: wordpiece tokenizer framewordk:huggingface trainsformers optimizer:adamW(lr:5e-5) github_url: BDG3)compared methods比较了不同的distractor generation CO-Att：出自Zhou DS-Att: 出自Gao GPT:baseline BDG: 没有应用P-MLM和Answer negative BDGPM BDGAN+PM4)token score comparisonBLEU和ROUGE(L)两种判断指标copying problem的效果5)MCQ Model Accuracy Comparison与回答系统相结合，将生成好的选项（一个正确答案三个误导选项）放入MCQ answering model，下面是回答正确率的表格可以看出作者的模型选项的误导性还是很高的。6）Parameter Study on γ之前使用P-MLM并行训练时候有个权重参数γ，下表显示了不同γ值的影响，对于只有PM的模型来说，γ=6，对于既有AN和PM来说，γ=7Conclusion现存的DG可以分为cloze-style distractor generation和 reading comprehension distractor generation，前者主要是word filling，后者主要看重语义信息，基于两者的设计出了很多模型，目前来看还是考虑语义信息生成的误导选项更好。我的看法文章中的模型提到了三种技术，第一是bert预训练模型使用。第二是P-MLM的并行使用， 它的使用让模型可以考虑段落的语义信息，那么生成的误导选项是sentence-level而不是之前模型所使用的类似word-filling这种word-level。第三是Answer negative loss的使用，它的使用相当于让模型不要考虑与正确答案语义很接近的误导选项，因为目前大多数DG生成多个选项时语义与正确答案都非常接近，这与实际情况不符，同时也起不到误导的作用。 同时文章提出了生成多个误导选项时使用不同模型生成的误导选项拼在一起作为选项是一种比较好的解决方法，让一次性生成多个误导选型有了一定的可用性。文章的代码开源，可以去github上看训练细节和网络结构细节。第二篇TitleBetter Distractions: Transformer-based Distractor Generation and Multiple Choice Question FilteringAuthorJeroen Offerijns, Suzan Verberne, Tessa VerhoefAbstract运用GPT2模型生成三个误导选项，同时用BERT模型去回答这个问题，只挑选出回答正确的问题。相当于使用了QA作为一个过滤器(QA filtering)。Method作者使用了Question generation model, distractor generation model和question answer filter，作者将从这三方面介绍，下图是大概的流程图。1)question generation 预训练模型：GPT-2 数据集：English SQuAD tokenizer：Byte-Pair-Encoding(BPE) tokenizer optimizer:Adam 下图展示了QG的输入，黑框内被tokenizer标记为特殊词元2)distractor generation 预训练模型：GPT-2 数据集：RACE tokenizer:BPE6 使用了repetition penalty技术，保证了尽量不会生成相似的text，并且过滤到那些不好的生成（比如生成了空字符串） 输入：经典的C(context)，A(answer),Q(question)，下图展示了输入格式3)QA filtering 预训练模型：DistilBERT 网络结构：CQA7输入到distilbert，再连接一个dropout，全连接层和softmax，最后输出一个答案，具体结构如下图Results1)quantitative evaluation下表中展示了和上一篇论文类似的指标,与现有的模型进行了比较：SEQ2SEQ,HSA8和CHN9。可以看出BLEU明显要比之前模型要好，但是ROUGE没有之前的高。2)question answering ability用GPT-2模型生成误导选项再输入到QAmodel中，具体结果见下图。3)human evaluation人工评估，从两方面评估distractor生成的好坏： Is the question well-formed and can you understand the meaning? If the question is at least understandable, does the answer make sense in relation to the question?评估过程中，使用了155个没有经过QA筛选和155经过QA筛选的，了解一下QA过滤模型的效果。整体来说QA过滤器还是有一点效果，具体结果如下：conclusion我认为作者使用的DG模型主要有两大特色，一个是使用了GPT2预训练模型，目前使用基于transformer的模型已经成为主流。第二个是使用了QA过滤器来筛选掉回答错误的，有一定提升但不显著。第三篇TitleGenerating Distractors for Reading Comprehension Questions from Real ExaminationsAuthorYifan Gao, Lidong Bing, Piji Li,Irwin King, Michael R. LyuAbstract上面两篇文献都有提到这篇文章。作者使用了Hierarchical encoder-decoder framework with static and dynamic attention mechanisms来生成有语义信息的误导选项。使用了编码器-解码器结构网络和静态和动态注意力机制。Framework Description 网络结构1)Task Definition输入：文章，问题和答案。P代表文章，s1,s2,s3…表示不同的句子，q和a分别表示问题和答案，那么我们的任务是生成误导选项$\\overline{d}$。2)Framework overview网络结构如下图所示，下面将从各个组成部分分别介绍：3)Hierarchical encoder word embedding:词嵌入，将每个句子si中的每个词元变成词向量(wi,1,wi,2,wi,3…) word encoder:将句子si的词向量(wi,1,wi,2,wi,3…)作为输入，用双向LSTM作为编码器，获得word-level representation hi,je sentence encoder:将word encoder中每个句子正向LSTM的最后一个隐藏状态和反向LSTM的最开始的隐藏状态作为输入到另一个双向LSTM中获得sentence-level representation(u1,u2,u3…)4)static attention mechanism目的：生成的误导选项必须和问题Q语义相关，但是和答案A必须语义不相关。我们从(s1,s2,s3…)学习到句子的权重分布(γ1,γ2,γ3…)，然后将问题q和答案a作为query。5)encoding layer我们希望把问题q，答案a和句子s都变成一样的长度的向量表示，也就是上图中紫色虚线部分。对于q和a，我们用两个独立的双向LSTM来获得(a1,a2…ak)和(q1,q2…ql)，然后用平均池化层平均一下：对于句子s，我们不用u而用h：6)matching layer目的：加重与问题q有关的句子，减轻与答案a有关的句子。oi表示不同句子的importance score7)nomalization layer目的：有些问题q和一两个句子有关，而有些问题q和很多句子有关，比如summarizing，下面的τ(temperature)就是这个作用作者介绍static attention mechanism用了很大篇幅8)distractor decoder解码器使用的也是LSTM，但是并没有使用编码器的最后一个隐藏状态作为初始状态，而是定义了一个question-based initializer来让生成的误导选项语法和问题q一致9)question-based initializer定义了一个question LSTM来编码问题q，使用最后一层的cell state和hidden state作为decoder初始状态，同时输入qlast，表示问题q的最后一个词元。10)dynamic hierarchical attention mechanism常规的注意力机制将一篇文章作为长句子，然后decoder的每一个时间步都与encoder中所有的hidden state进行比较，但是这种方法并不适合目前的模型。原因：首先LSTM不能处理这么长的输入，其次，一些问题只与部分句子有关。目的：每个decoder时间步只关注重要句子，作者将这种注意力机制称为动态注意力机制，因为不同的时间步，word-level和sentence-level 注意力分布都不同。每一个时间步的输入是词元dt-1和上一个隐藏状态ht-1α和β分别表示word-level,sentence-level权重，最后使用之前静态注意力机制获得的γ来调节α和β获得上下文变量ct获得attention vector $\\tilde{h}$11)training and inference损失函数：生成多个误导选项的方法是束搜索，但是生成的误导选项很相似，作者做了相应的处理方法，但我觉得效果还是很差experimental setting 实验设置1)datasetRACE数据集，作者做了相应的处理，去掉了很多不合理的和语义不相关的，作者的处理标准是：对于误导选项中的词元，如果它们在文章中出现的次数小于5次，那么将被保留，同时去掉了那些需要在句子中间和句子开始填空的问题。下表展示了处理后的数据集的一些信息：2)implementation details词表：保留了频率最高的50k个词元，同时使用GloVe作为词嵌入预训练模型。其他的细节都可以在文章中看见，这里不一一列出了，主要是超参数的设置。3)baselines and ablations与HRED10和seq2seq比较results and analysis 结果与分析人工评估：大致过程是这样：四个误导选项，分别来自seq2seq，HRED，作者的模型和原本的误导选项，让英语能力很好的人来选择最适合的选项，得出的结果可以发现，作者的模型生成的误导选项拥有最好的误导效果。下图直观展示了static attention distribution：我的看法这篇文章应该是第一个提出用处理后的RACE数据集来处理MCQ问题，处理后的RACE数据集在后面也有很多文献用到，这篇文章使用了seq2seq网络结构同时使用了静态和动态注意力机制，对于网络结构和注意力机制的解释非常完全和详细，虽然这篇文章的效果放到现在来看可能不是最好了，但是它提出来的评估标准可能会成为一个通用的标准。它的数据集和训练代码在github上也完全开源。第四篇TitleCo-attention hierarchical network: Generating coherent long distractors for reading comprehensionAuthorXiaorui Zhou, Senlin Luo, Yunfang WuAbstract这篇文献是针对上一篇Gao的文章(seq2seq)所作的改进。本篇文章提出了Gao的模型的两个问题：1.没有建立文章和问题的关系，他的解决方法是使用co-attention enhanced hierarchical architecture来捕获文章和问题之间的关系，让解码器生成更有关联的误导选项。2.没有加重整篇文章和误导选项的关系。作者的解决思路是添加一个额外的语义相关性损失函数，让生成的误导选项与整篇文章更有关联。Proposed Framework 网络结构1)notations and task definitionarticle T=(s1,s2…sk)，一篇文章有k个句子s，同时每个句子都有不同的长度l，si=(wi,1,wi,2…wi,l)，每个文章有m个问题和z个误导选项，Q=(q1,q2…qm),D=(d1,d2…dz),我们的任务是根据输入的T和Q生成D2)model overview整体结构如下图所示，下面将从各个部分分别介绍：3)encoding article and question文章和问题的编码器结构 hierarchical article encoder双向LSTM，和上一篇结构很像，很多部分我就简单列个式子。每一句最后的词元来表示整个句子sentence-level encoder：同样，用最后一个句子来表示整篇文章用H*来作为sentence-level representation of article,我们有H:t*=hts这样，通过使用两个双向LSTM获得word-level encoding和sentence-level encoding question encoder用U*来作为word-level representations of question, 我们有U:t*=htq4)Co-attention between article and questionCo-attention mechanism就是使用了两个方向的注意力机制，有从article到question的，也有question到article的。用一个“相似”矩阵S表示H和U的关系：Si,j就表示第i个句子和第j个问题词元的相似性我们可以获得两个特殊的矩阵SQ和ST article-to-question attention$\\tilde{U}$:j = $\\sum$ Si,jQU:,i question-to-article attention最后，将问题的词级表示H，两个方向的注意力结果$\\tilde{U}$和$\\tilde{H}$结合一下获得G5)Merging sentence representationZ表示final representation of sentence-level hidden states6)question initialization接下来就进入decoder环节，这里的question initialization和上篇文献处理方法相同7)hierarchical attention不同时间步有不同的句子相关，和上篇文献的处理方法动态注意力机制相同。8)semantic similarity loss目的：获得文章和误导选项的关系。还记得之前定义的eT吗，它表示整篇文章，那么我们通过下面的公式可以获得distractor representation:其中SM是decoder最后一个隐藏状态，那么我们通过cos计算相似关系，那么最终的损失函数Experimental Settings1)dataset使用了上篇文献处理过的RACE数据集。2)baselines and evaluation metrics与seq2seq，HRED，HCP11，HSA12比较。3)implementation details网络超参数设置技巧，不展开了Results and Analysis 结果与分析介绍一下上面这张表，这张表是人工评估的结果，从三个维度分析，分别是fluency,coherence,distracting ability。可以看出作者的模型并不是在所有维度都是最好的。下图是案例分析：我的看法这篇文献是基于上一篇文献的方法进行了两个改进：1.关联了整篇文章和问题，解决方法是使用了Co-attention mechanism。2.让distractor和article语义相关，方法是定义了相关性loss。补充RACE数据集简介RACE数据集是一个来源于中学考试题目的大规模阅读理解数据集，包含了大约 28000 个文章以及近 100000 个问题。它的形式类似于英语考试中的阅读理解（选择题），给定一篇文章，通过阅读并理解文章（Passage），针对提出的问题（Question）从四个选项中选择正确的答案（Answers）。BLEUBLEU是一个评价指标，最开始用于机器翻译任务，定义如下它的总体思想就是准确率，假如给定标准译文reference，神经网络生成的句子是candidate，句子长度为n，candidate中有m个单词出现在reference，m/n就是bleu的1-gram的计算公式。BLEU还有许多变种。根据n-gram可以划分成多种评价指标，常见的指标有BLEU-1、BLEU-2、BLEU-3、BLEU-4四种，其中n-gram指的是连续的单词个数为n。ROUGERouge(Recall-Oriented Understudy for Gisting Evaluation)，是评估自动文摘以及机器翻译的一组指标。它通过将自动生成的摘要或翻译与一组参考摘要（通常是人工生成的）进行比较计算，得出相应的分值，以衡量自动生成的摘要或翻译与参考摘要之间的“相似度”。它的定义如下：文献中使用的ROUGE-L是一种变种，L即是LCS(longest common subsequence，最长公共子序列)的首字母，因为Rouge-L使用了最长公共子序列。Rouge-L计算方式如下图：其中LCS(X, Y)是X和Y的最长公共子序列的长度,m、n分别表示参考摘要和自动摘要的长度（一般就是所含词的个数），Rlcs,Plcs分别表示召回率和准确率。最后的Flcs即是我们所说的Rouge-L。 distractor generation 误导选项生成，简称DG &#8617; 当我们test时，只需要Sequential MLM decoder来预测。 &#8617; multi-choice reading comprehension (MRC) model &#8617; P-MLM &#8617; Answer negative &#8617; Byte-Pair-Encoding &#8617; context，question，answer &#8617; hierarchical encoder-decoder model with static attention &#8617; hierarchical model enhanced with co-attention &#8617; hierarchical encoder-decoder &#8617; 相当于HRED+copy,是基于HRED的网络结构 &#8617; 就是上篇文献的网络 &#8617;"
    } ,
  
    {
      "title"       : "课程总结",
      "category"    : "",
      "tags"        : "school",
      "url"         : "./%E5%A4%A7%E5%9B%9B%E4%B8%8A%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93.html",
      "date"        : "2021-11-28 00:00:00 +0800",
      "description" : "记录课程和课程笔记",
      "content"     : "概率统计 简介 内容总览 流体力学 简介 内容总览 A4纸 报告 电磁辐射波 简介 内容总览 传感器 简介 内容总览 结构力学 简介 内容总览 A4纸 项目管理 简介 内容总览 工程热力学 简介 报告Pre 内容总览 Presse 简介 内容总览 Audiovisual 简介 内容总览 软件方法 简介 内容总览 机器人 简介 内容总览 报告 概率统计简介 授课老师：牛薇 授课材料：一份法语讲义，一份习题集（10个EX），上课用的PPT B站有录播，up主：却道成归 笔记记在侧边栏为大四上A的笔记本最前面内容总览一半时间概率一半时间统计 概率 先从之前学的概率空间讲起，介绍了概率分布（离散or连续），密度函数，期望方差，收敛性。 估计，比如说用平均值估计期望，用频率估计概率等等 估计又分为点估计和区间估计，点估计中介绍了似然函数以及最大似然法来找估计量 统计 主要介绍了几种检验方法来检验分布、估计量选择的好坏 包括了参数检验，分布检验，比较检验等等 A4纸流体力学简介 授课老师：方乐 授课材料：PPT，TD都是6个，分别对应六大章 B站有录播 笔记在侧边栏为大四上A的中后部分和大四上B前面内容总览第一章主要讲了流体的概念和动力学的公式。第二章从能量角度出发，介绍了NS方程（斯托克斯方程），和伯努利原理（压强和流速的关系）。第三章介绍了雷诺数，无量纲分析，雷诺数大的是湍流，雷诺数小的是层流。第四章介绍了边界层，第五章介绍了湍流，系统平均。第六章介绍了涡量。A4纸报告结课之前需要我们写一个报告，什么形式的都可以，我觉得这种方式挺好的，自由发挥，我做的实验，用牛奶和墨水还原了卡门涡街。电磁辐射波简介 授课老师: José Penuelas(负责前几章教学), Bertrand Vilquin(负责后几章教学), 孙鸣捷老师(负责TD) 授课形式：线上讲解原理，线下TD 授课材料：PPT，讲义，TD B站有录播 有笔记，侧边栏叫做电磁学(大四上) 考试闭卷，所以没有A4纸内容总览首先回顾了之前学的波动物理和电磁学，电磁辐射，顾名思义是要将辐射，讲了波导，腔和光电效应，能级跃迁等等传感器简介 授课老师：徐平 授课形式：线下授课，做实验 授课材料：大学生MOOC 没有考试，没有笔记内容总览讲解了传感器的基本原理，构造和常见传感器，每节课都需要在MOOC上做题，也有安排答辩，我和蔡卓江、宋正浩、刘亚林、马卫一一组讲解了机器狗。做实验是指去214玩小车，上面有不少传感器，也有大疆的线上模拟器，还是挺不错的一次动手实验。结构力学简介 授课老师：黄行蓉，Jean-Piere Lainé 授课形式：J-P录制ppt，黄老师线下授课 授课材料：讲义，PPT，TD B站有录播 笔记：侧边栏结构力学，还有最后第八章记在大四上C前面内容总览结构力学分为了两大部分，弹性力学和材料力学。在弹性力学部分，首先介绍了应力和应力张量的概念，张量可以写成3*3矩阵形式，其中对角线上的元素被称作正应力。第二章介绍了应变，首先介绍了很多种张量，F、H、C、E，然后介绍了形变张量ε。第三章介绍了本构方程（应力应变关系方程）。第四章介绍了能量，包括最小势能和最大余能等等。第二部分是材料力学，主题内容和弹性力学类似，但是引进了力螺旋的概念，这个概念在中国授课好像是没有的，它描述了合力和力矩。第一张介绍了内力，在材料力学部分我们主要研究梁这个结构，它包括了中轴线和截面，这部分内容和之前学的理论力学很相似。第二章介绍了应力，可以用内力表示应力，用一些惯性矩、艾力函数连接。第三章介绍了应变和本构方程，第四章介绍了能量部分，主要是三大定理：théorème de ménabréa;théorème de maxwell-betti;théorème de castigliano。A4纸项目管理简介 授课老师：张敏 授课形式：线下授课 授课材料：书(没买)，PPT B站有录播 开卷考试，没有笔记内容总览是上学期经济管理的一部分展开讲，讲了什么是项目，项目管理系统，基于关键路径的项目管理，项目管理的决策工程热力学简介 授课老师：Guillaume Merle 授课形式：看课本，付小尧定期答疑 授课材料：课本、PPT 闭卷考试，不能带A4纸，有三次小测 纯英文授课报告Pre 报告题目：SABRE发动机效率分析内容总览一共学习了课本上第1、2、3、4、5、6、7、9、10、11、12章内容，其中第一章介绍热力学基本单位和概念，第二章介绍能量转移的形式是功和热，顺便引出热力学第一定律，第三章介绍纯物质的相和相变，第四章介绍封闭系统的能量变化，即质量不变体积可变的系统，第五章介绍开放系统的质量变化，即体积可变质量不变，第六章介绍热机的效率基本概念，第七章介绍熵的概念以及热力学第二定律，到这为止都是之前热力学学过的东西，第九章介绍了内燃机的工作原理，包括了很多个不同的循环，比如otto、diesel、brayton循环。第十章介绍了热电机的原理和ranking循环。第十一章介绍了冰箱和热泵的基本原理。第十二章是用到的数学表达式的推理。Presse简介 法语课程 授课老师:Vanessa 授课形式：线下授课 授课材料：讲义 闭卷考试 期间写过几次PE内容总览 Règle de la classe Vingt ans qui ont déjà tout changé: 11 septembre(effodrement des Tours Jumelles), l’incendie de la cathédrale Notre-Dame de Paris, les confinements en France et dans le monde, la mort de Michael Jackson, l’accident nucléaire de Fukushima Le 11 Septembre apparaît comme la matrice du XXIe siècle les personnalités les plus marquantes des 20 dernières années: Barack Obama, Donald Trump, Oussama Ben Laden, Emmanuel Macron, Angela Merkel… François Hollande les chanteurs: Johnny Hallydat, Céline Dion, Stromae le confinement en France: haltères, machine à pain, graines de tomates… des solutions pour prospérer et progresser: Cité flottante, Télétravail, Agriculture spatiale, le train intelligent la voiture autonome ne tient pas encore la route: sensible, risque le tourisme spatial, bonne ou mauvaise idée: huit, Elon Musk l’agence spatiale européenne recrute une nouvelle promotion, Feriez-vous un bon astronaute les Français et Internet la 5G vous inquiète-t-elle? la 5G, amie ou ennemie? Sept innovations contre le handicap: le gant qui traduit la langue des signes, les lunettes qui parlent aux malvoyants Jeux vidéo Bientôt de l’e-sport aux Jeux OlympiquesAudiovisual简介 法语课程 授课老师：Fabien 授课形式：瞩目线上 授课材料：讲义 闭卷考试内容总览 Séance 1 - les étudiants français sous la Covid-19 Séance 2 - Afghanistan: partir ou rester Séance 3 - Réalité Virtuelle: une méthode d’emphaie Séance 4 - Utiliser des produits ménagers bio Séance 5 - Pandémies et environnement Séance 6 - Les transports du futur Séance 7 - Le tourisme spatial Séance 8 - Robots de guerre Séance 9 - Le bilan de la COP 26 Séance 10 - Révolution du vin Séance 11 – Rouler au whisky Séance 12 — Un champion de la pâtisserie软件方法简介 授课老师：于雷， Olivier Roux 授课形式：OR线上授课，于雷助教，有报告和TP 授课材料：PPT，TP 闭卷考试，手写代码内容总览在另一份博客里有具体整理，主要是介绍面向对象的开发方法，同时用java来展示如何进行面向对象的编程机器人简介 授课老师：严亮 授课形式：线下授课 授课材料：PPT 无考试，有大作业(报告) 英文授课内容总览只有四节课，介绍了自由度，动力学，逆动力学，速度和力报告用solidworks设计一个三自由度的机械臂，并附上运动学分析"
    } ,
  
    {
      "title"       : "MCQ文献阅读",
      "category"    : "",
      "tags"        : "paper",
      "url"         : "./%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html",
      "date"        : "2021-11-25 00:00:00 +0800",
      "description" : "整理最新MCQ研究进展和问题",
      "content"     : "整理集合多选题自动生成： 定义输入一篇文章，从这篇文章生成一系列多选题 模型结构待补充 工作流程六大步： 输入文章预处理 选择句子 从句子中选择关键字 生成疑问句 生成误导选项 后期处理 主流的研究关注点与现存的问题 目前大多数处理文本都不考虑公式、图片和图标等等信息，文本预处理只提取文本信息，今后MCQG的研究需要关注处理嵌入在文本中的信息的能力。 现有的MCQ生成方法侧重于从单个句子生成问题，然而，文本可能通过多个句子来生成句子，所以今后的研究应该侧重于从多个句子中生成问题。 关键字的选择取决于下游任务或者应用领域，早期的关键字选择依赖于基本的统计和句法信息。最新的研究趋势是使用机器学习或者语义信息作为选择的标准。 误导选项的选择同样与应用领域有关，目前的MCQ生成系统中使用的都是简单的误导选项生成，但在实际情况中，误导选项可以是非常多种类的，可以是不同的命名体，数字大小，多个单词的误导选项等等。作者认为文本的深层语义分析或使用基于神经嵌入的方法可能是复杂误导答案生成的一个可能的方向。 一篇关于MCQG的综述TitleAutomatic Multiple Choice Question Generation From Text: A SurveyAuthorDhawaleswar Rao CH and Sujan Kumar SahaAbstractMCQ1工作20年前已经开始研究，综述将概括目前常见的多选题自动生成的工作流和评估系统。1.introduction介绍了MCQ的重要性，是评估知识学习的工具之一，优点是耗时短但是人工出题需要很多时间，所以通过一段文章自动生成问题是人们关注的重点。2.multiple choice question介绍了MCQ和MCQ的基本结构，由题干，正确答案和误导答案组成，同时具体介绍了MCQ的优缺点。 优点 缺点 快速评估，耗时短 涵盖的知识面很小 可以实现机器阅卷 答案有猜测出来的可能 3.research motivation and objectivesMCQ的研究动机主要来源于人工出题繁琐且耗时间。4.review methodology作者从大量paper中挑选了86篇文章做来做MCQ的综述。介绍了一下检索文章的步骤同时介绍了不同的QG的方法分布5.discussion on the appproaches for MCQ generation自动生成MCQ和手动生成MCQ的步骤大致相同： Pre-processing of Input Text 预处理输入文章 Sentence Selection 句子选择 Key Selection 选择答案信息 Question Formation 问题生成 Distractor Generation 错误答案生成 Post-Processing 后期处理下面从这六个阶段分别分析：1.Pre-processing of Input Text 用到的技巧（每一个技巧都有对应的文献）： text normalization: 将文本格式变成我们需要的格式，不同的应用领域需要不同的格式化方法 structure analysis：给出段落结构 sentence simplification：把长句子变成短句子 lexical analysis：词汇分析，把文本分隔成单词，符号和数字。同时需要进行词根提取 statistical analysis：统计分析，包括不同的统计手段，比如词频，n元词频等 syntactic analysis：语法分析，包括POS2，NER3，syntactic parsing4。 coreference resolution: 代词通常不作为疑问句的主语，代词解析就是将代词映射到相应的名词。 word sense disambiguation：消除句子中单词的歧义 作者指出，对于text的预处理主要取决于输入文本的性质和下游任务的需求，比如说从web端爬下的文本会包含很多噪音和没必要的内容，那么文本清理就是必须的，再比如wikipedia文档作为输入时常常是一个长句子，需要把长句子简化变成短句子。目前大多数处理文本都不考虑公式、图片和图标等等信息，文本预处理只提取文本信息，今后MCQG的研究需要关注处理嵌入在文本中的信息的能力。2.sentence selection在对输入文本进行处理之后需要挑选出包含questionable fact的句子，我的理解是那些包含事实的句子。 一些技巧： sentence length：给出句子中单词的数量，一般来说，很短的句子不能包含足够的信息来生成问题，同样来说，很长的句子通常包含多个事实和关系，这会给生成问题带来困难。 occurrence of a particular word：查找特殊词汇 parts-of-speech information: 根据一个句子中出现词汇的词性挑选句子，比如说根据名词-形容词对的出来情况来选择句子。 parse information: 根据句子结构挑选，比如主谓宾 semantic information: 文本中包含的语义信息也作为选择句子的标准 machine learning: 使用机器学习算法，比如支持向量机、神经网络等 summarization：基于摘要的方法来选择句子 句子的选择同样需要根据任务的不同来选择。现有的MCQ生成方法侧重于从单个句子生成问题，然而，文本可能通过多个句子来生成句子，所以今后的研究应该侧重于从多个句子中生成问题。3.key Selection选择好句子后，我们从中挑选出关键词。我们不能将一个句子的全部词汇都作为关键词，因此，关键字的选择是确定句子中要被删除的单词（或者短语、n元词元） 一些技巧： frequency count: 统计单词的出现频率作为选择标准 part-of-speech and parse information: 在某些特定的应用领域中，一个特定的词性或者语法可以成为一个潜在的关键字。比如一些研究用动词作为关键字，一些研究用介词作为关键字。 semantic information: 语义信息。 pattern matching：模式匹配，从结构相似的句子中提取出常见的句型，这样有助于句子解析结构来寻找关键字 machine learning：利用机器学习来生成动词或者部分习语或者副词来作为关键字。 关键字的选择同样取决于下游任务或者应用领域，早期的关键字选择依赖于基本的统计和句法信息。最新的研究趋势是使用机器学习或者语义信息作为选择的标准。4.question formation选完关键字后，我们下一个任务就是把陈述句转化为疑问句。 一些技巧： by appropriate wh-word selection: 根据句子的语法结构和关键字来确定使用哪个wh subject-verb-object and their relationship：通过主谓宾结构来生成疑问句 knowledge in sentence：根据句子所包含的知识类型来确定转换规则，例如这个句子是概念，定义，示例等等。 dependency based patterns: 根据句子的依赖关系树来确定主要动词和将被问及的问题部分 syntactic transformation：通过句法结构来生成问题。 discourse connectives：通过不同的关系来转化，比如时间关系，空间关系。 semantic information based：基于语义来转化。 question generation，问题生成也是一个热门的研究方向，该领域的目标是根据输入文本生成问题。在MCQ中，我们首先选取一个句子，然后选择关键字，最后根据关键字转换成问句形式。5.distractor generation错误选项在MCQ中扮演重要的地位，如果错误选项不能很好的迷惑学生，那么这道多选题出的并不好。 一些技巧： parts-of-speech information：错误选项和关键字在语义上很接近，所以他们的词性也要一样 frequency：频率也是一个重要的指标，关键字和错误选项的出现频率应该相近。 wordnet：wordnet是一个词汇数据库，它将单词分组为同义词集并记录这些词集成员的关系。因此可以用wordnet来生成错误选项。 domain ontology：一些文献用web ontology language来寻找错误答案。 distributional hypothesis: 分布假设认为相似的词出现在相似的语境中，那么我们可以基于分布相似度来寻找错误答案。 semantic analysis：基于语义。 错误选项的选择同样与应用领域有关，目前的MCQ生成系统中使用的都是简单的错误选项生成，但在实际情况中，错误选项可以是非常多种类的，可以是不同的命名体，数字大小，多个单词的错误选项等等。作者认为文本的深层语义分析或使用基于神经嵌入的方法可能是复杂错误答案生成的一个可能的方向。6.post-processing后期处理是提高生成MCQ质量的阶段，系统生成的MCQ可能存在各种各样的错误。可能是标点符号错误，疑问词不恰当，问句过长等等，后期处理希望消除这些问题。 一些技巧： question post-editing：有些文献的方法是手动更改， 首先对于问题执行分类，是小问题就更正拼写和标点，如果是大问题就对题干进行重新措辞和替换等等。 question filtering：有些文献设计了一个过滤器来拒绝不对的问题，有的过滤器主要判断错误选项的质量，有的过滤器基于项目信息来过滤。 question ranking：对问题进行排名。 6.MCQ system evalutaion评估MCQ生成好坏，目前大多数系统采用人工评估的办法。由于MCQ生成包含了很多步骤，那么就产生了不同的度量标准。 evaluation of the stem and key: 目前还没有标准的公共数据集来评估MCQ，所以一般都是开发人员创建测试数据，下面有一张图展示了MCQ系统的评估过程，从表中可以看出，并没有一个标准的性能度量标准，开发人员采用了各种指标和参数。我们只能比较基于同一套评价体系下的MCQ。 evaluation of the distractors： 同样的，错误答案的评估也没有标准的数据集和评估指标。在许多应用领域中，MCQ有大量的干扰因素，所以一个标准的数据集可能无法容纳所有。所以目前还是有相关专家来评估。 7.conclusion总结了工作流程中的六个阶段，总结了目前的挑战和今后的研究方向，以及评价标准未确立等等。MCQ领域还有很多地方值得深入研究。 multiple choice question，多选题。 &#8617; part of speech 词性分析 &#8617; 命名实体识别 &#8617; 句子结构分析 &#8617;"
    } ,
  
    {
      "title"       : "练习Markdown",
      "category"    : "",
      "tags"        : "daily",
      "url"         : "./%E7%BB%83%E4%B9%A0Markdown.html",
      "date"        : "2021-11-24 00:00:00 +0800",
      "description" : "练习使用",
      "content"     : "1.标题一级标题二级标题三级标题一共有6级标题2.段落及格式用两个空格加回车表示换行当然也可以直接空一行出来表示换行1)各种文字表示斜体用两个或者两个_把需要斜体的文字围起来比如：*斜体斜体粗体用两个或者两个__把需要粗体的文字围起来比如:**粗体粗体粗斜体用两个或者两个___把需要粗体的文字围起来比如:**粗体*粗体2)分隔线你可以在一行中用三个以上的星号、减号、底线来建立一个分隔线，行内不能有其他东西。你也可以在星号或是减号中间插入空格。下面每种写法都可以建立分隔线：***3)删除线如果段落上的文字要添加删除线，只需要在文字的两端加上两个波浪线即可比如：~~哈哈哈哈4)下划线下划线可以通过HTML的&lt;u&gt;标签来实现比如：下划线5)脚注脚注是对文本的补充说明 创建脚注格式类似这样 1。脚注链接与脚注不能紧挨在一起。注脚默认在最后3.列表1)无序列表无序列表使用星号(*)、加号(+)或是减号(-)作为列表标记，这些标记后面要添加一个空格，然后再填写内容。比如： 第一项 第二项 第三项 第一项 第二项 第三项 第一项 第二项 第三项2)有序列表有序列表使用数字并加上 . 号来表示，如： 第一项 第二项 第三项3)列表嵌套列表嵌套只需在子列表中的选项前面添加四个空格即可。比如： 第一项： 第一项嵌套的第一个元素 第一项嵌套的第二个元素 第二项： 第二项嵌套的第一个元素 第二项嵌套的第二个元素 3.区块1)区块引用Markdown 区块引用是在段落开头使用 &gt; 符号 ，然后后面紧跟一个空格符号：比如： 区块引用2)区块嵌套另外区块是可以嵌套的，一个 &gt; 符号是最外层，两个 &gt; 符号是第一层嵌套，以此类推： 最外层 第一层嵌套 3)区块中使用列表比如： 区块中使用列表 第一项 第二项 第一项 第二项 第三项 4)列表中使用区块如果要在列表项目内放进区块，那么就需要在 &gt; 前添加四个空格的缩进。列表中使用区块实例如下： 第一项 菜鸟教程学的不仅是技术更是梦想 第二项4.使用代码1)代码如果是段落上的一个函数或片段的代码可以用反引号把它包起来(`)，例如：printf()函数2)指定一种语言可以用```包裹一段代码，并指定一种语言（也可以不指定）：def f(): qhr=qhr return qhr5.使用链接1)链接使用方法[链接名称](链接地址)或者比如：这是一个链接 菜鸟教程2)直接使用链接地址用&lt;&gt;把链接括起来。比如:http://www.runoob.com6.图片1)使用图片图片的语法格式：![alt 属性文本](图片地址 “可选标题”)2)链接图片大概长这样：&lt;img src=”http://static.runoob.com/images/runoob-logo.png” width=”50%”&gt;结果：7.表格1)格式Markdown 制作表格使用 | 来分隔不同的单元格，使用 - 来分隔表头和其他行。比如： 表头 表头 单元格 单元格 单元格 单元格 2)对齐方法在---前面加上:表示左对齐，在后面加上:表示右对齐，在两端加上:表示居中 左对齐 右对齐 居中对齐 单元格 单元格 单元格 单元格 单元格 单元格 8.高级技巧1)插入数学公式当你需要在编辑器中插入数学公式时，可以使用两个美元符$$包裹Tex或LaTeX格式的数学公式来实现。提交后，问答和文章页会根据需要加载Mathjax对数学公式进行渲染。比如：$$\\mathbf{V}_1 \\times \\mathbf{V}_2 = \\begin{vmatrix} \\mathbf{i} &amp; \\mathbf{j} &amp; \\mathbf{k} \\frac{\\partial X}{\\partial u} &amp; \\frac{\\partial Y}{\\partial u} &amp; 0 \\frac{\\partial X}{\\partial v} &amp; \\frac{\\partial Y}{\\partial v} &amp; 0 \\end{vmatrix}${$tep1}{\\style{visibility:hidden}{(x+1)(x+1)}}$$输出结果为：\\[\\mathbf{V}_1 \\times \\mathbf{V}_2 = \\begin{vmatrix} \\mathbf{i} &amp; \\mathbf{j} &amp; \\mathbf{k} \\\\\\frac{\\partial X}{\\partial u} &amp; \\frac{\\partial Y}{\\partial u} &amp; 0 \\\\\\frac{\\partial X}{\\partial v} &amp; \\frac{\\partial Y}{\\partial v} &amp; 0 \\\\\\end{vmatrix}${$tep1}{\\style{visibility:hidden}{(x+1)(x+1)}}\\]\\[\\sum_{i=0}^N\\int_{a}^{b}g(t,i)\\text{d}t\\]还没有整明白，用到的时候在看未完待续 我是脚注脚注脚注注脚 &#8617;"
    } ,
  
    {
      "title"       : "空天报国,敢为人先",
      "category"    : "",
      "tags"        : "daily",
      "url"         : "./%E7%A9%BA%E5%A4%A9%E6%8A%A5%E5%9B%BD%E6%95%A2%E4%B8%BA%E4%BA%BA%E5%85%88.html",
      "date"        : "2021-11-18 00:00:00 +0800",
      "description" : "听党课有感而发",
      "content"     : ""
    } ,
  
    {
      "title"       : "Who owns the copyright for an AI generated creative work?",
      "category"    : "opinion",
      "tags"        : "copyright, creativity, neural networks, machine learning, artificial intelligence",
      "url"         : "./AI-and-intellectual-property.html",
      "date"        : "2021-04-20 00:00:00 +0800",
      "description" : "As neural networks are used more and more in the creative process, text, images and even music are now created by AI, but who owns the copyright for those works?",
      "content"     : "Recently I was reading an article about a cool project that intends to have a neural network create songs of the late club of the 27 (artists that have tragically died at age 27 or near, and in the height of their respective careers), artists such as Amy Winehouse, Jimmy Hendrix, Curt Cobain and Jim Morrison.The project was created by Over the Bridge, an organization dedicated to increase awareness on mental health and substance abuse in the music industry, trying to denormalize and remove the glamour around such illnesses within the music community.They are using Google’s Magenta, which is a neural network that precisely was conceived to explore the role of machine learning within the creative process. Magenta has been used to create a brand new “Beatles” song or even there was a band that used it to write a full album in 2019.So, while reading the article, my immediate thought was: who owns the copyright of these new songs?Think about it, imagine one of this new songs becomes a massive hit with millions of youtube views and spotify streams, who can claim the royalties generated?At first it seems quite simple, Over the Bridge should be the ones reaping the benefits, since they are the ones who had the idea, gathered the data and then fed the neural network to get the “work of art”. But in a second thought, didn’t the original artists provide the basis for the work the neural network generated? shouldn’t their state get credit? what about Google whose tool was used, should they get credit too?Neural networks have been also used to create poetry, paintings and to write news articles, but how do they do it? A computer program developed for machine learning purposes is an algorithm that “learns” from data to make future decisions. When applied to art, music and literary works, machine learning algorithms are actually learning from some input data to generate a new piece of work, making independent decisions throughout the process to determine what the new work looks like. An important feature of this is that while programmers can set the parameters, the work is actually generated by the neural network itself, in a process akin to the thought processes of humans.Now, creative works qualify for copyright protection if they are original, with most definitions of originality requiring a human author. Most jurisdictions, including Spain and Germany, specifically state that only works created by a human can be protected by copyright. In the United States, for example, the Copyright Office has declared that it will “register an original work of authorship, provided that the work was created by a human being.”So as we currently stand, a human author is required to grant a copyright, which makes sense, there is no point of having a neural network be the beneficiary of royalties of a creative work (no bank would open an account for them anyways, lol).I think amendments have to be made to the law to ensure that the person who undertook all the arrangements necessary for the work to be created by the neural network gets the credit but also we need to modify copyright law to ensure the original authors of the body of work used as data input to produce the new piece get their corresponding share of credit. This will get messy if someone uses for example the #1 song of every month in a decade to create the decade song, then there would be as many as 120 different artists to credit.In a computer generated artistic work, both the person who undertook all the arrangements necessary for its creation as well as the original authors of the data input need to be credited.There will still be some ambiguity as to who undertook the arrangements necessary, only the one who gathered the data and pressed the button to let the network learn, or does the person who created the neural network’s model also get credit? Shall we go all the way and say that even the programmer of the neural network gets some credit as well?There are some countries, in particular the UK where some progress has been made to amend copyright laws to cater for computer generated works of art, but I believe this is one of those fields where technology will surpass our law making capacity and we will live under a grey area for a while, and maybe this is just what we need, by having these works ending up free for use by anyone in the world, perhaps a new model for remunerating creative work can be established, one that does not require commercial success to be necessary for artists to make a living, and thus they can become free to explore their art.Perhaps a new model for remunerating creative work can be established, one that does not require commercial success to be necessary for artists to make a living.The Next Rembrandt is a computer-generated 3-D–printed painting developed by a facial-recognition algorithm that scanned data from 346 known paintings by the Dutch painter in a process lasting 18 months. The portrait is based on 168,263 fragments from Rembrandt’s works."
    } ,
  
    {
      "title"       : "So, what is a neural network?",
      "category"    : "theory",
      "tags"        : "neural networks, machine learning, artificial intelligence",
      "url"         : "./back-to-basics.html",
      "date"        : "2021-04-02 00:00:00 +0800",
      "description" : "ELI5: what is a neural network.",
      "content"     : "The omnipresence of technology nowadays has made it commonplace to read news about AI, just a quick glance at today’s headlines, and I get: This Powerful AI Technique Led to Clashes at Google and Fierce Debate in Tech. How A.I.-powered companies dodged the worst damage from COVID AI technology detects ‘ticking time bomb’ arteries AI in Drug Discovery Starts to Live Up to the Hype Pentagon seeks commercial solutions to get its data ready for AITopics from business, manufacturing, supply chain, medicine and biotech and even defense are covered in those news headlines, definitively the advancements on the fields of artificial intelligence, in particular machine learning and deep neural networks have permeated into our daily lives and are here to stay. But, do the general population know what are we talking about when we say “an AI”? I assume most people correctly imagine a computer algorithm or perhaps the more adventurous minds think of a physical machine, an advanced computer entity or even a robot, getting smarter by itself with every use-case we throw at it. And most people will be right, when “an AI” is mentioned it is indeed an algorithm run by a computer, and there is where the boundary of their knowledge lies.They say that the best way to learn something is to try to explain it, so in a personal exercise I will try to do an ELI5 (Explain it Like I am 5) version of what is a neural network.Let’s start with a little history, humans have been tinkering with the idea of an intelligent machine for a while now, some even say that the idea of artificial intelligence was conceived by the ancient greeks (source), and several attempts at devising “intelligent” machines have been made through history, a notable one was ‘The Analytical Engine’ created by Charles Babbage in 1837:The Analytical Engine of Charles Babbage - 1837Then, in the middle of last century by trying to create a model of how our brain works, Neural Networks were born. Around that time, Frank Rosenblatt at Cornell trying to understand the simple decision system present in the eye of a common housefly, proposed the idea of a perceptron, a very simple system that processes certain inputs with basic math operations and produces an output.To illustrate, let’s say that the brain of the housefly is a perceptron, its inputs are whatever values are produced by the multiple cells in its eyes, when the eye cell detects “something” it’s output will be a 1, and if there is nothing a 0. Then the combination of all those inputs can be processed by the perceptron (the fly brain), and the output is a simple 0 or 1 value. If it is a 1 then the brain is telling the fly to flee and if it is a 0 it means it is safe to stay where it is.We can imagine then that if many of the eye cells of the fly produce 1s, it means that an object is quite near, and therefore the perceptron will calculate a 1, it is time to flee.The perceptron is just a math operation, one that multiplies certain input values with preset “parameters” (called weights) and adds up the resulting multiplications to generate a value.Then the magic spark was ignited, the parameters (weights) of the perceptron could be “learnt” by a process of minimizing the difference between known results of particular observations, and what the perceptron is actually calculating. It is this process of learning what we call training the neural network.This idea is so powerful that even today it is one of the fundamental building blocks of what we call AI.From this I will try to explain how this simple concept can have such diverse applications as natural language processing (think Alexa), image recognition like medical diagnosis from a CTR scan, autonomous vehicles, etc.A basic neural network is a combination of perceptrons in different arrangements, the perceptron therefore was downgraded from “fly brain” to “network neuron”.A neural network has different components, in its basic form it has: Input Hidden layers OutputInputThe inputs of a neural network are in their essence just numbers, therefore anything that can be converted to a number can become an input. Letters in a text, pixels in an image, frequencies in a sound wave, values from a sensor, etc. are all different things that when converted to a numerical value serve as inputs for the neural network. This is one of the reasons why applications of neural networks are so diverse.Inputs can be as many as one need for the task at hand, from maybe 9 inputs to teach a neural network how to play tic-tac-toe to thousands of pixels from a camera for an autonomous vehicle. Since the input of a perceptron needs to be a single value, if for example a color pixel is chosen as input, it most likely will be broken into three different values; its red, green and blue components, hence each pixel will become 3 different inputs for the neural network.Hidden layersA “layer” within a neural network is just a group of perceptrons that all perform the same exact mathematical operation to the inputs and produce an output. The catch is that each of them have different weights (parameters), therefore their output for a given input will be different amongst them. There are many types of layers, the most typical of them being a “dense” layer, which is another word to say that all the inputs are connected to all the neurons (individual perceptrons), and as said before, each of these connections have a weight associated with it, so that the operation that each neuron performs is a simple weighted sum of all the inputs.The hidden layer is then typically connected to another dense layer, and their connection means that each output of a neuron from the first layer is treated effectively as an input for the subsequent one, and it is thus connected to every neuron.A neural network can have from one to as many layers as one can think, and the number of layers depends solely on the experience we have gathered on the particular problem we would like to solve.Another critical parameter of a hidden layer is the number of neurons it has, and again, we need to rely on experience to determine how many neurons are needed for a given problem. I have seen networks that vary from a couple of neurons to the thousands. And of course each hidden layer can have as many neurons as we please, so the number of combinations is vast.To the number of layers, their type and how many neurons each have, is what we call the network topology (including the number of inputs and outputs).OutputAt the very end of the chain, another layer lies (which behaves just like a hidden layer), but has the peculiarity that it is the final layer, and therefore whatever it calculates will be the output values of the whole network. The number of outputs the network has is a function of the problem we would like to solve. It could be as simple as one output, with its value representing a probability of an action (like in the case of the flee reaction of the housefly), to many outputs, perhaps if our network is trying to distinguish images of animals, one would have an output for each animal species, and the output would represent how much confidence the network has that the particular image belongs to the corresponding species.As we said, the neural network is just a collection of individual neurons, doing basic math operations on certain inputs in series of layers that eventually generate an output. This mesh of neurons is then “trained” on certain output values from known cases of the inputs; once it has learned it can then process new inputs, values that it has never seen before with surprisingly accurate results.Many of the problems neural networks solve, could be certainly worked out by other algorithms, however, since neural networks are in their core very basic operations, once trained, they are extremely efficient, hence much quicker and economical to produce results.There are a few more details on how a simple neural network operate that I purposedly left out to make this explanation as simple as possible. Thinks like biases, the activation functions and the math behind learning, the backpropagation algorithm, I will leave to a more in depth article. I will also write (perhaps in a series) about the more complex topologies combining different types of layers and other building blocks, a part from the perceptron.Things like “Alexa”, are a bit more complex, but work on exactly the same principles. Let’s break down for example the case of asking “Alexa” to play a song in spotify. Alexa uses several different neural networks to acomplish this:1. Speech recognitionAs a basic input we have our speech: the command “Alexa, play Van Halen”. This might seem quite simple for us humans to process, but for a machine is an incredible difficult feat to be able to understand speech, things like each individual voice timbre, entonation, intention and many more nuances of human spoken language make it so that traditional algorithms have struggled a lot with this. In our simplified example let’s say that we use a neural network to transform our spoken speech into text characters a computer is much more familiarized to learn.2. Understanding what we mean (Natural Language Understanding)Once the previous network managed to succesfuly convert our spoken words into text, there comes the even more difficult task of making sense of what we said. Things that we humans take for granted such as context, intonation and non verbal communication, help give our words meaning in a very subtle, but powerful way, a machine will have to do with much less information to correctly understand what we mean. It has to correctly identify the intention of our sentence and the subject or entities of what we mean.The neural network has to identify that it received a command (by identifying its name), the command (“play music”), and our choice (“Van Halen”). And it does so by means of simple math operations as described before. Of course the network involved is quite complex and has different types of neurons and connection types, but the underlying principles remain.3. Replying to usOnce Alexa understood what we meant, it then proceeds to execute the action of the command it interpreted and it replies to us in turn using natural language. This is accomplished using a technique called speech synthesis, things like pitch, duration and intensity of the words and phonems are selected based on the “meaning” of what Alexa will respond to us: “Playing songs by Van Halen on Spotify” sounding quite naturally. And all is accomplished with neural networks executing many simple math operations.Although it seems quite complex, the process for AI to understand us can be boiled down to simple math operationsOf course Amazon’s Alexa neural networks have undergone quite a lot of training to get to the level where they are, the beauty is that once trained, to perform their magic they just need a few mathematical operations.As said before, I will continue to write about the basics of neural networks, the next article in the series will dive a bit deeper into the math behind a basic neural network."
    } ,
  
    {
      "title"       : "Starting the adventure",
      "category"    : "",
      "tags"        : "general blogging, thoughts, life",
      "url"         : "./starting-the-adventure.html",
      "date"        : "2021-03-24 00:00:00 +0800",
      "description" : "Midlife career change: a disaster or an opportunity?",
      "content"     : "In the midst of a global pandemic caused by the SARS-COV2 coronavirus; I decided to start blogging. I wanted to blog since a long time, I have always enjoyed writing, but many unknowns and having “no time” for it prevented me from taking it up. Things like: “I don’t really know who my target audience is”, “what would my topic or topics be?”, “I don’t think I am a world-class expert in anything”, and many more kept stopping me from setting up my own blog. Now seemed like a good time as any so with those and tons of other questions in my mind I decided it was time to start.Funnily, this is not my first post. The birth of the blog came very natural as a way to “document” my newly established pursuit for getting myself into Machine Learning. This new adventure of mine comprises several things, and if I want to succeed I need to be serious about them all: I want to start coding again! I used to code a long time ago, starting when I was 8 years old in a Tandy Color Computer hooked up to my parent’s TV. Machine Learning is a vast, wide subject, I want to learn the generals, but also to select a few areas to focus on. Setting up a blog to document my journey and share it: Establish a learning and blogging routine. If I don’t do this, I am sure this endeavour will die off soon.As for the focus areas I will start with: Neural Networks fundamentals: history, basic architecture and math behind them Deep Neural Networks Reinforcement Learning Current state of the art: what is at the cutting edge now in terms of Deep Neural Networks and Reinforcement Learning?I selected the above areas to focus on based on my personal interests, I have been fascinated by the developments in reinforcement learning for a long time, in particular Deep Mind’s awesome Go, Chess and Starcraft playing agents. Therefore, I started reading a lot about it and even started a personal project for coding a tic-tac-toe learning agent.With my limited knowledge I have drafted the following learning path: Youtube: Three Blue One Brown’s videos on Neural Networks, Calculus and Linear Algebra. I cannot recommend them enough, they are of sufficient depth and use animation superbly to facilitate the understanding of the subjects. Coursera: Andrew Ng’s Machine Learning course Book: Deep Learning with Python by Francois Chollet Book: Reinforcement Learning: An Introduction, by Richard S. Sutton and Andrew G. BartoAs for practical work I decided to start by coding my first models from scratch (without using libraries such as Tensorflow), to be able to deeply understand the math and logic behind the models, so far it has proven to be priceless.For my next project I think I will start to do the basic hand-written digits recognition, which is the Machine Learning Hello World, for this I think I will start to use Tensorflow already.I will continue to write about my learning road, what I find interesting and relevant, and to document all my practical exercises, as well as news and the state of the art in the world of AI.So far, all I have learned has been so engaging that I am seriously thinking of a career change. I have 17 years of international experience in multinational corporations across various functions, such as Information Services, Sales, Customer Care and New Products Introduction, and sincerely, I am finding more joy in artificial intelligence than anything else I have worked on before. Let’s see where the winds take us.Thanks for reading!P.S. For the geeks like me, here is a snippet on the technical side of the blog.Static Website GeneratorI researched a lot on this, when I started I didn’t even know I needed a static website generator. I was just sure of one thing, I wanted my blog site to look modern, be easy to update and not to have anything extra or additional content or functionality I did not need.There is a myriad of website generators nowadays, after a lengthy search the ones I ended up considering are: wordpress wix squarespace ghost webflow netlify hugo gatsby jekyllI started with the web interfaced generators with included hosting in their offerings:wordpress is the old standard, it is the one CMS I knew from before, and I thought I needed a fully fledged CMS, so I blindly ran towards it. Turns out, it has grown a lot since I remembered, it is now a fully fledged platform for complex websites and ecommerce development, even so I decided to give it a try, I picked a template and created a site. Even with the most simplistic and basic template I could find, there is a lot going on in the site. Setting it up was not as difficult or cumbersome as others claim, it took me about one hour to have it up and running, it looks good, but a bit crowded for my personal taste, and I found out it serves ads in your site for the readers, that is a big no for me.I have tried wix and squarespace before, they are fantastic for quick and easy website generation, but their free offering has ads, so again, a big no for me.I discovered ghost as the platform used by one of the bloggers I follow (Sebastian Ruder), turns out is a fantastic evolution over wordpress. It runs on the latest technologies, its interface is quite modern, and it is focused on one thing only: publishing. They have a paid hosting service, but the software is open sourced, therefore free to use in any hosting.I also tested webflow and even created a mockup there, the learning curve was quite smooth, and its CMS seems quite robust, but a bit too much for the functionalities I required.Next were the generators that don’t have a web interface, but can be easily set up:The first I tried was netlify, I also set up a test site in it. Netlify provides free hosting, and to keep your source files it uses GitHub (a repository keeps the source files where it publishes from). It has its own CMS, Netlify CMS, and you have a choice of site generators: Hugo, Gatsby, MiddleMan, Preact CLI, Next.js, Elevently and Nuxt.js, and once you choose there are some templates for each. I did not find the variety of templates enticing enough, and the set up process was much more cumbersome than with wordpress (at least for my knowledge level). I choose Hugo for my test site.I also tested gatsby with it’s own Gatsby Cloud hosting service, here is my test site. They also use GitHub as a base to host the source files to build the website, so you create a repository, and it is connected to it. I found the free template offerings quite limited for what I was looking for.Finally it came the turn for jekyll, although an older, and slower generator (compared to Hugo and Gatsby), it was created by one of the founders of GitHub, so it’s integration with GitHub Pages is quite natural and painless, so much so, that to use them together you don’t even have to install Jekyll in your machine! You have two choices: keep it all online, by having one repository in Github keep all the source files, modify or add them online, and having Jekyll build and publish your site to the special gh-pages repository everytime you change or add a new file to the source repository. Have a synchronized local copy of the source files for the website, this way you can edit your blog and customize it in your choice of IDE (Integrated Development Environment). Then, when you update any file on your computer, you just “push” the changes to GitHub, and GitHub Pages automatically uses Jekyll to build and publish your site.I chose the second option, specially because I can manipulate files, like images, in my laptop, and everytime I sync my local repository with GitHub, they are updated and published automatically. Quite convenient.After testing with several templates to get the feel for it, I decided to keep Jekyll for my blog for several reasons: the convenience of not having to install anything extra on my computer to build my blog, the integration with GitHub Pages, the ease of use, the future proofing via integration with modern technologies such as react or vue and the vast online community that has produced tons of templates and useful information for issue resolution, customization and added functionality.I picked up a template, just forked the repository and started modifying the files to customize it, it was fast and easy, I even took it upon myself to add some functionality to the template (it served as a coding little project) like: SEO meta tags Dark mode (configurable in _config.yml file) automatic sitemap.xml automatic archive page with infinite scrolling capability new page of posts filtered by a single tag (without needing autopages from paginator V2), also with infinite scrolling click to tweet functionality (just add a &lt;tweet&gt; &lt;/tweet&gt; tag in your markdown. custom and responsive 404 page responsive and automatic Table of Contents (optional per post) read time per post automatically calculated responsive post tags and social share icons (sticky or inline) included linkedin, reddit and bandcamp icons copy link to clipboard sharing option (and icon) view on github link button (optional per post) MathJax support (optional per post) tag cloud in the home page ‘back to top’ button comments ‘courtain’ to mask the disqus interface until the user clicks on it (configurable in _config.yml) CSS variables to make it easy to customize all colors and fonts added several pygments themes for code syntax highlight configurable from the _config.yml file. See the highlighter directory for reference on the options. responsive footer menu and footer logo (if setup in the config file) smoother menu animationsAs a summary, Hugo and Gatsby might be much faster than Jekyll to build the sites, but their complexity I think makes them useful for a big site with plenty of posts. For a small site like mine, Jekyll provides sufficient functionality and power without the hassle.You can use the modified template yourself by forking my repository. Let me know in the comments or feel free to contact me if you are interested in a detailed walkthrough on how to set it all up.HostingSince I decided on Jekyll to generate my site, the choice for hosting was quite obvious, Github Pages is very nicely integrated with it, it is free, and it has no ads! Plus the domain name isn’t too terrible (the-mvm.github.io).Interplanetary File SystemTo contribute to and test IPFS I also set up a mirror in IPFS by using fleek.co. I must confess that it was more troublesome than I imagined, it was definetively not plug and play because of the paths used to fetch resources. The nature of IPFS makes short absolute paths for website resources (like images, css and javascript files) inoperative; the easiest fix for this is to use relative paths, however the same relative path that works for the root directory (i.e. /index.html) does not work for links inside directories (i.e. /tags/), and since the site is static, while generating it, one must make the distinction between the different directory levels for the page to be rendered correctly.At first I tried a simple (but brute force solution):# determine the level of the current file{% assign lvl = page.url | append:'X' | split:'/' | size %}# create the relative base (i.e. \"../\"){% capture relativebase %}{% for i in (3..lvl) %}../{% endfor %}{% endcapture %}{% if relativebase == '' %} {% assign relativebase = './' %}{% endif %}...# Eliminate unecesary double backslashes{% capture post_url %}{{ relativebase }}{{ post.url }}{% endcapture %}{% assign post_url = post_url | replace: \"//\", \"/\" %}This jekyll/liquid code was executed in every page (or include) that needed to reference a resource hosted in the same server.But this fix did not work for the search function, because it relies on a search.json file (also generated programmatically to be served as a static file), therefore when generating this file one either use the relative path for the root directory or for a nested directory, thus the search results will only link correctly the corresponding pages if the page where the user searched for something is in the corresponding scope.So the final solution was to make the whole site flat, meaning to live in a single directory. All pages and posts will live under the root directory, and by doing so, I can control how to address the relative paths for resources."
    } ,
  
    {
      "title"       : "Deep Q Learning for Tic Tac Toe",
      "category"    : "",
      "tags"        : "machine learning, artificial intelligence, reinforcement learning, coding, python",
      "url"         : "./deep-q-learning-tic-tac-toe.html",
      "date"        : "2021-03-19 05:14:20 +0800",
      "description" : "Inspired by Deep Mind's astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe. How hard could it be?",
      "content"     : "BackgroundAfter many years of a corporate career (17) diverging from computer science, I have now decided to learn Machine Learning and in the process return to coding (something I have always loved!).To fully grasp the essence of ML I decided to start by coding a ML library myself, so I can fully understand the inner workings, linear algebra and calculus involved in Stochastic Gradient Descent. And on top learn Python (I used to code in C++ 20 years ago).I built a general purpose basic ML library that creates a Neural Network (only DENSE layers), saves and loads the weights into a file, does forward propagation and training (optimization of weights and biases) using SGD. I tested the ML library with the XOR problem to make sure it worked fine. You can read the blog post for it here.For the next challenge I am interested in reinforcement learning greatly inspired by Deep Mind’s astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe (or noughts and crosses).How hard could it be?Of course the first thing to do was to program the game itself, so I chose Python because I am learning it, so it gives me a good practice opportunity, and PyGame for the interface.Coding the game was quite straightforward, albeit for the hiccups of being my first PyGame and almost my first Python program ever.I created the game quite openly, in such a way that it can be played by two humans, by a human vs. an algorithmic AI, and a human vs. the neural network. And of course the neural network against a choice of 3 AI engines: random, minimax or hardcoded (an exercise I wanted to do since a long time).While training, the visuals of the game can be disabled to make training much faster.Now, for the fun part, training the network, I followed Deep Mind’s own DQN recommendations:The network will be an approximation for the Q value function or Bellman equation, meaning that the network will be trained to predict the \"value\" of each move available in a given game state.A replay experience memory was implemented. This meant that the neural network will not be trained after each move. Each move will be recorded in a special \"memory\" alongside with the state of the board and the reward it received for taking such an action (move).After the memory is sizable enough, batches of random experiences sampled from the replay memory are used for every training roundA secondary neural network (identical to the main one) is used to calculate part of the Q value function (Bellman equation), in particular the future Q values. And then it is updated with the main network's weights every n games. This is done so that we are not chasing a moving target.Designing the neural networkThe Neural Network chosen takes 9 inputs (the current state of the game) and outputs 9 Q values for each of the 9 squares in the board of the game (possible actions). Obviously some squares are illegal moves, hence while training there was a negative reward given to illegal moves hoping that the model would learn not to play illegal moves in a given position.I started out with two hidden layers of 36 neurons each, all fully connected and activated via ReLu. The output layer was initially activated using sigmoid to ensure that we get a nice value between 0 and 1 that represents the QValue of a given state action pair.The many models…Model 1 - the first tryAt first the model was trained by playing vs. a “perfect” AI, meaning a hard coded algorithm that never looses and that will win if it is given the chance. After several thousand training rounds, I noticed that the Neural Network was not learning much; so I switched to training vs. a completely random player, so that it will also learn how to win. After training vs. the random player, the Neural Network seems to have made progress and is steadily diminishing the loss function over time.However, the model was still generating many illegal moves, so I decided to modify the reinforcement learning algorithm to punish more the illegal moves. The change consisted in populating with zeros all the corresponding illegal moves for a given position at the target values to train the network. This seemed to work very well for diminishing the illegal moves:Nevertheless, the model was still performing quite poorly winning only around 50% of games vs. a completely random player (I expected it to win above 90% of the time). This was after only training 100,000 games, so I decided to keep training and see the results:Wins: 65.46% Losses: 30.32% Ties: 4.23%Note that when training restarts, the loss and illegal moves are still high in the beginning of the training round, and this is caused by the epsilon greedy strategy that prefers exploration (a completely random move) over exploitation, this preference diminishes over time.After another round of 100,000 games, I can see that the loss function actually started to diminish, and the win rate ended up at 65%, so with little hope I decided to carry on and do another round of 100,000 games (about 2 hours in an i7 MacBook Pro):Wins: 46.40% Losses: 41.33% Ties: 12.27%As you can see in the chart, the calculated loss not even plateaued, but it seemed to increase a bit over time, which tells me the model is not learning anymore. This was confirmed by the win rate decreasing with respect of the previous round to a meek 46.4% that looks no better than a random player.Model 2 - Linear activation for the outputAfter not getting the results I wanted, I decided to change the output activation function to linear, since the output is supposed to be a Q value, and not a probability of an action.Wins: 47.60% Losses: 39% Ties: 13.4%Initially I tested with only 1000 games to see if the new activation function was working, the loss function appears to be decreasing, however it reached a plateau around a value of 1, hence still not learning as expected. I came across a technique by Brad Kenstler, Carl Thome and Jeremy Jordan called Cyclical Learning Rate, which appears to solve some cases of stagnating loss functions in this type of networks. So I gave it a go using their Triangle 1 model.With the cycling learning rate in place, still no luck after a quick 1,000 games training round; so I decided to implement on top a decaying learning rate as per the following formula:The resulting learning rate combining the cycles and decay per epoch is:Learning Rate = 0.1, Decay = 0.0001, Cycle = 2048 epochs, max Learning Rate factor = 10xtrue_epoch = epoch - c.BATCH_SIZElearning_rate = self.learning_rate*(1/(1+c.DECAY_RATE*true_epoch))if c.CLR_ON: learning_rate = self.cyclic_learning_rate(learning_rate,true_epoch)@staticmethoddef cyclic_learning_rate(learning_rate, epoch): max_lr = learning_rate*c.MAX_LR_FACTOR cycle = np.floor(1+(epoch/(2*c.LR_STEP_SIZE))) x = np.abs((epoch/c.LR_STEP_SIZE)-(2*cycle)+1) return learning_rate+(max_lr-learning_rate)*np.maximum(0,(1-x))c.DECAY_RATE = learning rate decay ratec.MAX_LR_FACTOR = multiplier that determines the max learning ratec.LR_STEP_SIZE = the number of epochs each cycle lastsWith these many changes, I decided to restart with a fresh set of random weights and biases and try training more (much more) games.1,000,000 episodes, 7.5 million epochs with batches of 64 moves eachWins: 52.66% Losses: 36.02% Ties: 11.32%After 24 hours!, my computer was able to run 1,000,000 episodes (games played), which represented 7.5 million training epochs of batches of 64 plays (480 million plays learned), the learning rate did decreased (a bit), but is clearly still in a plateau; interestingly, the lower boundary of the loss function plot seems to continue to decrease as the upper bound and the moving average remains constant. This led me to believe that I might have hit a local minimum.Model 3 - new network topologyAfter all the failures I figured I had to rethink the topology of the network and play around with combinations of different networks and learning rates.100,000 episodes, 635,000 epochs with batches of 64 moves eachWins: 76.83% Losses: 17.35% Ties: 5.82%I increased to 200 neurons each hidden layer. In spite of this great improvement the loss function was still in a plateau at around 0.1 (Mean Squared Error). Which, although it is greatly reduced from what we had, still was giving out only 77% win rate vs. a random player, the network was playing tic tac toe as a toddler!*I can still beat the network most of the time! (I am playing with the red X)*100,000 more episodes, 620,000 epochs with batches of 64 moves eachWins: 82.25% Losses: 13.28% Ties: 4.46%Finally we crossed the 80% mark! This is quite an achievement, it seems that the change in network topology is working, although it also looks like the loss function is stagnating at around 0.15.After more training rounds and some experimenting with the learning rate and other parameters, I couldn’t improve past the 82.25% win rate.These have been the results so far:It is quite interesting to learn how the many parameters (hyper-parameters as most authors call them) of a neural network model affect its training performance, I have played with: the learning rate the network topology and activation functions the cycling and decaying learning rate parameters the batch size the target update cycle (when the target network is updated with the weights from the policy network) the rewards policy the epsilon greedy strategy whether to train vs. a random player or an “intelligent” AI.And so far the most effective change has been the network topology, but being so close but not quite there yet to my goal of 90% win rate vs. a random player, I will still try to optimize further.Network topology seems to have the biggest impact on a neural network's learning ability.Model 4 - implementing momentumI reached out to the reddit community and a kind soul pointed out that maybe what I need is to apply momentum to the optimization algorithm. So I did some research and ended up deciding to implement various optimization methods to experiment with: Stochastic Gradient Descent with Momentum RMSProp: Root Mean Square Plain Momentum NAG: Nezterov’s Accelerated Momentum Adam: Adaptive Moment Estimation and keep my old vanilla Gradient Descent (vGD) ☺Click here for a detailed explanation and code of all the implemented optimization algorithms.So far, I have not been able to get better results with Model 4, I have tried all the momentum optimization algorithms with little to no success.Model 5 - implementing one-hot encoding and changing topology (again)I came across an interesting project in Github that deals exactly with Deep Q Learning, and I noticed that he used “one-hot” encoding for the input as opposed to directly entering the values of the player into the 9 input slots. So I decided to give it a try and at the same time change my topology to match his:So, ‘one hot’ encoding is basically changing the input of a single square in the tic tac toe board to three numbers, so that each state is represented with different inputs, thus the network can clearly differentiate the three of them. As the original author puts it, the way I was encoding, having 0 for empty, 1 for X and 2 for O, the network couldn’t easily tell that, for instance, O and X both meant occupied states, because one is two times as far from 0 as the other. With the new encoding, the empty state will be 3 inputs: (1,0,0), the X will be (0,1,0) and the O (0,0,1) as in the diagram.Still, no luck even with Model 5, so I am starting to think that there could be a bug in my code.To test this hypothesis, I decided to implement the same model using Tensorflow / Keras.Model 6 - Tensorflow / Kerasself.PolicyNetwork = Sequential()for layer in hidden_layers: self.PolicyNetwork.add(Dense( units=layer, activation='relu', input_dim=inputs, kernel_initializer='random_uniform', bias_initializer='zeros'))self.PolicyNetwork.add(Dense( outputs, kernel_initializer='random_uniform', bias_initializer='zeros'))opt = Adam(learning_rate=c.LEARNING_RATE, beta_1=c.GAMMA_OPT, beta_2=c.BETA, epsilon=c.EPSILON, amsgrad=False)self.PolicyNetwork.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])As you can see I am reusing all of my old code, and just replacing my Neural Net library with Tensorflow/Keras, keeping even my hyper-parameter constants.The training function changed to:reduce_lr_on_plateau = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=25)history = self.PolicyNetwork.fit(np.asarray(states_to_train), np.asarray(targets_to_train), epochs=c.EPOCHS, batch_size=c.BATCH_SIZE, verbose=1, callbacks=[reduce_lr_on_plateau], shuffle=True)With Tensorflow implemented, the first thing I noticed, was that I had an error in the calculation of the loss, although this only affected reporting and didn’t change a thing on the training of the network, so the results kept being the same, the loss function was still stagnating! My code was not the issue.Model 7 - changing the training scheduleNext I tried to change the way the network was training as per u/elBarto015 advised me on reddit.The way I was training initially was: Games begin being simulated and the outcome recorded in the replay memory Once a sufficient ammount of experiences are recorded (at least equal to the batch size) the Network will train with a random sample of experiences from the replay memory. The ammount of experiences to sample is the batch size. The games continue to be played between the random player and the network. Every move from either player generates a new training round, again with a random sample from the replay memory. This continues until the number of games set up conclude.The first change was to train only after every game concludes with the same ammount of data (a batch). This was still not giving any good results.The second change was more drastic, it introduced the concept of epochs for every training round, it basically sampled the replay memory for epochs * batch size experiences, for instance if epochs selected were 10, and batch size was 81, then 810 experiences were sampled out of the replay memory. With this sample the network was then trained for 10 epochs randomly using the batch size.This meant that I was training now effectively 10 (or the number of epochs selected) times more per game, but in batches of the same size and randomly shuffling the experiences each epoch.After still playing around with some hyperparameters I managed to get similar performance as I got before, reaching 83.15% win rate vs. the random player, so I decided to keep training in rounds of 2,000 games each to evaluate performance. With almost every round I could see improvement:As of today, my best result so far is 87.5%, I will leave it rest for a while and keep investigating to find a reason for not being able to reach at least 90%. I read about self play, and it looks like a viable option to test and a fun coding challenge. However, before embarking in yet another big change I want to ensure I have been thorough with the model and have tested every option correctly.I feel the end is near… should I continue to update this post as new events unfold or shall I make it a multi post thread?"
    } ,
  
    {
      "title"       : "Neural Network Optimization Methods and Algorithms",
      "category"    : "",
      "tags"        : "coding, machine learning, optimization, deep Neural networks",
      "url"         : "./neural-network-optimization-methods.html",
      "date"        : "2021-03-13 03:32:20 +0800",
      "description" : "Some neural network optimization algorithms mostly to implement momentum when doing back propagation.",
      "content"     : "For the seemingly small project I undertook of creating a machine learning neural network that could learn by itself to play tic-tac-toe, I bumped into the necesity of implementing at least one momentum algorithm for the optimization of the network during backpropagation.And since my original post for the TicTacToe project is quite large already, I decided to post separately these optimization methods and how did I implement them in my code.AdamsourceAdaptive Moment Estimation (Adam) is an optimization method that computes adaptive learning rates for each weight and bias. In addition to storing an exponentially decaying average of past squared gradients \\(v_t\\) and an exponentially decaying average of past gradients \\(m_t\\), similar to momentum. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. We compute the decaying averages of past and past squared gradients \\(m_t\\) and \\(v_t\\) respectively as follows:\\(\\begin{align}\\begin{split}m_t &amp;= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\v_t &amp;= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\end{split}\\end{align}\\)\\(m_t\\) and \\(v_t\\) are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As \\(m_t\\) and \\(v_t\\) are initialized as vectors of 0's, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. \\(\\beta_1\\) and \\(\\beta_2\\) are close to 1).They counteract these biases by computing bias-corrected first and second moment estimates:\\(\\begin{align}\\begin{split}\\hat{m}_t &amp;= \\dfrac{m_t}{1 - \\beta^t_1} \\\\\\hat{v}_t &amp;= \\dfrac{v_t}{1 - \\beta^t_2} \\end{split}\\end{align}\\)We then use these to update the weights and biases which yields the Adam update rule:\\(\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\\).The authors propose defaults of 0.9 for \\(\\beta_1\\), 0.999 for \\(\\beta_2\\), and \\(10^{-8}\\) for \\(\\epsilon\\).view on github# decaying averages of past gradientsself.v[\"dW\" + str(i)] = ((c.BETA1 * self.v[\"dW\" + str(i)]) + ((1 - c.BETA1) * np.array(self.gradients[i]) ))self.v[\"db\" + str(i)] = ((c.BETA1 * self.v[\"db\" + str(i)]) + ((1 - c.BETA1) * np.array(self.bias_gradients[i]) ))# decaying averages of past squared gradientsself.s[\"dW\" + str(i)] = ((c.BETA2 * self.s[\"dW\"+str(i)]) + ((1 - c.BETA2) * (np.square(np.array(self.gradients[i]))) ))self.s[\"db\" + str(i)] = ((c.BETA2 * self.s[\"db\" + str(i)]) + ((1 - c.BETA2) * (np.square(np.array( self.bias_gradients[i]))) ))if c.ADAM_BIAS_Correction: # bias-corrected first and second moment estimates self.v[\"dW\" + str(i)] = self.v[\"dW\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.v[\"db\" + str(i)] = self.v[\"db\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.s[\"dW\" + str(i)] = self.s[\"dW\" + str(i)] / (1 - (c.BETA2 ** true_epoch)) self.s[\"db\" + str(i)] = self.s[\"db\" + str(i)] / (1 - (c.BETA2 ** true_epoch))# apply to weights and biasesweight_col -= ((eta * (self.v[\"dW\" + str(i)] / (np.sqrt(self.s[\"dW\" + str(i)]) + c.EPSILON))))self.bias[i] -= ((eta * (self.v[\"db\" + str(i)] / (np.sqrt(self.s[\"db\" + str(i)]) + c.EPSILON))))SGD MomentumsourceVanilla SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum.Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction \\(\\gamma\\) of the update vector of the past time step to the current update vector:\\(\\begin{align}\\begin{split}v_t &amp;= \\beta_1 v_{t-1} + \\eta \\nabla_\\theta J( \\theta) \\\\\\theta &amp;= \\theta - v_t\\end{split}\\end{align}\\)The momentum term \\(\\beta_1\\) is usually set to 0.9 or a similar value.Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. \\(\\beta_1 &lt; 1\\)). The same thing happens to our weight and biases updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.view on githubself.v[\"dW\"+str(i)] = ((c.BETA1*self.v[\"dW\" + str(i)]) +(eta*np.array(self.gradients[i]) ))self.v[\"db\"+str(i)] = ((c.BETA1*self.v[\"db\" + str(i)]) +(eta*np.array(self.bias_gradients[i]) ))weight_col -= self.v[\"dW\" + str(i)]self.bias[i] -= self.v[\"db\" + str(i)]Nesterov accelerated gradient (NAG)sourceHowever, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We'd like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.Nesterov accelerated gradient (NAG) is a way to give our momentum term this kind of prescience. We know that we will use our momentum term \\(\\beta_1 v_{t-1}\\) to move the weights and biases \\(\\theta\\). Computing \\( \\theta - \\beta_1 v_{t-1} \\) thus gives us an approximation of the next position of the weights and biases (the gradient is missing for the full update), a rough idea where our weights and biases are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current weights and biases \\(\\theta\\) but w.r.t. the approximate future position of our weights and biases:\\(\\begin{align}\\begin{split}v_t &amp;= \\beta_1 v_{t-1} + \\eta \\nabla_\\theta J( \\theta - \\beta_1 v_{t-1} ) \\\\\\theta &amp;= \\theta - v_t\\end{split}\\end{align}\\)Again, we set the momentum term \\(\\beta_1\\) to a value of around 0.9. While Momentum first computes the current gradient and then takes a big jump in the direction of the updated accumulated gradient, NAG first makes a big jump in the direction of the previous accumulated gradient, measures the gradient and then makes a correction, which results in the complete NAG update. This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of Neural Networks on a number of tasks.Now that we are able to adapt our updates to the slope of our error function and speed up SGD in turn, we would also like to adapt our updates to each individual weight and bias to perform larger or smaller updates depending on their importance.view on githubv_prev = {\"dW\" + str(i): self.v[\"dW\" + str(i)], \"db\" + str(i): self.v[\"db\" + str(i)]}self.v[\"dW\" + str(i)] = (c.NAG_COEFF * self.v[\"dW\" + str(i)] - eta * np.array(self.gradients[i]))self.v[\"db\" + str(i)] = (c.NAG_COEFF * self.v[\"db\" + str(i)] - eta * np.array(self.bias_gradients[i]))weight_col += ((-1 * c.BETA1 * v_prev[\"dW\" + str(i)]) + (1 + c.BETA1) * self.v[\"dW\" + str(i)])self.bias[i] += ((-1 * c.BETA1 * v_prev[\"db\" + str(i)]) + (1 + c.BETA1) * self.v[\"db\" + str(i)])RMSpropsourceRMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in Lecture 6e of his Coursera Class.RMSprop was developed stemming from the need to resolve other method's radically diminishing learning rates.\\(\\begin{align}\\begin{split}E[\\theta^2]_t &amp;= \\beta_1 E[\\theta^2]_{t-1} + (1-\\beta_1) \\theta^2_t \\\\\\theta_{t+1} &amp;= \\theta_{t} - \\dfrac{\\eta}{\\sqrt{E[\\theta^2]_t + \\epsilon}} \\theta_{t}\\end{split}\\end{align}\\)RMSprop divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests \\(\\beta_1\\) to be set to 0.9, while a good default value for the learning rate \\(\\eta\\) is 0.001.view on githubself.s[\"dW\" + str(i)] = ((c.BETA1 * self.s[\"dW\" + str(i)]) + ((1-c.BETA1) * (np.square(np.array(self.gradients[i]))) ))self.s[\"db\" + str(i)] = ((c.BETA1 * self.s[\"db\" + str(i)]) + ((1-c.BETA1) * (np.square(np.array(self.bias_gradients[i]))) ))weight_col -= (eta * (np.array(self.gradients[i]) / (np.sqrt(self.s[\"dW\"+str(i)]+c.EPSILON))) )self.bias[i] -= (eta * (np.array(self.bias_gradients[i]) / (np.sqrt(self.s[\"db\"+str(i)]+c.EPSILON))) )Complete codeAll in all the code ended up like this:view on github@staticmethoddef cyclic_learning_rate(learning_rate, epoch): max_lr = learning_rate * c.MAX_LR_FACTOR cycle = np.floor(1 + (epoch / (2 * c.LR_STEP_SIZE)) ) x = np.abs((epoch / c.LR_STEP_SIZE) - (2 * cycle) + 1) return learning_rate + (max_lr - learning_rate) * np.maximum(0, (1 - x))def apply_gradients(self, epoch): true_epoch = epoch - c.BATCH_SIZE eta = self.learning_rate * (1 / (1 + c.DECAY_RATE * true_epoch)) if c.CLR_ON: eta = self.cyclic_learning_rate(eta, true_epoch) for i, weight_col in enumerate(self.weights): if c.OPTIMIZATION == 'vanilla': weight_col -= eta * np.array(self.gradients[i]) / c.BATCH_SIZE self.bias[i] -= eta * np.array(self.bias_gradients[i]) / c.BATCH_SIZE elif c.OPTIMIZATION == 'SGD_momentum': self.v[\"dW\"+str(i)] = ((c.BETA1 *self.v[\"dW\" + str(i)]) +(eta *np.array(self.gradients[i]) )) self.v[\"db\"+str(i)] = ((c.BETA1 *self.v[\"db\" + str(i)]) +(eta *np.array(self.bias_gradients[i]) )) weight_col -= self.v[\"dW\" + str(i)] self.bias[i] -= self.v[\"db\" + str(i)] elif c.OPTIMIZATION == 'NAG': v_prev = {\"dW\" + str(i): self.v[\"dW\" + str(i)], \"db\" + str(i): self.v[\"db\" + str(i)]} self.v[\"dW\" + str(i)] = (c.NAG_COEFF * self.v[\"dW\" + str(i)] - eta * np.array(self.gradients[i])) self.v[\"db\" + str(i)] = (c.NAG_COEFF * self.v[\"db\" + str(i)] - eta * np.array(self.bias_gradients[i])) weight_col += ((-1 * c.BETA1 * v_prev[\"dW\" + str(i)]) + (1 + c.BETA1) * self.v[\"dW\" + str(i)]) self.bias[i] += ((-1 * c.BETA1 * v_prev[\"db\" + str(i)]) + (1 + c.BETA1) * self.v[\"db\" + str(i)]) elif c.OPTIMIZATION == 'RMSProp': self.s[\"dW\" + str(i)] = ((c.BETA1 *self.s[\"dW\" + str(i)]) +((1-c.BETA1) *(np.square(np.array(self.gradients[i]))) )) self.s[\"db\" + str(i)] = ((c.BETA1 *self.s[\"db\" + str(i)]) +((1-c.BETA1) *(np.square(np.array(self.bias_gradients[i]))) )) weight_col -= (eta *(np.array(self.gradients[i]) /(np.sqrt(self.s[\"dW\"+str(i)]+c.EPSILON))) ) self.bias[i] -= (eta *(np.array(self.bias_gradients[i]) /(np.sqrt(self.s[\"db\"+str(i)]+c.EPSILON))) ) if c.OPTIMIZATION == \"ADAM\": # decaying averages of past gradients self.v[\"dW\" + str(i)] = (( c.BETA1 * self.v[\"dW\" + str(i)]) + ((1 - c.BETA1) * np.array(self.gradients[i]) )) self.v[\"db\" + str(i)] = (( c.BETA1 * self.v[\"db\" + str(i)]) + ((1 - c.BETA1) * np.array(self.bias_gradients[i]) )) # decaying averages of past squared gradients self.s[\"dW\" + str(i)] = ((c.BETA2 * self.s[\"dW\"+str(i)]) + ((1 - c.BETA2) * (np.square( np.array( self.gradients[i]))) )) self.s[\"db\" + str(i)] = ((c.BETA2 * self.s[\"db\" + str(i)]) + ((1 - c.BETA2) * (np.square( np.array( self.bias_gradients[i]))) )) if c.ADAM_BIAS_Correction: # bias-corrected first and second moment estimates self.v[\"dW\" + str(i)] = self.v[\"dW\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.v[\"db\" + str(i)] = self.v[\"db\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.s[\"dW\" + str(i)] = self.s[\"dW\" + str(i)] / (1 - (c.BETA2 ** true_epoch)) self.s[\"db\" + str(i)] = self.s[\"db\" + str(i)] / (1 - (c.BETA2 ** true_epoch)) # apply to weights and biases weight_col -= ((eta * (self.v[\"dW\" + str(i)] / (np.sqrt(self.s[\"dW\" + str(i)]) + c.EPSILON)))) self.bias[i] -= ((eta * (self.v[\"db\" + str(i)] / (np.sqrt(self.s[\"db\" + str(i)]) + c.EPSILON)))) self.gradient_zeros()"
    } ,
  
    {
      "title"       : "Machine Learning Library in Python from scratch",
      "category"    : "",
      "tags"        : "machine learning, coding, neural networks, python",
      "url"         : "./ML-Library-from-scratch.html",
      "date"        : "2021-03-01 02:32:20 +0800",
      "description" : "Single neuron perceptron that classifies elements learning quite quickly.",
      "content"     : "It must sound crazy that in this day and age, when we have such a myriad of amazing machine learning libraries and toolkits all open sourced, all quite well documented and easy to use, I decided to create my own ML library from scratch.Let me try to explain; I am in the process of immersing myself into the world of Machine Learning, and to do so, I want to deeply understand the basic concepts and its foundations, and I think that there is no better way to do so than by creating myself all the code for a basic neural network library from scratch. This way I can gain in depth understanding of the math that underpins the ML algorithms.Another benefit of doing this is that since I am also learning Python, the experiment brings along good exercise for me.To call it a Machine Learning Library is perhaps a bit of a stretch, since I just intended to create a multi-neuron, multi-layered perceptron.The library started very narrowly, with just the following functionality: create a neural network based on the following parameters: number of inputs size and number of hidden layers number of outputs learning rate forward propagate or predict the output values when given some inputs learn through back propagation using gradient descentI restricted the model to be sequential, and the layers to be only dense / fully connected, this means that every neuron is connected to every neuron of the following layer. Also, as a restriction, the only activation function I implemented was sigmoid:With my neural network coded, I tested it with a very basic problem, the famous XOR problem.XOR is a logical operation that cannot be solved by a single perceptron because of its linearity restriction:As you can see, when plotted in an X,Y plane, the logical operators AND and OR have a line that can clearly separate the points that are false from the ones that are true, hence a perceptron can easily learn to classify them; however, for XOR there is no single straight line that can do so, therefore a multilayer perceptron is needed for the task.For the test I created a neural network with my library:import Neural_Network as nninputs = 3hidden_layers = [2, 1]outputs = 1learning_rate = 0.03NN = nn.NeuralNetwork(inputs, hidden_layers, outputs, learning_rate)The three inputs I decided to use (after a lot of trial and error) are the X and Y coordinate of a point (between X = 0, X = 1, Y = 0 and Y = 1) and as the third input the multiplication of both X and Y. Apparently it gives the network more information, and it ends up converging much more quickly with this third input.Then there is a single hidden layer with 2 neurons and one output value, that will represent False if the value is closer to 0 or True if the value is closer to 1.Then I created the learning data, which is quite trivial for this problem, since we know very easily how to compute XOR.training_data = []for n in range(learning_rounds): x = rnd.random() y = rnd.random() training_data.append([x, y, x * y, 0 if (x &lt; 0.5 and y &lt; 0.5) or (x &gt;= 0.5 and y &gt;= 0.5) else 1])And off we go into training:for data in training_data: NN.train(data[:3].reshape(inputs), data[3:].reshape(outputs))The ML library can only train on batches of 1 (another self-imposed coding restriction), therefore only one “observation” at a time, this is why the train function accepts two parameters, one is the inputs packed in an array, and the other one is the outputs, packed as well in an array.To see the neural net in action I decided to plot the predicted results in both a 3d X,Y,Z surface plot (z being the network’s predicted value), and a scatter plot with the color of the points representing the predicted value.This was plotted in MatPlotLib, so we needed to do some housekeeping first:fig = plt.figure()fig.canvas.set_window_title('Learning XOR Algorithm')fig.set_size_inches(11, 6)axs1 = fig.add_subplot(1, 2, 1, projection='3d')axs2 = fig.add_subplot(1, 2, 2)Then we need to prepare the data to be plotted by generating X and Y values distributed between 0 and 1, and having the network calculate the Z value:x = np.linspace(0, 1, num_surface_points)y = np.linspace(0, 1, num_surface_points)x, y = np.meshgrid(x, y)z = np.array(NN.forward_propagation([x, y, x * y])).reshape(num_surface_points, num_surface_points)As you can see, the z values array is reshaped as a 2d array of shape (x,y), since this is the way Matplotlib interprets it as a surface:axs1.plot_surface(x, y, z, rstride=1, cstride=1, cmap='viridis', vmin=0, vmax=1, antialiased=True)The end result looks something like this:Then we reshape the z array as a one dimensional array to use it to color the scatter plot:z = z.reshape(num_surface_points ** 2)scatter = axs2.scatter(x, y, marker='o', s=40, c=z.astype(float), cmap='viridis', vmin=0, vmax=1)To actually see the progress while learning, I created a Matplotlib animation, and it is quite interesting to see as it learns. So my baby ML library is completed for now, but still I would like to enhance it in several ways: include multiple activation functions (ReLu, linear, Tanh, etc.) allow for multiple optimizers (Adam, RMSProp, SGD Momentum, etc.) have batch and epoch training schedules functionality save and load trained model to fileI will get to it soon…"
    } ,
  
    {
      "title"       : "Conway&#39;s Game of Life",
      "category"    : "",
      "tags"        : "coding, python",
      "url"         : "./conways-game-of-life.html",
      "date"        : "2021-02-11 03:32:20 +0800",
      "description" : "Taking on the challenge of picking up coding again through interesting small projects, this time it is the turn of Conway's Game of Life.",
      "content"     : "I&nbsp;am lately trying to take on coding again. It had always been a part of my life since my early years when I&nbsp;learned to program a Tandy Color Computer at the age of 8, the good old days.Tandy Color Computer TRS80 IIIHaving already programed in Java, C# and of course BASIC, I&nbsp;thought it would be a great idea to learn Python since I&nbsp;have great interest in data science and machine learning, and those two topics seem to have an avid community within Python coders.For one of my starter quick programming tasks, I&nbsp;decided to code Conway's Game of Life, a very simple cellular automata that basically plays itself.The game consists of a grid of n size, and within each block of the grid a cell could either be dead or alive according to these rules:If a cell has less than 2 neighbors, meaning contiguous alive cells, the cell will die of lonelinessIf a cell has more than 3 neighbors, it will die of overpopulationIf an empty block has exactly 3 contiguous alive neighbors, a new cell will be born in that spotIf an alive cell has 2 or 3 alive neighbors, it continues to liveConway’s rules for the Game of LifeTo make it more of a challenge I&nbsp;also decided to implement an \"sparse\" method of recording the game board, this means that instead of the typical 2d array representing the whole board, I&nbsp;will only record the cells which are alive. Saving a lot of memory space and processing time, while adding some spice to the challenge.The trickiest part was figuring out how to calculate which empty blocks had exactly 3 alive neighbors so that a new cell will spring to life there, this is trivial in the case of recording the whole grid, because we just iterate all over the board and find the alive neighbors of ALL&nbsp;the blocks in the grid, but in the case of only keeping the alive cells proved quite a challenge.In the end the algorithm ended up as follows:Iterate through all the alive cells and get all of their neighborsdef get_neighbors(self, cell): neighbors = [] for x in range(-1, 2, 1): for y in range(-1, 2, 1): if not (x == 0 and y == 0): if (0 &amp;lt;= (cell[0] + x) &amp;lt;= self.size_x) and (0 &amp;lt;= (cell[1] + y) &amp;lt;= self.size_y): neighbors.append((cell[0] + x, cell[1] + y)) return neighborsMark all the neighboring blocks as having +1 neighbor each time a particular cell is encountered. This way, for each neighboring alive cell the counter of the particular block will increase, and in the end it will contain the total number of live cells which are contiguous to it.def next_state(self): alive_neighbors = {} for cell in self.alive_cells: if cell not in alive_neighbors: alive_neighbors[cell] = 0 neighbors = self.get_neighbors(cell) for neighbor in neighbors: if neighbor not in alive_neighbors: alive_neighbors[neighbor] = 1 else: alive_neighbors[neighbor] += 1The trick was using a dictionary to keep the record of the blocks that have alive neighbors and the cells who are alive in the current state but have zero alive neighbors (thus will die).With the dictionary it became easy just to add cells and increase their neighbor counter each time it was encountered as a neighbor of an alive cell.Having the dictionary now filled with all the cells that have alive neighbors and how many they have, it was just a matter of applying the rules of the game:for cell in alive_neighbors: if alive_neighbors[cell] &amp;lt; 2 or alive_neighbors[cell] &gt; 3: self.alive_cells.discard(cell) elif alive_neighbors[cell] == 3: self.alive_cells.add(cell)Notice that since I am keeping an array of the coordinates of only the cells who are alive, I could apply just 3 rules, die of loneliness, die of overpopulation and become alive from reproduction (exactly 3 alive neighbors) because the ones who have 2 or 3 neighbors and are already alive, can remain alive in the next iteration.I&nbsp;found it very interesting to implement the Game of Life like this, it was quite a refreshing challenge and I am beginning to feel my coding skills ramping up again."
    } ,
  
    {
      "title"       : "Single Neuron Perceptron",
      "category"    : "",
      "tags"        : "machine learning, coding, neural networks",
      "url"         : "./single-neuron-perceptron.html",
      "date"        : "2021-01-26 03:32:20 +0800",
      "description" : "Single neuron perceptron that classifies elements learning quite quickly.",
      "content"     : "As an entry point to learning python and getting into Machine Learning, I decided to code from scratch the Hello World! of the field, a single neuron perceptron.What is a perceptron?A perceptron is the basic building block of a neural network, it can be compared to a neuron, And its conception is what detonated the vast field of Artificial Intelligence nowadays.Back in the late 1950’s, a young Frank Rosenblatt devised a very simple algorithm as a foundation to construct a machine that could learn to perform different tasks.In its essence, a perceptron is nothing more than a collection of values and rules for passing information through them, but in its simplicity lies its power.Imagine you have a ‘neuron’ and to ‘activate’ it, you pass through several input signals, each signal connects to the neuron through a synapse, once the signal is aggregated in the perceptron, it is then passed on to one or as many outputs as defined. A perceptron is but a neuron and its collection of synapses to get a signal into it and to modify a signal to pass on.In more mathematical terms, a perceptron is an array of values (let’s call them weights), and the rules to apply such values to an input signal.For instance a perceptron could get 3 different inputs as in the image, lets pretend that the inputs it receives as signal are: $x_1 = 1, \\; x_2 = 2\\; and \\; x_3 = 3$, if it’s weights are $w_1 = 0.5,\\; w_2 = 1\\; and \\; w_3 = -1$ respectively, then what the perceptron will do when the signal is received is to multiply each input value by its corresponding weight, then add them up.\\(\\begin{align}\\begin{split}\\left(x_1 * w_1\\right) + \\left(x_2 * w_2\\right) + \\left(x_3 * w_3\\right)\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}\\left(0.5 * 1\\right) + \\left(1 * 2\\right) + \\left(-1 * 3\\right) = 0.5 + 2 - 3 = -0.5\\end{split}\\end{align}\\)Typically when this value is obtained, we need to apply an “activation” function to smooth the output, but let’s say that our activation function is linear, meaning that we keep the value as it is, then that’s it, that is the output of the perceptron, -0.5.In a practical application, the output means something, perhaps we want our perceptron to classify a set of data and if the perceptron outputs a negative number, then we know the data is of type A, and if it is a positive number then it is of type B.Once we understand this, the magic starts to happen through a process called backpropagation, where we “educate” our tiny one neuron brain to have it learn how to do its job.The magic starts to happen through a process called backpropagation, where we \"educate\" our tiny one neuron brain to have it learn how to do its job.For this we need a set of data that it is already classified, we call this a training set. This data has inputs and their corresponding correct output. So we can tell the little brain when it misses in its prediction, and by doing so, we also adjust the weights a bit in the direction where we know the perceptron committed the mistake hoping that after many iterations like this the weights will be so that most of the predictions will be correct.After the model trains successfully we can have it classify data it has never seen before, and we have a fairly high confidence that it will do so correctly.The math behind this magical property of the perceptron is called gradient descent, and is just a bit of differential calculus that helps us convert the error the brain is having into tiny nudges of value of the weights towards their optimum. This video series by 3 blue 1 brown explains it wonderfuly.My program creates a single neuron neural network tuned to guess if a point is above or below a randomly generated line and generates a visualization based on graphs to see how the neural network is learning through time.The neuron has 3 inputs and weights to calculate its output:input 1 is the X coordinate of the point,Input 2 is the y coordinate of the point,Input 3 is the bias and it is always 1Input 3 or the bias is required for lines that do not cross the origin (0,0)The Perceptron starts with weights all set to zero and learns by using 1,000 random points per each iteration.The output of the perceptron is calculated with the following activation function: if x * weight_x + y weight_y + weight_bias is positive then 1 else 0The error for each point is calculated as the expected outcome of the perceptron minus the real outcome therefore there are only 3 possible error values: Expected Calculated Error 1 -1 1 1 1 0 -1 -1 0 -1 1 -1 With every point that is learned if the error is not 0 the weights are adjusted according to:New_weight = Old_weight + error * input * learning_ratefor example: New_weight_x = Old_weight_x + error * x * learning rateA very useful parameter in all of neural networks is teh learning rate, which is basically a measure on how tiny our nudge to the weights is going to be.In this particular case, I coded the learning_rate to decrease with every iteration as follows:learning_rate = 0.01 / (iteration + 1)this is important to ensure that once the weights are nearing the optimal values the adjustment in each iteration is subsequently more subtle.In the end, the perceptron always converges into a solution and finds with great precision the line we are looking for.Perceptrons are quite a revelation in that they can resolve equations by learning, however they are very limited. By their nature they can only resolve linear equations, so their problem space is quite narrow.Nowadays the neural networks consist of combinations of many perceptrons, in many layers, and other types of “neurons”, like convolution, recurrent, etc. increasing significantly the types of problems they solve."
    } 
  
]
