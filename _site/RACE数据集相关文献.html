<!DOCTYPE html>
<html lang="en">
  




<head>
	<meta charset="utf-8">
	<title>RACE数据集相关文献 - Quehry</title>
	<link rel="canonical" href="http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html">
	<meta name="description" content="文献整理">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:4000"},
  "headline": "RACE数据集相关文献",
  "abstract": "文献整理",
    "keywords": "paper",
    "wordcount": "781",
    "image": ["http://localhost:4000/assets/imgposts/20211130/arxiv.jpg"],
  "datePublished": "2021-11-30 00:00:00 +0800",
  "dateModified": "2021-11-30 00:00:00 +0800",
  "author": {
    "@type": "Person",
    "name": "Quehry"},
  "publisher": {
    "@type":  "Organization",
    "logo": {
        "@type": "ImageObject",
        "encodingFormat": "image/png",
        "contentUrl": "http://localhost:4000/assets/img/branding/MVM-symbol-black.png",
        "url": "http://localhost:4000/assets/img/branding/MVM-symbol-black.png"},
    "name" : "Quehry"}
}
</script>
<!-- Open Graph data -->
<meta property="og:url" content="http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html"/>
<meta property="og:type" content="article"/>
<meta property="og:title" content="RACE数据集相关文献"/>
<meta property="og:description" content="文献整理"/>
<meta property="og:image" content="http://localhost:4000/assets/imgposts/20211130/arxiv.jpg"/>
<meta property="og:image:alt" content="RACE数据集相关文献"/>
<meta property="og:site_name" content="Quehry" />
<meta property="article:published_time" content="2021-11-30 00:00:00 +0800" />
<meta property="article:modified_time" content="2021-11-30 00:00:00 +0800" />
<meta property="article:tag" content="paper" />
<meta property="fb:admins" content="ar.maybach" />
<!-- Schema.org markup for Google -->
<meta itemprop="name" content="RACE数据集相关文献">
<meta itemprop="description" content="文献整理">
<meta itemprop="image" content="http://localhost:4000/assets/imgposts/20211130/arxiv.jpg">
<!-- Twitter Card data -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@QuehryS">
<meta name="twitter:title" content="RACE数据集相关文献">
<meta name="twitter:description" content="文献整理">
<meta name="twitter:creator" content="">
<meta data-rh="true" name="twitter:label1" content="Word count"/>
<meta data-rh="true" name="twitter:data1" content="781"/>
<meta name="twitter:image:src" content="http://localhost:4000/assets/img/posts/20211130/arxiv.jpg">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#311e3e">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#311e3e">
	<!-- Google Fonts -->
	<link rel="preconnect" href="https://fonts.gstatic.com" />
	<style>
/* latin */
@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 400;
  src: url(https://fonts.gstatic.com/s/lora/v17/0QIvMX1D_JOuMwr7I_FMl_E.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin */
@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 600;
  src: url(https://fonts.gstatic.com/s/lora/v17/0QIvMX1D_JOuMwr7I_FMl_E.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin */
@font-face {
  font-family: 'Source Sans Pro';
  font-style: normal;
  font-weight: 200;
  src: url(https://fonts.gstatic.com/s/sourcesanspro/v14/6xKydSBYKcSV-LCoeQqfX1RYOo3i94_wlxdu3cOWxw.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin */
@font-face {
  font-family: 'Source Sans Pro';
  font-style: normal;
  font-weight: 400;
  src: url(https://fonts.gstatic.com/s/sourcesanspro/v14/6xK3dSBYKcSV-LCoeQqfX1RYOo3qOK7lujVj9w.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin */
@font-face {
  font-family: 'Source Sans Pro';
  font-style: normal;
  font-weight: 700;
  src: url(https://fonts.gstatic.com/s/sourcesanspro/v14/6xKydSBYKcSV-LCoeQqfX1RYOo3ig4vwlxdu3cOWxw.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
	</style>
	<!-- <link href="https://fonts.googleapis.com/css?family=Lora:400,600|Source+Sans+Pro:200,400,700" rel="stylesheet"> -->
	<!-- Font Awesome -->
	<link rel="stylesheet" href="./assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="./assets/css/main.css">
	




<link rel="icon" href="./assets/img/favicon/favicon.ico" type="image/x-icon">
<link rel="apple-touch-icon" href="./assets/img/favicon/favicon.ico">
<link rel="apple-touch-icon" sizes="72x72" href="./assets/img/favicon/favicon.ico">
<link rel="apple-touch-icon" sizes="114x114" href="./assets/img/favicon/favicon.ico">
	
<head>
  <body>
    




<section class="hidden">
  <div class="post">
      <a  class="post-list-title" href="./RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html">RACE数据集相关文献</a>
      

  <span class = "post-card-meta">
  
  
    <span class="meta-pre"></span>
  
  
    
      
      <span class="page_meta-date">
        <time datetime="2021-11-30T00:00:00+08:00">November 30, 2021</time>
      </span>
    
    
      <span class="meta-sep"></span>
    
  
  
    
    
    <span class="page_meta-readtime">
      
        3 minute read
      
    </span>
  
  
  </span>

        <div class="post-excerpt">
            <h1 id="目录"><strong>目录</strong></h1><ul> <li><a href="#目录"><strong>目录</strong></a></li> <li><a href="#文献整理">文献整理</a> <ul> <li><a href="#要求">要求</a></li> <li><a href="#搜集到相关文献标题和地址">搜集到相关文献标题和地址</a></li> </ul> </li> <li><a href="#第一篇">第一篇</a> <ul> <li><a href="#title">Title</a></li> <li><a href="#author">Author</a></li> <li><a href="#abstract">Abstract</a></li> <li><a href="#introduction">Introduction</a></li> <li><a href="#bert-distractor-generation">BERT distractor generation</a> <ul> <li><a href="#1bert-based-distractor-generationbdg">1)BERT-based distractor generation(BDG)</a></li> <li><a href="#2multi-task-with-parallel-mlm">2)Multi-task with Parallel MLM</a></li> <li><a href="#3answer-negative-regularization">3)Answer Negative Regularization</a></li> </ul> </li> <li><a href="#multiple-distractor-generation">Multiple Distractor Generation</a> <ul> <li><a href="#1selecting-distractors-by-entropy-maximization">1)Selecting Distractors by Entropy Maximization</a></li> <li><a href="#2bdg-em">2)BDG-EM</a></li> </ul> </li> <li><a href="#performance-evaluation">Performance Evaluation</a> <ul> <li><a href="#1datasets">1)datasets</a></li> <li><a href="#2implementation-details">2)implementation details</a></li> <li><a href="#3compared-methods">3)compared methods</a></li> <li><a href="#4token-score-comparison">4)token score comparison</a></li> <li><a href="#5mcq-model-accuracy-comparison">5)MCQ Model Accuracy Comparison</a></li> <li><a href="#6parameter-study-on-γ">6）Parameter Study on γ</a></li> </ul> </li> <li><a href="#conclusion">Conclusion</a></li> <li><a href="#我的看法">我的看法</a></li> </ul> </li> <li><a href="#第二篇">第二篇</a> <ul> <li><a href="#title-1">Title</a></li> <li><a href="#author-1">Author</a></li>...<a class="read-more" href="./RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html"> read more</a>
        </div>
  </div>
</section>
<div class="flex-container transparent">
  




<header class="main-header">
  <div class="wrapper">
    <div class="header-flex">
      <div class="menu-icon-container">
        <span class="menu-icon"><i class="fa fa-bars" aria-hidden="true"></i></span>
      </div>
      <nav class="main-nav">
        <span class="menu-icon-close"><i class="fa fa-times" aria-hidden="true"></i></span>
        <ul>
          <li>
            <div class="theme-toggle night">
    <input class="night" type="checkbox" id="theme-switch">
    <label class="night" for="theme-switch">
        <div class="toggle night"></div>
        <div class="names night">             
        <p class="light night"><svg class="night" width="20" viewBox="0 0 25 20" fill="currentColor" xmlns="http://www.w3.org/2000/svg">
            <path class="night" d="M12.5 2.49871C11.3401 2.50016 10.2282 2.96156 9.40801 3.78171C8.58785 4.60187 8.12645 5.71383 8.125 6.87371C8.125 7.03947 8.19085 7.19844 8.30806 7.31565C8.42527 7.43286 8.58424 7.49871 8.75 7.49871C8.91576 7.49871 9.07473 7.43286 9.19194 7.31565C9.30915 7.19844 9.375 7.03947 9.375 6.87371C9.37593 6.04519 9.70547 5.25088 10.2913 4.66503C10.8772 4.07918 11.6715 3.74964 12.5 3.74871C12.6658 3.74871 12.8247 3.68286 12.9419 3.56565C13.0592 3.44844 13.125 3.28947 13.125 3.12371C13.125 2.95795 13.0592 2.79898 12.9419 2.68177C12.8247 2.56456 12.6658 2.49871 12.5 2.49871V2.49871ZM12.5 -0.00129131C8.47891 -0.00129131 5.62031 3.26238 5.625 6.88269C5.62487 8.54403 6.22974 10.1486 7.32656 11.3964C8.32891 12.5378 9.29062 14.4007 9.375 14.9987L9.37734 17.9358C9.37744 18.0587 9.41402 18.1787 9.48242 18.2807L10.4395 19.7198C10.4964 19.8055 10.5737 19.8758 10.6644 19.9245C10.7551 19.9731 10.8564 19.9986 10.9594 19.9987H14.0395C14.1426 19.9988 14.2441 19.9734 14.3351 19.9247C14.426 19.8761 14.5034 19.8057 14.5605 19.7198L15.5176 18.28C15.5854 18.1776 15.6219 18.0578 15.6227 17.935L15.625 14.9987C15.7129 14.3846 16.6797 12.5303 17.6734 11.3964C18.5434 10.4028 19.1087 9.17963 19.3015 7.87318C19.4944 6.56673 19.3066 5.23238 18.7608 4.02985C18.215 2.82732 17.3342 1.80757 16.2238 1.09264C15.1135 0.377721 13.8206 -0.00207746 12.5 -0.00129131V-0.00129131ZM14.3727 17.7452L13.7047 18.7487H11.2937L10.6273 17.7452V17.4987H14.3738L14.3727 17.7452ZM14.375 16.2487H10.625L10.6227 14.9987H14.375V16.2487ZM16.7348 10.5725C16.1879 11.1956 15.316 12.4511 14.7594 13.7479H10.243C9.68516 12.4507 8.81328 11.1956 8.26641 10.5725C7.36971 9.5491 6.87599 8.2344 6.87734 6.87371C6.87031 3.8659 9.23594 1.24871 12.5 1.24871C15.602 1.24871 18.125 3.77176 18.125 6.87371C18.1249 8.23456 17.6305 9.54904 16.7336 10.5725H16.7348ZM3.75 6.87371C3.75 6.70795 3.68415 6.54898 3.56694 6.43177C3.44973 6.31456 3.29076 6.24871 3.125 6.24871H0.625C0.45924 6.24871 0.300269 6.31456 0.183058 6.43177C0.065848 6.54898 0 6.70795 0 6.87371C0 7.03947 0.065848 7.19844 0.183058 7.31565C0.300269 7.43286 0.45924 7.49871 0.625 7.49871H3.125C3.29076 7.49871 3.44973 7.43286 3.56694 7.31565C3.68415 7.19844 3.75 7.03947 3.75 6.87371ZM20.625 2.49871C20.7221 2.49849 20.8178 2.4759 20.9047 2.43269L23.4047 1.18269C23.5529 1.10852 23.6657 0.978483 23.718 0.821201C23.7704 0.66392 23.7582 0.492273 23.684 0.344021C23.6473 0.270614 23.5964 0.205161 23.5344 0.151397C23.4724 0.0976336 23.4004 0.0566132 23.3225 0.0306781C23.1652 -0.0217002 22.9936 -0.00945342 22.8453 0.0647243L20.3453 1.31472C20.2194 1.37771 20.1184 1.48136 20.0588 1.60889C19.9991 1.73643 19.9843 1.88037 20.0166 2.0174C20.049 2.15442 20.1267 2.2765 20.2371 2.36386C20.3475 2.45122 20.4842 2.49873 20.625 2.49871ZM24.375 6.24871H21.875C21.7092 6.24871 21.5503 6.31456 21.4331 6.43177C21.3158 6.54898 21.25 6.70795 21.25 6.87371C21.25 7.03947 21.3158 7.19844 21.4331 7.31565C21.5503 7.43286 21.7092 7.49871 21.875 7.49871H24.375C24.5408 7.49871 24.6997 7.43286 24.8169 7.31565C24.9342 7.19844 25 7.03947 25 6.87371C25 6.70795 24.9342 6.54898 24.8169 6.43177C24.6997 6.31456 24.5408 6.24871 24.375 6.24871ZM4.65469 1.31472L2.15469 0.0647243C2.08128 0.0279952 2.00136 0.00608435 1.91948 0.00024269C1.83761 -0.00559897 1.75539 0.004743 1.67751 0.0306781C1.52023 0.0830564 1.39019 0.195769 1.31602 0.344021C1.24184 0.492273 1.22959 0.66392 1.28197 0.821201C1.33435 0.978483 1.44706 1.10852 1.59531 1.18269L4.09531 2.43269C4.18223 2.4759 4.27794 2.49849 4.375 2.49871C4.5158 2.49873 4.65248 2.45122 4.7629 2.36386C4.87332 2.2765 4.951 2.15442 4.98337 2.0174C5.01574 1.88037 5.0009 1.73643 4.94124 1.60889C4.88158 1.48136 4.78061 1.37771 4.65469 1.31472ZM23.4047 12.5647L20.9047 11.3147C20.7564 11.2405 20.5847 11.2283 20.4274 11.2807C20.2701 11.3332 20.14 11.4459 20.0658 11.5942C19.9916 11.7425 19.9794 11.9142 20.0318 12.0715C20.0842 12.2289 20.197 12.3589 20.3453 12.4331L22.8453 13.6831C22.9936 13.7573 23.1653 13.7695 23.3226 13.7171C23.4799 13.6647 23.61 13.5519 23.6842 13.4036C23.7584 13.2553 23.7706 13.0836 23.7182 12.9263C23.6658 12.769 23.553 12.6389 23.4047 12.5647V12.5647ZM4.375 11.2487C4.27794 11.2489 4.18223 11.2715 4.09531 11.3147L1.59531 12.5647C1.44701 12.6389 1.33425 12.769 1.28183 12.9263C1.25588 13.0042 1.24552 13.0864 1.25135 13.1683C1.25719 13.2502 1.27909 13.3302 1.31582 13.4036C1.35255 13.477 1.40338 13.5425 1.46542 13.5963C1.52745 13.6501 1.59947 13.6911 1.67737 13.7171C1.83469 13.7695 2.00638 13.7573 2.15469 13.6831L4.65469 12.4331C4.78083 12.3702 4.88202 12.2666 4.94183 12.1389C5.00164 12.0113 5.01656 11.8672 4.98417 11.7301C4.95178 11.5929 4.87397 11.4707 4.76339 11.3833C4.65281 11.2959 4.51594 11.2485 4.375 11.2487V11.2487Z" /></svg></p>
        <p class="dark night"><svg class="night" width="20" viewBox="0 0 25 21" fill="currentColor" xmlns="http://www.w3.org/2000/svg">
            <path class="night" d="M6.39614 3.72646C7.50591 1.56178 9.72622 0.00900831 12.4782 0.00114817C13.8006 -0.00388798 15.0965 0.375153 16.2101 1.09278C17.3237 1.8104 18.2079 2.83612 18.7564 4.04682C19.3049 5.25751 19.4945 6.60175 19.3024 7.91818C19.1103 9.23461 18.5447 10.4673 17.6735 11.4683C17.5227 11.6416 17.3516 11.859 17.1739 12.1069L9.47856 6.12184C9.65443 5.45016 10.046 4.85578 10.5924 4.43118C11.1387 4.00657 11.8093 3.77554 12.4997 3.77401C12.6654 3.77401 12.8244 3.70776 12.9416 3.58984C13.0588 3.47191 13.1247 3.31197 13.1247 3.1452C13.1247 2.97843 13.0588 2.81849 12.9416 2.70057C12.8244 2.58264 12.6654 2.51639 12.4997 2.51639C11.6212 2.5173 10.7634 2.78383 10.0374 3.28141C9.31146 3.77899 8.75092 4.48463 8.42856 5.30674L6.39614 3.72646ZM6.39614 10.0841C6.64968 10.5817 6.96225 11.0465 7.327 11.4683C7.97231 12.2091 8.98169 13.7568 9.36645 15.0624C9.36645 15.0726 9.36919 15.0828 9.37075 15.093H12.8372L6.39614 10.0841ZM9.37466 16.3502V17.8574C9.37584 18.1045 9.44934 18.3458 9.58599 18.5511L10.2536 19.5607C10.3675 19.7335 10.5221 19.8753 10.7037 19.9734C10.8852 20.0715 11.0881 20.1229 11.2942 20.1231H13.7047C13.9107 20.1231 14.1135 20.0718 14.2951 19.9739C14.4766 19.876 14.6313 19.7345 14.7454 19.5619L15.4129 18.5511C15.5492 18.3451 15.622 18.1033 15.6223 17.8558V17.2581L14.4528 16.3502H9.37466Z"/>
            <path class="night" d="M0.131556 1.2363L0.898352 0.243172C0.948738 0.177883 1.01142 0.123229 1.08282 0.0823368C1.15423 0.0414448 1.23294 0.0151171 1.31446 0.00486006C1.39598 -0.00539702 1.47872 0.000617709 1.55793 0.0225602C1.63714 0.0445026 1.71127 0.0819422 1.77609 0.132737L24.7585 18.0039C24.8894 18.1062 24.9745 18.2567 24.9952 18.4221C25.0158 18.5876 24.9703 18.7545 24.8687 18.8862L24.1015 19.8794C24.0511 19.9446 23.9884 19.9992 23.917 20.0401C23.8457 20.0809 23.767 20.1072 23.6855 20.1175C23.6041 20.1277 23.5214 20.1217 23.4422 20.0998C23.363 20.0779 23.2889 20.0405 23.2241 19.9898L0.241322 2.1186C0.110489 2.01624 0.0254259 1.86578 0.00484145 1.70032C-0.015743 1.53486 0.0298368 1.36795 0.131556 1.2363V1.2363Z"/>
            </svg></p>
        </div>
    </label>
</div>
          </li>
          <li>
            <a href="./">
              <div class="left">
                Home
              </div>  
              <div class="right">
                <svg width="24px" aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><g><rect x="83.534" y="40.929" width="3.997" height="20.071"/></g><path d="M16.466,41.931l33.548-25.123L92.81,48.877l2.396-3.198L50.015,11.814L4.794,45.679l2.396,3.199l5.279-3.954v42.763h75.062  V61h-3.997v22.69H64.598V54.068H35.402V83.69H16.466V41.931z M39.399,58.065h21.202V83.69H39.399V58.065z"/></svg>
              </div>
            </a>
          </li>
          <li>
            <a href="./archive.html">
              <div class="left">
                All Posts
              </div>
              <div class="right">
                <svg width="24px" aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="-3 3 64 64"><g><path d="M60.992,31.985c0-15.979-13-28.978-28.979-28.978c-15.994,0-29.006,12.999-29.006,28.978   c0,15.994,13.012,29.007,29.006,29.007v-2c-14.891,0-27.006-12.115-27.006-27.007c0-14.875,12.115-26.978,27.006-26.978   c14.876,0,26.979,12.103,26.979,26.978c0,8.945-4.479,17.329-11.804,22.338l0.874-10.062l-1.992-0.174l-1.135,13.071l13.042,1.136   l0.174-1.992l-9.183-0.799C56.443,50.079,60.992,41.321,60.992,31.985z"/><polygon points="33.014,12.682 31.014,12.682 31.014,32.398 39.811,41.224 41.227,39.812 33.014,31.572  "/></g></svg>
              </div>
            </a>
          </li>
          <li>
            <a href="./tags.html">
              <div class="left">
                Tags
              </div>
              <div class="right">
                <svg width="24px" aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><g><path d="M75.244,15.066c-2.59,0-5.027,1.012-6.857,2.843c-3.781,3.785-3.778,9.94,0.002,13.724    c1.831,1.833,4.266,2.843,6.857,2.843s5.026-1.01,6.861-2.843c3.781-3.785,3.781-9.943-0.002-13.724    C80.275,16.076,77.838,15.066,75.244,15.066z M78.766,28.252c-1.871,1.869-5.129,1.869-6.996,0c-1.929-1.931-1.931-5.069-0.002-7    c0.934-0.934,2.175-1.448,3.498-1.448c1.322,0,2.564,0.515,3.5,1.448C80.691,23.183,80.691,26.321,78.766,28.252z M94.632,41.027    l0.005-28.872c0-3.745-3.05-6.792-6.792-6.792L58.973,5.368l-1.237-0.004c-1.893,0-4.75,0-6.617,1.869L7.008,51.342    c-1.06,1.059-1.645,2.467-1.645,3.966s0.583,2.908,1.644,3.968l33.717,33.717c1.058,1.06,2.467,1.645,3.966,1.645    s2.908-0.585,3.968-1.645l44.106-44.111c1.893-1.886,1.88-4.604,1.869-7.227L94.632,41.027z M90.022,46.139L45.913,90.25    c-0.654,0.65-1.792,0.652-2.443,0L9.752,56.532c-0.328-0.327-0.507-0.762-0.507-1.225c0-0.462,0.18-0.894,0.507-1.221    L53.861,9.976c0.676-0.674,2.284-0.731,3.874-0.731l1.237,0.004l28.872-0.004c1.604,0,2.909,1.306,2.909,2.911l-0.005,28.872    l0.005,0.642C90.76,43.585,90.769,45.392,90.022,46.139z"/></g></svg>
              </div>
            </a>
          </li>
          <li>
            <a href="./about.html">
              <div class="left">
                About
              </div>
              <div class="right">
                <svg width='24px' aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 846.66 846.66"><g><path d="M351.26 453.22c-276.42,134.06 -224.86,336.22 -224.73,336.8 6.03,25.41 -32.58,34.56 -38.6,9.15 -0.15,-0.65 -55.78,-219.32 218.87,-367.66 -60.98,-39 -100.02,-106.82 -100.02,-182.56 0,-119.6 96.95,-216.55 216.55,-216.55 119.6,0 216.55,96.95 216.55,216.55 0,75.74 -39.04,143.56 -100.02,182.56 274.65,148.34 219.02,367.01 218.87,367.66 -6.02,25.41 -44.63,16.26 -38.6,-9.15 0.13,-0.58 51.69,-202.74 -224.73,-336.8 -22.55,7.96 -46.8,12.29 -72.07,12.29 -25.27,0 -49.52,-4.33 -72.07,-12.29zm72.07 -381.14c-97.68,0 -176.87,79.19 -176.87,176.87 0,97.69 79.19,176.87 176.87,176.87 97.68,0 176.87,-79.18 176.87,-176.87 0,-97.68 -79.19,-176.87 -176.87,-176.87z"/></g></svg>
              </div>
            </a>
          </li>
          <li>
            <a href="./feed.xml">
              <div class="left">
                Atom feed
              </div>
              <div class="right">
                <svg width='24px' aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M80 352c26.467 0 48 21.533 48 48s-21.533 48-48 48-48-21.533-48-48 21.533-48 48-48m0-32c-44.183 0-80 35.817-80 80s35.817 80 80 80 80-35.817 80-80-35.817-80-80-80zm367.996 147.615c-6.448-237.848-198.06-429.164-435.61-435.61C5.609 31.821 0 37.229 0 44.007v8.006c0 6.482 5.146 11.816 11.626 11.994 220.81 6.05 398.319 183.913 404.367 404.367.178 6.48 5.512 11.626 11.994 11.626h8.007c6.778 0 12.185-5.609 12.002-12.385zm-144.245-.05c-6.347-158.132-133.207-284.97-291.316-291.316C5.643 175.976 0 181.45 0 188.247v8.005c0 6.459 5.114 11.72 11.567 11.989 141.134 5.891 254.301 119.079 260.192 260.192.269 6.453 5.531 11.567 11.989 11.567h8.005c6.798 0 12.271-5.643 11.998-12.435z"></path></svg>
              </div>
            </a>
          </li>
        </ul>
      </nav>
      
      
      <div class="logo"><a href="./"><img class="logo" id="logo" src="./assets/img/branding/MVM-logo-full.svg" alt="Quehry"></a></div>
      <div class="search-icon-container">
        <span class="search-icon"><a><i class="fa fa-search" aria-hidden="true"></i></a></span>
      </div>
    </div>
  </div>
</header> <!-- End Header -->

  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="RACE数据集相关文献">
<meta itemprop="description" content="文献整理">
<meta itemprop="datePublished" content="2021-11-30T00:00:00+08:00">

    <div class="page-image">
      <div class="cover-image" style="background: url('./assets/img/posts/20211130/arxiv.jpg') center no-repeat; background-size: cover;"></div>
    </div>
    <div class="wrapper">
      <div class="page-content">
        <div class="header-page">
          <h1 class="page-title">RACE数据集相关文献</h1>
          

  <span class = "post-page-meta">
  
    <p class="page_meta">
  
  
  
    
      
      <span class="page_meta-date">
        <time datetime="2021-11-30T00:00:00+08:00">November 30, 2021</time>
      </span>
    
    
      <span class="meta-sep"></span>
    
  
  
    
    
    <span class="page_meta-readtime">
      
        3 minute read
      
    </span>
  
  
    </p>
  
  </span>

        </div>
        <aside class="sidebar side" id="sidebar">
    



<div class="tag-cloud">
    
        <ul class="tags side">
            
                <li><a href="./tag.html?tag=paper" class="tag side">paper</a></li>
            
    
        </ul>
</div>
    <div class="share-options side">
    <div class="share-hover side">
        <span class="share-button side"><svg fill="currentColor" width="25" height="25" class="side"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></span>
        <div class="share-icons side" id="sidebar-icons">
            <a class="twitter" href="https://twitter.com/intent/tweet?text=RACE数据集相关文献&url=http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html" title="Share on Twitter" rel="nofollow" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
            <a class="facebook" href="https://facebook.com/sharer.php?u=http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html" title="Share on Facebook" rel="nofollow" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
            <a class="reddit" href="http://www.reddit.com/submit?url=http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html&title=RACE数据集相关文献" title="Submit to Reddit" rel="nofollow" target="_blank"><i class="fa fa-reddit" aria-hidden="true"></i></a>
            <a class="linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html&title=RACE数据集相关文献&summary=文献整理&source=http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html" title="Share on LinkedIn" rel="nofollow" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
            <a class="email" href="mailto:?subject=RACE数据集相关文献&body=文献整理%0A%0ARead more here: http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html" title="Share via e-mail" rel="nofollow" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
            <a class="copy-link" onclick="copyToClipboard()" title="Copy to clipboard" rel="nofollow" target="_blank"><svg width="20px" fill="currentColor" class="side" viewBox="0 0 18 18"><path d="M16.94 1.1A3.7 3.7 0 0 0 14.3 0c-1 0-1.94.39-2.64 1.1L7.43 5.3c-.91.92-2.09 3.2 0 5.27a.75.75 0 0 0 .82.16c.09-.03.17-.09.24-.15a.74.74 0 0 0 0-1.06c-1.16-1.15-.77-2.39-.02-3.16l4.24-4.22a2.2 2.2 0 0 1 1.58-.65c.6 0 1.16.23 1.58.65.86.87.86 2.29 0 3.16L12.7 8.47a.74.74 0 0 0 1.04 1.05l3.17-3.16a3.73 3.73 0 0 0 0-5.27h.03zM9.54 7.4a.74.74 0 0 0 0 1.06c1.16 1.15.76 2.39 0 3.16l-4.22 4.22c-.42.42-.99.65-1.59.65a2.23 2.23 0 0 1-1.58-3.82l3.17-3.16A.73.73 0 0 0 5.54 9a.78.78 0 0 0-.22-.52.77.77 0 0 0-1.05 0L1.1 11.64A3.72 3.72 0 0 0 3.74 18c1 0 1.94-.39 2.65-1.1l4.23-4.2c.21-.22.94-1.02 1.13-2.2.18-1.12-.2-2.15-1.12-3.07-.27-.27-.78-.27-1.06 0l-.02-.02z" clip-rule="evenodd" fill-rule="evenodd"></path></svg></a>
        </div>
    </div>
    <div class='alert' style='font-size:.6em;color:var(--accent);text-align:center;'></div>
</div>
</aside>

        
        
        <h1 id="目录"><strong>目录</strong></h1>

<ul>
  <li><a href="#目录"><strong>目录</strong></a></li>
  <li><a href="#文献整理">文献整理</a>
    <ul>
      <li><a href="#要求">要求</a></li>
      <li><a href="#搜集到相关文献标题和地址">搜集到相关文献标题和地址</a></li>
    </ul>
  </li>
  <li><a href="#第一篇">第一篇</a>
    <ul>
      <li><a href="#title">Title</a></li>
      <li><a href="#author">Author</a></li>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#introduction">Introduction</a></li>
      <li><a href="#bert-distractor-generation">BERT distractor generation</a>
        <ul>
          <li><a href="#1bert-based-distractor-generationbdg">1)BERT-based distractor generation(BDG)</a></li>
          <li><a href="#2multi-task-with-parallel-mlm">2)Multi-task with Parallel MLM</a></li>
          <li><a href="#3answer-negative-regularization">3)Answer Negative Regularization</a></li>
        </ul>
      </li>
      <li><a href="#multiple-distractor-generation">Multiple Distractor Generation</a>
        <ul>
          <li><a href="#1selecting-distractors-by-entropy-maximization">1)Selecting Distractors by Entropy Maximization</a></li>
          <li><a href="#2bdg-em">2)BDG-EM</a></li>
        </ul>
      </li>
      <li><a href="#performance-evaluation">Performance Evaluation</a>
        <ul>
          <li><a href="#1datasets">1)datasets</a></li>
          <li><a href="#2implementation-details">2)implementation details</a></li>
          <li><a href="#3compared-methods">3)compared methods</a></li>
          <li><a href="#4token-score-comparison">4)token score comparison</a></li>
          <li><a href="#5mcq-model-accuracy-comparison">5)MCQ Model Accuracy Comparison</a></li>
          <li><a href="#6parameter-study-on-γ">6）Parameter Study on γ</a></li>
        </ul>
      </li>
      <li><a href="#conclusion">Conclusion</a></li>
      <li><a href="#我的看法">我的看法</a></li>
    </ul>
  </li>
  <li><a href="#第二篇">第二篇</a>
    <ul>
      <li><a href="#title-1">Title</a></li>
      <li><a href="#author-1">Author</a></li>
      <li><a href="#abstract-1">Abstract</a></li>
      <li><a href="#method">Method</a>
        <ul>
          <li><a href="#1question-generation">1)question generation</a></li>
          <li><a href="#2distractor-generation">2)distractor generation</a></li>
          <li><a href="#3qa-filtering">3)QA filtering</a></li>
        </ul>
      </li>
      <li><a href="#results">Results</a>
        <ul>
          <li><a href="#1quantitative-evaluation">1)quantitative evaluation</a></li>
          <li><a href="#2question-answering-ability">2)question answering ability</a></li>
          <li><a href="#3human-evaluation">3)human evaluation</a></li>
        </ul>
      </li>
      <li><a href="#conclusion">conclusion</a></li>
    </ul>
  </li>
  <li><a href="#第三篇">第三篇</a>
    <ul>
      <li><a href="#title-2">Title</a></li>
      <li><a href="#author-2">Author</a></li>
      <li><a href="#abstract-2">Abstract</a></li>
      <li><a href="#framework-description-网络结构">Framework Description 网络结构</a>
        <ul>
          <li><a href="#1task-definition">1)Task Definition</a></li>
          <li><a href="#2framework-overview">2)Framework overview</a></li>
          <li><a href="#3hierarchical-encoder">3)Hierarchical encoder</a></li>
          <li><a href="#4static-attention-mechanism">4)static attention mechanism</a></li>
          <li><a href="#5encoding-layer">5)encoding layer</a></li>
          <li><a href="#6matching-layer">6)matching layer</a></li>
          <li><a href="#7nomalization-layer">7)nomalization layer</a></li>
          <li><a href="#8distractor-decoder">8)distractor decoder</a></li>
          <li><a href="#9question-based-initializer">9)question-based initializer</a></li>
          <li><a href="#10dynamic-hierarchical-attention-mechanism">10)dynamic hierarchical attention mechanism</a></li>
          <li><a href="#11training-and-inference">11)training and inference</a></li>
        </ul>
      </li>
      <li><a href="#experimental-setting-实验设置">experimental setting 实验设置</a>
        <ul>
          <li><a href="#1dataset">1)dataset</a></li>
          <li><a href="#2implementation-details-1">2)implementation details</a></li>
          <li><a href="#3baselines-and-ablations">3)baselines and ablations</a></li>
        </ul>
      </li>
      <li><a href="#results-and-analysis-结果与分析">results and analysis 结果与分析</a></li>
      <li><a href="#我的看法-1">我的看法</a></li>
    </ul>
  </li>
  <li><a href="#第四篇">第四篇</a>
    <ul>
      <li><a href="#title-3">Title</a></li>
      <li><a href="#author-3">Author</a></li>
      <li><a href="#abstract-3">Abstract</a></li>
      <li><a href="#proposed-framework-网络结构">Proposed Framework 网络结构</a>
        <ul>
          <li><a href="#1notations-and-task-definition">1)notations and task definition</a></li>
          <li><a href="#2model-overview">2)model overview</a></li>
          <li><a href="#3encoding-article-and-question">3)encoding article and question</a></li>
          <li><a href="#4co-attention-between-article-and-question">4)Co-attention between article and question</a></li>
          <li><a href="#5merging-sentence-representation">5)Merging sentence representation</a></li>
          <li><a href="#6question-initialization">6)question initialization</a></li>
          <li><a href="#7hierarchical-attention">7)hierarchical attention</a></li>
          <li><a href="#8semantic-similarity-loss">8)semantic similarity loss</a></li>
        </ul>
      </li>
      <li><a href="#experimental-settings">Experimental Settings</a>
        <ul>
          <li><a href="#1dataset-1">1)dataset</a></li>
          <li><a href="#2baselines-and-evaluation-metrics">2)baselines and evaluation metrics</a></li>
          <li><a href="#3implementation-details">3)implementation details</a></li>
        </ul>
      </li>
      <li><a href="#results-and-analysis-结果与分析">Results and Analysis 结果与分析</a></li>
      <li><a href="#我的看法-2">我的看法</a></li>
    </ul>
  </li>
  <li><a href="#补充">补充</a>
    <ul>
      <li><a href="#race数据集简介">RACE数据集简介</a></li>
      <li><a href="#bleu">BLEU</a></li>
      <li><a href="#rouge">ROUGE</a></li>
    </ul>
  </li>
</ul>

<h1 id="文献整理">文献整理</h1>

<h2 id="要求">要求</h2>

<p><img src="../assets/img/posts/20211130/requirements.jpg" /></p>

<h2 id="搜集到相关文献标题和地址">搜集到相关文献标题和地址</h2>
<ul>
  <li><a href="https://arxiv.org/pdf/2010.05384.pdf">A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies</a></li>
  <li><a href="https://arxiv.org/pdf/2010.09598.pdf">Better Distractions: Transformer-based Distractor Generation and Multiple Choice Question Filtering</a></li>
  <li><a href="https://ojs.aaai.org//index.php/AAAI/article/view/4606">Generating Distractors for Reading Comprehension Questions from Real Examinations</a></li>
  <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6522">Co-attention hierarchical network: Generating coherent long distractors for reading comprehension</a></li>
  <li><a href="https://aclanthology.org/2020.coling-main.189.pdf">Automatic Distractor Generation for Multiple Choice Questions in Standard Tests</a></li>
  <li><a href="https://aclanthology.org/W18-0533.pdf">Distractor Generation for Multiple Choice Questions Using Learning to Rank</a></li>
  <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/16559">Knowledge-Driven Distractor Generation for Cloze-style Multiple Choice Questions</a></li>
</ul>

<h1 id="第一篇">第一篇</h1>
<h2 id="title">Title</h2>
<p>A BERT-based Distractor Generation Scheme with Multi-tasking and
Negative Answer Training Strategies</p>
<h2 id="author">Author</h2>
<p>Ho-Lam Chung, Ying-Hong Chan, Yao-Chung Fan</p>
<h2 id="abstract">Abstract</h2>
<p>现有的DG<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>局限在只能生成一个误导选项，我们需要生成多个误导选项，文章中提到他们团队用multi-tasking和negative answer training技巧来生成多个误导选项，模型结果达到了学界顶尖。</p>

<h2 id="introduction">Introduction</h2>
<p>DG效果不好，文章提出了两个提升的空间：</p>
<ol>
  <li>DG质量提升：<br />
 BERT模型来提升误导选项质量</li>
  <li>多个误导选项生成：
 运用了覆盖的方法来选择distractor，而不是选择概率最高但是语义很相近的distractor</li>
</ol>

<h2 id="bert-distractor-generation">BERT distractor generation</h2>
<h3 id="1bert-based-distractor-generationbdg">1)BERT-based distractor generation(BDG)</h3>
<p>输入：段落P，答案A，问题Q，用C表示这三者concatenate后的结果。<br />
BDG模型是一个自回归模型，在预测阶段，每次输入C和上一次预测的词元，BDG迭代预测词元，直到预测出特殊词元[S]停止。下面这张图简单介绍了这个过程。</p>

<p><img src="../assets/img/posts/20211130/2.jpg" /></p>

<p>网络结构简单介绍：h<sub>[M]</sub>表示bert输出的隐藏状态，隐藏状态再输入到一个全连接层中用来预测词元。</p>

<p><img src="../assets/img/posts/20211130/3.jpg" /></p>

<h3 id="2multi-task-with-parallel-mlm">2)Multi-task with Parallel MLM</h3>
<p>MLM全称masked language model，遮蔽语言模型,通过并行BDG和P-MLM来训练模型让模型有更好的效果。</p>

<p><img src="../assets/img/posts/20211130/4.jpg" /></p>

<p>上图中左边的sequential MLM就是之前提到的BDG，BDG模型是一个词接一个词的预测，P-MLM是对所有的masked token进行预测，最后的损失函数是这两者相加<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>，公式如下：</p>

<p><img src="../assets/img/posts/20211130/5.jpg" /></p>

<p><img src="../assets/img/posts/20211130/6.jpg" /></p>

<p><img src="../assets/img/posts/20211130/7.jpg" /></p>

<p>作者如此设计的思路是：BDG可能会忽略整体语义语义信息，但是会过拟合单个词预测。那么并行一个P-MLM可以防止过拟合。</p>

<h3 id="3answer-negative-regularization">3)Answer Negative Regularization</h3>
<p>目前机器预测的distractor和answer有很高的相似度，下面一张表可以展示相似度。其中PM表示机器，Gold表示人工，作者将这类问题称为answer copying problem。</p>

<p><img src="../assets/img/posts/20211130/8.jpg" /></p>

<p>为了解决这个问题，作者提出了answer negative loss来让机器更多的选择与answer不同的词来表示新的distractor，公式如下：</p>

<p><img src="../assets/img/posts/20211130/9.jpg" /></p>

<p>可以看出BDG的loss替换成了AN的loss，每一项都减去了Answer negative loss。</p>

<h2 id="multiple-distractor-generation">Multiple Distractor Generation</h2>
<h3 id="1selecting-distractors-by-entropy-maximization">1)Selecting Distractors by Entropy Maximization</h3>
<p>选择语义不同的distractor set。文章借鉴了MRC<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>的方法，让BDGmodel生成很多distractor组成 $\hat{D}$ = {$\hat{d}$<sub>1</sub>, $\hat{d}$<sub>2</sub>, $\hat{d}$<sub>3</sub>…}，然后找出最好的一组选项，一般情况下由三个误导选项和一个答案组成。选择的一句是最大化下面这个公式：</p>

<p><img src="../assets/img/posts/20211130/10.jpg" /></p>

<h3 id="2bdg-em">2)BDG-EM</h3>
<p>我们可以通过不同的BDG模型来生成不同的误导选项最后组合，不同的模型区别是有没有answer negative/multi-task training，比如我们有这几个模型:$\hat{D}$,$\hat{D}$<sub>PM</sub>,$\hat{D}$<sub>PM+AN</sub>，它们分别代表含PM<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>和含AN<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup></p>

<p><img src="../assets/img/posts/20211130/11.jpg" /></p>

<h2 id="performance-evaluation">Performance Evaluation</h2>
<h3 id="1datasets">1)datasets</h3>
<p>RACE,沿用了<a href="https://ojs.aaai.org//index.php/AAAI/article/view/4606">Gao</a>那篇论文的处理,后面也会梳理那篇论文</p>

<p><img src="../assets/img/posts/20211130/12.jpg" /></p>

<h3 id="2implementation-details">2)implementation details</h3>
<ul>
  <li>tokenizer: wordpiece tokenizer</li>
  <li>framewordk:huggingface trainsformers</li>
  <li>optimizer:adamW(lr:5e-5)</li>
  <li>github_url: <a href="https://github.com/voidful/BDG">BDG</a></li>
</ul>

<h3 id="3compared-methods">3)compared methods</h3>
<p>比较了不同的distractor generation</p>
<ul>
  <li>CO-Att：出自<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6522">Zhou</a></li>
  <li>DS-Att: 出自<a href="https://ojs.aaai.org//index.php/AAAI/article/view/4606">Gao</a></li>
  <li>GPT:baseline</li>
  <li>BDG: 没有应用P-MLM和Answer negative</li>
  <li>BDG<sub>PM</sub></li>
  <li>BDG<sub>AN+PM</sub></li>
</ul>

<h3 id="4token-score-comparison">4)token score comparison</h3>
<p>BLEU和ROUGE(L)两种判断指标</p>

<p><img src="../assets/img/posts/20211130/13.jpg" /></p>

<p>copying problem的效果</p>

<p><img src="../assets/img/posts/20211130/14.jpg" /></p>

<h3 id="5mcq-model-accuracy-comparison">5)MCQ Model Accuracy Comparison</h3>
<p>与回答系统相结合，将生成好的选项（一个正确答案三个误导选项）放入MCQ answering model，下面是回答正确率的表格</p>

<p><img src="../assets/img/posts/20211130/15.jpg" /></p>

<p>可以看出作者的模型选项的误导性还是很高的。</p>

<h3 id="6parameter-study-on-γ">6）Parameter Study on γ</h3>
<p>之前使用P-MLM并行训练时候有个权重参数γ，下表显示了不同γ值的影响，对于只有PM的模型来说，γ=6，对于既有AN和PM来说，γ=7</p>

<p><img src="../assets/img/posts/20211130/16.jpg" /></p>

<h2 id="conclusion">Conclusion</h2>
<p>现存的DG可以分为cloze-style distractor generation和 reading comprehension distractor generation，前者主要是word filling，后者主要看重语义信息，基于两者的设计出了很多模型，目前来看还是考虑语义信息生成的误导选项更好。</p>

<p><img src="../assets/img/posts/20211130/17.jpg" /></p>

<h2 id="我的看法">我的看法</h2>
<p>文章中的模型提到了三种技术，第一是bert预训练模型使用。第二是P-MLM的并行使用， 它的使用让模型可以考虑段落的语义信息，那么生成的误导选项是sentence-level而不是之前模型所使用的类似word-filling这种word-level。第三是Answer negative loss的使用，它的使用相当于让模型不要考虑与正确答案语义很接近的误导选项，因为目前大多数DG生成多个选项时语义与正确答案都非常接近，这与实际情况不符，同时也起不到误导的作用。  <br />
同时文章提出了生成多个误导选项时使用不同模型生成的误导选项拼在一起作为选项是一种比较好的解决方法，让一次性生成多个误导选型有了一定的可用性。<br />
文章的代码开源，可以去<a href="https://github.com/voidful/BDG">github</a>上看训练细节和网络结构细节。</p>

<h1 id="第二篇">第二篇</h1>
<h2 id="title-1">Title</h2>
<p>Better Distractions: Transformer-based Distractor Generation and Multiple Choice Question Filtering</p>
<h2 id="author-1">Author</h2>
<p>Jeroen Offerijns, Suzan Verberne, Tessa Verhoef</p>
<h2 id="abstract-1">Abstract</h2>
<p>运用GPT2模型生成三个误导选项，同时用BERT模型去回答这个问题，只挑选出回答正确的问题。相当于使用了QA作为一个过滤器(QA filtering)。</p>
<h2 id="method">Method</h2>
<p>作者使用了Question generation model, distractor generation model和question answer filter，作者将从这三方面介绍，下图是大概的流程图。</p>

<p><img src="../assets/img/posts/20211130/18.jpg" /></p>

<h3 id="1question-generation">1)question generation</h3>
<ul>
  <li>预训练模型：GPT-2</li>
  <li>数据集：English SQuAD</li>
  <li>tokenizer：Byte-Pair-Encoding(BPE) tokenizer</li>
  <li>optimizer:Adam</li>
  <li>下图展示了QG的输入，黑框内被tokenizer标记为特殊词元</li>
</ul>

<p><img src="../assets/img/posts/20211130/19.jpg" /></p>

<h3 id="2distractor-generation">2)distractor generation</h3>
<ul>
  <li>预训练模型：GPT-2</li>
  <li>数据集：RACE</li>
  <li>tokenizer:BPE<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup></li>
  <li>使用了repetition penalty技术，保证了尽量不会生成相似的text，并且过滤到那些不好的生成（比如生成了空字符串）</li>
  <li>输入：经典的C(context)，A(answer),Q(question)，下图展示了输入格式</li>
</ul>

<p><img src="../assets/img/posts/20211130/20.jpg" /></p>

<h3 id="3qa-filtering">3)QA filtering</h3>
<ul>
  <li>预训练模型：DistilBERT</li>
  <li>网络结构：CQA<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>输入到distilbert，再连接一个dropout，全连接层和softmax，最后输出一个答案，具体结构如下图</li>
</ul>

<p><img src="../assets/img/posts/20211130/21.jpg" /></p>

<h2 id="results">Results</h2>
<h3 id="1quantitative-evaluation">1)quantitative evaluation</h3>
<p>下表中展示了和上一篇论文类似的指标,与现有的模型进行了比较：SEQ2SEQ,HSA<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup>和CHN<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>。可以看出BLEU明显要比之前模型要好，但是ROUGE没有之前的高。</p>

<p><img src="../assets/img/posts/20211130/22.jpg" /></p>

<h3 id="2question-answering-ability">2)question answering ability</h3>
<p>用GPT-2模型生成误导选项再输入到QAmodel中，具体结果见下图。</p>

<p><img src="../assets/img/posts/20211130/23.jpg" /></p>

<h3 id="3human-evaluation">3)human evaluation</h3>
<p>人工评估，从两方面评估distractor生成的好坏：</p>
<ul>
  <li><strong>Is the question well-formed and can you understand the meaning?</strong></li>
  <li><strong>If the question is at least understandable, does the answer make sense in relation to the question?</strong>
评估过程中，使用了155个没有经过QA筛选和155经过QA筛选的，了解一下QA过滤模型的效果。整体来说QA过滤器还是有一点效果，具体结果如下：</li>
</ul>

<p><img src="../assets/img/posts/20211130/24.jpg" /></p>

<h2 id="conclusion-1">conclusion</h2>
<p>我认为作者使用的DG模型主要有两大特色，一个是使用了GPT2预训练模型，目前使用基于transformer的模型已经成为主流。第二个是使用了QA过滤器来筛选掉回答错误的，有一定提升但不显著。</p>

<h1 id="第三篇">第三篇</h1>
<h2 id="title-2">Title</h2>
<p>Generating Distractors for Reading Comprehension Questions from Real Examinations</p>
<h2 id="author-2">Author</h2>
<p>Yifan Gao, Lidong Bing, Piji Li,
Irwin King, Michael R. Lyu</p>
<h2 id="abstract-2">Abstract</h2>
<p>上面两篇文献都有提到这篇文章。作者使用了<strong>Hierarchical encoder-decoder framework</strong> with <strong>static</strong> and <strong>dynamic</strong> attention mechanisms来生成有语义信息的误导选项。使用了编码器-解码器结构网络和静态和动态注意力机制。</p>
<h2 id="framework-description-网络结构">Framework Description 网络结构</h2>
<h3 id="1task-definition">1)Task Definition</h3>
<p>输入：文章，问题和答案。P代表文章，s<sub>1</sub>,s<sub>2</sub>,s<sub>3</sub>…表示不同的句子，q和a分别表示问题和答案，那么我们的任务是生成误导选项$\overline{d}$。</p>

<p><img src="../assets/img/posts/20211130/25.jpg" /></p>

<h3 id="2framework-overview">2)Framework overview</h3>
<p>网络结构如下图所示，下面将从各个组成部分分别介绍：</p>

<p><img src="../assets/img/posts/20211130/26.jpg" /></p>

<h3 id="3hierarchical-encoder">3)Hierarchical encoder</h3>
<ul>
  <li><strong>word embedding</strong>:词嵌入，将每个句子s<sub>i</sub>中的每个词元变成词向量(w<sub>i,1</sub>,w<sub>i,2</sub>,w<sub>i,3</sub>…)</li>
  <li><strong>word encoder</strong>:将句子s<sub>i</sub>的词向量(w<sub>i,1</sub>,w<sub>i,2</sub>,w<sub>i,3</sub>…)作为输入，用<strong>双向LSTM</strong>作为编码器，获得word-level representation h<sub>i,j</sub><sup>e</sup></li>
</ul>

<p><img src="../assets/img/posts/20211130/27.jpg" /></p>

<ul>
  <li><strong>sentence encoder</strong>:将word encoder中每个句子正向LSTM的最后一个隐藏状态和反向LSTM的最开始的隐藏状态作为输入到另一个双向LSTM中获得<strong>sentence-level representation</strong>(u<sub>1</sub>,u<sub>2</sub>,u<sub>3</sub>…)</li>
</ul>

<h3 id="4static-attention-mechanism">4)static attention mechanism</h3>
<p>目的：生成的误导选项必须和问题Q语义相关，但是和答案A必须语义不相关。我们从(s<sub>1</sub>,s<sub>2</sub>,s<sub>3</sub>…)学习到句子的权重分布(γ<sub>1</sub>,γ<sub>2</sub>,γ<sub>3</sub>…)，然后将问题q和答案a作为query。</p>

<h3 id="5encoding-layer">5)encoding layer</h3>
<p>我们希望把问题q，答案a和句子s都变成一样的长度的向量表示，也就是上图中紫色虚线部分。对于q和a，我们用两个独立的双向LSTM来获得(<strong>a</strong><sub>1</sub>,<strong>a</strong><sub>2</sub>…<strong>a</strong><sub>k</sub>)和(<strong>q</strong><sub>1</sub>,<strong>q</strong><sub>2</sub>…<strong>q</strong><sub>l</sub>)，然后用平均池化层平均一下：</p>

<p><img src="../assets/img/posts/20211130/28.jpg" /></p>

<p>对于句子s，我们不用u而用h：</p>

<p><img src="../assets/img/posts/20211130/29.jpg" /></p>

<h3 id="6matching-layer">6)matching layer</h3>
<p>目的：加重与问题q有关的句子，减轻与答案a有关的句子。o<sub>i</sub>表示不同句子的importance score</p>

<p><img src="../assets/img/posts/20211130/30.jpg" /></p>

<h3 id="7nomalization-layer">7)nomalization layer</h3>
<p>目的：有些问题q和一两个句子有关，而有些问题q和很多句子有关，比如summarizing，下面的τ(temperature)就是这个作用</p>

<p><img src="../assets/img/posts/20211130/31.jpg" /></p>

<p><img src="../assets/img/posts/20211130/32.jpg" /></p>

<p>作者介绍static attention mechanism用了很大篇幅</p>

<h3 id="8distractor-decoder">8)distractor decoder</h3>
<p>解码器使用的也是LSTM，但是并没有使用编码器的最后一个隐藏状态作为初始状态，而是定义了一个
<strong>question-based initializer</strong>来让生成的误导选项语法和问题q一致</p>

<h3 id="9question-based-initializer">9)question-based initializer</h3>
<p>定义了一个question LSTM来编码问题q，使用最后一层的cell state和hidden state作为decoder初始状态，同时输入q<sub>last</sub>，表示问题q的最后一个词元。</p>

<h3 id="10dynamic-hierarchical-attention-mechanism">10)dynamic hierarchical attention mechanism</h3>
<p>常规的注意力机制将一篇文章作为长句子，然后decoder的每一个时间步都与encoder中所有的hidden state进行比较，但是这种方法并不适合目前的模型。原因：首先LSTM不能处理这么长的输入，其次，一些问题只与部分句子有关。<br />
目的：每个decoder时间步只关注<strong>重要句子</strong>，作者将这种注意力机制称为动态注意力机制，因为不同的时间步，word-level和sentence-level 注意力分布都不同。<br />
每一个时间步的输入是词元d<sub>t-1</sub>和上一个隐藏状态h<sub>t-1</sub></p>

<p><img src="../assets/img/posts/20211130/33.jpg" /></p>

<p><img src="../assets/img/posts/20211130/34.jpg" /></p>

<p>α和β分别表示word-level,sentence-level权重，最后使用之前静态注意力机制获得的γ来调节α和β</p>

<p><img src="../assets/img/posts/20211130/35.jpg" /></p>

<p><img src="../assets/img/posts/20211130/36.jpg" /></p>

<p>获得上下文变量<strong>c</strong><sub>t</sub></p>

<p><img src="../assets/img/posts/20211130/37.jpg" /></p>

<p>获得attention vector $\tilde{h}$</p>

<p><img src="../assets/img/posts/20211130/38.jpg" /></p>

<h3 id="11training-and-inference">11)training and inference</h3>
<p>损失函数：</p>

<p><img src="../assets/img/posts/20211130/39.jpg" /></p>

<p>生成多个误导选项的方法是束搜索，但是生成的误导选项很相似，作者做了相应的处理方法，但我觉得效果还是很差</p>

<h2 id="experimental-setting-实验设置">experimental setting 实验设置</h2>
<h3 id="1dataset">1)dataset</h3>
<p>RACE数据集，作者做了相应的处理，去掉了很多不合理的和语义不相关的，作者的处理标准是：对于误导选项中的词元，如果它们在文章中出现的次数小于5次，那么将被保留，同时去掉了那些需要在句子中间和句子开始填空的问题。下表展示了处理后的数据集的一些信息：</p>

<p><img src="../assets/img/posts/20211130/40.jpg" /></p>

<h3 id="2implementation-details-1">2)implementation details</h3>
<p>词表：保留了频率最高的50k个词元，同时使用GloVe作为词嵌入预训练模型。其他的细节都可以在文章中看见，这里不一一列出了，主要是超参数的设置。</p>

<h3 id="3baselines-and-ablations">3)baselines and ablations</h3>
<p>与HRED<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>和seq2seq比较</p>

<h2 id="results-and-analysis-结果与分析">results and analysis 结果与分析</h2>

<p><img src="../assets/img/posts/20211130/41.jpg" /></p>

<p>人工评估：</p>

<p><img src="../assets/img/posts/20211130/42.jpg" /></p>

<p>大致过程是这样：四个误导选项，分别来自seq2seq，HRED，作者的模型和原本的误导选项，让英语能力很好的人来选择最适合的选项，得出的结果可以发现，作者的模型生成的误导选项拥有最好的误导效果。</p>

<p>下图直观展示了static attention distribution：</p>

<p><img src="../assets/img/posts/20211130/43.jpg" /></p>

<h2 id="我的看法-1">我的看法</h2>
<p>这篇文章应该是第一个提出用处理后的RACE数据集来处理MCQ问题，处理后的RACE数据集在后面也有很多文献用到，这篇文章使用了seq2seq网络结构同时使用了静态和动态注意力机制，对于网络结构和注意力机制的解释非常完全和详细，虽然这篇文章的效果放到现在来看可能不是最好了，但是它提出来的评估标准可能会成为一个通用的标准。它的数据集和训练代码在<a href="https://github.com/Yifan-Gao/Distractor-Generation-RACE">github</a>上也完全开源。</p>

<h1 id="第四篇">第四篇</h1>
<h2 id="title-3">Title</h2>
<p>Co-attention hierarchical network: Generating coherent long distractors for reading comprehension</p>
<h2 id="author-3">Author</h2>
<p>Xiaorui Zhou, Senlin Luo, Yunfang Wu</p>
<h2 id="abstract-3">Abstract</h2>
<p>这篇文献是针对上一篇Gao的文章(seq2seq)所作的改进。本篇文章提出了Gao的模型的两个问题：1.没有建立文章和问题的关系，他的解决方法是使用<strong>co-attention enhanced hierarchical architecture</strong>来捕获文章和问题之间的关系，让解码器生成更有关联的误导选项。2.没有加重整篇文章和误导选项的关系。作者的解决思路是添加一个额外的语义相关性损失函数，让生成的误导选项与整篇文章更有关联。</p>
<h2 id="proposed-framework-网络结构">Proposed Framework 网络结构</h2>
<h3 id="1notations-and-task-definition">1)notations and task definition</h3>
<p>article T=(s<sub>1</sub>,s<sub>2</sub>…s<sub>k</sub>)，一篇文章有k个句子s，同时每个句子都有不同的长度l，s<sub>i</sub>=(w<sub>i,1</sub>,w<sub>i,2</sub>…w<sub>i,l</sub>)，每个文章有m个问题和z个误导选项，Q=(q<sub>1</sub>,q<sub>2</sub>…q<sub>m</sub>),D=(d<sub>1</sub>,d<sub>2</sub>…d<sub>z</sub>),我们的任务是根据输入的T和Q生成D</p>

<p><img src="../assets/img/posts/20211130/44.jpg" /></p>

<h3 id="2model-overview">2)model overview</h3>
<p>整体结构如下图所示，下面将从各个部分分别介绍：</p>

<p><img src="../assets/img/posts/20211130/45.jpg" /></p>

<h3 id="3encoding-article-and-question">3)encoding article and question</h3>
<p>文章和问题的编码器结构</p>
<ul>
  <li><strong>hierarchical article encoder</strong>
双向LSTM，和上一篇结构很像，很多部分我就简单列个式子。</li>
</ul>

<p><img src="../assets/img/posts/20211130/46.jpg" /></p>

<p>每一句最后的词元来表示整个句子</p>

<p><img src="../assets/img/posts/20211130/47.jpg" /></p>

<p>sentence-level encoder：</p>

<p><img src="../assets/img/posts/20211130/48.jpg" /></p>

<p>同样，用最后一个句子来表示整篇文章</p>

<p><img src="../assets/img/posts/20211130/49.jpg" /></p>

<p>用<strong>H</strong><sup>*</sup>来作为sentence-level representation of article,我们有<strong>H</strong><sub>:t</sub><sup>*</sup>=h<sub>t</sub><sup>s</sup></p>

<p>这样，通过使用两个双向LSTM获得word-level encoding和sentence-level encoding</p>
<ul>
  <li><strong>question encoder</strong></li>
</ul>

<p><img src="../assets/img/posts/20211130/50.jpg" /></p>

<p>用<strong>U</strong><sup>*</sup>来作为word-level representations of question, 我们有<strong>U</strong><sub>:t</sub><sup>*</sup>=h<sub>t</sub><sup>q</sup></p>

<h3 id="4co-attention-between-article-and-question">4)Co-attention between article and question</h3>
<p>Co-attention mechanism就是使用了两个方向的注意力机制，有从article到question的，也有question到article的。<br />
用一个“相似”矩阵S表示H和U的关系：</p>

<p><img src="../assets/img/posts/20211130/51.jpg" /></p>

<p>S<sub>i,j</sub>就表示第i个句子和第j个问题词元的相似性</p>

<p>我们可以获得两个特殊的矩阵<strong>S</strong><sup><strong>Q</strong></sup>和<strong>S</strong><sup><strong>T</strong></sup></p>

<p><img src="../assets/img/posts/20211130/52.jpg" /></p>

<ul>
  <li>article-to-question attention<br />
$\tilde{U}$<sub>:j</sub> = $\sum$ S<sub>i,j</sub><sup>Q</sup>U<sub>:,i</sub></li>
  <li>question-to-article attention</li>
</ul>

<p><img src="../assets/img/posts/20211130/53.jpg" /></p>

<p>最后，将问题的词级表示H，两个方向的注意力结果$\tilde{U}$和$\tilde{H}$结合一下获得G</p>

<p><img src="../assets/img/posts/20211130/54.jpg" /></p>

<h3 id="5merging-sentence-representation">5)Merging sentence representation</h3>

<p><img src="../assets/img/posts/20211130/55.jpg" /></p>

<p>Z表示final representation of sentence-level hidden states</p>

<h3 id="6question-initialization">6)question initialization</h3>
<p>接下来就进入decoder环节，这里的question initialization和上篇文献处理方法相同</p>

<h3 id="7hierarchical-attention">7)hierarchical attention</h3>
<p>不同时间步有不同的句子相关，和上篇文献的处理方法动态注意力机制相同。</p>

<p><img src="../assets/img/posts/20211130/56.jpg" /></p>

<p><img src="../assets/img/posts/20211130/57.jpg" /></p>

<p><img src="../assets/img/posts/20211130/58.jpg" /></p>

<p><img src="../assets/img/posts/20211130/59.jpg" /></p>

<h3 id="8semantic-similarity-loss">8)semantic similarity loss</h3>
<p>目的：获得文章和误导选项的关系。还记得之前定义的e<sub>T</sub>吗，它表示整篇文章，那么我们通过下面的公式可以获得distractor representation:</p>

<p><img src="../assets/img/posts/20211130/60.jpg" /></p>

<p>其中S<sub>M</sub>是decoder最后一个隐藏状态，那么我们通过cos计算相似关系，那么最终的损失函数</p>

<p><img src="../assets/img/posts/20211130/61.jpg" /></p>

<h2 id="experimental-settings">Experimental Settings</h2>
<h3 id="1dataset-1">1)dataset</h3>
<p>使用了上篇文献处理过的RACE数据集。</p>

<h3 id="2baselines-and-evaluation-metrics">2)baselines and evaluation metrics</h3>
<p>与seq2seq，HRED，HCP<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">11</a></sup>，HSA<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">12</a></sup>比较。</p>

<h3 id="3implementation-details">3)implementation details</h3>
<p>网络超参数设置技巧，不展开了</p>

<h2 id="results-and-analysis-结果与分析-1">Results and Analysis 结果与分析</h2>

<p><img src="../assets/img/posts/20211130/62.jpg" /></p>

<p><img src="../assets/img/posts/20211130/63.jpg" /></p>

<p><img src="../assets/img/posts/20211130/64.jpg" /></p>

<p>介绍一下上面这张表，这张表是人工评估的结果，从三个维度分析，分别是fluency,coherence,distracting ability。可以看出作者的模型并不是在所有维度都是最好的。</p>

<p>下图是案例分析：</p>

<p><img src="../assets/img/posts/20211130/65.jpg" /></p>

<h2 id="我的看法-2">我的看法</h2>
<p>这篇文献是基于上一篇文献的方法进行了两个改进：1.关联了整篇文章和问题，解决方法是使用了Co-attention mechanism。2.让distractor和article语义相关，方法是定义了相关性loss。</p>

<h1 id="补充">补充</h1>
<h2 id="race数据集简介">RACE数据集简介</h2>
<p>RACE数据集是一个来源于中学考试题目的大规模阅读理解数据集，包含了大约 28000 个文章以及近 100000 个问题。它的形式类似于英语考试中的阅读理解（选择题），给定一篇文章，通过阅读并理解文章（Passage），针对提出的问题（Question）从四个选项中选择正确的答案（Answers）。</p>
<h2 id="bleu">BLEU</h2>
<p>BLEU是一个评价指标，最开始用于机器翻译任务，定义如下</p>

<p><img src="../assets/img/posts/20211130/66.jpg" /></p>

<p>它的总体思想就是准确率，假如给定标准译文reference，神经网络生成的句子是candidate，句子长度为n，candidate中有m个单词出现在reference，m/n就是bleu的1-gram的计算公式。BLEU还有许多变种。根据n-gram可以划分成多种评价指标，常见的指标有BLEU-1、BLEU-2、BLEU-3、BLEU-4四种，其中n-gram指的是连续的单词个数为n。</p>

<h2 id="rouge">ROUGE</h2>
<p>Rouge(Recall-Oriented Understudy for Gisting Evaluation)，是评估自动文摘以及机器翻译的一组指标。它通过将自动生成的摘要或翻译与一组参考摘要（通常是人工生成的）进行比较计算，得出相应的分值，以衡量自动生成的摘要或翻译与参考摘要之间的“相似度”。它的定义如下：</p>

<p><img src="../assets/img/posts/20211130/67.jpg" /></p>

<p>文献中使用的ROUGE-L是一种变种，L即是LCS(longest common subsequence，最长公共子序列)的首字母，因为Rouge-L使用了最长公共子序列。Rouge-L计算方式如下图：</p>

<p><img src="../assets/img/posts/20211130/68.jpg" /></p>

<p>其中LCS(X, Y)是X和Y的最长公共子序列的长度,m、n分别表示参考摘要和自动摘要的长度（一般就是所含词的个数），R<sub>lcs</sub>,P<sub>lcs</sub>分别表示召回率和准确率。最后的F<sub>lcs</sub>即是我们所说的Rouge-L。</p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>distractor generation 误导选项生成，简称DG <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>当我们test时，只需要Sequential MLM decoder来预测。 <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>multi-choice reading comprehension (MRC) model <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>P-MLM <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Answer negative <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Byte-Pair-Encoding <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>context，question，answer <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>hierarchical encoder-decoder model with static attention <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>hierarchical model enhanced with co-attention <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p>hierarchical encoder-decoder <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p>相当于HRED+copy,是基于HRED的网络结构 <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:12" role="doc-endnote">
      <p>就是上篇文献的网络 <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

        <aside class="sidebar inline" id="post-end">
    



<div class="tag-cloud">
    
        <ul class="tags inline">
            
                <li><a href="./tag.html?tag=paper" class="tag inline">paper</a></li>
            
    
        </ul>
</div>
    <div class="share-options inline">
    <div class="share-hover inline">
        <span class="share-button inline"><svg fill="currentColor" width="25" height="25" class="inline"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></span>
        <div class="share-icons inline" id="post-end-icons">
            <a class="twitter" href="https://twitter.com/intent/tweet?text=RACE数据集相关文献&url=http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html" title="Share on Twitter" rel="nofollow" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
            <a class="facebook" href="https://facebook.com/sharer.php?u=http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html" title="Share on Facebook" rel="nofollow" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
            <a class="reddit" href="http://www.reddit.com/submit?url=http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html&title=RACE数据集相关文献" title="Submit to Reddit" rel="nofollow" target="_blank"><i class="fa fa-reddit" aria-hidden="true"></i></a>
            <a class="linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html&title=RACE数据集相关文献&summary=文献整理&source=http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html" title="Share on LinkedIn" rel="nofollow" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
            <a class="email" href="mailto:?subject=RACE数据集相关文献&body=文献整理%0A%0ARead more here: http://localhost:4000/RACE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE.html" title="Share via e-mail" rel="nofollow" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
            <a class="copy-link" onclick="copyToClipboard()" title="Copy to clipboard" rel="nofollow" target="_blank"><svg width="20px" fill="currentColor" class="inline" viewBox="0 0 18 18"><path d="M16.94 1.1A3.7 3.7 0 0 0 14.3 0c-1 0-1.94.39-2.64 1.1L7.43 5.3c-.91.92-2.09 3.2 0 5.27a.75.75 0 0 0 .82.16c.09-.03.17-.09.24-.15a.74.74 0 0 0 0-1.06c-1.16-1.15-.77-2.39-.02-3.16l4.24-4.22a2.2 2.2 0 0 1 1.58-.65c.6 0 1.16.23 1.58.65.86.87.86 2.29 0 3.16L12.7 8.47a.74.74 0 0 0 1.04 1.05l3.17-3.16a3.73 3.73 0 0 0 0-5.27h.03zM9.54 7.4a.74.74 0 0 0 0 1.06c1.16 1.15.76 2.39 0 3.16l-4.22 4.22c-.42.42-.99.65-1.59.65a2.23 2.23 0 0 1-1.58-3.82l3.17-3.16A.73.73 0 0 0 5.54 9a.78.78 0 0 0-.22-.52.77.77 0 0 0-1.05 0L1.1 11.64A3.72 3.72 0 0 0 3.74 18c1 0 1.94-.39 2.65-1.1l4.23-4.2c.21-.22.94-1.02 1.13-2.2.18-1.12-.2-2.15-1.12-3.07-.27-.27-.78-.27-1.06 0l-.02-.02z" clip-rule="evenodd" fill-rule="evenodd"></path></svg></a>
        </div>
    </div>
    <div class='alert' style='font-size:.6em;color:var(--accent);text-align:center;'></div>
</div>
</aside>

        <div class="separator"></div>
        



<section class="author-box">
  <div class="narrow-column">
    <a href='https://quehry.github.io'><img src="./assets/img/Dog.jpg" alt="Quehry" class="author-img"></a>
    <ul class="contact-icons">
      
      <li class="twitter"><a class="twitter" href="https://twitter.com/@QuehryS" target="_blank"><i class="fa fa-twitter"></i></a></li>
      
      
      <li class="linkedin"><a class="linkedin" href="https://in.linkedin.com/in/quehry" target="_blank"><i class="fa fa-linkedin"></i></a></li>
      
      
      <li class="github"><a class="github" href="http://github.com/Quehry" target="_blank"><i class="fa fa-github"></i></a></li>
      
      
      <li class="email"><a class="email" href="mailto:quehry@gmail.com"><i class="fa fa-envelope-o"></i></a></li>
      
    </ul>
  </div>
  <div class="author-desc">
    <h3>Quehry</h3>
    <p>Student</p>
  </div>
</section>

        



<div class="recent-box">
  <h2 class="page-subtitle">Recent posts</h2>
  <div class="recent-list">
    
      
        <div class="recent-item">
          
          

          <a href="./Machine-Learning.html" class="recent-item-img" style="background: url('./assets/img/posts/20211222/1.jpg') center no-repeat; background-size: cover;">
            <div class="recent-item-title">
              <span>机器学习</span>
              

  <span class = "recent-item-meta">
  
  
  
  
    
    
    <p class="page_meta-readtime">
      
        2 minute read
      
    </p>
  
  
  </span>

            </div>
          </a>
        </div>
      
    
      
        <div class="recent-item">
          
          

          <a href="./RACElike-datasets.html" class="recent-item-img" style="background: url('./assets/img/posts/20211221/2.jpg') center no-repeat; background-size: cover;">
            <div class="recent-item-title">
              <span>制作类RACE数据集</span>
              

  <span class = "recent-item-meta">
  
  
  
  
    
    
    <p class="page_meta-readtime">
      
        1 minute read
      
    </p>
  
  
  </span>

            </div>
          </a>
        </div>
      
    
      
        <div class="recent-item">
          
          

          <a href="./Computer_Graphics.html" class="recent-item-img" style="background: url('./assets/img/posts/20211221/1.jpg') center no-repeat; background-size: cover;">
            <div class="recent-item-title">
              <span>计算机图形学</span>
              

  <span class = "recent-item-meta">
  
  
  
  
    
    
    <p class="page_meta-readtime">
      
        8 minute read
      
    </p>
  
  
  </span>

            </div>
          </a>
        </div>
      
    
      
        <div class="recent-item">
          
          

          <a href="./1221%E7%BB%84%E4%BC%9A.html" class="recent-item-img" style="background: url('./assets/img/posts/20211221/6.jpg') center no-repeat; background-size: cover;">
            <div class="recent-item-title">
              <span>组会记录</span>
              

  <span class = "recent-item-meta">
  
  
  
  
    
    
    <p class="page_meta-readtime">
      
        less than 1 minute read
      
    </p>
  
  
  </span>

            </div>
          </a>
        </div>
      
    
  </div>
</div> <!-- End Recent-Box -->

        <div class="newsletter" id="mc_embed_signup">
  <h2 class="page-subtitle">Newsletter</h2>
  <div class="form-container">
    <p>Subscribe here to get our latest updates</p>
    <form action="https://github.us1.list-manage.com/subscribe/post?u=8ece198b3eb260e6838461a60&amp;id=397d90b5f4" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
      <label class="screen-reader-text" for="mce-EMAIL">Email Address</label>
      <div class="newsletter-box" id="mc_embed_signup_scroll">
        <input type="email" name="EMAIL" placeholder="Email address" class="email-input" id="mce-EMAIL" required>
        <input type="submit" value="Subscribe" name="subscribe" class="subscribe-btn" id="mc-embedded-subscribe">
      </div>
    </form>
  </div>
</div> <!-- End Newsletter -->

        
  <section class="comment-area">
    <div class="comment-wrapper">
        
            <div class="row" id="comment-curtain">
                <div class="col-lg-8 col-sm-10 mr-auto ml-auto">
                    <h2 class="page-subtitle">Comments</h2>
                    <div class="comments-trigger" onClick="toggle_comments()">
                        <i class="fa fa-comments"></i>&nbsp;&nbsp;Write a comment ...
                    </div>
                </div>
            </div>
        
      <div id="disqus_thread"></div>
          <script>
            (function() {
                var d = document, s = d.createElement('script');
                s.src = '//amaynez-github-io.disqus.com/embed.js';
                s.setAttribute('data-timestamp', +new Date());
                (d.head || d.body).appendChild(s);
            })();
          </script>
      <noscript>Please enable JavaScript to view the comments.</noscript>
    </div>
  </section> <!-- End Comment Area -->


      </div>
    </div> <!-- End Wrapper -->
  </article>
  <div class="search-box">
  <div class="wrapper">
    <div class="search-grid">
      <form class="search-form">
        <div id="search-container">
          <input type="text" id="search-input" class="search" placeholder="Search">
        </div>
      </form>
      <ul id="results-container" class="results-search"></ul>
      <div class="icon-close-container">
        <span class="search-icon-close"><i class="fa fa-times" aria-hidden="true"></i></span>
      </div>
    </div>
  </div>
</div>

  




<footer class="main-footer">
    <div class="footer-wrapper">
        <div class="logo-symbol">
            <a class="logo-link" title="Quehry" href="./">
                <svg width="45px" height="45px" class="logo-symbol" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid meet" version="1.0" viewBox="0 0 649 649"><g fill="currentColor" stroke="none"><path d="M2938 6380 c-551 -53 -1031 -220 -1478 -514 -306 -201 -588 -470 -810 -771 -276 -374 -475 -850 -559 -1336 -64 -371 -53 -836 29 -1211 155 -712 552 -1347 1130 -1810 450 -360 1021 -589 1640 -659 58 -6 202 -12 320 -12 282 0 503 26 756 87 902 220 1680 841 2079 1661 303 622 402 1333 279 2005 -90 496 -313 984 -634 1390 -111 140 -378 404 -519 513 -456 352 -957 560 -1549 643 -126 17 -558 26 -684 14z m687 -220 c77 -11 189 -32 250 -46 127 -29 345 -94 345 -103 0 -4 -341 -188 -757 -409 l-757 -403 -41 22 c-22 13 -58 26 -79 29 l-40 5 -148 370 c-82 204 -150 377 -153 386 -10 29 325 119 570 153 169 24 174 24 430 21 183 -3 273 -8 380 -25z m-1292 -546 l157 -387 -24 -28 c-13 -16 -30 -44 -36 -64 -6 -19 -17 -35 -23 -35 -7 -1 -382 -54 -834 -119 -453 -64 -823 -115 -823 -111 0 3 43 62 95 132 269 358 584 630 980 844 90 49 321 153 341 154 6 0 81 -174 167 -386z m2026 347 l37 -20 129 -440 128 -440 -35 -39 c-20 -22 -37 -44 -39 -50 -3 -8 -290 13 -929 66 -775 65 -925 80 -928 92 -2 11 223 134 785 432 433 228 794 416 801 417 7 0 30 -8 51 -18z m204 -92 c420 -211 850 -600 1137 -1027 34 -51 60 -95 57 -98 -4 -4 -805 157 -859 172 -10 3 -18 12 -18 21 0 58 -78 133 -137 133 -18 0 -33 2 -33 5 0 3 -52 184 -116 403 -63 218 -117 405 -120 415 -3 9 -3 17 1 17 3 0 42 -19 88 -41z m-913 -894 c494 -41 904 -75 912 -75 7 -1 24 -23 36 -51 l24 -50 -169 -269 -168 -270 -44 0 c-24 0 -63 -9 -86 -21 l-42 -21 -679 402 c-373 222 -688 409 -699 416 -18 13 -18 14 -1 14 10 0 422 -34 916 -75z m-1175 18 l48 -48 -31 -225 c-18 -124 -60 -417 -94 -652 l-62 -428 -44 0 c-24 0 -51 -4 -61 -10 -16 -8 -146 87 -777 566 -418 317 -761 578 -763 580 -3 3 0 9 5 14 8 8 1633 247 1700 249 25 1 42 -9 79 -46z m915 -414 l690 -411 0 -38 0 -37 -147 -46 c-82 -26 -453 -148 -825 -271 l-676 -223 -22 30 -22 29 98 669 c82 557 101 669 114 669 8 0 31 9 50 19 19 11 38 20 43 20 4 1 318 -184 697 -410z m1959 175 c251 -53 457 -97 457 -98 9 -9 123 -247 153 -320 113 -272 189 -599 209 -901 l9 -130 -231 -273 c-224 -265 -232 -273 -264 -270 l-34 3 -418 1013 -419 1013 26 30 c15 16 33 29 41 29 8 0 220 -43 471 -96z m-593 2 c6 -5 804 -1949 802 -1952 -2 -2 -274 272 -605 609 l-600 612 13 32 c20 46 17 82 -9 128 l-23 41 170 271 171 272 40 -6 c22 -3 41 -6 41 -7z m-3341 -611 c402 -307 733 -561 737 -564 31 -26 -38 -31 -947 -65 -516 -19 -941 -33 -944 -30 -2 2 6 77 18 165 49 346 162 693 324 986 44 82 52 91 66 79 9 -7 345 -264 746 -571z m2710 -142 c17 -16 39 -32 48 -38 15 -9 13 -64 -23 -695 -22 -377 -40 -686 -40 -687 0 -2 -8 -3 -19 -3 -10 0 -30 -9 -45 -21 l-27 -21 -342 200 c-188 110 -542 317 -787 460 -245 144 -445 266 -445 273 0 8 312 116 820 285 451 150 822 272 824 273 2 1 18 -11 36 -26z m590 -425 c220 -222 500 -505 623 -629 l223 -225 -17 -31 -18 -30 -585 -82 c-322 -45 -602 -85 -621 -88 -31 -4 -37 -1 -53 27 -11 17 -34 39 -53 47 -19 9 -34 21 -34 27 -1 6 16 316 37 688 l37 678 26 9 c14 5 27 10 30 10 3 1 185 -180 405 -401z m-2587 -114 c-2 -9 -224 -178 -494 -375 l-492 -359 -38 16 c-26 11 -53 14 -84 10 -25 -3 -55 -7 -67 -8 -16 -2 -106 76 -362 316 -204 192 -341 327 -341 338 0 16 10 18 83 19 45 0 431 13 857 28 984 36 942 35 938 15z m59 -88 l22 -23 -104 -467 c-58 -256 -105 -468 -105 -470 0 -2 -14 -6 -32 -10 -17 -3 -47 -18 -65 -32 l-34 -26 -339 122 -338 122 -7 45 c-5 37 -3 47 12 58 10 7 229 169 488 359 258 190 472 346 475 346 3 0 15 -11 27 -24z m1013 -430 c421 -245 769 -452 775 -460 5 -8 7 -18 3 -21 -3 -4 -417 -41 -920 -84 l-913 -77 -16 26 c-9 15 -29 36 -45 47 l-28 21 104 465 105 465 43 7 c27 4 55 17 75 35 18 16 37 28 42 26 6 -2 354 -204 775 -450z m-2307 -276 c-7 -71 -4 -98 10 -126 l15 -30 -206 -248 -206 -248 -27 59 c-140 314 -228 733 -229 1086 l0 138 323 -303 c270 -254 322 -307 320 -328z m5277 366 c-17 -270 -58 -489 -135 -721 -38 -117 -134 -350 -140 -343 -1 2 -35 117 -75 256 l-72 253 26 25 c51 52 58 134 15 189 -21 26 -21 26 -2 48 11 12 102 120 203 240 100 120 184 216 186 214 2 -2 -1 -74 -6 -161z m-642 -487 l21 -41 -456 -782 c-251 -429 -461 -787 -467 -794 -9 -9 -19 -10 -42 -2 l-29 10 -174 676 -174 675 35 37 c20 20 40 51 45 67 l10 30 584 82 c321 44 594 82 605 82 16 1 27 -10 42 -40z m-4015 -145 c301 -109 327 -120 327 -142 0 -12 7 -39 15 -58 l15 -36 -411 -469 c-226 -259 -413 -468 -415 -466 -2 2 1 219 6 483 5 263 10 538 10 610 l0 132 29 12 c16 6 39 23 52 36 13 14 28 23 34 21 5 -2 157 -57 338 -123z m-517 -413 c-3 -262 -9 -527 -12 -589 -8 -134 -2 -135 -120 20 -93 121 -192 278 -272 428 l-61 115 215 257 c181 216 218 255 235 251 l22 -6 -7 -476z m4695 460 c3 -3 43 -133 88 -289 l82 -282 -62 -108 c-75 -130 -175 -278 -265 -392 -140 -176 -514 -528 -595 -558 -13 -5 -72 -13 -130 -17 l-105 -7 -16 40 -16 40 462 796 461 796 45 -6 c25 -4 48 -9 51 -13z m-1736 -69 c-6 -5 -361 -192 -790 -417 -429 -224 -806 -422 -838 -439 -52 -28 -59 -29 -70 -15 -17 22 -78 59 -99 59 -19 0 -16 -14 -68 300 -29 169 -38 246 -30 248 27 9 62 50 74 87 11 32 19 41 43 44 67 8 1683 139 1733 140 30 0 50 -3 45 -7z m90 -62 c10 -11 38 -26 62 -31 38 -10 44 -16 52 -48 90 -346 333 -1280 337 -1292 3 -11 -7 -23 -29 -36 -19 -11 -42 -38 -53 -60 l-19 -40 -1050 297 c-859 244 -1050 301 -1050 315 0 12 239 141 860 465 473 247 863 449 866 449 3 1 14 -8 24 -19z m-2076 -210 c13 0 21 -9 25 -27 10 -47 86 -497 86 -508 0 -5 -13 -19 -29 -29 -35 -24 -57 -59 -66 -108 -4 -22 -14 -39 -24 -42 -143 -41 -861 -236 -870 -236 -7 0 -11 5 -9 11 2 6 190 224 418 484 349 399 416 472 431 464 10 -5 27 -9 38 -9z m1376 -1065 c741 -208 1053 -300 1060 -312 6 -9 24 -32 41 -52 l31 -35 -30 -68 c-34 -77 -25 -71 -205 -139 -310 -116 -653 -180 -984 -183 -97 0 -178 1 -181 4 -89 106 -817 1053 -817 1063 0 18 9 27 23 21 7 -2 485 -137 1062 -299z m-1319 264 c31 -35 81 -51 134 -44 l46 6 380 -487 c208 -268 385 -496 392 -506 13 -17 10 -18 -45 -12 -640 65 -1267 346 -1732 776 l-61 57 42 12 c271 74 805 218 814 218 6 1 19 -9 30 -20z m2814 -563 c0 -5 -223 -138 -278 -165 -24 -13 -46 -20 -49 -18 -2 3 3 19 11 36 12 22 27 33 59 42 27 7 56 26 81 52 35 37 44 41 105 47 36 4 67 8 69 9 1 0 2 -1 2 -3z" transform="translate(0.000000,644.000000) scale(0.100000,-0.100000)"/></g></svg>
            </a>
        </div>
        <div class="copyright">
          <p>2021 &copy; Quehry</p>
        </div>
        <div class="footer-nav">
            <div>
                <a href="./archive.html">
                    Posts
                </a>
            </div>
            <div>
                <a href="./tags.html">
                    Tags
                </a>
            </div>
            <div>
                <a href="./about.html">
                    About
                </a>
            </div>
        </div>
    </div>
</footer> <!-- End Footer -->

</div>

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

<style>
  table {
  margin: auto;
  }
</style>
    <div class="top" title="Top">
      <svg aria-hidden="true" focusable="false" data-prefix="fal" data-icon="angle-up" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="svg-inline--fa fa-angle-up fa-w-8 fa-2x"><path fill="currentColor" d="M136.5 185.1l116 117.8c4.7 4.7 4.7 12.3 0 17l-7.1 7.1c-4.7 4.7-12.3 4.7-17 0L128 224.7 27.6 326.9c-4.7 4.7-12.3 4.7-17 0l-7.1-7.1c-4.7-4.7-4.7-12.3 0-17l116-117.8c4.7-4.6 12.3-4.6 17 .1z" class=""></path></svg>
    </div>
    




<!-- JS -->








<script>
(function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var logo = document.getElementById('logo');
    var nightModeOption = ('auto' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
    storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
    var data = storage.getItem('theme');
    try {
        data = JSON.parse(data ? data : '');
    } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
    }
    return data;
    }

    function handleThemeToggle(nightShift) {
    themeData.nightShift = nightShift;
    saveThemeData(themeData);
    html.dataset.theme = nightShift ? 'dark' : 'light';
    if (nightShift) {
        logo.setAttribute("src", "./assets/img/branding/MVM-logo-full-dark.svg");
    } else {
        logo.setAttribute("src", "./assets/img/branding/MVM-logo-full.svg");
    }
    setTimeout(function() {
        sw.checked = nightShift ? true : false;
    }, 50);
    }

    function autoThemeToggle() {
    // Next time point of theme toggle
    var now = new Date();
    var toggleAt = new Date();
    var hours = now.getHours();
    var nightShift = hours >= 19 || hours <=7;

    if (nightShift) {
        if (hours > 7) {
        toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
    } else {
        toggleAt.setHours(19);
    }

    toggleAt.setMinutes(0);
    toggleAt.setSeconds(0);
    toggleAt.setMilliseconds(0)

    var delay = toggleAt.getTime() - now.getTime();

    // auto toggle theme mode
    setTimeout(function() {
        handleThemeToggle(!nightShift);
    }, delay);

    return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
    };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
    handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
    var data = autoThemeToggle();

    // Toggle theme by local setting
    if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
    } else {
        handleThemeToggle(themeData.nightShift);
    }
    } else if (nightModeOption == 'manual') {
    handleThemeToggle(themeData.nightShift);
    } else {
    var nightShift = themeData.nightShift;
    if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
    }
    handleThemeToggle(nightShift);
    }
})();
</script>

<script src="./assets/js/jekyll-search.js"></script>
<script src="./assets/js/jquery-3.6.0.min.js"></script>







<script src="./assets/js/main.js"></script>
<script>
  SimpleJekyllSearch({
      searchInput: document.getElementById('search-input'),
      resultsContainer: document.getElementById('results-container'),
      json: './search.json',
      searchResultTemplate: '<li><a href="{url}" title="{description}">{title}</a><p>{description}</p></li>',
      noResultsText: 'No results found',
      fuzzy: false,
      exclude: ['Welcome']
    });
</script>




    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R8SZS2YBZK"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-R8SZS2YBZK');
</script>
  </body>
</html>
