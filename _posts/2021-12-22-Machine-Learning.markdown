---
layout: postwithlatex
read_time: true
show_date: true
title:  机器学习
date:   2021-12-22  
description: 《机器学习》周志华读书笔记
img: posts/20211222/1.jpg 
tags: [note]
author: Quehry
---

# 目录

<!-- TOC -->

- [目录](#目录)
- [第2章 模型评估与选择](#第2章-模型评估与选择)
    - [2.0 思维导图](#20-思维导图)
    - [2.1 经验误差与过拟合](#21-经验误差与过拟合)
    - [2.2 评估方法](#22-评估方法)
        - [2.2.1 留出法](#221-留出法)
        - [2.2.2 交叉验证法](#222-交叉验证法)
        - [2.2.3 自助法](#223-自助法)
    - [2.3 性能度量](#23-性能度量)
        - [2.3.1 错误率与精度](#231-错误率与精度)
        - [2.3.2 查准率、查全率与F1](#232-查准率查全率与f1)
        - [2.3.3 ROC与AUC](#233-roc与auc)
        - [2.3.4 代价敏感错误率与代价曲线](#234-代价敏感错误率与代价曲线)
    - [2.4 比较检验](#24-比较检验)
        - [2.4.1 假设检验](#241-假设检验)
        - [2.4.2 交叉验证t检验](#242-交叉验证t检验)
        - [2.4.3 McNemar检验](#243-mcnemar检验)
        - [2.4.4 Friedman检验与Nemenyi后续检验](#244-friedman检验与nemenyi后续检验)
    - [2.5 偏差与方差](#25-偏差与方差)
- [第3章 线性模型](#第3章-线性模型)
    - [3.0 思维导图](#30-思维导图)
    - [3.1 基本形式](#31-基本形式)
    - [3.2 线性回归](#32-线性回归)
    - [3.3 对数几率回归](#33-对数几率回归)
    - [3.4 线性判别分析](#34-线性判别分析)
    - [3.5 多分类学习](#35-多分类学习)
    - [3.6 类别不平衡问题](#36-类别不平衡问题)

<!-- /TOC -->

# 第2章 模型评估与选择

## 2.0 思维导图

<center><img src='../assets/img/posts/20211222/16.jpg'></center>

## 2.1 经验误差与过拟合
**定义：**
- **错误率(error rate)**: 如果m个样本中有a个样本分类错误，则错误率E=a/m。
- **精度(accuracy)**：精度=1-错误率。
- **训练误差**：学习器在训练集上的误差称为训练误差或者经验误差。
- **泛化误差(generalization error)**：学习器在新样本上的误差称为泛化误差。
- **过拟合(overfitting)**：当学习器把训练样本学得太好了的时候，很可能把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这就会导致泛化性能下降。
- **欠拟合(underfitting)**：对训练样本的一般性质尚未学好。

## 2.2 评估方法
通常我们可通过实验测试来对学习器的泛化误差进行评估而做出选择，于是我们需要一个**测试集(testing set)**，然后以测试集上的测试误差作为泛化误差的近似。下面介绍一些常见的处理数据集D的方法(数据集D->训练集S+测试集T)[^1]

[^1]:注意这里的测试集其实是验证集，我们通常把实际情况中遇到的数据集称为测试集。

### 2.2.1 留出法
- 留出发(hold-out)直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T。在S上训练出模型后，用T来评估其测试误差。
- 以采样的角度来看待数据集划分的过程，则保留类别比例的采样方式通常称为**分层采样**。比如1000个样本，50%的正例，50%负例，以7：3划分数据集，那么训练集包含350个正例和350个负例就是分层采样。
- 在使用留出法评估结果时，一般要采用若干次随即划分、重复进行实验评估后取平均值作为留出法的评估结果。
- 常用做法时将大约2/3~4/5的样本用于训练。

### 2.2.2 交叉验证法
- **交叉验证法(cross validation)**先将数据集D划分为k个大小相似的互斥子集，每个子集D<sub>i</sub>都尽可能保持数据分布的一致性。然后每次用k-1个自己的并集作为训练集，余下的那个子集作为测试集，这样就可以获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。
- 交叉验证也称为k折交叉验证。k的常见取值有10、5、20。
- **留一法**就是k=m，其中数据集D有m个样本。

### 2.2.3 自助法
- **自助法(bootstrapping)**：给定包含m个样本的数据集D，每次随机从D中挑选一个样本，将其拷贝放入D', 然后再将该样本放回最初的数据集D中，这个过程重复执行m次后，我们获得了包含m个样本的数据集D'。我们将D'作为训练集，D\D'作为测试集。
- 不难发现大概有36.8%(m趋于无限大时)的样本再m次采样中始终不被采到。

<center>$\lim\limits_{m\rightarrow\infty}(1-\frac{1}{m})^m = \frac{1}{e} ≈ 0.368$</center>

- 缺点：自助法产生的数据集改变了初始数据集的分布，这样会引入估计偏差。因此，在初始数据量足够时，留出法和交叉验证法更常用一些。

## 2.3 性能度量
- 对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是**性能度量(performance measure)**。
- 在预测任务中，给定D = {(x<sub>1</sub>,y<sub>1</sub>), (x<sub>2</sub>,y<sub>2</sub>)...(x<sub>m</sub>,y<sub>m</sub>)}, 其中y<sub>i</sub>是x<sub>i</sub>的真实标记。学习器f。
- **均方误差(mean squared error)**：回归任务最常用的性能度量是均方误差：

<center>$E(f;D)=\frac{1}{m}\sum_1^m(f(x_i)-y_i)^2$</center>

接下来我将介绍**分类任务**中常用的性能度量

### 2.3.1 错误率与精度
本章开头提到了错误率和精度，这是分类任务中最常用的两种性能度量，既适用于二分类任务，也适用于多分类任务。

### 2.3.2 查准率、查全率与F1

- 针对二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例(true positive)、假正例(false positive)、真反例(true negative)、假反例(false negative)，分别记为TP、FP、TN、FN。
- 混淆矩阵(confusion matrix)


|真\预|正例|反例|
|正例|TP|FN|
|反例|FP|TN|

- 查准率(precision)，记为P，它表示选择的好瓜中有多少是真正的好瓜

<center>$P=\frac{TP}{TP+FP}$</center>

- 查全率(recall)，记为R，它表示好瓜中有多少被选出来了

<center>$R=\frac{TP}{TP+FN}$</center>

- 一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。

- P-R曲线：在很多情形下，我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器认为最可能是正例的样本，排在最后的则是学习器认为最不可能是正例的样本，按此顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率、查准率，也得到了P-R图。

<center><img src='../assets/img/posts/20211222/2.jpg'></center>

- 如果一个学习器的P-R曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者。如果两者曲线相交，则可以通过比较平衡点(break-even-point)来比较，越大越好。

- F1度量：F1综合考虑了查准率和查全率，是他们的调和平均

<center>$F1=\frac{2*P*R}{P+R}$</center>

- F1的一般形式：有时候我们希望赋予查准率和查重率不同的权重：

<center>$F_\beta=\frac{(1+\beta^2)*P*R}{\beta^2*P+R}$</center>

其中β大于1表示查全率有更大影响

- 有时候我们有多个二分类混淆矩阵，我们希望在这n个混淆矩阵上综合考虑查准率和查全率，那么我们有以下的度量

- 宏查准率macro-P，宏查全率macro-R，宏F1：

<center>$macro-P=\frac{1}{n}\sum_1^nP_i$</center>

<center>$macro-R=\frac{1}{n}\sum_1^nR_i$</center>

- 微查准率micro-P，微查全率micro-P，微F1：

对TP、FP、TN、FN进行平均

<center>$micro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}$</center>

<center>$micro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}$</center>

### 2.3.3 ROC与AUC
- 很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为反类。

- ROC曲线是从排序角度来出发研究学习器的泛化性能的工具。ROC的全称是Receiver Operating Characteristic。

- 与P-R曲线类似，我们根据学习器的预测结果进行排序，按此顺序逐个把样本作为正例进行预测，计算出真正例率TPR(true positive rate)与假正例率FPR(false positive rate)并以它们分别为横纵坐标画出ROC曲线

<center>$TPR=\frac{TP}{TP+FN}$</center>

<center>$FPR=\frac{FP}{FP+TN}$</center>

<center><img src='../assets/img/posts/20211222/3.jpg'></center>

- 同样的，如果一个学习器的ROC曲线被另一个学习器完全包住，则认为后者的性能优于前者。若两个曲线相交，则比较曲线下的面积AUC(area under curve)

- 形式化的看，AUC考虑的是样本预测的排序质量，那么我们可以定义一个排序损失l<sub>rank</sub>

<center><img src='../assets/img/posts/20211222/4.jpg'></center>

<center>AUC=1-l<sub>rank</sub></center>


### 2.3.4 代价敏感错误率与代价曲线
- 有时候将正例误判断为负例与将负例误判断为正例所带来的后果不一样，我们赋予它们非均等代价
- 代价矩阵(cost matrix)


|真\预|第0类|第1类|
|第0类|0|cost<sub>01</sub>|
|第1类|cost<sub>10</sub>|0|

其中cost<sub>ij</sub>表示将第i类样本预测为第j类样本的代价

- 代价敏感错误率

<center><img src='../assets/img/posts/20211222/5.jpg'></center>

- 在非均等代价下，ROC曲线不能直接反映出学习器的期望总代价，而代价曲线则可达到此目的，横轴是正例概率代价，纵轴是归一化代价

<center><img src='../assets/img/posts/20211222/6.jpg'></center>

<center><img src='../assets/img/posts/20211222/7.jpg'></center>

<center><img src='../assets/img/posts/20211222/8.jpg'></center>

## 2.4 比较检验
我们有了实验评估方法与性能度量，是否就可以对学习器的性能进行评估比较了呢？实际上，机器学习中性能比较这件事比大家想象的要复杂很多，我们需要考虑多种因素的影响，下面介绍一些对学习器性能进行比较的方法，本小节默认以错误率作为性能度量。(但我觉得似乎同一个数据集下就可以比较)

### 2.4.1 假设检验
- 假设检验中的假设是对学习器泛化错误率分布的某种猜想或者判断，例如“ε=ε<sub>0</sub>”这样的假设
- 现实任务中我们并不知道学习器的泛化错误率，我们只能获知其测试错误率$\hat{\epsilon}$，但直观上，两者接近的可能性比较大，因此可根据测试错误率估推出泛化错误率的分布。
- 泛化错误率为$\epsilon$的学习器被测得测试错误率为$\hat{\epsilon}$的概率：

<center><img src='../assets/img/posts/20211222/9.jpg'></center>

- 我们发现$\epsilon$符合二项分布

<center><img src='../assets/img/posts/20211222/10.jpg'></center>

- 二项检验：我们可以使用**二项检验(binomial test)**来对“$\epsilon$<0.3”这样的假设进行检验，即在$\alpha$显著度下，$1-\alpha$置信度下判断假设是否成立。

- t检验：我们也可以用t检验(t-test)来检验。

- 上面介绍的都是针对单个学习器泛化性能的假设进行检验

### 2.4.2 交叉验证t检验

- 对于两个学习器A和B，若我们使用k折交叉验证法得到的测试错误率分别是$\epsilon_1^A$, $\epsilon_2^A$...$\epsilon_k^A$和$\epsilon_1^B$, $\epsilon_2^B$...$\epsilon_k^B$。其中$\epsilon_i^A$和$\epsilon_i^B$是在相同第i折训练集上得到的结果。则可以用k折交叉验证“成对t检验”来进行比较检验。

- 我们这里的基本思想是若两个学习器的性能相同，则它们使用相同的训练测试集得到的错误率应该相同，即$\epsilon_i^A=\epsilon_1^B$

- $\Delta_i$ = $\epsilon_i^A$ - $\epsilon_i^B$，然后对$\Delta$进行分析

### 2.4.3 McNemar检验
- 对于二分类问题，使用留出法不仅可以估计出学习器A和B的测试错误率，还可获得两学习器分类结果的差别，即两者都正确、都错误...的样本数

<center><img src='../assets/img/posts/20211222/11.jpg'></center>

- 若我们假设两学习器性能相同，则应有e<sub>01</sub>=e<sub>10</sub>，那么变量\|e<sub>01</sub>-e<sub>10</sub>\|应该服从正态分布/卡方分布，然后用McNemar检验

### 2.4.4 Friedman检验与Nemenyi后续检验

- 交叉验证t检验与McNemar检验都是在一个数据集上比较两个算法的性能，但是很多时候，我们会在一组数据集上比较多个算法。

- 当有多个算法参与比较时，一种做法是在每个数据集上本别列出两两比较的结果。另一种方法则是基于算法排列的**Friedman检验**

- 假定我们用D<sub>1</sub>, D<sub>2</sub>, D<sub>3</sub>, D<sub>4</sub>四个数据集对算法A、B、C进行比较。首先，使用留出法或交叉验证法得到每个算法在每个数据集上的测试结果。然后在每个数据集上根据测试性能由好到坏排序，并赋予序值1,2,...若算法的测试性能相同，则平分序值。

<center><img src='../assets/img/posts/20211222/12.jpg'></center>

- 然后使用Fredman检验来判断这些算法是否性能都相同，若相同，那么它们的平均序值应当相同。r<sub>i</sub>表示第i个算法的平均序值，那么它的均值和方差应该满足...

- 若“所有算法的性能相同”这个假设被拒绝，则说明算法的性能显著不同，这时需要后续检验(post-hoc test)来进一步区分各算法，常用的有Nemenyi后续检验

- Nemenyi检验计算出平均序值差别的临界值域

<center><img src='../assets/img/posts/20211222/13.jpg'></center>

- 在表中找到k=3时q<sub>0.05</sub>=2.344，根据公式计算出临界值域CD=1.657，由表中的平均序值可知A与B算法的差距，以及算法B与C的差距均未超过临界值域，而算法A与C的差距超过临界值域，因此检验结果认为算法A与C的性能显著不同，而算法A与B以及算法B与C的性能没有显著差别。

<center><img src='../assets/img/posts/20211222/14.jpg'></center>

## 2.5 偏差与方差

- 对学习算法除了通过实验估计其泛化性能，人们往往还希望了解它“为什么”具有这样的性能。偏差-方差分解是解释学习算法泛化性能的一种重要工具

- **偏差**度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力
- **方差**度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响
- **噪声**则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度

- 一般来说，偏置与方差是有冲突的，也就是偏置大的方差小，偏置小的方差大

<center><img src='../assets/img/posts/20211222/15.jpg'></center>

# 第3章 线性模型
## 3.0 思维导图

<center><img src='../assets/img/posts/20211222/42.jpg'></center>

## 3.1 基本形式

- 给定由d个属性描述的示例$x=(x_1;x_2;...x_d)$，这是一个列向量，其中$x_i$是$x$在第i个属性上的取值。

- **线性模型(linear model)**试图学得一个通过属性线性组合来进行预测的函数：

<center><img src='../assets/img/posts/20211222/17.jpg'></center>

- 向量形式：

<center>$f(x)=\omega^Tx+b$</center>

其中$\omega=(\omega_1;\omega_2...\omega_d)$

- 当$\omega$和b学得后，模型就得以确定

- 线性模型的优点：形式简单，易于建模，良好的可解释性(comprehensibility)

## 3.2 线性回归
- 对离散属性的处理：1.若属性值存在“序”的关系，可通过连续化将其转化为连续值，比如高、矮变成1、0；2.若不存在序的关系，可转化为k维向量。

- 均方误差是回归任务中最常用的性能度量，试图让均方误差最小化：

<center><img src='../assets/img/posts/20211222/18.jpg'></center>

- 均方误差有非常好的几何意义，它对应了欧氏距离。基于均方误差最小化来进行模型求解的方法称为**最小二乘法(least square method)**。在线性回归中，最小二乘法就是试图找到一条直线，使得样本到直线上的欧氏距离之和最小

- 首先观察一个属性值的情况。求解$\omega$和$b$使得均方误差最小化的过程，称为线性回归的最小二乘“参数估计”，我们令均方误差分别对$\omega$和$b$求导令其为零，可以得到最优解的**闭式解(closed-form)**,即解析解

<center><img src='../assets/img/posts/20211222/19.jpg'></center>

<center><img src='../assets/img/posts/20211222/20.jpg'></center>

- 更一般的情况是d个属性，称其为“多元线性回归”，同样的步骤，只是$\omega$变成向量形式，自变量写成m*(d+1)的矩阵形式，m对应了m个样本，d+1对应了d个属性和偏置。同样的求导为0然后得出$\hat{\omega}$最优解的闭式解，其中$\hat{\omega}=(\omega;b)$。当$X^TX$为满秩矩阵[^2]或正定矩阵时，有唯一的解：

<center><img src='../assets/img/posts/20211222/21.jpg'></center>

<center><img src='../assets/img/posts/20211222/22.jpg'></center>

<center><img src='../assets/img/posts/20211222/23.jpg'></center>

<center><img src='../assets/img/posts/20211222/24.jpg'></center>

<center><img src='../assets/img/posts/20211222/25.jpg'></center>


[^2]:满秩矩阵指方阵的秩等于矩阵的行数/列数，满秩矩阵有逆矩阵且对于y=Xb有唯一的解


- 然而现实任务中往往不是满秩矩阵，例如在许多任务中我们会遇到大量的变量其数目甚至超过样例数。那么我们会求出$\hat{\omega}=(\omega;b)$的多个解，它们都能使均方误差最小化。选择哪一个解作为输出与学习算法的归纳偏好决定，常见的做法是**引入正则化(regularization)**

- **广义线性模型(generalized linear model)**:

<center><img src='../assets/img/posts/20211222/26.jpg'></center>

<center>$g(y)=\omega^Tx+b$</center>

其中g()单调可微，被称为联系函数。比如当g()=ln()时称为对数线性回归

## 3.3 对数几率回归
- 上一节讨论的是线性回归进行回归学习。那么如果我们要做的是分类任务应该怎么改变？我们只需要找到一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。

- 考虑二分类问题，那么我们可以用单位阶跃函数/Heaviside函数联系y与z，其中y是真是标记，z是预测值，$z=\omega^Tx+b$

<center><img src='../assets/img/posts/20211222/27.jpg'></center>

- 但是它有个问题就是不连续，所以我们希望找到能在一定程度上近似它的替代函数。

- **对数几率函数(logistic function)**就是一个替代函数：

<center>$y=\frac{1}{1+e^{-z}}$</center>

<center><img src='../assets/img/posts/20211222/28.jpg'></center>

- 那么这个式子可以转化为：可以把y视为样本x作为正例的可能性，则1-y是其反例的可能性，两者的比值称为几率(odds)：

<center><img src='../assets/img/posts/20211222/29.jpg'></center>

- 若将y视为类后验概率估计。则式子可以重写为：

<center><img src='../assets/img/posts/20211222/30.jpg'></center>

- 接下来我们可以通过**极大似然法(maximum likelihood method)**来估计$\omega$和$b$。给定数据集，对数似然函数[^3]为：

<center><img src='../assets/img/posts/20211222/31.jpg'></center>

即每个样本属于其真实标记的概率越大越好。

[^3]: 统计学中，似然函数是一种关于统计模型参数的函数。给定输出x时，关于参数θ的似然函数L(θ\|x)（在数值上）等于给定参数θ后变量X的概率：L(θ\|x)=P(X=x\|θ)。

- 推导过程：

<center><img src='../assets/img/posts/20211222/32.jpg'></center>

上面有个式子应该有问题，(3.26)应该是

<center>$p(y_i|x_i;\omega,b) = p_1(\hat{x_i};\beta)^{y_i}p_0(\hat{x_i};\beta)^{1-y_i}$</center>

因为$\beta$是高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法如**梯度下降法(gradient descent method)**和牛顿法都可以求得最优解

## 3.4 线性判别分析

- 线性判别分析(Linear Discriminant Analysis，简称LDA)是一种经典的线性学习方法。

- LDA的思想非常朴素: 给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离;在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。

<center><img src='../assets/img/posts/20211222/33.jpg'></center>

- 令$X_i$、$\mu_i$、$\Sigma_i$分别表示第i类示例的集合、均值向量[^4]、协方差矩阵[^5]。

[^4]: 随机变量的期望组成的向量称为期望向量或者均值向量

[^5]: 协方差矩阵的每个元素是各个向量元素之间的协方差。协方差就是Covariance

- 欲使同类样例的投影点尽可能接近，可以让同类样例投影点的协方差尽可能小，即$\omega^T\Sigma_0\omega+\omega^T\Sigma_1\omega$尽可能小;而欲使异类样例的投影点尽可能远离，可以让类中心之间的距离尽可能大，即$\|\|\omega^T\mu_0-\omega^T\mu_1\|\|$尽可能大:

<center><img src='../assets/img/posts/20211222/34.jpg'></center>

- 剩余推导过程：

<center><img src='../assets/img/posts/20211222/35.jpg'></center>

<center><img src='../assets/img/posts/20211222/36.jpg'></center>

- 值得一提的是，LDA可从贝时斯决策理论的角度来阐释，并可证明，当两类数据同先验、满足高斯分布且协方差相等时，LDA可达到最优分类。

## 3.5 多分类学习
- 现实中常遇到多分类学习任务，有些二分类学习方法可直接推广到多分类，但在更多情形下，我们是基于一些基本策略，利用二分类学习器来解决多分类问题。

- 不失一般性，考虑N个类别$C_1$、$C_2$...$C_N$，多分类学习的基本思路是"**拆解法**”，即将多分类任务拆为若干个二分类任务求解。具体来说，先对问题进行拆分，然后为拆出的每个二分类任务训练一个分类器;在测试时，对这些分类器的预测结果进行集成以获得最终的多分类结果。

- 这里我们着重介绍如何拆分，最经典的拆分策略有三种：一对一(One vs One)、一对其余(One vs Rest)、多对多(Many vs Many)

- 一对一：将这N个类别两两配对，从而产生 N(N-1)/2个二分类任务。在测试阶段，新样本将同时提交给所有分类器，于是我们将得到N(N-1)/2个分类结果，最终结果可通过投票产生:即把被预测得最
多的类别作为最终分类结果。

<center><img src='../assets/img/posts/20211222/37.jpg'></center>

- 一对其余：OvR则是每次将一个类的样例作为正例，所有其他类的样例作为反例来训练N个分类器。在测试时若仅有一个分类器预测为正类，则对应的类别标记作为最终分类结果。若有多个分类器预测为正类，则通常考虑各分类器的预测置信度，选择置信度最大的类别标记作为分类结果。

<center><img src='../assets/img/posts/20211222/38.jpg'></center>

- 多对多MvM是每次将若干个类作为正类，若干个其他类作为反类。这里我们介绍一种最常用的MvM技术：**纠错输出码(ECOC)**

- ECOC是将编码的思想引入类别拆分，主要分为两步：

1.编码： 对N个类别做M次划分，每次划分将一部分类别划为正类，一部分划为反类，从而形成一个二分类训练集;这样一共产生M个训练集，可训练出M个分类器

2.解码：:M个分类器分别对测试样本进行预测，这些预测标记组成一个编码.将这个预测编码与每个类别各自的编码进行比较，返回其中距离最小的类别作为最终预测结果。

- 类别划分通过**编码矩阵**指定。编码矩阵有多种形式，常见的有二元码和三元码，前者将每个类别分别指定为正类和反类，后者在正、反类之外，还可指定“停用类”

<center><img src='../assets/img/posts/20211222/39.jpg'></center>

## 3.6 类别不平衡问题
- 前面介绍的分类学习方法都有一个共同的基本假设：即不同类别的训练样例数目相当。如果不同类别的样例数差别很大，会对学习过程造成困扰。

- **类别不平衡(class-imbalance)**就是指分类任务中不同类别的训练样例数目差别很大的情况。

- “**再缩放(rescaling)**”是类别不平衡中的一个基本策略：比如在最简单的二分类问题中，我们假设y大于0.5为正例，y小于0.5为负例，但是在类别不平衡时，我们可以改变阈值来达到再平衡：

将

<center><img src='../assets/img/posts/20211222/40.jpg'></center>

变成

<center><img src='../assets/img/posts/20211222/41.jpg'></center>

- 现有的解决类别不平衡的技术大体上有三类做法(这里我们均假设正例样本少):

1.第一类是直接对训练集里的反类样例进行"欠采样" (undersampling)，即去除一些反例使得正、反例数日接近，然后再进行学习;

2.第二类是对训练集里的正类样例进行"过采样" (oversampling)，即增加一些正例使得正、反例数目接近，然后再进行学习;

3.第三类则是直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将上面的公式(改变阈值)嵌入到其决策过程中，称为"阔值移动" (threshold-moving).

- 需注意的是，过采样法不能简单地对初始正例样本进行重复来样，否则会招致严重的过拟合；过采样法的代表性算法SMOTE是通过对训练集里的正例进行插值来产生额外的正例.另一方面，欠采样法若随机丢弃反例可能丢失一些重要信息;欠采样法的代表性算法EasyEnsemble则是利用集成学习机制，将反例划分为若干个集合供不同学习器使用，这样对每个学习器来看都进行了欠采样，但在全局来看却不会丢失重要信息。


------------

